 {
  "label": "model_SmolLM17b_2Layer-onnx_dynamo-ir-d1rt1.onnx",
  "graphs": [
    {
      "id": "main_graph",
      "nodes": [
        {
          "id": "[value] input_ids",
          "label": "Input",
          "namespace": "",
          "subgraphIds": [],
          "attrs": [
            {
              "key": "name",
              "value": "input_ids"
            },
            {
              "key": "index",
              "value": "0"
            }
          ],
          "incomingEdges": [],
          "outputsMetadata": [
            {
              "id": "0",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "input_ids"
                },
                {
                  "key": "tensor_shape",
                  "value": "INT64[s34,s16]"
                },
                {
                  "key": "[metadata] pkg.torch.export.graph_signature.InputSpec.kind",
                  "value": "USER_INPUT"
                },
                {
                  "key": "[metadata] pkg.torch.export.graph_signature.InputSpec.persistent",
                  "value": "None"
                }
              ]
            }
          ],
          "inputsMetadata": [],
          "style": null,
          "config": null
        },
        {
          "id": "[value] attention_mask",
          "label": "Input",
          "namespace": "",
          "subgraphIds": [],
          "attrs": [
            {
              "key": "name",
              "value": "attention_mask"
            },
            {
              "key": "index",
              "value": "1"
            }
          ],
          "incomingEdges": [],
          "outputsMetadata": [
            {
              "id": "0",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "attention_mask"
                },
                {
                  "key": "tensor_shape",
                  "value": "INT64[s34,s16 + s17]"
                },
                {
                  "key": "[metadata] pkg.torch.export.graph_signature.InputSpec.kind",
                  "value": "USER_INPUT"
                },
                {
                  "key": "[metadata] pkg.torch.export.graph_signature.InputSpec.persistent",
                  "value": "None"
                }
              ]
            }
          ],
          "inputsMetadata": [],
          "style": null,
          "config": null
        },
        {
          "id": "[value] past_key_values_key_cache_0",
          "label": "Input",
          "namespace": "",
          "subgraphIds": [],
          "attrs": [
            {
              "key": "name",
              "value": "past_key_values_key_cache_0"
            },
            {
              "key": "index",
              "value": "2"
            }
          ],
          "incomingEdges": [],
          "outputsMetadata": [
            {
              "id": "0",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "past_key_values_key_cache_0"
                },
                {
                  "key": "tensor_shape",
                  "value": "FLOAT[s34,32,s17,64]"
                },
                {
                  "key": "[metadata] pkg.torch.export.graph_signature.InputSpec.kind",
                  "value": "USER_INPUT"
                },
                {
                  "key": "[metadata] pkg.torch.export.graph_signature.InputSpec.persistent",
                  "value": "None"
                }
              ]
            }
          ],
          "inputsMetadata": [],
          "style": null,
          "config": null
        },
        {
          "id": "[value] past_key_values_key_cache_1",
          "label": "Input",
          "namespace": "",
          "subgraphIds": [],
          "attrs": [
            {
              "key": "name",
              "value": "past_key_values_key_cache_1"
            },
            {
              "key": "index",
              "value": "3"
            }
          ],
          "incomingEdges": [],
          "outputsMetadata": [
            {
              "id": "0",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "past_key_values_key_cache_1"
                },
                {
                  "key": "tensor_shape",
                  "value": "FLOAT[s34,32,s17,64]"
                },
                {
                  "key": "[metadata] pkg.torch.export.graph_signature.InputSpec.kind",
                  "value": "USER_INPUT"
                },
                {
                  "key": "[metadata] pkg.torch.export.graph_signature.InputSpec.persistent",
                  "value": "None"
                }
              ]
            }
          ],
          "inputsMetadata": [],
          "style": null,
          "config": null
        },
        {
          "id": "[value] past_key_values_value_cache_0",
          "label": "Input",
          "namespace": "",
          "subgraphIds": [],
          "attrs": [
            {
              "key": "name",
              "value": "past_key_values_value_cache_0"
            },
            {
              "key": "index",
              "value": "4"
            }
          ],
          "incomingEdges": [],
          "outputsMetadata": [
            {
              "id": "0",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "past_key_values_value_cache_0"
                },
                {
                  "key": "tensor_shape",
                  "value": "FLOAT[s34,32,s17,64]"
                },
                {
                  "key": "[metadata] pkg.torch.export.graph_signature.InputSpec.kind",
                  "value": "USER_INPUT"
                },
                {
                  "key": "[metadata] pkg.torch.export.graph_signature.InputSpec.persistent",
                  "value": "None"
                }
              ]
            }
          ],
          "inputsMetadata": [],
          "style": null,
          "config": null
        },
        {
          "id": "[value] past_key_values_value_cache_1",
          "label": "Input",
          "namespace": "",
          "subgraphIds": [],
          "attrs": [
            {
              "key": "name",
              "value": "past_key_values_value_cache_1"
            },
            {
              "key": "index",
              "value": "5"
            }
          ],
          "incomingEdges": [],
          "outputsMetadata": [
            {
              "id": "0",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "past_key_values_value_cache_1"
                },
                {
                  "key": "tensor_shape",
                  "value": "FLOAT[s34,32,s17,64]"
                },
                {
                  "key": "[metadata] pkg.torch.export.graph_signature.InputSpec.kind",
                  "value": "USER_INPUT"
                },
                {
                  "key": "[metadata] pkg.torch.export.graph_signature.InputSpec.persistent",
                  "value": "None"
                }
              ]
            }
          ],
          "inputsMetadata": [],
          "style": null,
          "config": null
        },
        {
          "id": "node_Shape_2",
          "label": "Shape",
          "namespace": "main_graph/_empty_nn_module_stack_from_metadata_hook: _empty_nn_module_stack_from_metadata_hook/sym_size_int_61: aten.sym_size.int",
          "subgraphIds": [],
          "attrs": [
            {
              "key": "end",
              "value": "2"
            },
            {
              "key": "start",
              "value": "1"
            },
            {
              "key": "[metadata] namespace",
              "value": "_empty_nn_module_stack_from_metadata_hook: _empty_nn_module_stack_from_metadata_hook/sym_size_int_61: aten.sym_size.int"
            },
            {
              "key": "[metadata] pkg.torch.onnx.class_hierarchy",
              "value": "['_empty_nn_module_stack_from_metadata_hook', 'aten.sym_size.int']"
            },
            {
              "key": "[metadata] pkg.torch.onnx.fx_node",
              "value": "%sym_size_int_61 : [num_users=10] = call_function[target=torch.ops.aten.sym_size.int](args = (%input_ids, 1), kwargs = {})"
            },
            {
              "key": "[metadata] pkg.torch.onnx.name_scopes",
              "value": "['_empty_nn_module_stack_from_metadata_hook', 'sym_size_int_61']"
            },
            {
              "key": "[metadata] pkg.torch.onnx.stack_trace",
              "value": "File \"torch/fx/passes/runtime_assert.py\", line 24, in insert_deferred_runtime_asserts"
            }
          ],
          "incomingEdges": [
            {
              "sourceNodeId": "[value] input_ids",
              "sourceNodeOutputId": "0",
              "targetNodeInputId": "0"
            }
          ],
          "outputsMetadata": [
            {
              "id": "0",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "val_1"
                },
                {
                  "key": "tensor_shape",
                  "value": "INT64[1]"
                },
                {
                  "key": "param_name",
                  "value": "shape"
                }
              ]
            }
          ],
          "inputsMetadata": [
            {
              "id": "0",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "input_ids"
                },
                {
                  "key": "tensor_shape",
                  "value": "INT64[s34,s16]"
                },
                {
                  "key": "[metadata] pkg.torch.export.graph_signature.InputSpec.kind",
                  "value": "USER_INPUT"
                },
                {
                  "key": "[metadata] pkg.torch.export.graph_signature.InputSpec.persistent",
                  "value": "None"
                },
                {
                  "key": "param_name",
                  "value": "data"
                }
              ]
            }
          ],
          "style": null,
          "config": null
        },
        {
          "id": "node_Squeeze_3",
          "label": "Squeeze",
          "namespace": "main_graph/_empty_nn_module_stack_from_metadata_hook: _empty_nn_module_stack_from_metadata_hook/sym_size_int_61: aten.sym_size.int",
          "subgraphIds": [],
          "attrs": [
            {
              "key": "[metadata] namespace",
              "value": "_empty_nn_module_stack_from_metadata_hook: _empty_nn_module_stack_from_metadata_hook/sym_size_int_61: aten.sym_size.int"
            },
            {
              "key": "[metadata] pkg.torch.onnx.class_hierarchy",
              "value": "['_empty_nn_module_stack_from_metadata_hook', 'aten.sym_size.int']"
            },
            {
              "key": "[metadata] pkg.torch.onnx.fx_node",
              "value": "%sym_size_int_61 : [num_users=10] = call_function[target=torch.ops.aten.sym_size.int](args = (%input_ids, 1), kwargs = {})"
            },
            {
              "key": "[metadata] pkg.torch.onnx.name_scopes",
              "value": "['_empty_nn_module_stack_from_metadata_hook', 'sym_size_int_61']"
            },
            {
              "key": "[metadata] pkg.torch.onnx.stack_trace",
              "value": "File \"torch/fx/passes/runtime_assert.py\", line 24, in insert_deferred_runtime_asserts"
            }
          ],
          "incomingEdges": [
            {
              "sourceNodeId": "node_Shape_2",
              "sourceNodeOutputId": "0",
              "targetNodeInputId": "0"
            }
          ],
          "outputsMetadata": [
            {
              "id": "0",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "sym_size_int_61"
                },
                {
                  "key": "tensor_shape",
                  "value": "INT64[]"
                },
                {
                  "key": "param_name",
                  "value": "squeezed"
                }
              ]
            }
          ],
          "inputsMetadata": [
            {
              "id": "0",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "val_1"
                },
                {
                  "key": "tensor_shape",
                  "value": "INT64[1]"
                },
                {
                  "key": "param_name",
                  "value": "data"
                }
              ]
            }
          ],
          "style": null,
          "config": null
        },
        {
          "id": "node_Shape_10",
          "label": "Shape",
          "namespace": "main_graph/_empty_nn_module_stack_from_metadata_hook: _empty_nn_module_stack_from_metadata_hook/sym_size_int_67: aten.sym_size.int",
          "subgraphIds": [],
          "attrs": [
            {
              "key": "end",
              "value": "3"
            },
            {
              "key": "start",
              "value": "2"
            },
            {
              "key": "[metadata] namespace",
              "value": "_empty_nn_module_stack_from_metadata_hook: _empty_nn_module_stack_from_metadata_hook/sym_size_int_67: aten.sym_size.int"
            },
            {
              "key": "[metadata] pkg.torch.onnx.class_hierarchy",
              "value": "['_empty_nn_module_stack_from_metadata_hook', 'aten.sym_size.int']"
            },
            {
              "key": "[metadata] pkg.torch.onnx.fx_node",
              "value": "%sym_size_int_67 : [num_users=5] = call_function[target=torch.ops.aten.sym_size.int](args = (%past_key_values_key_cache_1, 2), kwargs = {})"
            },
            {
              "key": "[metadata] pkg.torch.onnx.name_scopes",
              "value": "['_empty_nn_module_stack_from_metadata_hook', 'sym_size_int_67']"
            },
            {
              "key": "[metadata] pkg.torch.onnx.stack_trace",
              "value": "File \"torch/fx/passes/runtime_assert.py\", line 24, in insert_deferred_runtime_asserts"
            }
          ],
          "incomingEdges": [
            {
              "sourceNodeId": "[value] past_key_values_key_cache_1",
              "sourceNodeOutputId": "0",
              "targetNodeInputId": "0"
            }
          ],
          "outputsMetadata": [
            {
              "id": "0",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "val_5"
                },
                {
                  "key": "tensor_shape",
                  "value": "INT64[1]"
                },
                {
                  "key": "param_name",
                  "value": "shape"
                }
              ]
            }
          ],
          "inputsMetadata": [
            {
              "id": "0",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "past_key_values_key_cache_1"
                },
                {
                  "key": "tensor_shape",
                  "value": "FLOAT[s34,32,s17,64]"
                },
                {
                  "key": "[metadata] pkg.torch.export.graph_signature.InputSpec.kind",
                  "value": "USER_INPUT"
                },
                {
                  "key": "[metadata] pkg.torch.export.graph_signature.InputSpec.persistent",
                  "value": "None"
                },
                {
                  "key": "param_name",
                  "value": "data"
                }
              ]
            }
          ],
          "style": null,
          "config": null
        },
        {
          "id": "node_Squeeze_11",
          "label": "Squeeze",
          "namespace": "main_graph/_empty_nn_module_stack_from_metadata_hook: _empty_nn_module_stack_from_metadata_hook/sym_size_int_67: aten.sym_size.int",
          "subgraphIds": [],
          "attrs": [
            {
              "key": "[metadata] namespace",
              "value": "_empty_nn_module_stack_from_metadata_hook: _empty_nn_module_stack_from_metadata_hook/sym_size_int_67: aten.sym_size.int"
            },
            {
              "key": "[metadata] pkg.torch.onnx.class_hierarchy",
              "value": "['_empty_nn_module_stack_from_metadata_hook', 'aten.sym_size.int']"
            },
            {
              "key": "[metadata] pkg.torch.onnx.fx_node",
              "value": "%sym_size_int_67 : [num_users=5] = call_function[target=torch.ops.aten.sym_size.int](args = (%past_key_values_key_cache_1, 2), kwargs = {})"
            },
            {
              "key": "[metadata] pkg.torch.onnx.name_scopes",
              "value": "['_empty_nn_module_stack_from_metadata_hook', 'sym_size_int_67']"
            },
            {
              "key": "[metadata] pkg.torch.onnx.stack_trace",
              "value": "File \"torch/fx/passes/runtime_assert.py\", line 24, in insert_deferred_runtime_asserts"
            }
          ],
          "incomingEdges": [
            {
              "sourceNodeId": "node_Shape_10",
              "sourceNodeOutputId": "0",
              "targetNodeInputId": "0"
            }
          ],
          "outputsMetadata": [
            {
              "id": "0",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "sym_size_int_67"
                },
                {
                  "key": "tensor_shape",
                  "value": "INT64[]"
                },
                {
                  "key": "param_name",
                  "value": "squeezed"
                }
              ]
            }
          ],
          "inputsMetadata": [
            {
              "id": "0",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "val_5"
                },
                {
                  "key": "tensor_shape",
                  "value": "INT64[1]"
                },
                {
                  "key": "param_name",
                  "value": "data"
                }
              ]
            }
          ],
          "style": null,
          "config": null
        },
        {
          "id": "node_Shape_12",
          "label": "Shape",
          "namespace": "main_graph/_empty_nn_module_stack_from_metadata_hook: _empty_nn_module_stack_from_metadata_hook/sym_size_int_68: aten.sym_size.int",
          "subgraphIds": [],
          "attrs": [
            {
              "key": "end",
              "value": "1"
            },
            {
              "key": "start",
              "value": "0"
            },
            {
              "key": "[metadata] namespace",
              "value": "_empty_nn_module_stack_from_metadata_hook: _empty_nn_module_stack_from_metadata_hook/sym_size_int_68: aten.sym_size.int"
            },
            {
              "key": "[metadata] pkg.torch.onnx.class_hierarchy",
              "value": "['_empty_nn_module_stack_from_metadata_hook', 'aten.sym_size.int']"
            },
            {
              "key": "[metadata] pkg.torch.onnx.fx_node",
              "value": "%sym_size_int_68 : [num_users=16] = call_function[target=torch.ops.aten.sym_size.int](args = (%past_key_values_value_cache_0, 0), kwargs = {})"
            },
            {
              "key": "[metadata] pkg.torch.onnx.name_scopes",
              "value": "['_empty_nn_module_stack_from_metadata_hook', 'sym_size_int_68']"
            },
            {
              "key": "[metadata] pkg.torch.onnx.stack_trace",
              "value": "File \"torch/fx/passes/runtime_assert.py\", line 24, in insert_deferred_runtime_asserts"
            }
          ],
          "incomingEdges": [
            {
              "sourceNodeId": "[value] past_key_values_value_cache_0",
              "sourceNodeOutputId": "0",
              "targetNodeInputId": "0"
            }
          ],
          "outputsMetadata": [
            {
              "id": "0",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "val_6"
                },
                {
                  "key": "tensor_shape",
                  "value": "INT64[1]"
                },
                {
                  "key": "param_name",
                  "value": "shape"
                }
              ]
            }
          ],
          "inputsMetadata": [
            {
              "id": "0",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "past_key_values_value_cache_0"
                },
                {
                  "key": "tensor_shape",
                  "value": "FLOAT[s34,32,s17,64]"
                },
                {
                  "key": "[metadata] pkg.torch.export.graph_signature.InputSpec.kind",
                  "value": "USER_INPUT"
                },
                {
                  "key": "[metadata] pkg.torch.export.graph_signature.InputSpec.persistent",
                  "value": "None"
                },
                {
                  "key": "param_name",
                  "value": "data"
                }
              ]
            }
          ],
          "style": null,
          "config": null
        },
        {
          "id": "node_Gather_20",
          "label": "Gather",
          "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.embed_tokens: torch.nn.modules.sparse.Embedding/embedding: aten.embedding.default",
          "subgraphIds": [],
          "attrs": [
            {
              "key": "axis",
              "value": "0"
            },
            {
              "key": "[metadata] namespace",
              "value": ": transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.embed_tokens: torch.nn.modules.sparse.Embedding/embedding: aten.embedding.default"
            },
            {
              "key": "[metadata] pkg.torch.onnx.class_hierarchy",
              "value": "['transformers.models.llama.modeling_llama.LlamaForCausalLM', 'transformers.models.llama.modeling_llama.LlamaModel', 'torch.nn.modules.sparse.Embedding', 'aten.embedding.default']"
            },
            {
              "key": "[metadata] pkg.torch.onnx.fx_node",
              "value": "%embedding : [num_users=3] = call_function[target=torch.ops.aten.embedding.default](args = (%p_lm_head_weight, %input_ids), kwargs = {})"
            },
            {
              "key": "[metadata] pkg.torch.onnx.name_scopes",
              "value": "['', 'model', 'model.embed_tokens', 'embedding']"
            },
            {
              "key": "[metadata] pkg.torch.onnx.stack_trace",
              "value": "File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 688, in forward\n    outputs: BaseModelOutputWithPast = self.model(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 422, in forward\n    inputs_embeds = self.embed_tokens(input_ids)\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/torch/nn/modules/sparse.py\", line 190, in forward\n    return F.embedding("
            }
          ],
          "incomingEdges": [
            {
              "sourceNodeId": "[value] lm_head.weight",
              "sourceNodeOutputId": "0",
              "targetNodeInputId": "0"
            },
            {
              "sourceNodeId": "[value] input_ids",
              "sourceNodeOutputId": "0",
              "targetNodeInputId": "1"
            }
          ],
          "outputsMetadata": [
            {
              "id": "0",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "embedding"
                },
                {
                  "key": "tensor_shape",
                  "value": "FLOAT[s34,s16,2048]"
                },
                {
                  "key": "param_name",
                  "value": "output"
                }
              ]
            }
          ],
          "inputsMetadata": [
            {
              "id": "0",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "lm_head.weight"
                },
                {
                  "key": "tensor_shape",
                  "value": "FLOAT[49152,2048]"
                },
                {
                  "key": "param_name",
                  "value": "data"
                }
              ]
            },
            {
              "id": "1",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "input_ids"
                },
                {
                  "key": "tensor_shape",
                  "value": "INT64[s34,s16]"
                },
                {
                  "key": "[metadata] pkg.torch.export.graph_signature.InputSpec.kind",
                  "value": "USER_INPUT"
                },
                {
                  "key": "[metadata] pkg.torch.export.graph_signature.InputSpec.persistent",
                  "value": "None"
                },
                {
                  "key": "param_name",
                  "value": "indices"
                }
              ]
            }
          ],
          "style": null,
          "config": null
        },
        {
          "id": "node_Add_33",
          "label": "Add",
          "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/add_4: <built-in function add>",
          "subgraphIds": [],
          "attrs": [
            {
              "key": "[metadata] namespace",
              "value": ": transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/add_4: <built-in function add>"
            },
            {
              "key": "[metadata] pkg.torch.onnx.class_hierarchy",
              "value": "['transformers.models.llama.modeling_llama.LlamaForCausalLM', 'transformers.models.llama.modeling_llama.LlamaModel', '<built-in function add>']"
            },
            {
              "key": "[metadata] pkg.torch.onnx.fx_node",
              "value": "%add_4 : [num_users=7] = call_function[target=operator.add](args = (%sym_size_int_67, %sym_size_int_61), kwargs = {})"
            },
            {
              "key": "[metadata] pkg.torch.onnx.name_scopes",
              "value": "['', 'model', 'add_4']"
            },
            {
              "key": "[metadata] pkg.torch.onnx.stack_trace",
              "value": "File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 688, in forward\n    outputs: BaseModelOutputWithPast = self.model(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 430, in forward\n    past_seen_tokens, past_seen_tokens + inputs_embeds.shape[1], device=inputs_embeds.device"
            }
          ],
          "incomingEdges": [
            {
              "sourceNodeId": "node_Squeeze_11",
              "sourceNodeOutputId": "0",
              "targetNodeInputId": "0"
            },
            {
              "sourceNodeId": "node_Squeeze_3",
              "sourceNodeOutputId": "0",
              "targetNodeInputId": "1"
            }
          ],
          "outputsMetadata": [
            {
              "id": "0",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "add_4"
                },
                {
                  "key": "tensor_shape",
                  "value": "INT64[]"
                },
                {
                  "key": "param_name",
                  "value": "C"
                }
              ]
            }
          ],
          "inputsMetadata": [
            {
              "id": "0",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "sym_size_int_67"
                },
                {
                  "key": "tensor_shape",
                  "value": "INT64[]"
                },
                {
                  "key": "param_name",
                  "value": "A"
                }
              ]
            },
            {
              "id": "1",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "sym_size_int_61"
                },
                {
                  "key": "tensor_shape",
                  "value": "INT64[]"
                },
                {
                  "key": "param_name",
                  "value": "B"
                }
              ]
            }
          ],
          "style": null,
          "config": null
        },
        {
          "id": "node_Constant_704",
          "label": "Constant",
          "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/arange: aten.arange.start",
          "subgraphIds": [],
          "attrs": [
            {
              "key": "__value",
              "value": "1"
            },
            {
              "key": "value",
              "value": "TensorProtoTensor<INT64,[]>(array(1), name='val_11')"
            }
          ],
          "incomingEdges": [],
          "outputsMetadata": [
            {
              "id": "0",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "val_11"
                },
                {
                  "key": "tensor_shape",
                  "value": "INT64[]"
                },
                {
                  "key": "param_name",
                  "value": "output"
                }
              ]
            }
          ],
          "inputsMetadata": [],
          "style": null,
          "config": null
        },
        {
          "id": "node_Range_36",
          "label": "Range",
          "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/arange: aten.arange.start",
          "subgraphIds": [],
          "attrs": [
            {
              "key": "[metadata] namespace",
              "value": ": transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/arange: aten.arange.start"
            },
            {
              "key": "[metadata] pkg.torch.onnx.class_hierarchy",
              "value": "['transformers.models.llama.modeling_llama.LlamaForCausalLM', 'transformers.models.llama.modeling_llama.LlamaModel', 'aten.arange.start']"
            },
            {
              "key": "[metadata] pkg.torch.onnx.fx_node",
              "value": "%arange : [num_users=2] = call_function[target=torch.ops.aten.arange.start](args = (%sym_size_int_67, %add_4), kwargs = {device: cuda:0, pin_memory: False})"
            },
            {
              "key": "[metadata] pkg.torch.onnx.name_scopes",
              "value": "['', 'model', 'arange']"
            },
            {
              "key": "[metadata] pkg.torch.onnx.stack_trace",
              "value": "File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 688, in forward\n    outputs: BaseModelOutputWithPast = self.model(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 429, in forward\n    cache_position = torch.arange("
            }
          ],
          "incomingEdges": [
            {
              "sourceNodeId": "node_Squeeze_11",
              "sourceNodeOutputId": "0",
              "targetNodeInputId": "0"
            },
            {
              "sourceNodeId": "node_Add_33",
              "sourceNodeOutputId": "0",
              "targetNodeInputId": "1"
            },
            {
              "sourceNodeId": "node_Constant_704",
              "sourceNodeOutputId": "0",
              "targetNodeInputId": "2"
            }
          ],
          "outputsMetadata": [
            {
              "id": "0",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "arange"
                },
                {
                  "key": "tensor_shape",
                  "value": "INT64[s16]"
                },
                {
                  "key": "param_name",
                  "value": "output"
                }
              ]
            }
          ],
          "inputsMetadata": [
            {
              "id": "0",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "sym_size_int_67"
                },
                {
                  "key": "tensor_shape",
                  "value": "INT64[]"
                },
                {
                  "key": "param_name",
                  "value": "start"
                }
              ]
            },
            {
              "id": "1",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "add_4"
                },
                {
                  "key": "tensor_shape",
                  "value": "INT64[]"
                },
                {
                  "key": "param_name",
                  "value": "limit"
                }
              ]
            },
            {
              "id": "2",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "val_11"
                },
                {
                  "key": "tensor_shape",
                  "value": "INT64[]"
                },
                {
                  "key": "param_name",
                  "value": "delta"
                }
              ]
            }
          ],
          "style": null,
          "config": null
        },
        {
          "id": "node_Constant_37",
          "label": "Constant",
          "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/unsqueeze: aten.unsqueeze.default",
          "subgraphIds": [],
          "attrs": [
            {
              "key": "__value",
              "value": "[0]"
            },
            {
              "key": "value",
              "value": "TensorProtoTensor<INT64,[1]>(array([0]), name='')"
            },
            {
              "key": "[metadata] namespace",
              "value": ": transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/unsqueeze: aten.unsqueeze.default"
            },
            {
              "key": "[metadata] pkg.torch.onnx.class_hierarchy",
              "value": "['transformers.models.llama.modeling_llama.LlamaForCausalLM', 'transformers.models.llama.modeling_llama.LlamaModel', 'aten.unsqueeze.default']"
            },
            {
              "key": "[metadata] pkg.torch.onnx.fx_node",
              "value": "%unsqueeze : [num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%arange, 0), kwargs = {})"
            },
            {
              "key": "[metadata] pkg.torch.onnx.name_scopes",
              "value": "['', 'model', 'unsqueeze']"
            },
            {
              "key": "[metadata] pkg.torch.onnx.stack_trace",
              "value": "File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 688, in forward\n    outputs: BaseModelOutputWithPast = self.model(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 434, in forward\n    position_ids = cache_position.unsqueeze(0)"
            }
          ],
          "incomingEdges": [],
          "outputsMetadata": [
            {
              "id": "0",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "val_12"
                },
                {
                  "key": "tensor_shape",
                  "value": "INT64[1]"
                },
                {
                  "key": "param_name",
                  "value": "output"
                }
              ]
            }
          ],
          "inputsMetadata": [],
          "style": null,
          "config": null
        },
        {
          "id": "node_Unsqueeze_38",
          "label": "Unsqueeze",
          "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/unsqueeze: aten.unsqueeze.default",
          "subgraphIds": [],
          "attrs": [
            {
              "key": "[metadata] namespace",
              "value": ": transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/unsqueeze: aten.unsqueeze.default"
            },
            {
              "key": "[metadata] pkg.torch.onnx.class_hierarchy",
              "value": "['transformers.models.llama.modeling_llama.LlamaForCausalLM', 'transformers.models.llama.modeling_llama.LlamaModel', 'aten.unsqueeze.default']"
            },
            {
              "key": "[metadata] pkg.torch.onnx.fx_node",
              "value": "%unsqueeze : [num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%arange, 0), kwargs = {})"
            },
            {
              "key": "[metadata] pkg.torch.onnx.name_scopes",
              "value": "['', 'model', 'unsqueeze']"
            },
            {
              "key": "[metadata] pkg.torch.onnx.stack_trace",
              "value": "File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 688, in forward\n    outputs: BaseModelOutputWithPast = self.model(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 434, in forward\n    position_ids = cache_position.unsqueeze(0)"
            }
          ],
          "incomingEdges": [
            {
              "sourceNodeId": "node_Range_36",
              "sourceNodeOutputId": "0",
              "targetNodeInputId": "0"
            },
            {
              "sourceNodeId": "node_Constant_37",
              "sourceNodeOutputId": "0",
              "targetNodeInputId": "1"
            }
          ],
          "outputsMetadata": [
            {
              "id": "0",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "unsqueeze"
                },
                {
                  "key": "tensor_shape",
                  "value": "INT64[1,s16]"
                },
                {
                  "key": "param_name",
                  "value": "expanded"
                }
              ]
            }
          ],
          "inputsMetadata": [
            {
              "id": "0",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "arange"
                },
                {
                  "key": "tensor_shape",
                  "value": "INT64[s16]"
                },
                {
                  "key": "param_name",
                  "value": "data"
                }
              ]
            },
            {
              "id": "1",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "val_12"
                },
                {
                  "key": "tensor_shape",
                  "value": "INT64[1]"
                },
                {
                  "key": "param_name",
                  "value": "axes"
                }
              ]
            }
          ],
          "style": null,
          "config": null
        },
        {
          "id": "node_Constant_706",
          "label": "Constant",
          "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/full: aten.full.default",
          "subgraphIds": [],
          "attrs": [
            {
              "key": "__value",
              "value": "-3.4028234663852886e+38"
            },
            {
              "key": "value",
              "value": "TensorProtoTensor<FLOAT,[]>(array(-3.4028235e+38, dtype=float32), name='val_14')"
            }
          ],
          "incomingEdges": [],
          "outputsMetadata": [
            {
              "id": "0",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "val_14"
                },
                {
                  "key": "tensor_shape",
                  "value": "FLOAT[]"
                },
                {
                  "key": "param_name",
                  "value": "output"
                }
              ]
            }
          ],
          "inputsMetadata": [],
          "style": null,
          "config": null
        },
        {
          "id": "node_Constant_43",
          "label": "Constant",
          "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/full: aten.full.default",
          "subgraphIds": [],
          "attrs": [
            {
              "key": "__value",
              "value": "[-1]"
            },
            {
              "key": "value",
              "value": "TensorProtoTensor<INT64,[1]>(array([-1]), name='')"
            },
            {
              "key": "[metadata] namespace",
              "value": ": transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/full: aten.full.default"
            },
            {
              "key": "[metadata] pkg.torch.onnx.class_hierarchy",
              "value": "['transformers.models.llama.modeling_llama.LlamaForCausalLM', 'transformers.models.llama.modeling_llama.LlamaModel', 'aten.full.default']"
            },
            {
              "key": "[metadata] pkg.torch.onnx.fx_node",
              "value": "%full : [num_users=1] = call_function[target=torch.ops.aten.full.default](args = ([%sym_size_int_61, %add_4], -3.4028234663852886e+38), kwargs = {dtype: torch.float32, device: cuda:0, pin_memory: False})"
            },
            {
              "key": "[metadata] pkg.torch.onnx.name_scopes",
              "value": "['', 'model', 'full']"
            },
            {
              "key": "[metadata] pkg.torch.onnx.stack_trace",
              "value": "File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 688, in forward\n    outputs: BaseModelOutputWithPast = self.model(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 436, in forward\n    causal_mask = self._update_causal_mask("
            }
          ],
          "incomingEdges": [],
          "outputsMetadata": [
            {
              "id": "0",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "val_17"
                },
                {
                  "key": "tensor_shape",
                  "value": "INT64[1]"
                },
                {
                  "key": "param_name",
                  "value": "output"
                }
              ]
            }
          ],
          "inputsMetadata": [],
          "style": null,
          "config": null
        },
        {
          "id": "node_Reshape_44",
          "label": "Reshape",
          "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/full: aten.full.default",
          "subgraphIds": [],
          "attrs": [
            {
              "key": "allowzero",
              "value": "0"
            },
            {
              "key": "[metadata] namespace",
              "value": ": transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/full: aten.full.default"
            },
            {
              "key": "[metadata] pkg.torch.onnx.class_hierarchy",
              "value": "['transformers.models.llama.modeling_llama.LlamaForCausalLM', 'transformers.models.llama.modeling_llama.LlamaModel', 'aten.full.default']"
            },
            {
              "key": "[metadata] pkg.torch.onnx.fx_node",
              "value": "%full : [num_users=1] = call_function[target=torch.ops.aten.full.default](args = ([%sym_size_int_61, %add_4], -3.4028234663852886e+38), kwargs = {dtype: torch.float32, device: cuda:0, pin_memory: False})"
            },
            {
              "key": "[metadata] pkg.torch.onnx.name_scopes",
              "value": "['', 'model', 'full']"
            },
            {
              "key": "[metadata] pkg.torch.onnx.stack_trace",
              "value": "File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 688, in forward\n    outputs: BaseModelOutputWithPast = self.model(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 436, in forward\n    causal_mask = self._update_causal_mask("
            }
          ],
          "incomingEdges": [
            {
              "sourceNodeId": "node_Add_33",
              "sourceNodeOutputId": "0",
              "targetNodeInputId": "0"
            },
            {
              "sourceNodeId": "node_Constant_43",
              "sourceNodeOutputId": "0",
              "targetNodeInputId": "1"
            }
          ],
          "outputsMetadata": [
            {
              "id": "0",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "val_18"
                },
                {
                  "key": "tensor_shape",
                  "value": "INT64[1]"
                },
                {
                  "key": "param_name",
                  "value": "reshaped"
                }
              ]
            }
          ],
          "inputsMetadata": [
            {
              "id": "0",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "add_4"
                },
                {
                  "key": "tensor_shape",
                  "value": "INT64[]"
                },
                {
                  "key": "param_name",
                  "value": "data"
                }
              ]
            },
            {
              "id": "1",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "val_17"
                },
                {
                  "key": "tensor_shape",
                  "value": "INT64[1]"
                },
                {
                  "key": "param_name",
                  "value": "shape"
                }
              ]
            }
          ],
          "style": null,
          "config": null
        },
        {
          "id": "node_Concat_45",
          "label": "Concat",
          "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/full: aten.full.default",
          "subgraphIds": [],
          "attrs": [
            {
              "key": "axis",
              "value": "0"
            },
            {
              "key": "[metadata] namespace",
              "value": ": transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/full: aten.full.default"
            },
            {
              "key": "[metadata] pkg.torch.onnx.class_hierarchy",
              "value": "['transformers.models.llama.modeling_llama.LlamaForCausalLM', 'transformers.models.llama.modeling_llama.LlamaModel', 'aten.full.default']"
            },
            {
              "key": "[metadata] pkg.torch.onnx.fx_node",
              "value": "%full : [num_users=1] = call_function[target=torch.ops.aten.full.default](args = ([%sym_size_int_61, %add_4], -3.4028234663852886e+38), kwargs = {dtype: torch.float32, device: cuda:0, pin_memory: False})"
            },
            {
              "key": "[metadata] pkg.torch.onnx.name_scopes",
              "value": "['', 'model', 'full']"
            },
            {
              "key": "[metadata] pkg.torch.onnx.stack_trace",
              "value": "File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 688, in forward\n    outputs: BaseModelOutputWithPast = self.model(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 436, in forward\n    causal_mask = self._update_causal_mask("
            }
          ],
          "incomingEdges": [
            {
              "sourceNodeId": "node_Shape_2",
              "sourceNodeOutputId": "0",
              "targetNodeInputId": "0"
            },
            {
              "sourceNodeId": "node_Reshape_44",
              "sourceNodeOutputId": "0",
              "targetNodeInputId": "1"
            }
          ],
          "outputsMetadata": [
            {
              "id": "0",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "val_19"
                },
                {
                  "key": "tensor_shape",
                  "value": "INT64[2]"
                },
                {
                  "key": "param_name",
                  "value": "concat_result"
                }
              ]
            }
          ],
          "inputsMetadata": [
            {
              "id": "0",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "val_1"
                },
                {
                  "key": "tensor_shape",
                  "value": "INT64[1]"
                },
                {
                  "key": "param_name",
                  "value": "inputs"
                }
              ]
            },
            {
              "id": "1",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "val_18"
                },
                {
                  "key": "tensor_shape",
                  "value": "INT64[1]"
                },
                {
                  "key": "param_name",
                  "value": "inputs"
                }
              ]
            }
          ],
          "style": null,
          "config": null
        },
        {
          "id": "node_Expand_47",
          "label": "Expand",
          "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/full: aten.full.default",
          "subgraphIds": [],
          "attrs": [
            {
              "key": "[metadata] namespace",
              "value": ": transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/full: aten.full.default"
            },
            {
              "key": "[metadata] pkg.torch.onnx.class_hierarchy",
              "value": "['transformers.models.llama.modeling_llama.LlamaForCausalLM', 'transformers.models.llama.modeling_llama.LlamaModel', 'aten.full.default']"
            },
            {
              "key": "[metadata] pkg.torch.onnx.fx_node",
              "value": "%full : [num_users=1] = call_function[target=torch.ops.aten.full.default](args = ([%sym_size_int_61, %add_4], -3.4028234663852886e+38), kwargs = {dtype: torch.float32, device: cuda:0, pin_memory: False})"
            },
            {
              "key": "[metadata] pkg.torch.onnx.name_scopes",
              "value": "['', 'model', 'full']"
            },
            {
              "key": "[metadata] pkg.torch.onnx.stack_trace",
              "value": "File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 688, in forward\n    outputs: BaseModelOutputWithPast = self.model(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 436, in forward\n    causal_mask = self._update_causal_mask("
            }
          ],
          "incomingEdges": [
            {
              "sourceNodeId": "node_Constant_706",
              "sourceNodeOutputId": "0",
              "targetNodeInputId": "0"
            },
            {
              "sourceNodeId": "node_Concat_45",
              "sourceNodeOutputId": "0",
              "targetNodeInputId": "1"
            }
          ],
          "outputsMetadata": [
            {
              "id": "0",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "full"
                },
                {
                  "key": "tensor_shape",
                  "value": "FLOAT[s16,s16 + s17]"
                },
                {
                  "key": "param_name",
                  "value": "output"
                }
              ]
            }
          ],
          "inputsMetadata": [
            {
              "id": "0",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "val_14"
                },
                {
                  "key": "tensor_shape",
                  "value": "FLOAT[]"
                },
                {
                  "key": "param_name",
                  "value": "input"
                }
              ]
            },
            {
              "id": "1",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "val_19"
                },
                {
                  "key": "tensor_shape",
                  "value": "INT64[2]"
                },
                {
                  "key": "param_name",
                  "value": "shape"
                }
              ]
            }
          ],
          "style": null,
          "config": null
        },
        {
          "id": "n0",
          "label": "Constant",
          "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/triu: aten.triu.default",
          "subgraphIds": [],
          "attrs": [
            {
              "key": "__value",
              "value": "1"
            },
            {
              "key": "value_int",
              "value": "1"
            },
            {
              "key": "[metadata] namespace",
              "value": ": transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/triu: aten.triu.default"
            },
            {
              "key": "[metadata] pkg.torch.onnx.class_hierarchy",
              "value": "['transformers.models.llama.modeling_llama.LlamaForCausalLM', 'transformers.models.llama.modeling_llama.LlamaModel', 'aten.triu.default']"
            },
            {
              "key": "[metadata] pkg.torch.onnx.fx_node",
              "value": "%triu : [num_users=1] = call_function[target=torch.ops.aten.triu.default](args = (%full, 1), kwargs = {})"
            },
            {
              "key": "[metadata] pkg.torch.onnx.name_scopes",
              "value": "['', 'model', 'triu']"
            },
            {
              "key": "[metadata] pkg.torch.onnx.stack_trace",
              "value": "File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 688, in forward\n    outputs: BaseModelOutputWithPast = self.model(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 436, in forward\n    causal_mask = self._update_causal_mask("
            }
          ],
          "incomingEdges": [],
          "outputsMetadata": [
            {
              "id": "0",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "diagonal"
                },
                {
                  "key": "tensor_shape",
                  "value": "INT64[]"
                },
                {
                  "key": "param_name",
                  "value": "output"
                }
              ]
            }
          ],
          "inputsMetadata": [],
          "style": null,
          "config": null
        },
        {
          "id": "n1",
          "label": "Trilu",
          "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/triu: aten.triu.default",
          "subgraphIds": [],
          "attrs": [
            {
              "key": "upper",
              "value": "1"
            },
            {
              "key": "[metadata] namespace",
              "value": ": transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/triu: aten.triu.default"
            },
            {
              "key": "[metadata] pkg.torch.onnx.class_hierarchy",
              "value": "['transformers.models.llama.modeling_llama.LlamaForCausalLM', 'transformers.models.llama.modeling_llama.LlamaModel', 'aten.triu.default']"
            },
            {
              "key": "[metadata] pkg.torch.onnx.fx_node",
              "value": "%triu : [num_users=1] = call_function[target=torch.ops.aten.triu.default](args = (%full, 1), kwargs = {})"
            },
            {
              "key": "[metadata] pkg.torch.onnx.name_scopes",
              "value": "['', 'model', 'triu']"
            },
            {
              "key": "[metadata] pkg.torch.onnx.stack_trace",
              "value": "File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 688, in forward\n    outputs: BaseModelOutputWithPast = self.model(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 436, in forward\n    causal_mask = self._update_causal_mask("
            }
          ],
          "incomingEdges": [
            {
              "sourceNodeId": "node_Expand_47",
              "sourceNodeOutputId": "0",
              "targetNodeInputId": "0"
            },
            {
              "sourceNodeId": "n0",
              "sourceNodeOutputId": "0",
              "targetNodeInputId": "1"
            }
          ],
          "outputsMetadata": [
            {
              "id": "0",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "triu"
                },
                {
                  "key": "tensor_shape",
                  "value": "FLOAT[s16,s16 + s17]"
                },
                {
                  "key": "param_name",
                  "value": "output"
                }
              ]
            }
          ],
          "inputsMetadata": [
            {
              "id": "0",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "full"
                },
                {
                  "key": "tensor_shape",
                  "value": "FLOAT[s16,s16 + s17]"
                },
                {
                  "key": "param_name",
                  "value": "input"
                }
              ]
            },
            {
              "id": "1",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "diagonal"
                },
                {
                  "key": "tensor_shape",
                  "value": "INT64[]"
                },
                {
                  "key": "param_name",
                  "value": "k"
                }
              ]
            }
          ],
          "style": null,
          "config": null
        },
        {
          "id": "node_Constant_709",
          "label": "Constant",
          "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/arange_1: aten.arange.default",
          "subgraphIds": [],
          "attrs": [
            {
              "key": "__value",
              "value": "0"
            },
            {
              "key": "value",
              "value": "TensorProtoTensor<INT64,[]>(array(0), name='val_22')"
            }
          ],
          "incomingEdges": [],
          "outputsMetadata": [
            {
              "id": "0",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "val_22"
                },
                {
                  "key": "tensor_shape",
                  "value": "INT64[]"
                },
                {
                  "key": "param_name",
                  "value": "output"
                }
              ]
            }
          ],
          "inputsMetadata": [],
          "style": null,
          "config": null
        },
        {
          "id": "node_Constant_711",
          "label": "Constant",
          "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/arange_1: aten.arange.default",
          "subgraphIds": [],
          "attrs": [
            {
              "key": "__value",
              "value": "1"
            },
            {
              "key": "value",
              "value": "TensorProtoTensor<INT64,[]>(array(1), name='val_23')"
            }
          ],
          "incomingEdges": [],
          "outputsMetadata": [
            {
              "id": "0",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "val_23"
                },
                {
                  "key": "tensor_shape",
                  "value": "INT64[]"
                },
                {
                  "key": "param_name",
                  "value": "output"
                }
              ]
            }
          ],
          "inputsMetadata": [],
          "style": null,
          "config": null
        },
        {
          "id": "node_Range_52",
          "label": "Range",
          "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/arange_1: aten.arange.default",
          "subgraphIds": [],
          "attrs": [
            {
              "key": "[metadata] namespace",
              "value": ": transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/arange_1: aten.arange.default"
            },
            {
              "key": "[metadata] pkg.torch.onnx.class_hierarchy",
              "value": "['transformers.models.llama.modeling_llama.LlamaForCausalLM', 'transformers.models.llama.modeling_llama.LlamaModel', 'aten.arange.default']"
            },
            {
              "key": "[metadata] pkg.torch.onnx.fx_node",
              "value": "%arange_1 : [num_users=1] = call_function[target=torch.ops.aten.arange.default](args = (%add_4,), kwargs = {device: cuda:0, pin_memory: False})"
            },
            {
              "key": "[metadata] pkg.torch.onnx.name_scopes",
              "value": "['', 'model', 'arange_1']"
            },
            {
              "key": "[metadata] pkg.torch.onnx.stack_trace",
              "value": "File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 688, in forward\n    outputs: BaseModelOutputWithPast = self.model(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 436, in forward\n    causal_mask = self._update_causal_mask("
            }
          ],
          "incomingEdges": [
            {
              "sourceNodeId": "node_Constant_709",
              "sourceNodeOutputId": "0",
              "targetNodeInputId": "0"
            },
            {
              "sourceNodeId": "node_Add_33",
              "sourceNodeOutputId": "0",
              "targetNodeInputId": "1"
            },
            {
              "sourceNodeId": "node_Constant_711",
              "sourceNodeOutputId": "0",
              "targetNodeInputId": "2"
            }
          ],
          "outputsMetadata": [
            {
              "id": "0",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "arange_1"
                },
                {
                  "key": "tensor_shape",
                  "value": "INT64[s16 + s17]"
                },
                {
                  "key": "param_name",
                  "value": "output"
                }
              ]
            }
          ],
          "inputsMetadata": [
            {
              "id": "0",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "val_22"
                },
                {
                  "key": "tensor_shape",
                  "value": "INT64[]"
                },
                {
                  "key": "param_name",
                  "value": "start"
                }
              ]
            },
            {
              "id": "1",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "add_4"
                },
                {
                  "key": "tensor_shape",
                  "value": "INT64[]"
                },
                {
                  "key": "param_name",
                  "value": "limit"
                }
              ]
            },
            {
              "id": "2",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "val_23"
                },
                {
                  "key": "tensor_shape",
                  "value": "INT64[]"
                },
                {
                  "key": "param_name",
                  "value": "delta"
                }
              ]
            }
          ],
          "style": null,
          "config": null
        },
        {
          "id": "node_Constant_713",
          "label": "Constant",
          "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/view: aten.view.default",
          "subgraphIds": [],
          "attrs": [
            {
              "key": "__value",
              "value": "[-1,1]"
            },
            {
              "key": "value",
              "value": "TensorProtoTensor<INT64,[2]>(array([-1,  1]), name='val_25')"
            }
          ],
          "incomingEdges": [],
          "outputsMetadata": [
            {
              "id": "0",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "val_25"
                },
                {
                  "key": "tensor_shape",
                  "value": "INT64[2]"
                },
                {
                  "key": "param_name",
                  "value": "output"
                }
              ]
            }
          ],
          "inputsMetadata": [],
          "style": null,
          "config": null
        },
        {
          "id": "node_Reshape_55",
          "label": "Reshape",
          "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/view: aten.view.default",
          "subgraphIds": [],
          "attrs": [
            {
              "key": "allowzero",
              "value": "1"
            },
            {
              "key": "[metadata] namespace",
              "value": ": transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/view: aten.view.default"
            },
            {
              "key": "[metadata] pkg.torch.onnx.class_hierarchy",
              "value": "['transformers.models.llama.modeling_llama.LlamaForCausalLM', 'transformers.models.llama.modeling_llama.LlamaModel', 'aten.view.default']"
            },
            {
              "key": "[metadata] pkg.torch.onnx.fx_node",
              "value": "%view : [num_users=1] = call_function[target=torch.ops.aten.view.default](args = (%arange, [-1, 1]), kwargs = {})"
            },
            {
              "key": "[metadata] pkg.torch.onnx.name_scopes",
              "value": "['', 'model', 'view']"
            },
            {
              "key": "[metadata] pkg.torch.onnx.stack_trace",
              "value": "File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 688, in forward\n    outputs: BaseModelOutputWithPast = self.model(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 436, in forward\n    causal_mask = self._update_causal_mask("
            }
          ],
          "incomingEdges": [
            {
              "sourceNodeId": "node_Range_36",
              "sourceNodeOutputId": "0",
              "targetNodeInputId": "0"
            },
            {
              "sourceNodeId": "node_Constant_713",
              "sourceNodeOutputId": "0",
              "targetNodeInputId": "1"
            }
          ],
          "outputsMetadata": [
            {
              "id": "0",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "view"
                },
                {
                  "key": "tensor_shape",
                  "value": "INT64[s16,1]"
                },
                {
                  "key": "param_name",
                  "value": "reshaped"
                }
              ]
            }
          ],
          "inputsMetadata": [
            {
              "id": "0",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "arange"
                },
                {
                  "key": "tensor_shape",
                  "value": "INT64[s16]"
                },
                {
                  "key": "param_name",
                  "value": "data"
                }
              ]
            },
            {
              "id": "1",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "val_25"
                },
                {
                  "key": "tensor_shape",
                  "value": "INT64[2]"
                },
                {
                  "key": "param_name",
                  "value": "shape"
                }
              ]
            }
          ],
          "style": null,
          "config": null
        },
        {
          "id": "node_Greater_56",
          "label": "Greater",
          "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/gt: aten.gt.Tensor",
          "subgraphIds": [],
          "attrs": [
            {
              "key": "[metadata] namespace",
              "value": ": transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/gt: aten.gt.Tensor"
            },
            {
              "key": "[metadata] pkg.torch.onnx.class_hierarchy",
              "value": "['transformers.models.llama.modeling_llama.LlamaForCausalLM', 'transformers.models.llama.modeling_llama.LlamaModel', 'aten.gt.Tensor']"
            },
            {
              "key": "[metadata] pkg.torch.onnx.fx_node",
              "value": "%gt : [num_users=1] = call_function[target=torch.ops.aten.gt.Tensor](args = (%arange_1, %view), kwargs = {})"
            },
            {
              "key": "[metadata] pkg.torch.onnx.name_scopes",
              "value": "['', 'model', 'gt']"
            },
            {
              "key": "[metadata] pkg.torch.onnx.stack_trace",
              "value": "File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 688, in forward\n    outputs: BaseModelOutputWithPast = self.model(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 436, in forward\n    causal_mask = self._update_causal_mask("
            }
          ],
          "incomingEdges": [
            {
              "sourceNodeId": "node_Range_52",
              "sourceNodeOutputId": "0",
              "targetNodeInputId": "0"
            },
            {
              "sourceNodeId": "node_Reshape_55",
              "sourceNodeOutputId": "0",
              "targetNodeInputId": "1"
            }
          ],
          "outputsMetadata": [
            {
              "id": "0",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "gt"
                },
                {
                  "key": "tensor_shape",
                  "value": "BOOL[s16,s16 + s17]"
                },
                {
                  "key": "param_name",
                  "value": "C"
                }
              ]
            }
          ],
          "inputsMetadata": [
            {
              "id": "0",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "arange_1"
                },
                {
                  "key": "tensor_shape",
                  "value": "INT64[s16 + s17]"
                },
                {
                  "key": "param_name",
                  "value": "A"
                }
              ]
            },
            {
              "id": "1",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "view"
                },
                {
                  "key": "tensor_shape",
                  "value": "INT64[s16,1]"
                },
                {
                  "key": "param_name",
                  "value": "B"
                }
              ]
            }
          ],
          "style": null,
          "config": null
        },
        {
          "id": "node_Cast_57",
          "label": "Cast",
          "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/convert_element_type_default: prims.convert_element_type.default",
          "subgraphIds": [],
          "attrs": [
            {
              "key": "to",
              "value": "FLOAT"
            },
            {
              "key": "[metadata] namespace",
              "value": ": transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/convert_element_type_default: prims.convert_element_type.default"
            },
            {
              "key": "[metadata] pkg.torch.onnx.class_hierarchy",
              "value": "['transformers.models.llama.modeling_llama.LlamaForCausalLM', 'transformers.models.llama.modeling_llama.LlamaModel', 'prims.convert_element_type.default']"
            },
            {
              "key": "[metadata] pkg.torch.onnx.fx_node",
              "value": "%convert_element_type_default : [num_users=1] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%gt,), kwargs = {dtype: torch.float32})"
            },
            {
              "key": "[metadata] pkg.torch.onnx.name_scopes",
              "value": "['', 'model', 'convert_element_type_default']"
            },
            {
              "key": "[metadata] pkg.torch.onnx.stack_trace",
              "value": "File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 688, in forward\n    outputs: BaseModelOutputWithPast = self.model(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 436, in forward\n    causal_mask = self._update_causal_mask("
            }
          ],
          "incomingEdges": [
            {
              "sourceNodeId": "node_Greater_56",
              "sourceNodeOutputId": "0",
              "targetNodeInputId": "0"
            }
          ],
          "outputsMetadata": [
            {
              "id": "0",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "convert_element_type_default"
                },
                {
                  "key": "tensor_shape",
                  "value": "FLOAT[s16,s16 + s17]"
                },
                {
                  "key": "param_name",
                  "value": "output"
                }
              ]
            }
          ],
          "inputsMetadata": [
            {
              "id": "0",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "gt"
                },
                {
                  "key": "tensor_shape",
                  "value": "BOOL[s16,s16 + s17]"
                },
                {
                  "key": "param_name",
                  "value": "input"
                }
              ]
            }
          ],
          "style": null,
          "config": null
        },
        {
          "id": "node_Mul_58",
          "label": "Mul",
          "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/mul_16: aten.mul.Tensor",
          "subgraphIds": [],
          "attrs": [
            {
              "key": "[metadata] namespace",
              "value": ": transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/mul_16: aten.mul.Tensor"
            },
            {
              "key": "[metadata] pkg.torch.onnx.class_hierarchy",
              "value": "['transformers.models.llama.modeling_llama.LlamaForCausalLM', 'transformers.models.llama.modeling_llama.LlamaModel', 'aten.mul.Tensor']"
            },
            {
              "key": "[metadata] pkg.torch.onnx.fx_node",
              "value": "%mul_16 : [num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%triu, %convert_element_type_default), kwargs = {})"
            },
            {
              "key": "[metadata] pkg.torch.onnx.name_scopes",
              "value": "['', 'model', 'mul_16']"
            },
            {
              "key": "[metadata] pkg.torch.onnx.stack_trace",
              "value": "File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 688, in forward\n    outputs: BaseModelOutputWithPast = self.model(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 436, in forward\n    causal_mask = self._update_causal_mask("
            }
          ],
          "incomingEdges": [
            {
              "sourceNodeId": "n1",
              "sourceNodeOutputId": "0",
              "targetNodeInputId": "0"
            },
            {
              "sourceNodeId": "node_Cast_57",
              "sourceNodeOutputId": "0",
              "targetNodeInputId": "1"
            }
          ],
          "outputsMetadata": [
            {
              "id": "0",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "mul_16"
                },
                {
                  "key": "tensor_shape",
                  "value": "FLOAT[s16,s16 + s17]"
                },
                {
                  "key": "param_name",
                  "value": "C"
                }
              ]
            }
          ],
          "inputsMetadata": [
            {
              "id": "0",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "triu"
                },
                {
                  "key": "tensor_shape",
                  "value": "FLOAT[s16,s16 + s17]"
                },
                {
                  "key": "param_name",
                  "value": "A"
                }
              ]
            },
            {
              "id": "1",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "convert_element_type_default"
                },
                {
                  "key": "tensor_shape",
                  "value": "FLOAT[s16,s16 + s17]"
                },
                {
                  "key": "param_name",
                  "value": "B"
                }
              ]
            }
          ],
          "style": null,
          "config": null
        },
        {
          "id": "node_Constant_60",
          "label": "Constant",
          "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel",
          "subgraphIds": [],
          "attrs": [
            {
              "key": "__value",
              "value": "[1]"
            },
            {
              "key": "value",
              "value": "TensorProtoTensor<INT64,[1]>(array([1]), name='')"
            },
            {
              "key": "[metadata] namespace",
              "value": ": transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/unsqueeze_4: aten.unsqueeze.default"
            },
            {
              "key": "[metadata] pkg.torch.onnx.class_hierarchy",
              "value": "['transformers.models.llama.modeling_llama.LlamaForCausalLM', 'transformers.models.llama.modeling_llama.LlamaModel', 'aten.unsqueeze.default']"
            },
            {
              "key": "[metadata] pkg.torch.onnx.fx_node",
              "value": "%unsqueeze_4 : [num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%unsqueeze_3, 1), kwargs = {})"
            },
            {
              "key": "[metadata] pkg.torch.onnx.name_scopes",
              "value": "['', 'model', 'unsqueeze_4']"
            },
            {
              "key": "[metadata] pkg.torch.onnx.stack_trace",
              "value": "File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 688, in forward\n    outputs: BaseModelOutputWithPast = self.model(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 436, in forward\n    causal_mask = self._update_causal_mask("
            }
          ],
          "incomingEdges": [],
          "outputsMetadata": [
            {
              "id": "0",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "val_26"
                },
                {
                  "key": "tensor_shape",
                  "value": "INT64[1]"
                },
                {
                  "key": "param_name",
                  "value": "output"
                }
              ]
            }
          ],
          "inputsMetadata": [],
          "style": null,
          "config": null
        },
        {
          "id": "node_Constant_1026",
          "label": "Constant",
          "namespace": "main_graph",
          "subgraphIds": [],
          "attrs": [
            {
              "key": "__value",
              "value": "[0,1]"
            },
            {
              "key": "value",
              "value": "TensorProtoTensor<INT64,[2]>(array([0, 1]), name='val_491')"
            }
          ],
          "incomingEdges": [],
          "outputsMetadata": [
            {
              "id": "0",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "val_491"
                },
                {
                  "key": "tensor_shape",
                  "value": "INT64[2]"
                },
                {
                  "key": "param_name",
                  "value": "output"
                }
              ]
            }
          ],
          "inputsMetadata": [],
          "style": null,
          "config": null
        },
        {
          "id": "node_Unsqueeze_1027",
          "label": "Unsqueeze",
          "namespace": "main_graph",
          "subgraphIds": [],
          "attrs": [],
          "incomingEdges": [
            {
              "sourceNodeId": "node_Mul_58",
              "sourceNodeOutputId": "0",
              "targetNodeInputId": "0"
            },
            {
              "sourceNodeId": "node_Constant_1026",
              "sourceNodeOutputId": "0",
              "targetNodeInputId": "1"
            }
          ],
          "outputsMetadata": [
            {
              "id": "0",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "unsqueeze_4"
                },
                {
                  "key": "tensor_shape",
                  "value": "FLOAT[1,1,s16,s16 + s17]"
                },
                {
                  "key": "param_name",
                  "value": "expanded"
                }
              ]
            }
          ],
          "inputsMetadata": [
            {
              "id": "0",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "mul_16"
                },
                {
                  "key": "tensor_shape",
                  "value": "FLOAT[s16,s16 + s17]"
                },
                {
                  "key": "param_name",
                  "value": "data"
                }
              ]
            },
            {
              "id": "1",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "val_491"
                },
                {
                  "key": "tensor_shape",
                  "value": "INT64[2]"
                },
                {
                  "key": "param_name",
                  "value": "axes"
                }
              ]
            }
          ],
          "style": null,
          "config": null
        },
        {
          "id": "node_Constant_62",
          "label": "Constant",
          "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel",
          "subgraphIds": [],
          "attrs": [
            {
              "key": "__value",
              "value": "0"
            },
            {
              "key": "value",
              "value": "TensorProtoTensor<INT64,[]>(array(0), name='')"
            },
            {
              "key": "[metadata] namespace",
              "value": ": transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/slice_3: aten.slice.Tensor"
            },
            {
              "key": "[metadata] pkg.torch.onnx.class_hierarchy",
              "value": "['transformers.models.llama.modeling_llama.LlamaForCausalLM', 'transformers.models.llama.modeling_llama.LlamaModel', 'aten.slice.Tensor']"
            },
            {
              "key": "[metadata] pkg.torch.onnx.fx_node",
              "value": "%slice_3 : [num_users=1] = call_function[target=torch.ops.aten.slice.Tensor](args = (%unsqueeze_4, 2, 0, 9223372036854775807), kwargs = {})"
            },
            {
              "key": "[metadata] pkg.torch.onnx.name_scopes",
              "value": "['', 'model', 'slice_3']"
            },
            {
              "key": "[metadata] pkg.torch.onnx.stack_trace",
              "value": "File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 688, in forward\n    outputs: BaseModelOutputWithPast = self.model(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 436, in forward\n    causal_mask = self._update_causal_mask("
            }
          ],
          "incomingEdges": [],
          "outputsMetadata": [
            {
              "id": "0",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "val_27"
                },
                {
                  "key": "tensor_shape",
                  "value": "INT64[]"
                },
                {
                  "key": "param_name",
                  "value": "output"
                }
              ]
            }
          ],
          "inputsMetadata": [],
          "style": null,
          "config": null
        },
        {
          "id": "node_Constant_70",
          "label": "Constant",
          "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/slice_scatter: aten.slice_scatter.default",
          "subgraphIds": [],
          "attrs": [
            {
              "key": "__value",
              "value": "2"
            },
            {
              "key": "value",
              "value": "TensorProtoTensor<INT64,[]>(array(2), name='')"
            },
            {
              "key": "[metadata] namespace",
              "value": ": transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/slice_3: aten.slice.Tensor"
            },
            {
              "key": "[metadata] pkg.torch.onnx.class_hierarchy",
              "value": "['transformers.models.llama.modeling_llama.LlamaForCausalLM', 'transformers.models.llama.modeling_llama.LlamaModel', 'aten.slice.Tensor']"
            },
            {
              "key": "[metadata] pkg.torch.onnx.fx_node",
              "value": "%slice_3 : [num_users=1] = call_function[target=torch.ops.aten.slice.Tensor](args = (%unsqueeze_4, 2, 0, 9223372036854775807), kwargs = {})"
            },
            {
              "key": "[metadata] pkg.torch.onnx.name_scopes",
              "value": "['', 'model', 'slice_3']"
            },
            {
              "key": "[metadata] pkg.torch.onnx.stack_trace",
              "value": "File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 688, in forward\n    outputs: BaseModelOutputWithPast = self.model(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 436, in forward\n    causal_mask = self._update_causal_mask("
            }
          ],
          "incomingEdges": [],
          "outputsMetadata": [
            {
              "id": "0",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "val_35"
                },
                {
                  "key": "tensor_shape",
                  "value": "INT64[]"
                },
                {
                  "key": "param_name",
                  "value": "output"
                }
              ]
            }
          ],
          "inputsMetadata": [],
          "style": null,
          "config": null
        },
        {
          "id": "node_Constant_90",
          "label": "Constant",
          "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel",
          "subgraphIds": [],
          "attrs": [
            {
              "key": "__value",
              "value": "[-1]"
            },
            {
              "key": "value",
              "value": "TensorProtoTensor<INT64,[1]>(array([-1]), name='')"
            },
            {
              "key": "[metadata] namespace",
              "value": ": transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/expand_1: aten.expand.default"
            },
            {
              "key": "[metadata] pkg.torch.onnx.class_hierarchy",
              "value": "['transformers.models.llama.modeling_llama.LlamaForCausalLM', 'transformers.models.llama.modeling_llama.LlamaModel', 'aten.expand.default']"
            },
            {
              "key": "[metadata] pkg.torch.onnx.fx_node",
              "value": "%expand_1 : [num_users=1] = call_function[target=torch.ops.aten.expand.default](args = (%slice_4, [%sym_size_int_68, 1, -1, -1]), kwargs = {})"
            },
            {
              "key": "[metadata] pkg.torch.onnx.name_scopes",
              "value": "['', 'model', 'expand_1']"
            },
            {
              "key": "[metadata] pkg.torch.onnx.stack_trace",
              "value": "File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 688, in forward\n    outputs: BaseModelOutputWithPast = self.model(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 436, in forward\n    causal_mask = self._update_causal_mask("
            }
          ],
          "incomingEdges": [],
          "outputsMetadata": [
            {
              "id": "0",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "val_53"
                },
                {
                  "key": "tensor_shape",
                  "value": "INT64[1]"
                },
                {
                  "key": "param_name",
                  "value": "output"
                }
              ]
            }
          ],
          "inputsMetadata": [],
          "style": null,
          "config": null
        },
        {
          "id": "node_Concat_91",
          "label": "Concat",
          "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/expand_1: aten.expand.default",
          "subgraphIds": [],
          "attrs": [
            {
              "key": "axis",
              "value": "0"
            },
            {
              "key": "[metadata] namespace",
              "value": ": transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/expand_1: aten.expand.default"
            },
            {
              "key": "[metadata] pkg.torch.onnx.class_hierarchy",
              "value": "['transformers.models.llama.modeling_llama.LlamaForCausalLM', 'transformers.models.llama.modeling_llama.LlamaModel', 'aten.expand.default']"
            },
            {
              "key": "[metadata] pkg.torch.onnx.fx_node",
              "value": "%expand_1 : [num_users=1] = call_function[target=torch.ops.aten.expand.default](args = (%slice_4, [%sym_size_int_68, 1, -1, -1]), kwargs = {})"
            },
            {
              "key": "[metadata] pkg.torch.onnx.name_scopes",
              "value": "['', 'model', 'expand_1']"
            },
            {
              "key": "[metadata] pkg.torch.onnx.stack_trace",
              "value": "File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 688, in forward\n    outputs: BaseModelOutputWithPast = self.model(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 436, in forward\n    causal_mask = self._update_causal_mask("
            }
          ],
          "incomingEdges": [
            {
              "sourceNodeId": "node_Shape_12",
              "sourceNodeOutputId": "0",
              "targetNodeInputId": "0"
            },
            {
              "sourceNodeId": "node_Constant_60",
              "sourceNodeOutputId": "0",
              "targetNodeInputId": "1"
            },
            {
              "sourceNodeId": "node_Constant_90",
              "sourceNodeOutputId": "0",
              "targetNodeInputId": "2"
            },
            {
              "sourceNodeId": "node_Constant_90",
              "sourceNodeOutputId": "0",
              "targetNodeInputId": "3"
            }
          ],
          "outputsMetadata": [
            {
              "id": "0",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "val_54"
                },
                {
                  "key": "tensor_shape",
                  "value": "INT64[4]"
                },
                {
                  "key": "param_name",
                  "value": "concat_result"
                }
              ]
            }
          ],
          "inputsMetadata": [
            {
              "id": "0",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "val_6"
                },
                {
                  "key": "tensor_shape",
                  "value": "INT64[1]"
                },
                {
                  "key": "param_name",
                  "value": "inputs"
                }
              ]
            },
            {
              "id": "1",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "val_26"
                },
                {
                  "key": "tensor_shape",
                  "value": "INT64[1]"
                },
                {
                  "key": "param_name",
                  "value": "inputs"
                }
              ]
            },
            {
              "id": "2",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "val_53"
                },
                {
                  "key": "tensor_shape",
                  "value": "INT64[1]"
                },
                {
                  "key": "param_name",
                  "value": "inputs"
                }
              ]
            },
            {
              "id": "3",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "val_53"
                },
                {
                  "key": "tensor_shape",
                  "value": "INT64[1]"
                },
                {
                  "key": "param_name",
                  "value": "inputs"
                }
              ]
            }
          ],
          "style": null,
          "config": null
        },
        {
          "id": "node_Abs_93",
          "label": "Abs",
          "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/expand_1: aten.expand.default",
          "subgraphIds": [],
          "attrs": [
            {
              "key": "[metadata] namespace",
              "value": ": transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/expand_1: aten.expand.default"
            },
            {
              "key": "[metadata] pkg.torch.onnx.class_hierarchy",
              "value": "['transformers.models.llama.modeling_llama.LlamaForCausalLM', 'transformers.models.llama.modeling_llama.LlamaModel', 'aten.expand.default']"
            },
            {
              "key": "[metadata] pkg.torch.onnx.fx_node",
              "value": "%expand_1 : [num_users=1] = call_function[target=torch.ops.aten.expand.default](args = (%slice_4, [%sym_size_int_68, 1, -1, -1]), kwargs = {})"
            },
            {
              "key": "[metadata] pkg.torch.onnx.name_scopes",
              "value": "['', 'model', 'expand_1']"
            },
            {
              "key": "[metadata] pkg.torch.onnx.stack_trace",
              "value": "File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 688, in forward\n    outputs: BaseModelOutputWithPast = self.model(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 436, in forward\n    causal_mask = self._update_causal_mask("
            }
          ],
          "incomingEdges": [
            {
              "sourceNodeId": "node_Concat_91",
              "sourceNodeOutputId": "0",
              "targetNodeInputId": "0"
            }
          ],
          "outputsMetadata": [
            {
              "id": "0",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "val_56"
                },
                {
                  "key": "tensor_shape",
                  "value": "INT64[4]"
                },
                {
                  "key": "param_name",
                  "value": "Y"
                }
              ]
            }
          ],
          "inputsMetadata": [
            {
              "id": "0",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "val_54"
                },
                {
                  "key": "tensor_shape",
                  "value": "INT64[4]"
                },
                {
                  "key": "param_name",
                  "value": "X"
                }
              ]
            }
          ],
          "style": null,
          "config": null
        },
        {
          "id": "node_Expand_94",
          "label": "Expand",
          "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/expand_1: aten.expand.default",
          "subgraphIds": [],
          "attrs": [
            {
              "key": "[metadata] namespace",
              "value": ": transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/expand_1: aten.expand.default"
            },
            {
              "key": "[metadata] pkg.torch.onnx.class_hierarchy",
              "value": "['transformers.models.llama.modeling_llama.LlamaForCausalLM', 'transformers.models.llama.modeling_llama.LlamaModel', 'aten.expand.default']"
            },
            {
              "key": "[metadata] pkg.torch.onnx.fx_node",
              "value": "%expand_1 : [num_users=1] = call_function[target=torch.ops.aten.expand.default](args = (%slice_4, [%sym_size_int_68, 1, -1, -1]), kwargs = {})"
            },
            {
              "key": "[metadata] pkg.torch.onnx.name_scopes",
              "value": "['', 'model', 'expand_1']"
            },
            {
              "key": "[metadata] pkg.torch.onnx.stack_trace",
              "value": "File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 688, in forward\n    outputs: BaseModelOutputWithPast = self.model(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 436, in forward\n    causal_mask = self._update_causal_mask("
            }
          ],
          "incomingEdges": [
            {
              "sourceNodeId": "node_Unsqueeze_1027",
              "sourceNodeOutputId": "0",
              "targetNodeInputId": "0"
            },
            {
              "sourceNodeId": "node_Abs_93",
              "sourceNodeOutputId": "0",
              "targetNodeInputId": "1"
            }
          ],
          "outputsMetadata": [
            {
              "id": "0",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "expand_1"
                },
                {
                  "key": "tensor_shape",
                  "value": "FLOAT[s34,1,s16,s16 + s17]"
                },
                {
                  "key": "param_name",
                  "value": "output"
                }
              ]
            }
          ],
          "inputsMetadata": [
            {
              "id": "0",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "unsqueeze_4"
                },
                {
                  "key": "tensor_shape",
                  "value": "FLOAT[1,1,s16,s16 + s17]"
                },
                {
                  "key": "param_name",
                  "value": "input"
                }
              ]
            },
            {
              "id": "1",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "val_56"
                },
                {
                  "key": "tensor_shape",
                  "value": "INT64[4]"
                },
                {
                  "key": "param_name",
                  "value": "shape"
                }
              ]
            }
          ],
          "style": null,
          "config": null
        },
        {
          "id": "node_Constant_105",
          "label": "Constant",
          "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel",
          "subgraphIds": [],
          "attrs": [
            {
              "key": "__value",
              "value": "1"
            },
            {
              "key": "value",
              "value": "TensorProtoTensor<INT64,[]>(array(1), name='')"
            },
            {
              "key": "[metadata] namespace",
              "value": ": transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/slice_6: aten.slice.Tensor"
            },
            {
              "key": "[metadata] pkg.torch.onnx.class_hierarchy",
              "value": "['transformers.models.llama.modeling_llama.LlamaForCausalLM', 'transformers.models.llama.modeling_llama.LlamaModel', 'aten.slice.Tensor']"
            },
            {
              "key": "[metadata] pkg.torch.onnx.fx_node",
              "value": "%slice_6 : [num_users=1] = call_function[target=torch.ops.aten.slice.Tensor](args = (%slice_5, 1), kwargs = {})"
            },
            {
              "key": "[metadata] pkg.torch.onnx.name_scopes",
              "value": "['', 'model', 'slice_6']"
            },
            {
              "key": "[metadata] pkg.torch.onnx.stack_trace",
              "value": "File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 688, in forward\n    outputs: BaseModelOutputWithPast = self.model(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 436, in forward\n    causal_mask = self._update_causal_mask("
            }
          ],
          "incomingEdges": [],
          "outputsMetadata": [
            {
              "id": "0",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "val_65"
                },
                {
                  "key": "tensor_shape",
                  "value": "INT64[]"
                },
                {
                  "key": "param_name",
                  "value": "output"
                }
              ]
            }
          ],
          "inputsMetadata": [],
          "style": null,
          "config": null
        },
        {
          "id": "node_Constant_118",
          "label": "Constant",
          "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/slice_8: aten.slice.Tensor",
          "subgraphIds": [],
          "attrs": [
            {
              "key": "__value",
              "value": "[0]"
            },
            {
              "key": "value_ints",
              "value": "[0]"
            },
            {
              "key": "[metadata] namespace",
              "value": ": transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/slice_8: aten.slice.Tensor"
            },
            {
              "key": "[metadata] pkg.torch.onnx.class_hierarchy",
              "value": "['transformers.models.llama.modeling_llama.LlamaForCausalLM', 'transformers.models.llama.modeling_llama.LlamaModel', 'aten.slice.Tensor']"
            },
            {
              "key": "[metadata] pkg.torch.onnx.fx_node",
              "value": "%slice_8 : [num_users=1] = call_function[target=torch.ops.aten.slice.Tensor](args = (%slice_7, 3, None, %add_4), kwargs = {})"
            },
            {
              "key": "[metadata] pkg.torch.onnx.name_scopes",
              "value": "['', 'model', 'slice_8']"
            },
            {
              "key": "[metadata] pkg.torch.onnx.stack_trace",
              "value": "File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 688, in forward\n    outputs: BaseModelOutputWithPast = self.model(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 436, in forward\n    causal_mask = self._update_causal_mask("
            }
          ],
          "incomingEdges": [],
          "outputsMetadata": [
            {
              "id": "0",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "val_76"
                },
                {
                  "key": "tensor_shape",
                  "value": "INT64[1]"
                },
                {
                  "key": "param_name",
                  "value": "output"
                }
              ]
            }
          ],
          "inputsMetadata": [],
          "style": null,
          "config": null
        },
        {
          "id": "node_Constant_120",
          "label": "Constant",
          "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/slice_8: aten.slice.Tensor",
          "subgraphIds": [],
          "attrs": [
            {
              "key": "__value",
              "value": "[-1]"
            },
            {
              "key": "value_ints",
              "value": "[-1]"
            },
            {
              "key": "[metadata] namespace",
              "value": ": transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/slice_8: aten.slice.Tensor"
            },
            {
              "key": "[metadata] pkg.torch.onnx.class_hierarchy",
              "value": "['transformers.models.llama.modeling_llama.LlamaForCausalLM', 'transformers.models.llama.modeling_llama.LlamaModel', 'aten.slice.Tensor']"
            },
            {
              "key": "[metadata] pkg.torch.onnx.fx_node",
              "value": "%slice_8 : [num_users=1] = call_function[target=torch.ops.aten.slice.Tensor](args = (%slice_7, 3, None, %add_4), kwargs = {})"
            },
            {
              "key": "[metadata] pkg.torch.onnx.name_scopes",
              "value": "['', 'model', 'slice_8']"
            },
            {
              "key": "[metadata] pkg.torch.onnx.stack_trace",
              "value": "File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 688, in forward\n    outputs: BaseModelOutputWithPast = self.model(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 436, in forward\n    causal_mask = self._update_causal_mask("
            }
          ],
          "incomingEdges": [],
          "outputsMetadata": [
            {
              "id": "0",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "val_78"
                },
                {
                  "key": "tensor_shape",
                  "value": "INT64[1]"
                },
                {
                  "key": "param_name",
                  "value": "output"
                }
              ]
            }
          ],
          "inputsMetadata": [],
          "style": null,
          "config": null
        },
        {
          "id": "node_Reshape_121",
          "label": "Reshape",
          "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/slice_8: aten.slice.Tensor",
          "subgraphIds": [],
          "attrs": [
            {
              "key": "allowzero",
              "value": "0"
            },
            {
              "key": "[metadata] namespace",
              "value": ": transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/slice_8: aten.slice.Tensor"
            },
            {
              "key": "[metadata] pkg.torch.onnx.class_hierarchy",
              "value": "['transformers.models.llama.modeling_llama.LlamaForCausalLM', 'transformers.models.llama.modeling_llama.LlamaModel', 'aten.slice.Tensor']"
            },
            {
              "key": "[metadata] pkg.torch.onnx.fx_node",
              "value": "%slice_8 : [num_users=1] = call_function[target=torch.ops.aten.slice.Tensor](args = (%slice_7, 3, None, %add_4), kwargs = {})"
            },
            {
              "key": "[metadata] pkg.torch.onnx.name_scopes",
              "value": "['', 'model', 'slice_8']"
            },
            {
              "key": "[metadata] pkg.torch.onnx.stack_trace",
              "value": "File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 688, in forward\n    outputs: BaseModelOutputWithPast = self.model(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 436, in forward\n    causal_mask = self._update_causal_mask("
            }
          ],
          "incomingEdges": [
            {
              "sourceNodeId": "node_Add_33",
              "sourceNodeOutputId": "0",
              "targetNodeInputId": "0"
            },
            {
              "sourceNodeId": "node_Constant_120",
              "sourceNodeOutputId": "0",
              "targetNodeInputId": "1"
            }
          ],
          "outputsMetadata": [
            {
              "id": "0",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "val_79"
                },
                {
                  "key": "tensor_shape",
                  "value": "INT64[1]"
                },
                {
                  "key": "param_name",
                  "value": "reshaped"
                }
              ]
            }
          ],
          "inputsMetadata": [
            {
              "id": "0",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "add_4"
                },
                {
                  "key": "tensor_shape",
                  "value": "INT64[]"
                },
                {
                  "key": "param_name",
                  "value": "data"
                }
              ]
            },
            {
              "id": "1",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "val_78"
                },
                {
                  "key": "tensor_shape",
                  "value": "INT64[1]"
                },
                {
                  "key": "param_name",
                  "value": "shape"
                }
              ]
            }
          ],
          "style": null,
          "config": null
        },
        {
          "id": "node_Constant_745",
          "label": "Constant",
          "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/slice_8: aten.slice.Tensor",
          "subgraphIds": [],
          "attrs": [
            {
              "key": "__value",
              "value": "[3]"
            },
            {
              "key": "value",
              "value": "TensorProtoTensor<INT64,[1]>(array([3]), name='val_82')"
            }
          ],
          "incomingEdges": [],
          "outputsMetadata": [
            {
              "id": "0",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "val_82"
                },
                {
                  "key": "tensor_shape",
                  "value": "INT64[1]"
                },
                {
                  "key": "param_name",
                  "value": "output"
                }
              ]
            }
          ],
          "inputsMetadata": [],
          "style": null,
          "config": null
        },
        {
          "id": "node_Constant_125",
          "label": "Constant",
          "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/slice_8: aten.slice.Tensor",
          "subgraphIds": [],
          "attrs": [
            {
              "key": "__value",
              "value": "[1]"
            },
            {
              "key": "value_ints",
              "value": "[1]"
            },
            {
              "key": "[metadata] namespace",
              "value": ": transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/slice_8: aten.slice.Tensor"
            },
            {
              "key": "[metadata] pkg.torch.onnx.class_hierarchy",
              "value": "['transformers.models.llama.modeling_llama.LlamaForCausalLM', 'transformers.models.llama.modeling_llama.LlamaModel', 'aten.slice.Tensor']"
            },
            {
              "key": "[metadata] pkg.torch.onnx.fx_node",
              "value": "%slice_8 : [num_users=1] = call_function[target=torch.ops.aten.slice.Tensor](args = (%slice_7, 3, None, %add_4), kwargs = {})"
            },
            {
              "key": "[metadata] pkg.torch.onnx.name_scopes",
              "value": "['', 'model', 'slice_8']"
            },
            {
              "key": "[metadata] pkg.torch.onnx.stack_trace",
              "value": "File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 688, in forward\n    outputs: BaseModelOutputWithPast = self.model(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 436, in forward\n    causal_mask = self._update_causal_mask("
            }
          ],
          "incomingEdges": [],
          "outputsMetadata": [
            {
              "id": "0",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "val_83"
                },
                {
                  "key": "tensor_shape",
                  "value": "INT64[1]"
                },
                {
                  "key": "param_name",
                  "value": "output"
                }
              ]
            }
          ],
          "inputsMetadata": [],
          "style": null,
          "config": null
        },
        {
          "id": "node_Slice_126",
          "label": "Slice",
          "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/slice_8: aten.slice.Tensor",
          "subgraphIds": [],
          "attrs": [
            {
              "key": "[metadata] namespace",
              "value": ": transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/slice_8: aten.slice.Tensor"
            },
            {
              "key": "[metadata] pkg.torch.onnx.class_hierarchy",
              "value": "['transformers.models.llama.modeling_llama.LlamaForCausalLM', 'transformers.models.llama.modeling_llama.LlamaModel', 'aten.slice.Tensor']"
            },
            {
              "key": "[metadata] pkg.torch.onnx.fx_node",
              "value": "%slice_8 : [num_users=1] = call_function[target=torch.ops.aten.slice.Tensor](args = (%slice_7, 3, None, %add_4), kwargs = {})"
            },
            {
              "key": "[metadata] pkg.torch.onnx.name_scopes",
              "value": "['', 'model', 'slice_8']"
            },
            {
              "key": "[metadata] pkg.torch.onnx.stack_trace",
              "value": "File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 688, in forward\n    outputs: BaseModelOutputWithPast = self.model(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 436, in forward\n    causal_mask = self._update_causal_mask("
            }
          ],
          "incomingEdges": [
            {
              "sourceNodeId": "node_Expand_94",
              "sourceNodeOutputId": "0",
              "targetNodeInputId": "0"
            },
            {
              "sourceNodeId": "node_Constant_118",
              "sourceNodeOutputId": "0",
              "targetNodeInputId": "1"
            },
            {
              "sourceNodeId": "node_Reshape_121",
              "sourceNodeOutputId": "0",
              "targetNodeInputId": "2"
            },
            {
              "sourceNodeId": "node_Constant_745",
              "sourceNodeOutputId": "0",
              "targetNodeInputId": "3"
            },
            {
              "sourceNodeId": "node_Constant_125",
              "sourceNodeOutputId": "0",
              "targetNodeInputId": "4"
            }
          ],
          "outputsMetadata": [
            {
              "id": "0",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "slice_8"
                },
                {
                  "key": "tensor_shape",
                  "value": "FLOAT[s34,1,s16,s16 + s17]"
                },
                {
                  "key": "param_name",
                  "value": "output"
                }
              ]
            }
          ],
          "inputsMetadata": [
            {
              "id": "0",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "expand_1"
                },
                {
                  "key": "tensor_shape",
                  "value": "FLOAT[s34,1,s16,s16 + s17]"
                },
                {
                  "key": "param_name",
                  "value": "data"
                }
              ]
            },
            {
              "id": "1",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "val_76"
                },
                {
                  "key": "tensor_shape",
                  "value": "INT64[1]"
                },
                {
                  "key": "param_name",
                  "value": "starts"
                }
              ]
            },
            {
              "id": "2",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "val_79"
                },
                {
                  "key": "tensor_shape",
                  "value": "INT64[1]"
                },
                {
                  "key": "param_name",
                  "value": "ends"
                }
              ]
            },
            {
              "id": "3",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "val_82"
                },
                {
                  "key": "tensor_shape",
                  "value": "INT64[1]"
                },
                {
                  "key": "param_name",
                  "value": "axes"
                }
              ]
            },
            {
              "id": "4",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "val_83"
                },
                {
                  "key": "tensor_shape",
                  "value": "INT64[1]"
                },
                {
                  "key": "param_name",
                  "value": "steps"
                }
              ]
            }
          ],
          "style": null,
          "config": null
        },
        {
          "id": "node_Constant_1028",
          "label": "Constant",
          "namespace": "main_graph",
          "subgraphIds": [],
          "attrs": [
            {
              "key": "__value",
              "value": "[1,2]"
            },
            {
              "key": "value",
              "value": "TensorProtoTensor<INT64,[2]>(array([1, 2]), name='val_492')"
            }
          ],
          "incomingEdges": [],
          "outputsMetadata": [
            {
              "id": "0",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "val_492"
                },
                {
                  "key": "tensor_shape",
                  "value": "INT64[2]"
                },
                {
                  "key": "param_name",
                  "value": "output"
                }
              ]
            }
          ],
          "inputsMetadata": [],
          "style": null,
          "config": null
        },
        {
          "id": "node_Unsqueeze_1029",
          "label": "Unsqueeze",
          "namespace": "main_graph",
          "subgraphIds": [],
          "attrs": [],
          "incomingEdges": [
            {
              "sourceNodeId": "[value] attention_mask",
              "sourceNodeOutputId": "0",
              "targetNodeInputId": "0"
            },
            {
              "sourceNodeId": "node_Constant_1028",
              "sourceNodeOutputId": "0",
              "targetNodeInputId": "1"
            }
          ],
          "outputsMetadata": [
            {
              "id": "0",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "unsqueeze_6"
                },
                {
                  "key": "tensor_shape",
                  "value": "INT64[s34,1,1,s16 + s17]"
                },
                {
                  "key": "param_name",
                  "value": "expanded"
                }
              ]
            }
          ],
          "inputsMetadata": [
            {
              "id": "0",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "attention_mask"
                },
                {
                  "key": "tensor_shape",
                  "value": "INT64[s34,s16 + s17]"
                },
                {
                  "key": "[metadata] pkg.torch.export.graph_signature.InputSpec.kind",
                  "value": "USER_INPUT"
                },
                {
                  "key": "[metadata] pkg.torch.export.graph_signature.InputSpec.persistent",
                  "value": "None"
                },
                {
                  "key": "param_name",
                  "value": "data"
                }
              ]
            },
            {
              "id": "1",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "val_492"
                },
                {
                  "key": "tensor_shape",
                  "value": "INT64[2]"
                },
                {
                  "key": "param_name",
                  "value": "axes"
                }
              ]
            }
          ],
          "style": null,
          "config": null
        },
        {
          "id": "node_Cast_152",
          "label": "Cast",
          "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/convert_element_type_default_1: prims.convert_element_type.default",
          "subgraphIds": [],
          "attrs": [
            {
              "key": "to",
              "value": "FLOAT"
            },
            {
              "key": "[metadata] namespace",
              "value": ": transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/convert_element_type_default_1: prims.convert_element_type.default"
            },
            {
              "key": "[metadata] pkg.torch.onnx.class_hierarchy",
              "value": "['transformers.models.llama.modeling_llama.LlamaForCausalLM', 'transformers.models.llama.modeling_llama.LlamaModel', 'prims.convert_element_type.default']"
            },
            {
              "key": "[metadata] pkg.torch.onnx.fx_node",
              "value": "%convert_element_type_default_1 : [num_users=1] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%slice_10,), kwargs = {dtype: torch.float32})"
            },
            {
              "key": "[metadata] pkg.torch.onnx.name_scopes",
              "value": "['', 'model', 'convert_element_type_default_1']"
            },
            {
              "key": "[metadata] pkg.torch.onnx.stack_trace",
              "value": "File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 688, in forward\n    outputs: BaseModelOutputWithPast = self.model(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 436, in forward\n    causal_mask = self._update_causal_mask("
            }
          ],
          "incomingEdges": [
            {
              "sourceNodeId": "node_Unsqueeze_1029",
              "sourceNodeOutputId": "0",
              "targetNodeInputId": "0"
            }
          ],
          "outputsMetadata": [
            {
              "id": "0",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "convert_element_type_default_1"
                },
                {
                  "key": "tensor_shape",
                  "value": "FLOAT[s34,1,1,s16 + s17]"
                },
                {
                  "key": "param_name",
                  "value": "output"
                }
              ]
            }
          ],
          "inputsMetadata": [
            {
              "id": "0",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "unsqueeze_6"
                },
                {
                  "key": "tensor_shape",
                  "value": "INT64[s34,1,1,s16 + s17]"
                },
                {
                  "key": "param_name",
                  "value": "input"
                }
              ]
            }
          ],
          "style": null,
          "config": null
        },
        {
          "id": "node_Add_153",
          "label": "Add",
          "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/add_89: aten.add.Tensor",
          "subgraphIds": [],
          "attrs": [
            {
              "key": "[metadata] namespace",
              "value": ": transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/add_89: aten.add.Tensor"
            },
            {
              "key": "[metadata] pkg.torch.onnx.class_hierarchy",
              "value": "['transformers.models.llama.modeling_llama.LlamaForCausalLM', 'transformers.models.llama.modeling_llama.LlamaModel', 'aten.add.Tensor']"
            },
            {
              "key": "[metadata] pkg.torch.onnx.fx_node",
              "value": "%add_89 : [num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%slice_8, %convert_element_type_default_1), kwargs = {})"
            },
            {
              "key": "[metadata] pkg.torch.onnx.name_scopes",
              "value": "['', 'model', 'add_89']"
            },
            {
              "key": "[metadata] pkg.torch.onnx.stack_trace",
              "value": "File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 688, in forward\n    outputs: BaseModelOutputWithPast = self.model(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 436, in forward\n    causal_mask = self._update_causal_mask("
            }
          ],
          "incomingEdges": [
            {
              "sourceNodeId": "node_Slice_126",
              "sourceNodeOutputId": "0",
              "targetNodeInputId": "0"
            },
            {
              "sourceNodeId": "node_Cast_152",
              "sourceNodeOutputId": "0",
              "targetNodeInputId": "1"
            }
          ],
          "outputsMetadata": [
            {
              "id": "0",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "add_89"
                },
                {
                  "key": "tensor_shape",
                  "value": "FLOAT[s34,1,s16,s16 + s17]"
                },
                {
                  "key": "param_name",
                  "value": "C"
                }
              ]
            }
          ],
          "inputsMetadata": [
            {
              "id": "0",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "slice_8"
                },
                {
                  "key": "tensor_shape",
                  "value": "FLOAT[s34,1,s16,s16 + s17]"
                },
                {
                  "key": "param_name",
                  "value": "A"
                }
              ]
            },
            {
              "id": "1",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "convert_element_type_default_1"
                },
                {
                  "key": "tensor_shape",
                  "value": "FLOAT[s34,1,1,s16 + s17]"
                },
                {
                  "key": "param_name",
                  "value": "B"
                }
              ]
            }
          ],
          "style": null,
          "config": null
        },
        {
          "id": "node_Constant_764",
          "label": "Constant",
          "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/eq_67: aten.eq.Tensor",
          "subgraphIds": [],
          "attrs": [
            {
              "key": "__value",
              "value": "0.0"
            },
            {
              "key": "value",
              "value": "TensorProtoTensor<FLOAT,[]>(array(0., dtype=float32), name='scalar_tensor_default')"
            }
          ],
          "incomingEdges": [],
          "outputsMetadata": [
            {
              "id": "0",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "scalar_tensor_default"
                },
                {
                  "key": "tensor_shape",
                  "value": "FLOAT[]"
                },
                {
                  "key": "param_name",
                  "value": "output"
                }
              ]
            }
          ],
          "inputsMetadata": [],
          "style": null,
          "config": null
        },
        {
          "id": "node_Equal_155",
          "label": "Equal",
          "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/eq_67: aten.eq.Tensor",
          "subgraphIds": [],
          "attrs": [
            {
              "key": "[metadata] namespace",
              "value": ": transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/eq_67: aten.eq.Tensor"
            },
            {
              "key": "[metadata] pkg.torch.onnx.class_hierarchy",
              "value": "['transformers.models.llama.modeling_llama.LlamaForCausalLM', 'transformers.models.llama.modeling_llama.LlamaModel', 'aten.eq.Tensor']"
            },
            {
              "key": "[metadata] pkg.torch.onnx.fx_node",
              "value": "%eq_67 : [num_users=1] = call_function[target=torch.ops.aten.eq.Tensor](args = (%add_89, %scalar_tensor_default), kwargs = {})"
            },
            {
              "key": "[metadata] pkg.torch.onnx.name_scopes",
              "value": "['', 'model', 'eq_67']"
            },
            {
              "key": "[metadata] pkg.torch.onnx.stack_trace",
              "value": "File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 688, in forward\n    outputs: BaseModelOutputWithPast = self.model(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 436, in forward\n    causal_mask = self._update_causal_mask("
            }
          ],
          "incomingEdges": [
            {
              "sourceNodeId": "node_Add_153",
              "sourceNodeOutputId": "0",
              "targetNodeInputId": "0"
            },
            {
              "sourceNodeId": "node_Constant_764",
              "sourceNodeOutputId": "0",
              "targetNodeInputId": "1"
            }
          ],
          "outputsMetadata": [
            {
              "id": "0",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "eq_67"
                },
                {
                  "key": "tensor_shape",
                  "value": "BOOL[s34,1,s16,s16 + s17]"
                },
                {
                  "key": "param_name",
                  "value": "C"
                }
              ]
            }
          ],
          "inputsMetadata": [
            {
              "id": "0",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "add_89"
                },
                {
                  "key": "tensor_shape",
                  "value": "FLOAT[s34,1,s16,s16 + s17]"
                },
                {
                  "key": "param_name",
                  "value": "A"
                }
              ]
            },
            {
              "id": "1",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "scalar_tensor_default"
                },
                {
                  "key": "tensor_shape",
                  "value": "FLOAT[]"
                },
                {
                  "key": "param_name",
                  "value": "B"
                }
              ]
            }
          ],
          "style": null,
          "config": null
        },
        {
          "id": "node_Constant_177",
          "label": "Constant",
          "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/slice_14: aten.slice.Tensor",
          "subgraphIds": [],
          "attrs": [
            {
              "key": "__value",
              "value": "[0]"
            },
            {
              "key": "value_ints",
              "value": "[0]"
            },
            {
              "key": "[metadata] namespace",
              "value": ": transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/slice_14: aten.slice.Tensor"
            },
            {
              "key": "[metadata] pkg.torch.onnx.class_hierarchy",
              "value": "['transformers.models.llama.modeling_llama.LlamaForCausalLM', 'transformers.models.llama.modeling_llama.LlamaModel', 'aten.slice.Tensor']"
            },
            {
              "key": "[metadata] pkg.torch.onnx.fx_node",
              "value": "%slice_14 : [num_users=1] = call_function[target=torch.ops.aten.slice.Tensor](args = (%slice_13, 3, None, %add_4), kwargs = {})"
            },
            {
              "key": "[metadata] pkg.torch.onnx.name_scopes",
              "value": "['', 'model', 'slice_14']"
            },
            {
              "key": "[metadata] pkg.torch.onnx.stack_trace",
              "value": "File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 688, in forward\n    outputs: BaseModelOutputWithPast = self.model(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 436, in forward\n    causal_mask = self._update_causal_mask("
            }
          ],
          "incomingEdges": [],
          "outputsMetadata": [
            {
              "id": "0",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "val_123"
                },
                {
                  "key": "tensor_shape",
                  "value": "INT64[1]"
                },
                {
                  "key": "param_name",
                  "value": "output"
                }
              ]
            }
          ],
          "inputsMetadata": [],
          "style": null,
          "config": null
        },
        {
          "id": "node_Constant_179",
          "label": "Constant",
          "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/slice_14: aten.slice.Tensor",
          "subgraphIds": [],
          "attrs": [
            {
              "key": "__value",
              "value": "[-1]"
            },
            {
              "key": "value_ints",
              "value": "[-1]"
            },
            {
              "key": "[metadata] namespace",
              "value": ": transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/slice_14: aten.slice.Tensor"
            },
            {
              "key": "[metadata] pkg.torch.onnx.class_hierarchy",
              "value": "['transformers.models.llama.modeling_llama.LlamaForCausalLM', 'transformers.models.llama.modeling_llama.LlamaModel', 'aten.slice.Tensor']"
            },
            {
              "key": "[metadata] pkg.torch.onnx.fx_node",
              "value": "%slice_14 : [num_users=1] = call_function[target=torch.ops.aten.slice.Tensor](args = (%slice_13, 3, None, %add_4), kwargs = {})"
            },
            {
              "key": "[metadata] pkg.torch.onnx.name_scopes",
              "value": "['', 'model', 'slice_14']"
            },
            {
              "key": "[metadata] pkg.torch.onnx.stack_trace",
              "value": "File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 688, in forward\n    outputs: BaseModelOutputWithPast = self.model(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 436, in forward\n    causal_mask = self._update_causal_mask("
            }
          ],
          "incomingEdges": [],
          "outputsMetadata": [
            {
              "id": "0",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "val_125"
                },
                {
                  "key": "tensor_shape",
                  "value": "INT64[1]"
                },
                {
                  "key": "param_name",
                  "value": "output"
                }
              ]
            }
          ],
          "inputsMetadata": [],
          "style": null,
          "config": null
        },
        {
          "id": "node_Reshape_180",
          "label": "Reshape",
          "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/slice_14: aten.slice.Tensor",
          "subgraphIds": [],
          "attrs": [
            {
              "key": "allowzero",
              "value": "0"
            },
            {
              "key": "[metadata] namespace",
              "value": ": transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/slice_14: aten.slice.Tensor"
            },
            {
              "key": "[metadata] pkg.torch.onnx.class_hierarchy",
              "value": "['transformers.models.llama.modeling_llama.LlamaForCausalLM', 'transformers.models.llama.modeling_llama.LlamaModel', 'aten.slice.Tensor']"
            },
            {
              "key": "[metadata] pkg.torch.onnx.fx_node",
              "value": "%slice_14 : [num_users=1] = call_function[target=torch.ops.aten.slice.Tensor](args = (%slice_13, 3, None, %add_4), kwargs = {})"
            },
            {
              "key": "[metadata] pkg.torch.onnx.name_scopes",
              "value": "['', 'model', 'slice_14']"
            },
            {
              "key": "[metadata] pkg.torch.onnx.stack_trace",
              "value": "File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 688, in forward\n    outputs: BaseModelOutputWithPast = self.model(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 436, in forward\n    causal_mask = self._update_causal_mask("
            }
          ],
          "incomingEdges": [
            {
              "sourceNodeId": "node_Add_33",
              "sourceNodeOutputId": "0",
              "targetNodeInputId": "0"
            },
            {
              "sourceNodeId": "node_Constant_179",
              "sourceNodeOutputId": "0",
              "targetNodeInputId": "1"
            }
          ],
          "outputsMetadata": [
            {
              "id": "0",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "val_126"
                },
                {
                  "key": "tensor_shape",
                  "value": "INT64[1]"
                },
                {
                  "key": "param_name",
                  "value": "reshaped"
                }
              ]
            }
          ],
          "inputsMetadata": [
            {
              "id": "0",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "add_4"
                },
                {
                  "key": "tensor_shape",
                  "value": "INT64[]"
                },
                {
                  "key": "param_name",
                  "value": "data"
                }
              ]
            },
            {
              "id": "1",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "val_125"
                },
                {
                  "key": "tensor_shape",
                  "value": "INT64[1]"
                },
                {
                  "key": "param_name",
                  "value": "shape"
                }
              ]
            }
          ],
          "style": null,
          "config": null
        },
        {
          "id": "node_Constant_777",
          "label": "Constant",
          "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/slice_14: aten.slice.Tensor",
          "subgraphIds": [],
          "attrs": [
            {
              "key": "__value",
              "value": "[3]"
            },
            {
              "key": "value",
              "value": "TensorProtoTensor<INT64,[1]>(array([3]), name='val_129')"
            }
          ],
          "incomingEdges": [],
          "outputsMetadata": [
            {
              "id": "0",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "val_129"
                },
                {
                  "key": "tensor_shape",
                  "value": "INT64[1]"
                },
                {
                  "key": "param_name",
                  "value": "output"
                }
              ]
            }
          ],
          "inputsMetadata": [],
          "style": null,
          "config": null
        },
        {
          "id": "node_Constant_184",
          "label": "Constant",
          "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/slice_14: aten.slice.Tensor",
          "subgraphIds": [],
          "attrs": [
            {
              "key": "__value",
              "value": "[1]"
            },
            {
              "key": "value_ints",
              "value": "[1]"
            },
            {
              "key": "[metadata] namespace",
              "value": ": transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/slice_14: aten.slice.Tensor"
            },
            {
              "key": "[metadata] pkg.torch.onnx.class_hierarchy",
              "value": "['transformers.models.llama.modeling_llama.LlamaForCausalLM', 'transformers.models.llama.modeling_llama.LlamaModel', 'aten.slice.Tensor']"
            },
            {
              "key": "[metadata] pkg.torch.onnx.fx_node",
              "value": "%slice_14 : [num_users=1] = call_function[target=torch.ops.aten.slice.Tensor](args = (%slice_13, 3, None, %add_4), kwargs = {})"
            },
            {
              "key": "[metadata] pkg.torch.onnx.name_scopes",
              "value": "['', 'model', 'slice_14']"
            },
            {
              "key": "[metadata] pkg.torch.onnx.stack_trace",
              "value": "File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 688, in forward\n    outputs: BaseModelOutputWithPast = self.model(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 436, in forward\n    causal_mask = self._update_causal_mask("
            }
          ],
          "incomingEdges": [],
          "outputsMetadata": [
            {
              "id": "0",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "val_130"
                },
                {
                  "key": "tensor_shape",
                  "value": "INT64[1]"
                },
                {
                  "key": "param_name",
                  "value": "output"
                }
              ]
            }
          ],
          "inputsMetadata": [],
          "style": null,
          "config": null
        },
        {
          "id": "node_Slice_185",
          "label": "Slice",
          "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/slice_14: aten.slice.Tensor",
          "subgraphIds": [],
          "attrs": [
            {
              "key": "[metadata] namespace",
              "value": ": transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/slice_14: aten.slice.Tensor"
            },
            {
              "key": "[metadata] pkg.torch.onnx.class_hierarchy",
              "value": "['transformers.models.llama.modeling_llama.LlamaForCausalLM', 'transformers.models.llama.modeling_llama.LlamaModel', 'aten.slice.Tensor']"
            },
            {
              "key": "[metadata] pkg.torch.onnx.fx_node",
              "value": "%slice_14 : [num_users=1] = call_function[target=torch.ops.aten.slice.Tensor](args = (%slice_13, 3, None, %add_4), kwargs = {})"
            },
            {
              "key": "[metadata] pkg.torch.onnx.name_scopes",
              "value": "['', 'model', 'slice_14']"
            },
            {
              "key": "[metadata] pkg.torch.onnx.stack_trace",
              "value": "File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 688, in forward\n    outputs: BaseModelOutputWithPast = self.model(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 436, in forward\n    causal_mask = self._update_causal_mask("
            }
          ],
          "incomingEdges": [
            {
              "sourceNodeId": "node_Expand_94",
              "sourceNodeOutputId": "0",
              "targetNodeInputId": "0"
            },
            {
              "sourceNodeId": "node_Constant_177",
              "sourceNodeOutputId": "0",
              "targetNodeInputId": "1"
            },
            {
              "sourceNodeId": "node_Reshape_180",
              "sourceNodeOutputId": "0",
              "targetNodeInputId": "2"
            },
            {
              "sourceNodeId": "node_Constant_777",
              "sourceNodeOutputId": "0",
              "targetNodeInputId": "3"
            },
            {
              "sourceNodeId": "node_Constant_184",
              "sourceNodeOutputId": "0",
              "targetNodeInputId": "4"
            }
          ],
          "outputsMetadata": [
            {
              "id": "0",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "slice_14"
                },
                {
                  "key": "tensor_shape",
                  "value": "FLOAT[s34,1,s16,s16 + s17]"
                },
                {
                  "key": "param_name",
                  "value": "output"
                }
              ]
            }
          ],
          "inputsMetadata": [
            {
              "id": "0",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "expand_1"
                },
                {
                  "key": "tensor_shape",
                  "value": "FLOAT[s34,1,s16,s16 + s17]"
                },
                {
                  "key": "param_name",
                  "value": "data"
                }
              ]
            },
            {
              "id": "1",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "val_123"
                },
                {
                  "key": "tensor_shape",
                  "value": "INT64[1]"
                },
                {
                  "key": "param_name",
                  "value": "starts"
                }
              ]
            },
            {
              "id": "2",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "val_126"
                },
                {
                  "key": "tensor_shape",
                  "value": "INT64[1]"
                },
                {
                  "key": "param_name",
                  "value": "ends"
                }
              ]
            },
            {
              "id": "3",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "val_129"
                },
                {
                  "key": "tensor_shape",
                  "value": "INT64[1]"
                },
                {
                  "key": "param_name",
                  "value": "axes"
                }
              ]
            },
            {
              "id": "4",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "val_130"
                },
                {
                  "key": "tensor_shape",
                  "value": "INT64[1]"
                },
                {
                  "key": "param_name",
                  "value": "steps"
                }
              ]
            }
          ],
          "style": null,
          "config": null
        },
        {
          "id": "node_Constant_779",
          "label": "Constant",
          "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/masked_fill: aten.masked_fill.Scalar",
          "subgraphIds": [],
          "attrs": [
            {
              "key": "__value",
              "value": "-3.4028234663852886e+38"
            },
            {
              "key": "value",
              "value": "TensorProtoTensor<FLOAT,[]>(array(-3.4028235e+38, dtype=float32), name='val_131')"
            }
          ],
          "incomingEdges": [],
          "outputsMetadata": [
            {
              "id": "0",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "val_131"
                },
                {
                  "key": "tensor_shape",
                  "value": "FLOAT[]"
                },
                {
                  "key": "param_name",
                  "value": "output"
                }
              ]
            }
          ],
          "inputsMetadata": [],
          "style": null,
          "config": null
        },
        {
          "id": "node_Where_187",
          "label": "Where",
          "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/masked_fill: aten.masked_fill.Scalar",
          "subgraphIds": [],
          "attrs": [
            {
              "key": "[metadata] namespace",
              "value": ": transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/masked_fill: aten.masked_fill.Scalar"
            },
            {
              "key": "[metadata] pkg.torch.onnx.class_hierarchy",
              "value": "['transformers.models.llama.modeling_llama.LlamaForCausalLM', 'transformers.models.llama.modeling_llama.LlamaModel', 'aten.masked_fill.Scalar']"
            },
            {
              "key": "[metadata] pkg.torch.onnx.fx_node",
              "value": "%masked_fill : [num_users=1] = call_function[target=torch.ops.aten.masked_fill.Scalar](args = (%slice_14, %eq_67, -3.4028234663852886e+38), kwargs = {})"
            },
            {
              "key": "[metadata] pkg.torch.onnx.name_scopes",
              "value": "['', 'model', 'masked_fill']"
            },
            {
              "key": "[metadata] pkg.torch.onnx.stack_trace",
              "value": "File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 688, in forward\n    outputs: BaseModelOutputWithPast = self.model(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 436, in forward\n    causal_mask = self._update_causal_mask("
            }
          ],
          "incomingEdges": [
            {
              "sourceNodeId": "node_Equal_155",
              "sourceNodeOutputId": "0",
              "targetNodeInputId": "0"
            },
            {
              "sourceNodeId": "node_Constant_779",
              "sourceNodeOutputId": "0",
              "targetNodeInputId": "1"
            },
            {
              "sourceNodeId": "node_Slice_185",
              "sourceNodeOutputId": "0",
              "targetNodeInputId": "2"
            }
          ],
          "outputsMetadata": [
            {
              "id": "0",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "masked_fill"
                },
                {
                  "key": "tensor_shape",
                  "value": "FLOAT[s34,1,s16,s16 + s17]"
                },
                {
                  "key": "param_name",
                  "value": "output"
                }
              ]
            }
          ],
          "inputsMetadata": [
            {
              "id": "0",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "eq_67"
                },
                {
                  "key": "tensor_shape",
                  "value": "BOOL[s34,1,s16,s16 + s17]"
                },
                {
                  "key": "param_name",
                  "value": "condition"
                }
              ]
            },
            {
              "id": "1",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "val_131"
                },
                {
                  "key": "tensor_shape",
                  "value": "FLOAT[]"
                },
                {
                  "key": "param_name",
                  "value": "X"
                }
              ]
            },
            {
              "id": "2",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "slice_14"
                },
                {
                  "key": "tensor_shape",
                  "value": "FLOAT[s34,1,s16,s16 + s17]"
                },
                {
                  "key": "param_name",
                  "value": "Y"
                }
              ]
            }
          ],
          "style": null,
          "config": null
        },
        {
          "id": "node_Shape_245",
          "label": "Shape",
          "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/slice_scatter: aten.slice_scatter.default",
          "subgraphIds": [],
          "attrs": [
            {
              "key": "start",
              "value": "0"
            },
            {
              "key": "[metadata] namespace",
              "value": ": transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/slice_scatter: aten.slice_scatter.default"
            },
            {
              "key": "[metadata] pkg.torch.onnx.class_hierarchy",
              "value": "['transformers.models.llama.modeling_llama.LlamaForCausalLM', 'transformers.models.llama.modeling_llama.LlamaModel', 'aten.slice_scatter.default']"
            },
            {
              "key": "[metadata] pkg.torch.onnx.fx_node",
              "value": "%slice_scatter : [num_users=1] = call_function[target=torch.ops.aten.slice_scatter.default](args = (%slice_19, %copy, 2, 0, 9223372036854775807), kwargs = {})"
            },
            {
              "key": "[metadata] pkg.torch.onnx.name_scopes",
              "value": "['', 'model', 'slice_scatter']"
            },
            {
              "key": "[metadata] pkg.torch.onnx.stack_trace",
              "value": "File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 688, in forward\n    outputs: BaseModelOutputWithPast = self.model(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 436, in forward\n    causal_mask = self._update_causal_mask("
            }
          ],
          "incomingEdges": [
            {
              "sourceNodeId": "node_Expand_94",
              "sourceNodeOutputId": "0",
              "targetNodeInputId": "0"
            }
          ],
          "outputsMetadata": [
            {
              "id": "0",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "val_183"
                },
                {
                  "key": "tensor_shape",
                  "value": "INT64[4]"
                },
                {
                  "key": "param_name",
                  "value": "shape"
                }
              ]
            }
          ],
          "inputsMetadata": [
            {
              "id": "0",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "expand_1"
                },
                {
                  "key": "tensor_shape",
                  "value": "FLOAT[s34,1,s16,s16 + s17]"
                },
                {
                  "key": "param_name",
                  "value": "data"
                }
              ]
            }
          ],
          "style": null,
          "config": null
        },
        {
          "id": "node_Gather_246",
          "label": "Gather",
          "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/slice_scatter: aten.slice_scatter.default",
          "subgraphIds": [],
          "attrs": [
            {
              "key": "axis",
              "value": "0"
            },
            {
              "key": "[metadata] namespace",
              "value": ": transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/slice_scatter: aten.slice_scatter.default"
            },
            {
              "key": "[metadata] pkg.torch.onnx.class_hierarchy",
              "value": "['transformers.models.llama.modeling_llama.LlamaForCausalLM', 'transformers.models.llama.modeling_llama.LlamaModel', 'aten.slice_scatter.default']"
            },
            {
              "key": "[metadata] pkg.torch.onnx.fx_node",
              "value": "%slice_scatter : [num_users=1] = call_function[target=torch.ops.aten.slice_scatter.default](args = (%slice_19, %copy, 2, 0, 9223372036854775807), kwargs = {})"
            },
            {
              "key": "[metadata] pkg.torch.onnx.name_scopes",
              "value": "['', 'model', 'slice_scatter']"
            },
            {
              "key": "[metadata] pkg.torch.onnx.stack_trace",
              "value": "File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 688, in forward\n    outputs: BaseModelOutputWithPast = self.model(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 436, in forward\n    causal_mask = self._update_causal_mask("
            }
          ],
          "incomingEdges": [
            {
              "sourceNodeId": "node_Shape_245",
              "sourceNodeOutputId": "0",
              "targetNodeInputId": "0"
            },
            {
              "sourceNodeId": "node_Constant_70",
              "sourceNodeOutputId": "0",
              "targetNodeInputId": "1"
            }
          ],
          "outputsMetadata": [
            {
              "id": "0",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "val_184"
                },
                {
                  "key": "tensor_shape",
                  "value": "INT64[]"
                },
                {
                  "key": "param_name",
                  "value": "output"
                }
              ]
            }
          ],
          "inputsMetadata": [
            {
              "id": "0",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "val_183"
                },
                {
                  "key": "tensor_shape",
                  "value": "INT64[4]"
                },
                {
                  "key": "param_name",
                  "value": "data"
                }
              ]
            },
            {
              "id": "1",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "val_35"
                },
                {
                  "key": "tensor_shape",
                  "value": "INT64[]"
                },
                {
                  "key": "param_name",
                  "value": "indices"
                }
              ]
            }
          ],
          "style": null,
          "config": null
        },
        {
          "id": "node_Range_247",
          "label": "Range",
          "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/slice_scatter: aten.slice_scatter.default",
          "subgraphIds": [],
          "attrs": [
            {
              "key": "[metadata] namespace",
              "value": ": transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/slice_scatter: aten.slice_scatter.default"
            },
            {
              "key": "[metadata] pkg.torch.onnx.class_hierarchy",
              "value": "['transformers.models.llama.modeling_llama.LlamaForCausalLM', 'transformers.models.llama.modeling_llama.LlamaModel', 'aten.slice_scatter.default']"
            },
            {
              "key": "[metadata] pkg.torch.onnx.fx_node",
              "value": "%slice_scatter : [num_users=1] = call_function[target=torch.ops.aten.slice_scatter.default](args = (%slice_19, %copy, 2, 0, 9223372036854775807), kwargs = {})"
            },
            {
              "key": "[metadata] pkg.torch.onnx.name_scopes",
              "value": "['', 'model', 'slice_scatter']"
            },
            {
              "key": "[metadata] pkg.torch.onnx.stack_trace",
              "value": "File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 688, in forward\n    outputs: BaseModelOutputWithPast = self.model(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 436, in forward\n    causal_mask = self._update_causal_mask("
            }
          ],
          "incomingEdges": [
            {
              "sourceNodeId": "node_Constant_62",
              "sourceNodeOutputId": "0",
              "targetNodeInputId": "0"
            },
            {
              "sourceNodeId": "node_Gather_246",
              "sourceNodeOutputId": "0",
              "targetNodeInputId": "1"
            },
            {
              "sourceNodeId": "node_Constant_105",
              "sourceNodeOutputId": "0",
              "targetNodeInputId": "2"
            }
          ],
          "outputsMetadata": [
            {
              "id": "0",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "val_185"
                },
                {
                  "key": "tensor_shape",
                  "value": "INT64[unk__16]"
                },
                {
                  "key": "param_name",
                  "value": "output"
                }
              ]
            }
          ],
          "inputsMetadata": [
            {
              "id": "0",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "val_27"
                },
                {
                  "key": "tensor_shape",
                  "value": "INT64[]"
                },
                {
                  "key": "param_name",
                  "value": "start"
                }
              ]
            },
            {
              "id": "1",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "val_184"
                },
                {
                  "key": "tensor_shape",
                  "value": "INT64[]"
                },
                {
                  "key": "param_name",
                  "value": "limit"
                }
              ]
            },
            {
              "id": "2",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "val_65"
                },
                {
                  "key": "tensor_shape",
                  "value": "INT64[]"
                },
                {
                  "key": "param_name",
                  "value": "delta"
                }
              ]
            }
          ],
          "style": null,
          "config": null
        },
        {
          "id": "node_Unsqueeze_252",
          "label": "Unsqueeze",
          "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/slice_scatter: aten.slice_scatter.default",
          "subgraphIds": [],
          "attrs": [
            {
              "key": "[metadata] namespace",
              "value": ": transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/slice_scatter: aten.slice_scatter.default"
            },
            {
              "key": "[metadata] pkg.torch.onnx.class_hierarchy",
              "value": "['transformers.models.llama.modeling_llama.LlamaForCausalLM', 'transformers.models.llama.modeling_llama.LlamaModel', 'aten.slice_scatter.default']"
            },
            {
              "key": "[metadata] pkg.torch.onnx.fx_node",
              "value": "%slice_scatter : [num_users=1] = call_function[target=torch.ops.aten.slice_scatter.default](args = (%slice_19, %copy, 2, 0, 9223372036854775807), kwargs = {})"
            },
            {
              "key": "[metadata] pkg.torch.onnx.name_scopes",
              "value": "['', 'model', 'slice_scatter']"
            },
            {
              "key": "[metadata] pkg.torch.onnx.stack_trace",
              "value": "File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 688, in forward\n    outputs: BaseModelOutputWithPast = self.model(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 436, in forward\n    causal_mask = self._update_causal_mask("
            }
          ],
          "incomingEdges": [
            {
              "sourceNodeId": "node_Range_247",
              "sourceNodeOutputId": "0",
              "targetNodeInputId": "0"
            },
            {
              "sourceNodeId": "node_Constant_90",
              "sourceNodeOutputId": "0",
              "targetNodeInputId": "1"
            }
          ],
          "outputsMetadata": [
            {
              "id": "0",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "val_190"
                },
                {
                  "key": "tensor_shape",
                  "value": "INT64[unk__16,1]"
                },
                {
                  "key": "param_name",
                  "value": "expanded"
                }
              ]
            }
          ],
          "inputsMetadata": [
            {
              "id": "0",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "val_185"
                },
                {
                  "key": "tensor_shape",
                  "value": "INT64[unk__16]"
                },
                {
                  "key": "param_name",
                  "value": "data"
                }
              ]
            },
            {
              "id": "1",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "val_53"
                },
                {
                  "key": "tensor_shape",
                  "value": "INT64[1]"
                },
                {
                  "key": "param_name",
                  "value": "axes"
                }
              ]
            }
          ],
          "style": null,
          "config": null
        },
        {
          "id": "node_Transpose_253",
          "label": "Transpose",
          "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/slice_scatter: aten.slice_scatter.default",
          "subgraphIds": [],
          "attrs": [
            {
              "key": "perm",
              "value": "[2, 1, 0, 3]"
            },
            {
              "key": "[metadata] namespace",
              "value": ": transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/slice_scatter: aten.slice_scatter.default"
            },
            {
              "key": "[metadata] pkg.torch.onnx.class_hierarchy",
              "value": "['transformers.models.llama.modeling_llama.LlamaForCausalLM', 'transformers.models.llama.modeling_llama.LlamaModel', 'aten.slice_scatter.default']"
            },
            {
              "key": "[metadata] pkg.torch.onnx.fx_node",
              "value": "%slice_scatter : [num_users=1] = call_function[target=torch.ops.aten.slice_scatter.default](args = (%slice_19, %copy, 2, 0, 9223372036854775807), kwargs = {})"
            },
            {
              "key": "[metadata] pkg.torch.onnx.name_scopes",
              "value": "['', 'model', 'slice_scatter']"
            },
            {
              "key": "[metadata] pkg.torch.onnx.stack_trace",
              "value": "File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 688, in forward\n    outputs: BaseModelOutputWithPast = self.model(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 436, in forward\n    causal_mask = self._update_causal_mask("
            }
          ],
          "incomingEdges": [
            {
              "sourceNodeId": "node_Where_187",
              "sourceNodeOutputId": "0",
              "targetNodeInputId": "0"
            }
          ],
          "outputsMetadata": [
            {
              "id": "0",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "val_191"
                },
                {
                  "key": "tensor_shape",
                  "value": "FLOAT[s16,1,s34,s16 + s17]"
                },
                {
                  "key": "param_name",
                  "value": "transposed"
                }
              ]
            }
          ],
          "inputsMetadata": [
            {
              "id": "0",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "masked_fill"
                },
                {
                  "key": "tensor_shape",
                  "value": "FLOAT[s34,1,s16,s16 + s17]"
                },
                {
                  "key": "param_name",
                  "value": "data"
                }
              ]
            }
          ],
          "style": null,
          "config": null
        },
        {
          "id": "node_Transpose_254",
          "label": "Transpose",
          "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/slice_scatter: aten.slice_scatter.default",
          "subgraphIds": [],
          "attrs": [
            {
              "key": "perm",
              "value": "[2, 1, 0, 3]"
            },
            {
              "key": "[metadata] namespace",
              "value": ": transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/slice_scatter: aten.slice_scatter.default"
            },
            {
              "key": "[metadata] pkg.torch.onnx.class_hierarchy",
              "value": "['transformers.models.llama.modeling_llama.LlamaForCausalLM', 'transformers.models.llama.modeling_llama.LlamaModel', 'aten.slice_scatter.default']"
            },
            {
              "key": "[metadata] pkg.torch.onnx.fx_node",
              "value": "%slice_scatter : [num_users=1] = call_function[target=torch.ops.aten.slice_scatter.default](args = (%slice_19, %copy, 2, 0, 9223372036854775807), kwargs = {})"
            },
            {
              "key": "[metadata] pkg.torch.onnx.name_scopes",
              "value": "['', 'model', 'slice_scatter']"
            },
            {
              "key": "[metadata] pkg.torch.onnx.stack_trace",
              "value": "File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 688, in forward\n    outputs: BaseModelOutputWithPast = self.model(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 436, in forward\n    causal_mask = self._update_causal_mask("
            }
          ],
          "incomingEdges": [
            {
              "sourceNodeId": "node_Expand_94",
              "sourceNodeOutputId": "0",
              "targetNodeInputId": "0"
            }
          ],
          "outputsMetadata": [
            {
              "id": "0",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "val_192"
                },
                {
                  "key": "tensor_shape",
                  "value": "FLOAT[s16,1,s34,s16 + s17]"
                },
                {
                  "key": "param_name",
                  "value": "transposed"
                }
              ]
            }
          ],
          "inputsMetadata": [
            {
              "id": "0",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "expand_1"
                },
                {
                  "key": "tensor_shape",
                  "value": "FLOAT[s34,1,s16,s16 + s17]"
                },
                {
                  "key": "param_name",
                  "value": "data"
                }
              ]
            }
          ],
          "style": null,
          "config": null
        },
        {
          "id": "node_ScatterND_255",
          "label": "ScatterND",
          "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/slice_scatter: aten.slice_scatter.default",
          "subgraphIds": [],
          "attrs": [
            {
              "key": "reduction",
              "value": "none"
            },
            {
              "key": "[metadata] namespace",
              "value": ": transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/slice_scatter: aten.slice_scatter.default"
            },
            {
              "key": "[metadata] pkg.torch.onnx.class_hierarchy",
              "value": "['transformers.models.llama.modeling_llama.LlamaForCausalLM', 'transformers.models.llama.modeling_llama.LlamaModel', 'aten.slice_scatter.default']"
            },
            {
              "key": "[metadata] pkg.torch.onnx.fx_node",
              "value": "%slice_scatter : [num_users=1] = call_function[target=torch.ops.aten.slice_scatter.default](args = (%slice_19, %copy, 2, 0, 9223372036854775807), kwargs = {})"
            },
            {
              "key": "[metadata] pkg.torch.onnx.name_scopes",
              "value": "['', 'model', 'slice_scatter']"
            },
            {
              "key": "[metadata] pkg.torch.onnx.stack_trace",
              "value": "File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 688, in forward\n    outputs: BaseModelOutputWithPast = self.model(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 436, in forward\n    causal_mask = self._update_causal_mask("
            }
          ],
          "incomingEdges": [
            {
              "sourceNodeId": "node_Transpose_254",
              "sourceNodeOutputId": "0",
              "targetNodeInputId": "0"
            },
            {
              "sourceNodeId": "node_Unsqueeze_252",
              "sourceNodeOutputId": "0",
              "targetNodeInputId": "1"
            },
            {
              "sourceNodeId": "node_Transpose_253",
              "sourceNodeOutputId": "0",
              "targetNodeInputId": "2"
            }
          ],
          "outputsMetadata": [
            {
              "id": "0",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "val_193"
                },
                {
                  "key": "tensor_shape",
                  "value": "FLOAT[s16,1,s34,s16 + s17]"
                },
                {
                  "key": "param_name",
                  "value": "output"
                }
              ]
            }
          ],
          "inputsMetadata": [
            {
              "id": "0",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "val_192"
                },
                {
                  "key": "tensor_shape",
                  "value": "FLOAT[s16,1,s34,s16 + s17]"
                },
                {
                  "key": "param_name",
                  "value": "data"
                }
              ]
            },
            {
              "id": "1",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "val_190"
                },
                {
                  "key": "tensor_shape",
                  "value": "INT64[unk__16,1]"
                },
                {
                  "key": "param_name",
                  "value": "indices"
                }
              ]
            },
            {
              "id": "2",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "val_191"
                },
                {
                  "key": "tensor_shape",
                  "value": "FLOAT[s16,1,s34,s16 + s17]"
                },
                {
                  "key": "param_name",
                  "value": "updates"
                }
              ]
            }
          ],
          "style": null,
          "config": null
        },
        {
          "id": "node_Shape_258",
          "label": "Shape",
          "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/slice_scatter_1: aten.slice_scatter.default",
          "subgraphIds": [],
          "attrs": [
            {
              "key": "start",
              "value": "0"
            },
            {
              "key": "[metadata] namespace",
              "value": ": transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/slice_scatter_1: aten.slice_scatter.default"
            },
            {
              "key": "[metadata] pkg.torch.onnx.class_hierarchy",
              "value": "['transformers.models.llama.modeling_llama.LlamaForCausalLM', 'transformers.models.llama.modeling_llama.LlamaModel', 'aten.slice_scatter.default']"
            },
            {
              "key": "[metadata] pkg.torch.onnx.fx_node",
              "value": "%slice_scatter_1 : [num_users=1] = call_function[target=torch.ops.aten.slice_scatter.default](args = (%slice_18, %slice_scatter, 1, 0, 9223372036854775807), kwargs = {})"
            },
            {
              "key": "[metadata] pkg.torch.onnx.name_scopes",
              "value": "['', 'model', 'slice_scatter_1']"
            },
            {
              "key": "[metadata] pkg.torch.onnx.stack_trace",
              "value": "File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 688, in forward\n    outputs: BaseModelOutputWithPast = self.model(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 436, in forward\n    causal_mask = self._update_causal_mask("
            }
          ],
          "incomingEdges": [
            {
              "sourceNodeId": "node_Expand_94",
              "sourceNodeOutputId": "0",
              "targetNodeInputId": "0"
            }
          ],
          "outputsMetadata": [
            {
              "id": "0",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "val_195"
                },
                {
                  "key": "tensor_shape",
                  "value": "INT64[4]"
                },
                {
                  "key": "param_name",
                  "value": "shape"
                }
              ]
            }
          ],
          "inputsMetadata": [
            {
              "id": "0",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "expand_1"
                },
                {
                  "key": "tensor_shape",
                  "value": "FLOAT[s34,1,s16,s16 + s17]"
                },
                {
                  "key": "param_name",
                  "value": "data"
                }
              ]
            }
          ],
          "style": null,
          "config": null
        },
        {
          "id": "node_Gather_259",
          "label": "Gather",
          "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/slice_scatter_1: aten.slice_scatter.default",
          "subgraphIds": [],
          "attrs": [
            {
              "key": "axis",
              "value": "0"
            },
            {
              "key": "[metadata] namespace",
              "value": ": transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/slice_scatter_1: aten.slice_scatter.default"
            },
            {
              "key": "[metadata] pkg.torch.onnx.class_hierarchy",
              "value": "['transformers.models.llama.modeling_llama.LlamaForCausalLM', 'transformers.models.llama.modeling_llama.LlamaModel', 'aten.slice_scatter.default']"
            },
            {
              "key": "[metadata] pkg.torch.onnx.fx_node",
              "value": "%slice_scatter_1 : [num_users=1] = call_function[target=torch.ops.aten.slice_scatter.default](args = (%slice_18, %slice_scatter, 1, 0, 9223372036854775807), kwargs = {})"
            },
            {
              "key": "[metadata] pkg.torch.onnx.name_scopes",
              "value": "['', 'model', 'slice_scatter_1']"
            },
            {
              "key": "[metadata] pkg.torch.onnx.stack_trace",
              "value": "File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 688, in forward\n    outputs: BaseModelOutputWithPast = self.model(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 436, in forward\n    causal_mask = self._update_causal_mask("
            }
          ],
          "incomingEdges": [
            {
              "sourceNodeId": "node_Shape_258",
              "sourceNodeOutputId": "0",
              "targetNodeInputId": "0"
            },
            {
              "sourceNodeId": "node_Constant_105",
              "sourceNodeOutputId": "0",
              "targetNodeInputId": "1"
            }
          ],
          "outputsMetadata": [
            {
              "id": "0",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "val_196"
                },
                {
                  "key": "tensor_shape",
                  "value": "INT64[]"
                },
                {
                  "key": "param_name",
                  "value": "output"
                }
              ]
            }
          ],
          "inputsMetadata": [
            {
              "id": "0",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "val_195"
                },
                {
                  "key": "tensor_shape",
                  "value": "INT64[4]"
                },
                {
                  "key": "param_name",
                  "value": "data"
                }
              ]
            },
            {
              "id": "1",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "val_65"
                },
                {
                  "key": "tensor_shape",
                  "value": "INT64[]"
                },
                {
                  "key": "param_name",
                  "value": "indices"
                }
              ]
            }
          ],
          "style": null,
          "config": null
        },
        {
          "id": "node_Range_260",
          "label": "Range",
          "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/slice_scatter_1: aten.slice_scatter.default",
          "subgraphIds": [],
          "attrs": [
            {
              "key": "[metadata] namespace",
              "value": ": transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/slice_scatter_1: aten.slice_scatter.default"
            },
            {
              "key": "[metadata] pkg.torch.onnx.class_hierarchy",
              "value": "['transformers.models.llama.modeling_llama.LlamaForCausalLM', 'transformers.models.llama.modeling_llama.LlamaModel', 'aten.slice_scatter.default']"
            },
            {
              "key": "[metadata] pkg.torch.onnx.fx_node",
              "value": "%slice_scatter_1 : [num_users=1] = call_function[target=torch.ops.aten.slice_scatter.default](args = (%slice_18, %slice_scatter, 1, 0, 9223372036854775807), kwargs = {})"
            },
            {
              "key": "[metadata] pkg.torch.onnx.name_scopes",
              "value": "['', 'model', 'slice_scatter_1']"
            },
            {
              "key": "[metadata] pkg.torch.onnx.stack_trace",
              "value": "File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 688, in forward\n    outputs: BaseModelOutputWithPast = self.model(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 436, in forward\n    causal_mask = self._update_causal_mask("
            }
          ],
          "incomingEdges": [
            {
              "sourceNodeId": "node_Constant_62",
              "sourceNodeOutputId": "0",
              "targetNodeInputId": "0"
            },
            {
              "sourceNodeId": "node_Gather_259",
              "sourceNodeOutputId": "0",
              "targetNodeInputId": "1"
            },
            {
              "sourceNodeId": "node_Constant_105",
              "sourceNodeOutputId": "0",
              "targetNodeInputId": "2"
            }
          ],
          "outputsMetadata": [
            {
              "id": "0",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "val_197"
                },
                {
                  "key": "tensor_shape",
                  "value": "INT64[unk__17]"
                },
                {
                  "key": "param_name",
                  "value": "output"
                }
              ]
            }
          ],
          "inputsMetadata": [
            {
              "id": "0",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "val_27"
                },
                {
                  "key": "tensor_shape",
                  "value": "INT64[]"
                },
                {
                  "key": "param_name",
                  "value": "start"
                }
              ]
            },
            {
              "id": "1",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "val_196"
                },
                {
                  "key": "tensor_shape",
                  "value": "INT64[]"
                },
                {
                  "key": "param_name",
                  "value": "limit"
                }
              ]
            },
            {
              "id": "2",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "val_65"
                },
                {
                  "key": "tensor_shape",
                  "value": "INT64[]"
                },
                {
                  "key": "param_name",
                  "value": "delta"
                }
              ]
            }
          ],
          "style": null,
          "config": null
        },
        {
          "id": "node_Unsqueeze_265",
          "label": "Unsqueeze",
          "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/slice_scatter_1: aten.slice_scatter.default",
          "subgraphIds": [],
          "attrs": [
            {
              "key": "[metadata] namespace",
              "value": ": transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/slice_scatter_1: aten.slice_scatter.default"
            },
            {
              "key": "[metadata] pkg.torch.onnx.class_hierarchy",
              "value": "['transformers.models.llama.modeling_llama.LlamaForCausalLM', 'transformers.models.llama.modeling_llama.LlamaModel', 'aten.slice_scatter.default']"
            },
            {
              "key": "[metadata] pkg.torch.onnx.fx_node",
              "value": "%slice_scatter_1 : [num_users=1] = call_function[target=torch.ops.aten.slice_scatter.default](args = (%slice_18, %slice_scatter, 1, 0, 9223372036854775807), kwargs = {})"
            },
            {
              "key": "[metadata] pkg.torch.onnx.name_scopes",
              "value": "['', 'model', 'slice_scatter_1']"
            },
            {
              "key": "[metadata] pkg.torch.onnx.stack_trace",
              "value": "File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 688, in forward\n    outputs: BaseModelOutputWithPast = self.model(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 436, in forward\n    causal_mask = self._update_causal_mask("
            }
          ],
          "incomingEdges": [
            {
              "sourceNodeId": "node_Range_260",
              "sourceNodeOutputId": "0",
              "targetNodeInputId": "0"
            },
            {
              "sourceNodeId": "node_Constant_90",
              "sourceNodeOutputId": "0",
              "targetNodeInputId": "1"
            }
          ],
          "outputsMetadata": [
            {
              "id": "0",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "val_202"
                },
                {
                  "key": "tensor_shape",
                  "value": "INT64[unk__17,1]"
                },
                {
                  "key": "param_name",
                  "value": "expanded"
                }
              ]
            }
          ],
          "inputsMetadata": [
            {
              "id": "0",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "val_197"
                },
                {
                  "key": "tensor_shape",
                  "value": "INT64[unk__17]"
                },
                {
                  "key": "param_name",
                  "value": "data"
                }
              ]
            },
            {
              "id": "1",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "val_53"
                },
                {
                  "key": "tensor_shape",
                  "value": "INT64[1]"
                },
                {
                  "key": "param_name",
                  "value": "axes"
                }
              ]
            }
          ],
          "style": null,
          "config": null
        },
        {
          "id": "node_Transpose_1025",
          "label": "Transpose",
          "namespace": "main_graph",
          "subgraphIds": [],
          "attrs": [
            {
              "key": "perm",
              "value": "[1, 2, 0, 3]"
            }
          ],
          "incomingEdges": [
            {
              "sourceNodeId": "node_ScatterND_255",
              "sourceNodeOutputId": "0",
              "targetNodeInputId": "0"
            }
          ],
          "outputsMetadata": [
            {
              "id": "0",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "val_203"
                },
                {
                  "key": "tensor_shape",
                  "value": "FLOAT[1,s34,s16,s16 + s17]"
                },
                {
                  "key": "param_name",
                  "value": "transposed"
                }
              ]
            }
          ],
          "inputsMetadata": [
            {
              "id": "0",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "val_193"
                },
                {
                  "key": "tensor_shape",
                  "value": "FLOAT[s16,1,s34,s16 + s17]"
                },
                {
                  "key": "param_name",
                  "value": "data"
                }
              ]
            }
          ],
          "style": null,
          "config": null
        },
        {
          "id": "node_Transpose_267",
          "label": "Transpose",
          "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/slice_scatter_1: aten.slice_scatter.default",
          "subgraphIds": [],
          "attrs": [
            {
              "key": "perm",
              "value": "[1, 0, 2, 3]"
            },
            {
              "key": "[metadata] namespace",
              "value": ": transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/slice_scatter_1: aten.slice_scatter.default"
            },
            {
              "key": "[metadata] pkg.torch.onnx.class_hierarchy",
              "value": "['transformers.models.llama.modeling_llama.LlamaForCausalLM', 'transformers.models.llama.modeling_llama.LlamaModel', 'aten.slice_scatter.default']"
            },
            {
              "key": "[metadata] pkg.torch.onnx.fx_node",
              "value": "%slice_scatter_1 : [num_users=1] = call_function[target=torch.ops.aten.slice_scatter.default](args = (%slice_18, %slice_scatter, 1, 0, 9223372036854775807), kwargs = {})"
            },
            {
              "key": "[metadata] pkg.torch.onnx.name_scopes",
              "value": "['', 'model', 'slice_scatter_1']"
            },
            {
              "key": "[metadata] pkg.torch.onnx.stack_trace",
              "value": "File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 688, in forward\n    outputs: BaseModelOutputWithPast = self.model(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 436, in forward\n    causal_mask = self._update_causal_mask("
            }
          ],
          "incomingEdges": [
            {
              "sourceNodeId": "node_Expand_94",
              "sourceNodeOutputId": "0",
              "targetNodeInputId": "0"
            }
          ],
          "outputsMetadata": [
            {
              "id": "0",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "val_204"
                },
                {
                  "key": "tensor_shape",
                  "value": "FLOAT[1,s34,s16,s16 + s17]"
                },
                {
                  "key": "param_name",
                  "value": "transposed"
                }
              ]
            }
          ],
          "inputsMetadata": [
            {
              "id": "0",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "expand_1"
                },
                {
                  "key": "tensor_shape",
                  "value": "FLOAT[s34,1,s16,s16 + s17]"
                },
                {
                  "key": "param_name",
                  "value": "data"
                }
              ]
            }
          ],
          "style": null,
          "config": null
        },
        {
          "id": "node_ScatterND_268",
          "label": "ScatterND",
          "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/slice_scatter_1: aten.slice_scatter.default",
          "subgraphIds": [],
          "attrs": [
            {
              "key": "reduction",
              "value": "none"
            },
            {
              "key": "[metadata] namespace",
              "value": ": transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/slice_scatter_1: aten.slice_scatter.default"
            },
            {
              "key": "[metadata] pkg.torch.onnx.class_hierarchy",
              "value": "['transformers.models.llama.modeling_llama.LlamaForCausalLM', 'transformers.models.llama.modeling_llama.LlamaModel', 'aten.slice_scatter.default']"
            },
            {
              "key": "[metadata] pkg.torch.onnx.fx_node",
              "value": "%slice_scatter_1 : [num_users=1] = call_function[target=torch.ops.aten.slice_scatter.default](args = (%slice_18, %slice_scatter, 1, 0, 9223372036854775807), kwargs = {})"
            },
            {
              "key": "[metadata] pkg.torch.onnx.name_scopes",
              "value": "['', 'model', 'slice_scatter_1']"
            },
            {
              "key": "[metadata] pkg.torch.onnx.stack_trace",
              "value": "File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 688, in forward\n    outputs: BaseModelOutputWithPast = self.model(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 436, in forward\n    causal_mask = self._update_causal_mask("
            }
          ],
          "incomingEdges": [
            {
              "sourceNodeId": "node_Transpose_267",
              "sourceNodeOutputId": "0",
              "targetNodeInputId": "0"
            },
            {
              "sourceNodeId": "node_Unsqueeze_265",
              "sourceNodeOutputId": "0",
              "targetNodeInputId": "1"
            },
            {
              "sourceNodeId": "node_Transpose_1025",
              "sourceNodeOutputId": "0",
              "targetNodeInputId": "2"
            }
          ],
          "outputsMetadata": [
            {
              "id": "0",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "val_205"
                },
                {
                  "key": "tensor_shape",
                  "value": "FLOAT[1,s34,s16,s16 + s17]"
                },
                {
                  "key": "param_name",
                  "value": "output"
                }
              ]
            }
          ],
          "inputsMetadata": [
            {
              "id": "0",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "val_204"
                },
                {
                  "key": "tensor_shape",
                  "value": "FLOAT[1,s34,s16,s16 + s17]"
                },
                {
                  "key": "param_name",
                  "value": "data"
                }
              ]
            },
            {
              "id": "1",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "val_202"
                },
                {
                  "key": "tensor_shape",
                  "value": "INT64[unk__17,1]"
                },
                {
                  "key": "param_name",
                  "value": "indices"
                }
              ]
            },
            {
              "id": "2",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "val_203"
                },
                {
                  "key": "tensor_shape",
                  "value": "FLOAT[1,s34,s16,s16 + s17]"
                },
                {
                  "key": "param_name",
                  "value": "updates"
                }
              ]
            }
          ],
          "style": null,
          "config": null
        },
        {
          "id": "node_Transpose_269",
          "label": "Transpose",
          "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/slice_scatter_1: aten.slice_scatter.default",
          "subgraphIds": [],
          "attrs": [
            {
              "key": "perm",
              "value": "[1, 0, 2, 3]"
            },
            {
              "key": "[metadata] namespace",
              "value": ": transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/slice_scatter_1: aten.slice_scatter.default"
            },
            {
              "key": "[metadata] pkg.torch.onnx.class_hierarchy",
              "value": "['transformers.models.llama.modeling_llama.LlamaForCausalLM', 'transformers.models.llama.modeling_llama.LlamaModel', 'aten.slice_scatter.default']"
            },
            {
              "key": "[metadata] pkg.torch.onnx.fx_node",
              "value": "%slice_scatter_1 : [num_users=1] = call_function[target=torch.ops.aten.slice_scatter.default](args = (%slice_18, %slice_scatter, 1, 0, 9223372036854775807), kwargs = {})"
            },
            {
              "key": "[metadata] pkg.torch.onnx.name_scopes",
              "value": "['', 'model', 'slice_scatter_1']"
            },
            {
              "key": "[metadata] pkg.torch.onnx.stack_trace",
              "value": "File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 688, in forward\n    outputs: BaseModelOutputWithPast = self.model(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 436, in forward\n    causal_mask = self._update_causal_mask("
            }
          ],
          "incomingEdges": [
            {
              "sourceNodeId": "node_ScatterND_268",
              "sourceNodeOutputId": "0",
              "targetNodeInputId": "0"
            }
          ],
          "outputsMetadata": [
            {
              "id": "0",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "slice_scatter_1"
                },
                {
                  "key": "tensor_shape",
                  "value": "FLOAT[s34,1,s16,s16 + s17]"
                },
                {
                  "key": "param_name",
                  "value": "transposed"
                }
              ]
            }
          ],
          "inputsMetadata": [
            {
              "id": "0",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "val_205"
                },
                {
                  "key": "tensor_shape",
                  "value": "FLOAT[1,s34,s16,s16 + s17]"
                },
                {
                  "key": "param_name",
                  "value": "data"
                }
              ]
            }
          ],
          "style": null,
          "config": null
        },
        {
          "id": "node_Shape_271",
          "label": "Shape",
          "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/slice_scatter_2: aten.slice_scatter.default",
          "subgraphIds": [],
          "attrs": [
            {
              "key": "start",
              "value": "0"
            },
            {
              "key": "[metadata] namespace",
              "value": ": transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/slice_scatter_2: aten.slice_scatter.default"
            },
            {
              "key": "[metadata] pkg.torch.onnx.class_hierarchy",
              "value": "['transformers.models.llama.modeling_llama.LlamaForCausalLM', 'transformers.models.llama.modeling_llama.LlamaModel', 'aten.slice_scatter.default']"
            },
            {
              "key": "[metadata] pkg.torch.onnx.fx_node",
              "value": "%slice_scatter_2 : [num_users=2] = call_function[target=torch.ops.aten.slice_scatter.default](args = (%clone, %slice_scatter_1, 0, 0, 9223372036854775807), kwargs = {})"
            },
            {
              "key": "[metadata] pkg.torch.onnx.name_scopes",
              "value": "['', 'model', 'slice_scatter_2']"
            },
            {
              "key": "[metadata] pkg.torch.onnx.stack_trace",
              "value": "File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 688, in forward\n    outputs: BaseModelOutputWithPast = self.model(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 436, in forward\n    causal_mask = self._update_causal_mask("
            }
          ],
          "incomingEdges": [
            {
              "sourceNodeId": "node_Expand_94",
              "sourceNodeOutputId": "0",
              "targetNodeInputId": "0"
            }
          ],
          "outputsMetadata": [
            {
              "id": "0",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "val_207"
                },
                {
                  "key": "tensor_shape",
                  "value": "INT64[4]"
                },
                {
                  "key": "param_name",
                  "value": "shape"
                }
              ]
            }
          ],
          "inputsMetadata": [
            {
              "id": "0",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "expand_1"
                },
                {
                  "key": "tensor_shape",
                  "value": "FLOAT[s34,1,s16,s16 + s17]"
                },
                {
                  "key": "param_name",
                  "value": "data"
                }
              ]
            }
          ],
          "style": null,
          "config": null
        },
        {
          "id": "node_Gather_272",
          "label": "Gather",
          "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/slice_scatter_2: aten.slice_scatter.default",
          "subgraphIds": [],
          "attrs": [
            {
              "key": "axis",
              "value": "0"
            },
            {
              "key": "[metadata] namespace",
              "value": ": transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/slice_scatter_2: aten.slice_scatter.default"
            },
            {
              "key": "[metadata] pkg.torch.onnx.class_hierarchy",
              "value": "['transformers.models.llama.modeling_llama.LlamaForCausalLM', 'transformers.models.llama.modeling_llama.LlamaModel', 'aten.slice_scatter.default']"
            },
            {
              "key": "[metadata] pkg.torch.onnx.fx_node",
              "value": "%slice_scatter_2 : [num_users=2] = call_function[target=torch.ops.aten.slice_scatter.default](args = (%clone, %slice_scatter_1, 0, 0, 9223372036854775807), kwargs = {})"
            },
            {
              "key": "[metadata] pkg.torch.onnx.name_scopes",
              "value": "['', 'model', 'slice_scatter_2']"
            },
            {
              "key": "[metadata] pkg.torch.onnx.stack_trace",
              "value": "File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 688, in forward\n    outputs: BaseModelOutputWithPast = self.model(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 436, in forward\n    causal_mask = self._update_causal_mask("
            }
          ],
          "incomingEdges": [
            {
              "sourceNodeId": "node_Shape_271",
              "sourceNodeOutputId": "0",
              "targetNodeInputId": "0"
            },
            {
              "sourceNodeId": "node_Constant_62",
              "sourceNodeOutputId": "0",
              "targetNodeInputId": "1"
            }
          ],
          "outputsMetadata": [
            {
              "id": "0",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "val_208"
                },
                {
                  "key": "tensor_shape",
                  "value": "INT64[]"
                },
                {
                  "key": "param_name",
                  "value": "output"
                }
              ]
            }
          ],
          "inputsMetadata": [
            {
              "id": "0",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "val_207"
                },
                {
                  "key": "tensor_shape",
                  "value": "INT64[4]"
                },
                {
                  "key": "param_name",
                  "value": "data"
                }
              ]
            },
            {
              "id": "1",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "val_27"
                },
                {
                  "key": "tensor_shape",
                  "value": "INT64[]"
                },
                {
                  "key": "param_name",
                  "value": "indices"
                }
              ]
            }
          ],
          "style": null,
          "config": null
        },
        {
          "id": "node_Range_273",
          "label": "Range",
          "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/slice_scatter_2: aten.slice_scatter.default",
          "subgraphIds": [],
          "attrs": [
            {
              "key": "[metadata] namespace",
              "value": ": transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/slice_scatter_2: aten.slice_scatter.default"
            },
            {
              "key": "[metadata] pkg.torch.onnx.class_hierarchy",
              "value": "['transformers.models.llama.modeling_llama.LlamaForCausalLM', 'transformers.models.llama.modeling_llama.LlamaModel', 'aten.slice_scatter.default']"
            },
            {
              "key": "[metadata] pkg.torch.onnx.fx_node",
              "value": "%slice_scatter_2 : [num_users=2] = call_function[target=torch.ops.aten.slice_scatter.default](args = (%clone, %slice_scatter_1, 0, 0, 9223372036854775807), kwargs = {})"
            },
            {
              "key": "[metadata] pkg.torch.onnx.name_scopes",
              "value": "['', 'model', 'slice_scatter_2']"
            },
            {
              "key": "[metadata] pkg.torch.onnx.stack_trace",
              "value": "File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 688, in forward\n    outputs: BaseModelOutputWithPast = self.model(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 436, in forward\n    causal_mask = self._update_causal_mask("
            }
          ],
          "incomingEdges": [
            {
              "sourceNodeId": "node_Constant_62",
              "sourceNodeOutputId": "0",
              "targetNodeInputId": "0"
            },
            {
              "sourceNodeId": "node_Gather_272",
              "sourceNodeOutputId": "0",
              "targetNodeInputId": "1"
            },
            {
              "sourceNodeId": "node_Constant_105",
              "sourceNodeOutputId": "0",
              "targetNodeInputId": "2"
            }
          ],
          "outputsMetadata": [
            {
              "id": "0",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "val_209"
                },
                {
                  "key": "tensor_shape",
                  "value": "INT64[unk__18]"
                },
                {
                  "key": "param_name",
                  "value": "output"
                }
              ]
            }
          ],
          "inputsMetadata": [
            {
              "id": "0",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "val_27"
                },
                {
                  "key": "tensor_shape",
                  "value": "INT64[]"
                },
                {
                  "key": "param_name",
                  "value": "start"
                }
              ]
            },
            {
              "id": "1",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "val_208"
                },
                {
                  "key": "tensor_shape",
                  "value": "INT64[]"
                },
                {
                  "key": "param_name",
                  "value": "limit"
                }
              ]
            },
            {
              "id": "2",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "val_65"
                },
                {
                  "key": "tensor_shape",
                  "value": "INT64[]"
                },
                {
                  "key": "param_name",
                  "value": "delta"
                }
              ]
            }
          ],
          "style": null,
          "config": null
        },
        {
          "id": "node_Unsqueeze_278",
          "label": "Unsqueeze",
          "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/slice_scatter_2: aten.slice_scatter.default",
          "subgraphIds": [],
          "attrs": [
            {
              "key": "[metadata] namespace",
              "value": ": transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/slice_scatter_2: aten.slice_scatter.default"
            },
            {
              "key": "[metadata] pkg.torch.onnx.class_hierarchy",
              "value": "['transformers.models.llama.modeling_llama.LlamaForCausalLM', 'transformers.models.llama.modeling_llama.LlamaModel', 'aten.slice_scatter.default']"
            },
            {
              "key": "[metadata] pkg.torch.onnx.fx_node",
              "value": "%slice_scatter_2 : [num_users=2] = call_function[target=torch.ops.aten.slice_scatter.default](args = (%clone, %slice_scatter_1, 0, 0, 9223372036854775807), kwargs = {})"
            },
            {
              "key": "[metadata] pkg.torch.onnx.name_scopes",
              "value": "['', 'model', 'slice_scatter_2']"
            },
            {
              "key": "[metadata] pkg.torch.onnx.stack_trace",
              "value": "File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 688, in forward\n    outputs: BaseModelOutputWithPast = self.model(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 436, in forward\n    causal_mask = self._update_causal_mask("
            }
          ],
          "incomingEdges": [
            {
              "sourceNodeId": "node_Range_273",
              "sourceNodeOutputId": "0",
              "targetNodeInputId": "0"
            },
            {
              "sourceNodeId": "node_Constant_90",
              "sourceNodeOutputId": "0",
              "targetNodeInputId": "1"
            }
          ],
          "outputsMetadata": [
            {
              "id": "0",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "val_214"
                },
                {
                  "key": "tensor_shape",
                  "value": "INT64[unk__18,1]"
                },
                {
                  "key": "param_name",
                  "value": "expanded"
                }
              ]
            }
          ],
          "inputsMetadata": [
            {
              "id": "0",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "val_209"
                },
                {
                  "key": "tensor_shape",
                  "value": "INT64[unk__18]"
                },
                {
                  "key": "param_name",
                  "value": "data"
                }
              ]
            },
            {
              "id": "1",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "val_53"
                },
                {
                  "key": "tensor_shape",
                  "value": "INT64[1]"
                },
                {
                  "key": "param_name",
                  "value": "axes"
                }
              ]
            }
          ],
          "style": null,
          "config": null
        },
        {
          "id": "node_ScatterND_279",
          "label": "ScatterND",
          "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/slice_scatter_2: aten.slice_scatter.default",
          "subgraphIds": [],
          "attrs": [
            {
              "key": "reduction",
              "value": "none"
            },
            {
              "key": "[metadata] namespace",
              "value": ": transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/slice_scatter_2: aten.slice_scatter.default"
            },
            {
              "key": "[metadata] pkg.torch.onnx.class_hierarchy",
              "value": "['transformers.models.llama.modeling_llama.LlamaForCausalLM', 'transformers.models.llama.modeling_llama.LlamaModel', 'aten.slice_scatter.default']"
            },
            {
              "key": "[metadata] pkg.torch.onnx.fx_node",
              "value": "%slice_scatter_2 : [num_users=2] = call_function[target=torch.ops.aten.slice_scatter.default](args = (%clone, %slice_scatter_1, 0, 0, 9223372036854775807), kwargs = {})"
            },
            {
              "key": "[metadata] pkg.torch.onnx.name_scopes",
              "value": "['', 'model', 'slice_scatter_2']"
            },
            {
              "key": "[metadata] pkg.torch.onnx.stack_trace",
              "value": "File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 688, in forward\n    outputs: BaseModelOutputWithPast = self.model(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 436, in forward\n    causal_mask = self._update_causal_mask("
            }
          ],
          "incomingEdges": [
            {
              "sourceNodeId": "node_Expand_94",
              "sourceNodeOutputId": "0",
              "targetNodeInputId": "0"
            },
            {
              "sourceNodeId": "node_Unsqueeze_278",
              "sourceNodeOutputId": "0",
              "targetNodeInputId": "1"
            },
            {
              "sourceNodeId": "node_Transpose_269",
              "sourceNodeOutputId": "0",
              "targetNodeInputId": "2"
            }
          ],
          "outputsMetadata": [
            {
              "id": "0",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "slice_scatter_2"
                },
                {
                  "key": "tensor_shape",
                  "value": "FLOAT[s34,1,s16,s16 + s17]"
                },
                {
                  "key": "param_name",
                  "value": "output"
                }
              ]
            }
          ],
          "inputsMetadata": [
            {
              "id": "0",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "expand_1"
                },
                {
                  "key": "tensor_shape",
                  "value": "FLOAT[s34,1,s16,s16 + s17]"
                },
                {
                  "key": "param_name",
                  "value": "data"
                }
              ]
            },
            {
              "id": "1",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "val_214"
                },
                {
                  "key": "tensor_shape",
                  "value": "INT64[unk__18,1]"
                },
                {
                  "key": "param_name",
                  "value": "indices"
                }
              ]
            },
            {
              "id": "2",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "slice_scatter_1"
                },
                {
                  "key": "tensor_shape",
                  "value": "FLOAT[s34,1,s16,s16 + s17]"
                },
                {
                  "key": "param_name",
                  "value": "updates"
                }
              ]
            }
          ],
          "style": null,
          "config": null
        },
        {
          "id": "node_Unsqueeze_308",
          "label": "Unsqueeze",
          "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.rotary_emb: transformers.models.llama.modeling_llama.LlamaRotaryEmbedding/unsqueeze_9: aten.unsqueeze.default",
          "subgraphIds": [],
          "attrs": [
            {
              "key": "[metadata] namespace",
              "value": ": transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.rotary_emb: transformers.models.llama.modeling_llama.LlamaRotaryEmbedding/unsqueeze_9: aten.unsqueeze.default"
            },
            {
              "key": "[metadata] pkg.torch.onnx.class_hierarchy",
              "value": "['transformers.models.llama.modeling_llama.LlamaForCausalLM', 'transformers.models.llama.modeling_llama.LlamaModel', 'transformers.models.llama.modeling_llama.LlamaRotaryEmbedding', 'aten.unsqueeze.default']"
            },
            {
              "key": "[metadata] pkg.torch.onnx.fx_node",
              "value": "%unsqueeze_9 : [num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%slice_24, 1), kwargs = {})"
            },
            {
              "key": "[metadata] pkg.torch.onnx.name_scopes",
              "value": "['', 'model', 'model.rotary_emb', 'unsqueeze_9']"
            },
            {
              "key": "[metadata] pkg.torch.onnx.stack_trace",
              "value": "File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 688, in forward\n    outputs: BaseModelOutputWithPast = self.model(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 443, in forward\n    position_embeddings = self.rotary_emb(hidden_states, position_ids)\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 104, in forward\n    position_ids_expanded = position_ids[:, None, :].float()"
            }
          ],
          "incomingEdges": [
            {
              "sourceNodeId": "node_Unsqueeze_38",
              "sourceNodeOutputId": "0",
              "targetNodeInputId": "0"
            },
            {
              "sourceNodeId": "node_Constant_60",
              "sourceNodeOutputId": "0",
              "targetNodeInputId": "1"
            }
          ],
          "outputsMetadata": [
            {
              "id": "0",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "unsqueeze_9"
                },
                {
                  "key": "tensor_shape",
                  "value": "INT64[1,1,s16]"
                },
                {
                  "key": "param_name",
                  "value": "expanded"
                }
              ]
            }
          ],
          "inputsMetadata": [
            {
              "id": "0",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "unsqueeze"
                },
                {
                  "key": "tensor_shape",
                  "value": "INT64[1,s16]"
                },
                {
                  "key": "param_name",
                  "value": "data"
                }
              ]
            },
            {
              "id": "1",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "val_26"
                },
                {
                  "key": "tensor_shape",
                  "value": "INT64[1]"
                },
                {
                  "key": "param_name",
                  "value": "axes"
                }
              ]
            }
          ],
          "style": null,
          "config": null
        },
        {
          "id": "node_Cast_320",
          "label": "Cast",
          "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.rotary_emb: transformers.models.llama.modeling_llama.LlamaRotaryEmbedding/_to_copy: aten._to_copy.default",
          "subgraphIds": [],
          "attrs": [
            {
              "key": "to",
              "value": "FLOAT"
            },
            {
              "key": "[metadata] namespace",
              "value": ": transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.rotary_emb: transformers.models.llama.modeling_llama.LlamaRotaryEmbedding/_to_copy: aten._to_copy.default"
            },
            {
              "key": "[metadata] pkg.torch.onnx.class_hierarchy",
              "value": "['transformers.models.llama.modeling_llama.LlamaForCausalLM', 'transformers.models.llama.modeling_llama.LlamaModel', 'transformers.models.llama.modeling_llama.LlamaRotaryEmbedding', 'aten._to_copy.default']"
            },
            {
              "key": "[metadata] pkg.torch.onnx.fx_node",
              "value": "%_to_copy : [num_users=1] = call_function[target=torch.ops.aten._to_copy.default](args = (%slice_25,), kwargs = {dtype: torch.float32})"
            },
            {
              "key": "[metadata] pkg.torch.onnx.name_scopes",
              "value": "['', 'model', 'model.rotary_emb', '_to_copy']"
            },
            {
              "key": "[metadata] pkg.torch.onnx.stack_trace",
              "value": "File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 688, in forward\n    outputs: BaseModelOutputWithPast = self.model(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 443, in forward\n    position_embeddings = self.rotary_emb(hidden_states, position_ids)\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 104, in forward\n    position_ids_expanded = position_ids[:, None, :].float()"
            }
          ],
          "incomingEdges": [
            {
              "sourceNodeId": "node_Unsqueeze_308",
              "sourceNodeOutputId": "0",
              "targetNodeInputId": "0"
            }
          ],
          "outputsMetadata": [
            {
              "id": "0",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "_to_copy"
                },
                {
                  "key": "tensor_shape",
                  "value": "FLOAT[1,1,s16]"
                },
                {
                  "key": "param_name",
                  "value": "output"
                }
              ]
            }
          ],
          "inputsMetadata": [
            {
              "id": "0",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "unsqueeze_9"
                },
                {
                  "key": "tensor_shape",
                  "value": "INT64[1,1,s16]"
                },
                {
                  "key": "param_name",
                  "value": "input"
                }
              ]
            }
          ],
          "style": null,
          "config": null
        },
        {
          "id": "node_MatMul_321",
          "label": "MatMul",
          "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.rotary_emb: transformers.models.llama.modeling_llama.LlamaRotaryEmbedding/matmul: aten.matmul.default",
          "subgraphIds": [],
          "attrs": [
            {
              "key": "[metadata] namespace",
              "value": ": transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.rotary_emb: transformers.models.llama.modeling_llama.LlamaRotaryEmbedding/matmul: aten.matmul.default"
            },
            {
              "key": "[metadata] pkg.torch.onnx.class_hierarchy",
              "value": "['transformers.models.llama.modeling_llama.LlamaForCausalLM', 'transformers.models.llama.modeling_llama.LlamaModel', 'transformers.models.llama.modeling_llama.LlamaRotaryEmbedding', 'aten.matmul.default']"
            },
            {
              "key": "[metadata] pkg.torch.onnx.fx_node",
              "value": "%matmul : [num_users=1] = call_function[target=torch.ops.aten.matmul.default](args = (%expand_2, %_to_copy), kwargs = {})"
            },
            {
              "key": "[metadata] pkg.torch.onnx.name_scopes",
              "value": "['', 'model', 'model.rotary_emb', 'matmul']"
            },
            {
              "key": "[metadata] pkg.torch.onnx.stack_trace",
              "value": "File \"<eval_with_key>.20\", line 9, in forward\n    matmul = torch.ops.aten.matmul.default(to_4, to_5);  to_4 = to_5 = None"
            }
          ],
          "incomingEdges": [
            {
              "sourceNodeId": "[value] expand_2",
              "sourceNodeOutputId": "0",
              "targetNodeInputId": "0"
            },
            {
              "sourceNodeId": "node_Cast_320",
              "sourceNodeOutputId": "0",
              "targetNodeInputId": "1"
            }
          ],
          "outputsMetadata": [
            {
              "id": "0",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "matmul"
                },
                {
                  "key": "tensor_shape",
                  "value": "FLOAT[1,32,s16]"
                },
                {
                  "key": "param_name",
                  "value": "Y"
                }
              ]
            }
          ],
          "inputsMetadata": [
            {
              "id": "0",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "expand_2"
                },
                {
                  "key": "tensor_shape",
                  "value": "FLOAT[1,32,1]"
                },
                {
                  "key": "param_name",
                  "value": "A"
                }
              ]
            },
            {
              "id": "1",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "_to_copy"
                },
                {
                  "key": "tensor_shape",
                  "value": "FLOAT[1,1,s16]"
                },
                {
                  "key": "param_name",
                  "value": "B"
                }
              ]
            }
          ],
          "style": null,
          "config": null
        },
        {
          "id": "node_Transpose_322",
          "label": "Transpose",
          "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.rotary_emb: transformers.models.llama.modeling_llama.LlamaRotaryEmbedding/transpose: aten.transpose.int",
          "subgraphIds": [],
          "attrs": [
            {
              "key": "perm",
              "value": "[0, 2, 1]"
            },
            {
              "key": "[metadata] namespace",
              "value": ": transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.rotary_emb: transformers.models.llama.modeling_llama.LlamaRotaryEmbedding/transpose: aten.transpose.int"
            },
            {
              "key": "[metadata] pkg.torch.onnx.class_hierarchy",
              "value": "['transformers.models.llama.modeling_llama.LlamaForCausalLM', 'transformers.models.llama.modeling_llama.LlamaModel', 'transformers.models.llama.modeling_llama.LlamaRotaryEmbedding', 'aten.transpose.int']"
            },
            {
              "key": "[metadata] pkg.torch.onnx.fx_node",
              "value": "%transpose : [num_users=1] = call_function[target=torch.ops.aten.transpose.int](args = (%matmul, 1, 2), kwargs = {})"
            },
            {
              "key": "[metadata] pkg.torch.onnx.name_scopes",
              "value": "['', 'model', 'model.rotary_emb', 'transpose']"
            },
            {
              "key": "[metadata] pkg.torch.onnx.stack_trace",
              "value": "File \"<eval_with_key>.20\", line 10, in forward\n    transpose = torch.ops.aten.transpose.int(matmul, 1, 2);  matmul = None"
            }
          ],
          "incomingEdges": [
            {
              "sourceNodeId": "node_MatMul_321",
              "sourceNodeOutputId": "0",
              "targetNodeInputId": "0"
            }
          ],
          "outputsMetadata": [
            {
              "id": "0",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "transpose"
                },
                {
                  "key": "tensor_shape",
                  "value": "FLOAT[1,s16,32]"
                },
                {
                  "key": "param_name",
                  "value": "transposed"
                }
              ]
            }
          ],
          "inputsMetadata": [
            {
              "id": "0",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "matmul"
                },
                {
                  "key": "tensor_shape",
                  "value": "FLOAT[1,32,s16]"
                },
                {
                  "key": "param_name",
                  "value": "data"
                }
              ]
            }
          ],
          "style": null,
          "config": null
        },
        {
          "id": "node_Concat_323",
          "label": "Concat",
          "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.rotary_emb: transformers.models.llama.modeling_llama.LlamaRotaryEmbedding/cat: aten.cat.default",
          "subgraphIds": [],
          "attrs": [
            {
              "key": "axis",
              "value": "-1"
            },
            {
              "key": "[metadata] namespace",
              "value": ": transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.rotary_emb: transformers.models.llama.modeling_llama.LlamaRotaryEmbedding/cat: aten.cat.default"
            },
            {
              "key": "[metadata] pkg.torch.onnx.class_hierarchy",
              "value": "['transformers.models.llama.modeling_llama.LlamaForCausalLM', 'transformers.models.llama.modeling_llama.LlamaModel', 'transformers.models.llama.modeling_llama.LlamaRotaryEmbedding', 'aten.cat.default']"
            },
            {
              "key": "[metadata] pkg.torch.onnx.fx_node",
              "value": "%cat : [num_users=2] = call_function[target=torch.ops.aten.cat.default](args = ([%transpose, %transpose], -1), kwargs = {})"
            },
            {
              "key": "[metadata] pkg.torch.onnx.name_scopes",
              "value": "['', 'model', 'model.rotary_emb', 'cat']"
            },
            {
              "key": "[metadata] pkg.torch.onnx.stack_trace",
              "value": "File \"<eval_with_key>.20\", line 11, in forward\n    cat = torch.ops.aten.cat.default([transpose, transpose], -1);  transpose = None"
            }
          ],
          "incomingEdges": [
            {
              "sourceNodeId": "node_Transpose_322",
              "sourceNodeOutputId": "0",
              "targetNodeInputId": "0"
            },
            {
              "sourceNodeId": "node_Transpose_322",
              "sourceNodeOutputId": "0",
              "targetNodeInputId": "1"
            }
          ],
          "outputsMetadata": [
            {
              "id": "0",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "cat"
                },
                {
                  "key": "tensor_shape",
                  "value": "FLOAT[1,s16,64]"
                },
                {
                  "key": "param_name",
                  "value": "concat_result"
                }
              ]
            }
          ],
          "inputsMetadata": [
            {
              "id": "0",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "transpose"
                },
                {
                  "key": "tensor_shape",
                  "value": "FLOAT[1,s16,32]"
                },
                {
                  "key": "param_name",
                  "value": "inputs"
                }
              ]
            },
            {
              "id": "1",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "transpose"
                },
                {
                  "key": "tensor_shape",
                  "value": "FLOAT[1,s16,32]"
                },
                {
                  "key": "param_name",
                  "value": "inputs"
                }
              ]
            }
          ],
          "style": null,
          "config": null
        },
        {
          "id": "node_Cos_324",
          "label": "Cos",
          "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.rotary_emb: transformers.models.llama.modeling_llama.LlamaRotaryEmbedding/cos: aten.cos.default",
          "subgraphIds": [],
          "attrs": [
            {
              "key": "[metadata] namespace",
              "value": ": transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.rotary_emb: transformers.models.llama.modeling_llama.LlamaRotaryEmbedding/cos: aten.cos.default"
            },
            {
              "key": "[metadata] pkg.torch.onnx.class_hierarchy",
              "value": "['transformers.models.llama.modeling_llama.LlamaForCausalLM', 'transformers.models.llama.modeling_llama.LlamaModel', 'transformers.models.llama.modeling_llama.LlamaRotaryEmbedding', 'aten.cos.default']"
            },
            {
              "key": "[metadata] pkg.torch.onnx.fx_node",
              "value": "%cos : [num_users=1] = call_function[target=torch.ops.aten.cos.default](args = (%cat,), kwargs = {})"
            },
            {
              "key": "[metadata] pkg.torch.onnx.name_scopes",
              "value": "['', 'model', 'model.rotary_emb', 'cos']"
            },
            {
              "key": "[metadata] pkg.torch.onnx.stack_trace",
              "value": "File \"<eval_with_key>.20\", line 12, in forward\n    cos = torch.ops.aten.cos.default(cat)"
            }
          ],
          "incomingEdges": [
            {
              "sourceNodeId": "node_Concat_323",
              "sourceNodeOutputId": "0",
              "targetNodeInputId": "0"
            }
          ],
          "outputsMetadata": [
            {
              "id": "0",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "cos"
                },
                {
                  "key": "tensor_shape",
                  "value": "FLOAT[1,s16,64]"
                },
                {
                  "key": "param_name",
                  "value": "output"
                }
              ]
            }
          ],
          "inputsMetadata": [
            {
              "id": "0",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "cat"
                },
                {
                  "key": "tensor_shape",
                  "value": "FLOAT[1,s16,64]"
                },
                {
                  "key": "param_name",
                  "value": "input"
                }
              ]
            }
          ],
          "style": null,
          "config": null
        },
        {
          "id": "node_Sin_326",
          "label": "Sin",
          "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.rotary_emb: transformers.models.llama.modeling_llama.LlamaRotaryEmbedding/sin: aten.sin.default",
          "subgraphIds": [],
          "attrs": [
            {
              "key": "[metadata] namespace",
              "value": ": transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.rotary_emb: transformers.models.llama.modeling_llama.LlamaRotaryEmbedding/sin: aten.sin.default"
            },
            {
              "key": "[metadata] pkg.torch.onnx.class_hierarchy",
              "value": "['transformers.models.llama.modeling_llama.LlamaForCausalLM', 'transformers.models.llama.modeling_llama.LlamaModel', 'transformers.models.llama.modeling_llama.LlamaRotaryEmbedding', 'aten.sin.default']"
            },
            {
              "key": "[metadata] pkg.torch.onnx.fx_node",
              "value": "%sin : [num_users=1] = call_function[target=torch.ops.aten.sin.default](args = (%cat,), kwargs = {})"
            },
            {
              "key": "[metadata] pkg.torch.onnx.name_scopes",
              "value": "['', 'model', 'model.rotary_emb', 'sin']"
            },
            {
              "key": "[metadata] pkg.torch.onnx.stack_trace",
              "value": "File \"<eval_with_key>.20\", line 14, in forward\n    sin = torch.ops.aten.sin.default(cat);  cat = None"
            }
          ],
          "incomingEdges": [
            {
              "sourceNodeId": "node_Concat_323",
              "sourceNodeOutputId": "0",
              "targetNodeInputId": "0"
            }
          ],
          "outputsMetadata": [
            {
              "id": "0",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "sin"
                },
                {
                  "key": "tensor_shape",
                  "value": "FLOAT[1,s16,64]"
                },
                {
                  "key": "param_name",
                  "value": "output"
                }
              ]
            }
          ],
          "inputsMetadata": [
            {
              "id": "0",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "cat"
                },
                {
                  "key": "tensor_shape",
                  "value": "FLOAT[1,s16,64]"
                },
                {
                  "key": "param_name",
                  "value": "input"
                }
              ]
            }
          ],
          "style": null,
          "config": null
        },
        {
          "id": "node_Constant_328",
          "label": "Constant",
          "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.0: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.0.input_layernorm: transformers.models.llama.modeling_llama.LlamaRMSNorm/pow_1: aten.pow.Tensor_Scalar",
          "subgraphIds": [],
          "attrs": [
            {
              "key": "__value",
              "value": "2.0"
            },
            {
              "key": "value",
              "value": "TensorProtoTensor<FLOAT,[]>(array(2., dtype=float32), name='')"
            },
            {
              "key": "[metadata] namespace",
              "value": ": transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.0: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.0.input_layernorm: transformers.models.llama.modeling_llama.LlamaRMSNorm/pow_1: aten.pow.Tensor_Scalar"
            },
            {
              "key": "[metadata] pkg.torch.onnx.class_hierarchy",
              "value": "['transformers.models.llama.modeling_llama.LlamaForCausalLM', 'transformers.models.llama.modeling_llama.LlamaModel', 'transformers.models.llama.modeling_llama.LlamaDecoderLayer', 'transformers.models.llama.modeling_llama.LlamaRMSNorm', 'aten.pow.Tensor_Scalar']"
            },
            {
              "key": "[metadata] pkg.torch.onnx.fx_node",
              "value": "%pow_1 : [num_users=1] = call_function[target=torch.ops.aten.pow.Tensor_Scalar](args = (%embedding, 2), kwargs = {})"
            },
            {
              "key": "[metadata] pkg.torch.onnx.name_scopes",
              "value": "['', 'model', 'model.layers.0', 'model.layers.0.input_layernorm', 'pow_1']"
            },
            {
              "key": "[metadata] pkg.torch.onnx.stack_trace",
              "value": "File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 688, in forward\n    outputs: BaseModelOutputWithPast = self.model(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 453, in forward\n    layer_outputs = decoder_layer(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 305, in forward\n    hidden_states = self.input_layernorm(hidden_states)\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 71, in forward\n    variance = hidden_states.pow(2).mean(-1, keepdim=True)"
            }
          ],
          "incomingEdges": [],
          "outputsMetadata": [
            {
              "id": "0",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "val_248"
                },
                {
                  "key": "tensor_shape",
                  "value": "FLOAT[]"
                },
                {
                  "key": "param_name",
                  "value": "output"
                }
              ]
            }
          ],
          "inputsMetadata": [],
          "style": null,
          "config": null
        },
        {
          "id": "node_Pow_329",
          "label": "Pow",
          "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.0: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.0.input_layernorm: transformers.models.llama.modeling_llama.LlamaRMSNorm/pow_1: aten.pow.Tensor_Scalar",
          "subgraphIds": [],
          "attrs": [
            {
              "key": "[metadata] namespace",
              "value": ": transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.0: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.0.input_layernorm: transformers.models.llama.modeling_llama.LlamaRMSNorm/pow_1: aten.pow.Tensor_Scalar"
            },
            {
              "key": "[metadata] pkg.torch.onnx.class_hierarchy",
              "value": "['transformers.models.llama.modeling_llama.LlamaForCausalLM', 'transformers.models.llama.modeling_llama.LlamaModel', 'transformers.models.llama.modeling_llama.LlamaDecoderLayer', 'transformers.models.llama.modeling_llama.LlamaRMSNorm', 'aten.pow.Tensor_Scalar']"
            },
            {
              "key": "[metadata] pkg.torch.onnx.fx_node",
              "value": "%pow_1 : [num_users=1] = call_function[target=torch.ops.aten.pow.Tensor_Scalar](args = (%embedding, 2), kwargs = {})"
            },
            {
              "key": "[metadata] pkg.torch.onnx.name_scopes",
              "value": "['', 'model', 'model.layers.0', 'model.layers.0.input_layernorm', 'pow_1']"
            },
            {
              "key": "[metadata] pkg.torch.onnx.stack_trace",
              "value": "File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 688, in forward\n    outputs: BaseModelOutputWithPast = self.model(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 453, in forward\n    layer_outputs = decoder_layer(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 305, in forward\n    hidden_states = self.input_layernorm(hidden_states)\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 71, in forward\n    variance = hidden_states.pow(2).mean(-1, keepdim=True)"
            }
          ],
          "incomingEdges": [
            {
              "sourceNodeId": "node_Gather_20",
              "sourceNodeOutputId": "0",
              "targetNodeInputId": "0"
            },
            {
              "sourceNodeId": "node_Constant_328",
              "sourceNodeOutputId": "0",
              "targetNodeInputId": "1"
            }
          ],
          "outputsMetadata": [
            {
              "id": "0",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "pow_1"
                },
                {
                  "key": "tensor_shape",
                  "value": "FLOAT[s34,s16,2048]"
                },
                {
                  "key": "param_name",
                  "value": "Z"
                }
              ]
            }
          ],
          "inputsMetadata": [
            {
              "id": "0",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "embedding"
                },
                {
                  "key": "tensor_shape",
                  "value": "FLOAT[s34,s16,2048]"
                },
                {
                  "key": "param_name",
                  "value": "X"
                }
              ]
            },
            {
              "id": "1",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "val_248"
                },
                {
                  "key": "tensor_shape",
                  "value": "FLOAT[]"
                },
                {
                  "key": "param_name",
                  "value": "Y"
                }
              ]
            }
          ],
          "style": null,
          "config": null
        },
        {
          "id": "node_Constant_869",
          "label": "Constant",
          "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.0: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.0.input_layernorm: transformers.models.llama.modeling_llama.LlamaRMSNorm/mean: aten.mean.dim",
          "subgraphIds": [],
          "attrs": [
            {
              "key": "__value",
              "value": "[-1]"
            },
            {
              "key": "value",
              "value": "TensorProtoTensor<INT64,[1]>(array([-1]), name='val_250')"
            }
          ],
          "incomingEdges": [],
          "outputsMetadata": [
            {
              "id": "0",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "val_250"
                },
                {
                  "key": "tensor_shape",
                  "value": "INT64[1]"
                },
                {
                  "key": "param_name",
                  "value": "output"
                }
              ]
            }
          ],
          "inputsMetadata": [],
          "style": null,
          "config": null
        },
        {
          "id": "node_ReduceMean_332",
          "label": "ReduceMean",
          "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.0: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.0.input_layernorm: transformers.models.llama.modeling_llama.LlamaRMSNorm/mean: aten.mean.dim",
          "subgraphIds": [],
          "attrs": [
            {
              "key": "noop_with_empty_axes",
              "value": "0"
            },
            {
              "key": "keepdims",
              "value": "1"
            },
            {
              "key": "[metadata] namespace",
              "value": ": transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.0: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.0.input_layernorm: transformers.models.llama.modeling_llama.LlamaRMSNorm/mean: aten.mean.dim"
            },
            {
              "key": "[metadata] pkg.torch.onnx.class_hierarchy",
              "value": "['transformers.models.llama.modeling_llama.LlamaForCausalLM', 'transformers.models.llama.modeling_llama.LlamaModel', 'transformers.models.llama.modeling_llama.LlamaDecoderLayer', 'transformers.models.llama.modeling_llama.LlamaRMSNorm', 'aten.mean.dim']"
            },
            {
              "key": "[metadata] pkg.torch.onnx.fx_node",
              "value": "%mean : [num_users=1] = call_function[target=torch.ops.aten.mean.dim](args = (%pow_1, [-1], True), kwargs = {})"
            },
            {
              "key": "[metadata] pkg.torch.onnx.name_scopes",
              "value": "['', 'model', 'model.layers.0', 'model.layers.0.input_layernorm', 'mean']"
            },
            {
              "key": "[metadata] pkg.torch.onnx.stack_trace",
              "value": "File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 688, in forward\n    outputs: BaseModelOutputWithPast = self.model(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 453, in forward\n    layer_outputs = decoder_layer(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 305, in forward\n    hidden_states = self.input_layernorm(hidden_states)\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 71, in forward\n    variance = hidden_states.pow(2).mean(-1, keepdim=True)"
            }
          ],
          "incomingEdges": [
            {
              "sourceNodeId": "node_Pow_329",
              "sourceNodeOutputId": "0",
              "targetNodeInputId": "0"
            },
            {
              "sourceNodeId": "node_Constant_869",
              "sourceNodeOutputId": "0",
              "targetNodeInputId": "1"
            }
          ],
          "outputsMetadata": [
            {
              "id": "0",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "mean"
                },
                {
                  "key": "tensor_shape",
                  "value": "FLOAT[s34,s16,1]"
                },
                {
                  "key": "param_name",
                  "value": "reduced"
                }
              ]
            }
          ],
          "inputsMetadata": [
            {
              "id": "0",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "pow_1"
                },
                {
                  "key": "tensor_shape",
                  "value": "FLOAT[s34,s16,2048]"
                },
                {
                  "key": "param_name",
                  "value": "data"
                }
              ]
            },
            {
              "id": "1",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "val_250"
                },
                {
                  "key": "tensor_shape",
                  "value": "INT64[1]"
                },
                {
                  "key": "param_name",
                  "value": "axes"
                }
              ]
            }
          ],
          "style": null,
          "config": null
        },
        {
          "id": "node_Constant_333",
          "label": "Constant",
          "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel",
          "subgraphIds": [],
          "attrs": [
            {
              "key": "__value",
              "value": "9.999999747378752e-06"
            },
            {
              "key": "value",
              "value": "TensorProtoTensor<FLOAT,[]>(array(1.e-05, dtype=float32), name='')"
            },
            {
              "key": "[metadata] namespace",
              "value": ": transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.0: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.0.input_layernorm: transformers.models.llama.modeling_llama.LlamaRMSNorm/add_192: aten.add.Tensor"
            },
            {
              "key": "[metadata] pkg.torch.onnx.class_hierarchy",
              "value": "['transformers.models.llama.modeling_llama.LlamaForCausalLM', 'transformers.models.llama.modeling_llama.LlamaModel', 'transformers.models.llama.modeling_llama.LlamaDecoderLayer', 'transformers.models.llama.modeling_llama.LlamaRMSNorm', 'aten.add.Tensor']"
            },
            {
              "key": "[metadata] pkg.torch.onnx.fx_node",
              "value": "%add_192 : [num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%mean, 1e-05), kwargs = {})"
            },
            {
              "key": "[metadata] pkg.torch.onnx.name_scopes",
              "value": "['', 'model', 'model.layers.0', 'model.layers.0.input_layernorm', 'add_192']"
            },
            {
              "key": "[metadata] pkg.torch.onnx.stack_trace",
              "value": "File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 688, in forward\n    outputs: BaseModelOutputWithPast = self.model(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 453, in forward\n    layer_outputs = decoder_layer(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 305, in forward\n    hidden_states = self.input_layernorm(hidden_states)\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 72, in forward\n    hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)"
            }
          ],
          "incomingEdges": [],
          "outputsMetadata": [
            {
              "id": "0",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "val_251"
                },
                {
                  "key": "tensor_shape",
                  "value": "FLOAT[]"
                },
                {
                  "key": "param_name",
                  "value": "output"
                }
              ]
            }
          ],
          "inputsMetadata": [],
          "style": null,
          "config": null
        },
        {
          "id": "node_Add_334",
          "label": "Add",
          "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.0: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.0.input_layernorm: transformers.models.llama.modeling_llama.LlamaRMSNorm/add_192: aten.add.Tensor",
          "subgraphIds": [],
          "attrs": [
            {
              "key": "[metadata] namespace",
              "value": ": transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.0: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.0.input_layernorm: transformers.models.llama.modeling_llama.LlamaRMSNorm/add_192: aten.add.Tensor"
            },
            {
              "key": "[metadata] pkg.torch.onnx.class_hierarchy",
              "value": "['transformers.models.llama.modeling_llama.LlamaForCausalLM', 'transformers.models.llama.modeling_llama.LlamaModel', 'transformers.models.llama.modeling_llama.LlamaDecoderLayer', 'transformers.models.llama.modeling_llama.LlamaRMSNorm', 'aten.add.Tensor']"
            },
            {
              "key": "[metadata] pkg.torch.onnx.fx_node",
              "value": "%add_192 : [num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%mean, 1e-05), kwargs = {})"
            },
            {
              "key": "[metadata] pkg.torch.onnx.name_scopes",
              "value": "['', 'model', 'model.layers.0', 'model.layers.0.input_layernorm', 'add_192']"
            },
            {
              "key": "[metadata] pkg.torch.onnx.stack_trace",
              "value": "File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 688, in forward\n    outputs: BaseModelOutputWithPast = self.model(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 453, in forward\n    layer_outputs = decoder_layer(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 305, in forward\n    hidden_states = self.input_layernorm(hidden_states)\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 72, in forward\n    hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)"
            }
          ],
          "incomingEdges": [
            {
              "sourceNodeId": "node_ReduceMean_332",
              "sourceNodeOutputId": "0",
              "targetNodeInputId": "0"
            },
            {
              "sourceNodeId": "node_Constant_333",
              "sourceNodeOutputId": "0",
              "targetNodeInputId": "1"
            }
          ],
          "outputsMetadata": [
            {
              "id": "0",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "add_192"
                },
                {
                  "key": "tensor_shape",
                  "value": "FLOAT[s34,s16,1]"
                },
                {
                  "key": "param_name",
                  "value": "C"
                }
              ]
            }
          ],
          "inputsMetadata": [
            {
              "id": "0",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "mean"
                },
                {
                  "key": "tensor_shape",
                  "value": "FLOAT[s34,s16,1]"
                },
                {
                  "key": "param_name",
                  "value": "A"
                }
              ]
            },
            {
              "id": "1",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "val_251"
                },
                {
                  "key": "tensor_shape",
                  "value": "FLOAT[]"
                },
                {
                  "key": "param_name",
                  "value": "B"
                }
              ]
            }
          ],
          "style": null,
          "config": null
        },
        {
          "id": "node_Sqrt_335",
          "label": "Sqrt",
          "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.0: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.0.input_layernorm: transformers.models.llama.modeling_llama.LlamaRMSNorm/rsqrt: aten.rsqrt.default",
          "subgraphIds": [],
          "attrs": [
            {
              "key": "[metadata] namespace",
              "value": ": transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.0: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.0.input_layernorm: transformers.models.llama.modeling_llama.LlamaRMSNorm/rsqrt: aten.rsqrt.default"
            },
            {
              "key": "[metadata] pkg.torch.onnx.class_hierarchy",
              "value": "['transformers.models.llama.modeling_llama.LlamaForCausalLM', 'transformers.models.llama.modeling_llama.LlamaModel', 'transformers.models.llama.modeling_llama.LlamaDecoderLayer', 'transformers.models.llama.modeling_llama.LlamaRMSNorm', 'aten.rsqrt.default']"
            },
            {
              "key": "[metadata] pkg.torch.onnx.fx_node",
              "value": "%rsqrt : [num_users=1] = call_function[target=torch.ops.aten.rsqrt.default](args = (%add_192,), kwargs = {})"
            },
            {
              "key": "[metadata] pkg.torch.onnx.name_scopes",
              "value": "['', 'model', 'model.layers.0', 'model.layers.0.input_layernorm', 'rsqrt']"
            },
            {
              "key": "[metadata] pkg.torch.onnx.stack_trace",
              "value": "File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 688, in forward\n    outputs: BaseModelOutputWithPast = self.model(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 453, in forward\n    layer_outputs = decoder_layer(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 305, in forward\n    hidden_states = self.input_layernorm(hidden_states)\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 72, in forward\n    hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)"
            }
          ],
          "incomingEdges": [
            {
              "sourceNodeId": "node_Add_334",
              "sourceNodeOutputId": "0",
              "targetNodeInputId": "0"
            }
          ],
          "outputsMetadata": [
            {
              "id": "0",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "val_252"
                },
                {
                  "key": "tensor_shape",
                  "value": "FLOAT[s34,s16,1]"
                },
                {
                  "key": "param_name",
                  "value": "Y"
                }
              ]
            }
          ],
          "inputsMetadata": [
            {
              "id": "0",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "add_192"
                },
                {
                  "key": "tensor_shape",
                  "value": "FLOAT[s34,s16,1]"
                },
                {
                  "key": "param_name",
                  "value": "X"
                }
              ]
            }
          ],
          "style": null,
          "config": null
        },
        {
          "id": "node_Reciprocal_336",
          "label": "Reciprocal",
          "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.0: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.0.input_layernorm: transformers.models.llama.modeling_llama.LlamaRMSNorm/rsqrt: aten.rsqrt.default",
          "subgraphIds": [],
          "attrs": [
            {
              "key": "[metadata] namespace",
              "value": ": transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.0: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.0.input_layernorm: transformers.models.llama.modeling_llama.LlamaRMSNorm/rsqrt: aten.rsqrt.default"
            },
            {
              "key": "[metadata] pkg.torch.onnx.class_hierarchy",
              "value": "['transformers.models.llama.modeling_llama.LlamaForCausalLM', 'transformers.models.llama.modeling_llama.LlamaModel', 'transformers.models.llama.modeling_llama.LlamaDecoderLayer', 'transformers.models.llama.modeling_llama.LlamaRMSNorm', 'aten.rsqrt.default']"
            },
            {
              "key": "[metadata] pkg.torch.onnx.fx_node",
              "value": "%rsqrt : [num_users=1] = call_function[target=torch.ops.aten.rsqrt.default](args = (%add_192,), kwargs = {})"
            },
            {
              "key": "[metadata] pkg.torch.onnx.name_scopes",
              "value": "['', 'model', 'model.layers.0', 'model.layers.0.input_layernorm', 'rsqrt']"
            },
            {
              "key": "[metadata] pkg.torch.onnx.stack_trace",
              "value": "File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 688, in forward\n    outputs: BaseModelOutputWithPast = self.model(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 453, in forward\n    layer_outputs = decoder_layer(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 305, in forward\n    hidden_states = self.input_layernorm(hidden_states)\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 72, in forward\n    hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)"
            }
          ],
          "incomingEdges": [
            {
              "sourceNodeId": "node_Sqrt_335",
              "sourceNodeOutputId": "0",
              "targetNodeInputId": "0"
            }
          ],
          "outputsMetadata": [
            {
              "id": "0",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "rsqrt"
                },
                {
                  "key": "tensor_shape",
                  "value": "FLOAT[s34,s16,1]"
                },
                {
                  "key": "param_name",
                  "value": "Y"
                }
              ]
            }
          ],
          "inputsMetadata": [
            {
              "id": "0",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "val_252"
                },
                {
                  "key": "tensor_shape",
                  "value": "FLOAT[s34,s16,1]"
                },
                {
                  "key": "param_name",
                  "value": "X"
                }
              ]
            }
          ],
          "style": null,
          "config": null
        },
        {
          "id": "node_Mul_337",
          "label": "Mul",
          "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.0: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.0.input_layernorm: transformers.models.llama.modeling_llama.LlamaRMSNorm/mul_169: aten.mul.Tensor",
          "subgraphIds": [],
          "attrs": [
            {
              "key": "[metadata] namespace",
              "value": ": transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.0: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.0.input_layernorm: transformers.models.llama.modeling_llama.LlamaRMSNorm/mul_169: aten.mul.Tensor"
            },
            {
              "key": "[metadata] pkg.torch.onnx.class_hierarchy",
              "value": "['transformers.models.llama.modeling_llama.LlamaForCausalLM', 'transformers.models.llama.modeling_llama.LlamaModel', 'transformers.models.llama.modeling_llama.LlamaDecoderLayer', 'transformers.models.llama.modeling_llama.LlamaRMSNorm', 'aten.mul.Tensor']"
            },
            {
              "key": "[metadata] pkg.torch.onnx.fx_node",
              "value": "%mul_169 : [num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%embedding, %rsqrt), kwargs = {})"
            },
            {
              "key": "[metadata] pkg.torch.onnx.name_scopes",
              "value": "['', 'model', 'model.layers.0', 'model.layers.0.input_layernorm', 'mul_169']"
            },
            {
              "key": "[metadata] pkg.torch.onnx.stack_trace",
              "value": "File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 688, in forward\n    outputs: BaseModelOutputWithPast = self.model(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 453, in forward\n    layer_outputs = decoder_layer(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 305, in forward\n    hidden_states = self.input_layernorm(hidden_states)\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 72, in forward\n    hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)"
            }
          ],
          "incomingEdges": [
            {
              "sourceNodeId": "node_Gather_20",
              "sourceNodeOutputId": "0",
              "targetNodeInputId": "0"
            },
            {
              "sourceNodeId": "node_Reciprocal_336",
              "sourceNodeOutputId": "0",
              "targetNodeInputId": "1"
            }
          ],
          "outputsMetadata": [
            {
              "id": "0",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "mul_169"
                },
                {
                  "key": "tensor_shape",
                  "value": "FLOAT[s34,s16,2048]"
                },
                {
                  "key": "param_name",
                  "value": "C"
                }
              ]
            }
          ],
          "inputsMetadata": [
            {
              "id": "0",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "embedding"
                },
                {
                  "key": "tensor_shape",
                  "value": "FLOAT[s34,s16,2048]"
                },
                {
                  "key": "param_name",
                  "value": "A"
                }
              ]
            },
            {
              "id": "1",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "rsqrt"
                },
                {
                  "key": "tensor_shape",
                  "value": "FLOAT[s34,s16,1]"
                },
                {
                  "key": "param_name",
                  "value": "B"
                }
              ]
            }
          ],
          "style": null,
          "config": null
        },
        {
          "id": "node_Mul_338",
          "label": "Mul",
          "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.0: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.0.input_layernorm: transformers.models.llama.modeling_llama.LlamaRMSNorm/mul_173: aten.mul.Tensor",
          "subgraphIds": [],
          "attrs": [
            {
              "key": "[metadata] namespace",
              "value": ": transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.0: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.0.input_layernorm: transformers.models.llama.modeling_llama.LlamaRMSNorm/mul_173: aten.mul.Tensor"
            },
            {
              "key": "[metadata] pkg.torch.onnx.class_hierarchy",
              "value": "['transformers.models.llama.modeling_llama.LlamaForCausalLM', 'transformers.models.llama.modeling_llama.LlamaModel', 'transformers.models.llama.modeling_llama.LlamaDecoderLayer', 'transformers.models.llama.modeling_llama.LlamaRMSNorm', 'aten.mul.Tensor']"
            },
            {
              "key": "[metadata] pkg.torch.onnx.fx_node",
              "value": "%mul_173 : [num_users=3] = call_function[target=torch.ops.aten.mul.Tensor](args = (%p_model_layers_0_input_layernorm_weight, %mul_169), kwargs = {})"
            },
            {
              "key": "[metadata] pkg.torch.onnx.name_scopes",
              "value": "['', 'model', 'model.layers.0', 'model.layers.0.input_layernorm', 'mul_173']"
            },
            {
              "key": "[metadata] pkg.torch.onnx.stack_trace",
              "value": "File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 688, in forward\n    outputs: BaseModelOutputWithPast = self.model(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 453, in forward\n    layer_outputs = decoder_layer(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 305, in forward\n    hidden_states = self.input_layernorm(hidden_states)\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 73, in forward\n    return self.weight * hidden_states.to(input_dtype)"
            }
          ],
          "incomingEdges": [
            {
              "sourceNodeId": "[value] model.layers.0.input_layernorm.weight",
              "sourceNodeOutputId": "0",
              "targetNodeInputId": "0"
            },
            {
              "sourceNodeId": "node_Mul_337",
              "sourceNodeOutputId": "0",
              "targetNodeInputId": "1"
            }
          ],
          "outputsMetadata": [
            {
              "id": "0",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "mul_173"
                },
                {
                  "key": "tensor_shape",
                  "value": "FLOAT[s34,s16,2048]"
                },
                {
                  "key": "param_name",
                  "value": "C"
                }
              ]
            }
          ],
          "inputsMetadata": [
            {
              "id": "0",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "model.layers.0.input_layernorm.weight"
                },
                {
                  "key": "tensor_shape",
                  "value": "FLOAT[2048]"
                },
                {
                  "key": "param_name",
                  "value": "A"
                }
              ]
            },
            {
              "id": "1",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "mul_169"
                },
                {
                  "key": "tensor_shape",
                  "value": "FLOAT[s34,s16,2048]"
                },
                {
                  "key": "param_name",
                  "value": "B"
                }
              ]
            }
          ],
          "style": null,
          "config": null
        },
        {
          "id": "node_Transpose_339",
          "label": "Transpose",
          "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.0: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.0.self_attn: transformers.models.llama.modeling_llama.LlamaAttention/model.layers.0.self_attn.q_proj: torch.nn.modules.linear.Linear/linear: aten.linear.default",
          "subgraphIds": [],
          "attrs": [
            {
              "key": "perm",
              "value": "[1, 0]"
            },
            {
              "key": "[metadata] namespace",
              "value": ": transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.0: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.0.self_attn: transformers.models.llama.modeling_llama.LlamaAttention/model.layers.0.self_attn.q_proj: torch.nn.modules.linear.Linear/linear: aten.linear.default"
            },
            {
              "key": "[metadata] pkg.torch.onnx.class_hierarchy",
              "value": "['transformers.models.llama.modeling_llama.LlamaForCausalLM', 'transformers.models.llama.modeling_llama.LlamaModel', 'transformers.models.llama.modeling_llama.LlamaDecoderLayer', 'transformers.models.llama.modeling_llama.LlamaAttention', 'torch.nn.modules.linear.Linear', 'aten.linear.default']"
            },
            {
              "key": "[metadata] pkg.torch.onnx.fx_node",
              "value": "%linear : [num_users=1] = call_function[target=torch.ops.aten.linear.default](args = (%mul_173, %p_model_layers_0_self_attn_q_proj_weight), kwargs = {})"
            },
            {
              "key": "[metadata] pkg.torch.onnx.name_scopes",
              "value": "['', 'model', 'model.layers.0', 'model.layers.0.self_attn', 'model.layers.0.self_attn.q_proj', 'linear']"
            },
            {
              "key": "[metadata] pkg.torch.onnx.stack_trace",
              "value": "File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 688, in forward\n    outputs: BaseModelOutputWithPast = self.model(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 453, in forward\n    layer_outputs = decoder_layer(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 308, in forward\n    hidden_states, self_attn_weights = self.self_attn(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 242, in forward\n    query_states = self.q_proj(hidden_states).view(hidden_shape).transpose(1, 2)\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/torch/nn/modules/linear.py\", line 125, in forward\n    return F.linear(input, self.weight, self.bias)"
            }
          ],
          "incomingEdges": [
            {
              "sourceNodeId": "[value] model.layers.0.self_attn.q_proj.weight",
              "sourceNodeOutputId": "0",
              "targetNodeInputId": "0"
            }
          ],
          "outputsMetadata": [
            {
              "id": "0",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "val_253"
                },
                {
                  "key": "tensor_shape",
                  "value": "FLOAT[2048,2048]"
                },
                {
                  "key": "param_name",
                  "value": "transposed"
                }
              ]
            }
          ],
          "inputsMetadata": [
            {
              "id": "0",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "model.layers.0.self_attn.q_proj.weight"
                },
                {
                  "key": "tensor_shape",
                  "value": "FLOAT[2048,2048]"
                },
                {
                  "key": "param_name",
                  "value": "data"
                }
              ]
            }
          ],
          "style": null,
          "config": null
        },
        {
          "id": "node_MatMul_340",
          "label": "MatMul",
          "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.0: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.0.self_attn: transformers.models.llama.modeling_llama.LlamaAttention/model.layers.0.self_attn.q_proj: torch.nn.modules.linear.Linear/linear: aten.linear.default",
          "subgraphIds": [],
          "attrs": [
            {
              "key": "[metadata] namespace",
              "value": ": transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.0: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.0.self_attn: transformers.models.llama.modeling_llama.LlamaAttention/model.layers.0.self_attn.q_proj: torch.nn.modules.linear.Linear/linear: aten.linear.default"
            },
            {
              "key": "[metadata] pkg.torch.onnx.class_hierarchy",
              "value": "['transformers.models.llama.modeling_llama.LlamaForCausalLM', 'transformers.models.llama.modeling_llama.LlamaModel', 'transformers.models.llama.modeling_llama.LlamaDecoderLayer', 'transformers.models.llama.modeling_llama.LlamaAttention', 'torch.nn.modules.linear.Linear', 'aten.linear.default']"
            },
            {
              "key": "[metadata] pkg.torch.onnx.fx_node",
              "value": "%linear : [num_users=1] = call_function[target=torch.ops.aten.linear.default](args = (%mul_173, %p_model_layers_0_self_attn_q_proj_weight), kwargs = {})"
            },
            {
              "key": "[metadata] pkg.torch.onnx.name_scopes",
              "value": "['', 'model', 'model.layers.0', 'model.layers.0.self_attn', 'model.layers.0.self_attn.q_proj', 'linear']"
            },
            {
              "key": "[metadata] pkg.torch.onnx.stack_trace",
              "value": "File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 688, in forward\n    outputs: BaseModelOutputWithPast = self.model(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 453, in forward\n    layer_outputs = decoder_layer(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 308, in forward\n    hidden_states, self_attn_weights = self.self_attn(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 242, in forward\n    query_states = self.q_proj(hidden_states).view(hidden_shape).transpose(1, 2)\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/torch/nn/modules/linear.py\", line 125, in forward\n    return F.linear(input, self.weight, self.bias)"
            }
          ],
          "incomingEdges": [
            {
              "sourceNodeId": "node_Mul_338",
              "sourceNodeOutputId": "0",
              "targetNodeInputId": "0"
            },
            {
              "sourceNodeId": "node_Transpose_339",
              "sourceNodeOutputId": "0",
              "targetNodeInputId": "1"
            }
          ],
          "outputsMetadata": [
            {
              "id": "0",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "linear"
                },
                {
                  "key": "tensor_shape",
                  "value": "FLOAT[s34,s16,2048]"
                },
                {
                  "key": "param_name",
                  "value": "Y"
                }
              ]
            }
          ],
          "inputsMetadata": [
            {
              "id": "0",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "mul_173"
                },
                {
                  "key": "tensor_shape",
                  "value": "FLOAT[s34,s16,2048]"
                },
                {
                  "key": "param_name",
                  "value": "A"
                }
              ]
            },
            {
              "id": "1",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "val_253"
                },
                {
                  "key": "tensor_shape",
                  "value": "FLOAT[2048,2048]"
                },
                {
                  "key": "param_name",
                  "value": "B"
                }
              ]
            }
          ],
          "style": null,
          "config": null
        },
        {
          "id": "node_Constant_345",
          "label": "Constant",
          "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel",
          "subgraphIds": [],
          "attrs": [
            {
              "key": "__value",
              "value": "[64]"
            },
            {
              "key": "value",
              "value": "TensorProtoTensor<INT64,[1]>(array([64]), name='')"
            },
            {
              "key": "[metadata] namespace",
              "value": ": transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.0: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.0.self_attn: transformers.models.llama.modeling_llama.LlamaAttention/view_1: aten.view.default"
            },
            {
              "key": "[metadata] pkg.torch.onnx.class_hierarchy",
              "value": "['transformers.models.llama.modeling_llama.LlamaForCausalLM', 'transformers.models.llama.modeling_llama.LlamaModel', 'transformers.models.llama.modeling_llama.LlamaDecoderLayer', 'transformers.models.llama.modeling_llama.LlamaAttention', 'aten.view.default']"
            },
            {
              "key": "[metadata] pkg.torch.onnx.fx_node",
              "value": "%view_1 : [num_users=1] = call_function[target=torch.ops.aten.view.default](args = (%linear, [%sym_size_int_68, %sym_size_int_61, -1, 64]), kwargs = {})"
            },
            {
              "key": "[metadata] pkg.torch.onnx.name_scopes",
              "value": "['', 'model', 'model.layers.0', 'model.layers.0.self_attn', 'view_1']"
            },
            {
              "key": "[metadata] pkg.torch.onnx.stack_trace",
              "value": "File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 688, in forward\n    outputs: BaseModelOutputWithPast = self.model(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 453, in forward\n    layer_outputs = decoder_layer(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 308, in forward\n    hidden_states, self_attn_weights = self.self_attn(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 242, in forward\n    query_states = self.q_proj(hidden_states).view(hidden_shape).transpose(1, 2)"
            }
          ],
          "incomingEdges": [],
          "outputsMetadata": [
            {
              "id": "0",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "val_258"
                },
                {
                  "key": "tensor_shape",
                  "value": "INT64[1]"
                },
                {
                  "key": "param_name",
                  "value": "output"
                }
              ]
            }
          ],
          "inputsMetadata": [],
          "style": null,
          "config": null
        },
        {
          "id": "node_Concat_346",
          "label": "Concat",
          "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.0: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.0.self_attn: transformers.models.llama.modeling_llama.LlamaAttention/view_1: aten.view.default",
          "subgraphIds": [],
          "attrs": [
            {
              "key": "axis",
              "value": "0"
            },
            {
              "key": "[metadata] namespace",
              "value": ": transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.0: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.0.self_attn: transformers.models.llama.modeling_llama.LlamaAttention/view_1: aten.view.default"
            },
            {
              "key": "[metadata] pkg.torch.onnx.class_hierarchy",
              "value": "['transformers.models.llama.modeling_llama.LlamaForCausalLM', 'transformers.models.llama.modeling_llama.LlamaModel', 'transformers.models.llama.modeling_llama.LlamaDecoderLayer', 'transformers.models.llama.modeling_llama.LlamaAttention', 'aten.view.default']"
            },
            {
              "key": "[metadata] pkg.torch.onnx.fx_node",
              "value": "%view_1 : [num_users=1] = call_function[target=torch.ops.aten.view.default](args = (%linear, [%sym_size_int_68, %sym_size_int_61, -1, 64]), kwargs = {})"
            },
            {
              "key": "[metadata] pkg.torch.onnx.name_scopes",
              "value": "['', 'model', 'model.layers.0', 'model.layers.0.self_attn', 'view_1']"
            },
            {
              "key": "[metadata] pkg.torch.onnx.stack_trace",
              "value": "File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 688, in forward\n    outputs: BaseModelOutputWithPast = self.model(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 453, in forward\n    layer_outputs = decoder_layer(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 308, in forward\n    hidden_states, self_attn_weights = self.self_attn(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 242, in forward\n    query_states = self.q_proj(hidden_states).view(hidden_shape).transpose(1, 2)"
            }
          ],
          "incomingEdges": [
            {
              "sourceNodeId": "node_Shape_12",
              "sourceNodeOutputId": "0",
              "targetNodeInputId": "0"
            },
            {
              "sourceNodeId": "node_Shape_2",
              "sourceNodeOutputId": "0",
              "targetNodeInputId": "1"
            },
            {
              "sourceNodeId": "node_Constant_90",
              "sourceNodeOutputId": "0",
              "targetNodeInputId": "2"
            },
            {
              "sourceNodeId": "node_Constant_345",
              "sourceNodeOutputId": "0",
              "targetNodeInputId": "3"
            }
          ],
          "outputsMetadata": [
            {
              "id": "0",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "val_259"
                },
                {
                  "key": "tensor_shape",
                  "value": "INT64[4]"
                },
                {
                  "key": "param_name",
                  "value": "concat_result"
                }
              ]
            }
          ],
          "inputsMetadata": [
            {
              "id": "0",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "val_6"
                },
                {
                  "key": "tensor_shape",
                  "value": "INT64[1]"
                },
                {
                  "key": "param_name",
                  "value": "inputs"
                }
              ]
            },
            {
              "id": "1",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "val_1"
                },
                {
                  "key": "tensor_shape",
                  "value": "INT64[1]"
                },
                {
                  "key": "param_name",
                  "value": "inputs"
                }
              ]
            },
            {
              "id": "2",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "val_53"
                },
                {
                  "key": "tensor_shape",
                  "value": "INT64[1]"
                },
                {
                  "key": "param_name",
                  "value": "inputs"
                }
              ]
            },
            {
              "id": "3",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "val_258"
                },
                {
                  "key": "tensor_shape",
                  "value": "INT64[1]"
                },
                {
                  "key": "param_name",
                  "value": "inputs"
                }
              ]
            }
          ],
          "style": null,
          "config": null
        },
        {
          "id": "node_Reshape_348",
          "label": "Reshape",
          "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.0: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.0.self_attn: transformers.models.llama.modeling_llama.LlamaAttention/view_1: aten.view.default",
          "subgraphIds": [],
          "attrs": [
            {
              "key": "allowzero",
              "value": "1"
            },
            {
              "key": "[metadata] namespace",
              "value": ": transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.0: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.0.self_attn: transformers.models.llama.modeling_llama.LlamaAttention/view_1: aten.view.default"
            },
            {
              "key": "[metadata] pkg.torch.onnx.class_hierarchy",
              "value": "['transformers.models.llama.modeling_llama.LlamaForCausalLM', 'transformers.models.llama.modeling_llama.LlamaModel', 'transformers.models.llama.modeling_llama.LlamaDecoderLayer', 'transformers.models.llama.modeling_llama.LlamaAttention', 'aten.view.default']"
            },
            {
              "key": "[metadata] pkg.torch.onnx.fx_node",
              "value": "%view_1 : [num_users=1] = call_function[target=torch.ops.aten.view.default](args = (%linear, [%sym_size_int_68, %sym_size_int_61, -1, 64]), kwargs = {})"
            },
            {
              "key": "[metadata] pkg.torch.onnx.name_scopes",
              "value": "['', 'model', 'model.layers.0', 'model.layers.0.self_attn', 'view_1']"
            },
            {
              "key": "[metadata] pkg.torch.onnx.stack_trace",
              "value": "File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 688, in forward\n    outputs: BaseModelOutputWithPast = self.model(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 453, in forward\n    layer_outputs = decoder_layer(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 308, in forward\n    hidden_states, self_attn_weights = self.self_attn(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 242, in forward\n    query_states = self.q_proj(hidden_states).view(hidden_shape).transpose(1, 2)"
            }
          ],
          "incomingEdges": [
            {
              "sourceNodeId": "node_MatMul_340",
              "sourceNodeOutputId": "0",
              "targetNodeInputId": "0"
            },
            {
              "sourceNodeId": "node_Concat_346",
              "sourceNodeOutputId": "0",
              "targetNodeInputId": "1"
            }
          ],
          "outputsMetadata": [
            {
              "id": "0",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "view_1"
                },
                {
                  "key": "tensor_shape",
                  "value": "FLOAT[s34,s16,32,64]"
                },
                {
                  "key": "param_name",
                  "value": "reshaped"
                }
              ]
            }
          ],
          "inputsMetadata": [
            {
              "id": "0",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "linear"
                },
                {
                  "key": "tensor_shape",
                  "value": "FLOAT[s34,s16,2048]"
                },
                {
                  "key": "param_name",
                  "value": "data"
                }
              ]
            },
            {
              "id": "1",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "val_259"
                },
                {
                  "key": "tensor_shape",
                  "value": "INT64[4]"
                },
                {
                  "key": "param_name",
                  "value": "shape"
                }
              ]
            }
          ],
          "style": null,
          "config": null
        },
        {
          "id": "node_Transpose_349",
          "label": "Transpose",
          "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.0: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.0.self_attn: transformers.models.llama.modeling_llama.LlamaAttention/transpose_1: aten.transpose.int",
          "subgraphIds": [],
          "attrs": [
            {
              "key": "perm",
              "value": "[0, 2, 1, 3]"
            },
            {
              "key": "[metadata] namespace",
              "value": ": transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.0: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.0.self_attn: transformers.models.llama.modeling_llama.LlamaAttention/transpose_1: aten.transpose.int"
            },
            {
              "key": "[metadata] pkg.torch.onnx.class_hierarchy",
              "value": "['transformers.models.llama.modeling_llama.LlamaForCausalLM', 'transformers.models.llama.modeling_llama.LlamaModel', 'transformers.models.llama.modeling_llama.LlamaDecoderLayer', 'transformers.models.llama.modeling_llama.LlamaAttention', 'aten.transpose.int']"
            },
            {
              "key": "[metadata] pkg.torch.onnx.fx_node",
              "value": "%transpose_1 : [num_users=3] = call_function[target=torch.ops.aten.transpose.int](args = (%view_1, 1, 2), kwargs = {})"
            },
            {
              "key": "[metadata] pkg.torch.onnx.name_scopes",
              "value": "['', 'model', 'model.layers.0', 'model.layers.0.self_attn', 'transpose_1']"
            },
            {
              "key": "[metadata] pkg.torch.onnx.stack_trace",
              "value": "File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 688, in forward\n    outputs: BaseModelOutputWithPast = self.model(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 453, in forward\n    layer_outputs = decoder_layer(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 308, in forward\n    hidden_states, self_attn_weights = self.self_attn(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 242, in forward\n    query_states = self.q_proj(hidden_states).view(hidden_shape).transpose(1, 2)"
            }
          ],
          "incomingEdges": [
            {
              "sourceNodeId": "node_Reshape_348",
              "sourceNodeOutputId": "0",
              "targetNodeInputId": "0"
            }
          ],
          "outputsMetadata": [
            {
              "id": "0",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "transpose_1"
                },
                {
                  "key": "tensor_shape",
                  "value": "FLOAT[s34,32,s16,64]"
                },
                {
                  "key": "param_name",
                  "value": "transposed"
                }
              ]
            }
          ],
          "inputsMetadata": [
            {
              "id": "0",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "view_1"
                },
                {
                  "key": "tensor_shape",
                  "value": "FLOAT[s34,s16,32,64]"
                },
                {
                  "key": "param_name",
                  "value": "data"
                }
              ]
            }
          ],
          "style": null,
          "config": null
        },
        {
          "id": "node_Transpose_350",
          "label": "Transpose",
          "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.0: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.0.self_attn: transformers.models.llama.modeling_llama.LlamaAttention/model.layers.0.self_attn.k_proj: torch.nn.modules.linear.Linear/linear_1: aten.linear.default",
          "subgraphIds": [],
          "attrs": [
            {
              "key": "perm",
              "value": "[1, 0]"
            },
            {
              "key": "[metadata] namespace",
              "value": ": transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.0: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.0.self_attn: transformers.models.llama.modeling_llama.LlamaAttention/model.layers.0.self_attn.k_proj: torch.nn.modules.linear.Linear/linear_1: aten.linear.default"
            },
            {
              "key": "[metadata] pkg.torch.onnx.class_hierarchy",
              "value": "['transformers.models.llama.modeling_llama.LlamaForCausalLM', 'transformers.models.llama.modeling_llama.LlamaModel', 'transformers.models.llama.modeling_llama.LlamaDecoderLayer', 'transformers.models.llama.modeling_llama.LlamaAttention', 'torch.nn.modules.linear.Linear', 'aten.linear.default']"
            },
            {
              "key": "[metadata] pkg.torch.onnx.fx_node",
              "value": "%linear_1 : [num_users=1] = call_function[target=torch.ops.aten.linear.default](args = (%mul_173, %p_model_layers_0_self_attn_k_proj_weight), kwargs = {})"
            },
            {
              "key": "[metadata] pkg.torch.onnx.name_scopes",
              "value": "['', 'model', 'model.layers.0', 'model.layers.0.self_attn', 'model.layers.0.self_attn.k_proj', 'linear_1']"
            },
            {
              "key": "[metadata] pkg.torch.onnx.stack_trace",
              "value": "File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 688, in forward\n    outputs: BaseModelOutputWithPast = self.model(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 453, in forward\n    layer_outputs = decoder_layer(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 308, in forward\n    hidden_states, self_attn_weights = self.self_attn(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 243, in forward\n    key_states = self.k_proj(hidden_states).view(hidden_shape).transpose(1, 2)\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/torch/nn/modules/linear.py\", line 125, in forward\n    return F.linear(input, self.weight, self.bias)"
            }
          ],
          "incomingEdges": [
            {
              "sourceNodeId": "[value] model.layers.0.self_attn.k_proj.weight",
              "sourceNodeOutputId": "0",
              "targetNodeInputId": "0"
            }
          ],
          "outputsMetadata": [
            {
              "id": "0",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "val_261"
                },
                {
                  "key": "tensor_shape",
                  "value": "FLOAT[2048,2048]"
                },
                {
                  "key": "param_name",
                  "value": "transposed"
                }
              ]
            }
          ],
          "inputsMetadata": [
            {
              "id": "0",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "model.layers.0.self_attn.k_proj.weight"
                },
                {
                  "key": "tensor_shape",
                  "value": "FLOAT[2048,2048]"
                },
                {
                  "key": "param_name",
                  "value": "data"
                }
              ]
            }
          ],
          "style": null,
          "config": null
        },
        {
          "id": "node_MatMul_351",
          "label": "MatMul",
          "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.0: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.0.self_attn: transformers.models.llama.modeling_llama.LlamaAttention/model.layers.0.self_attn.k_proj: torch.nn.modules.linear.Linear/linear_1: aten.linear.default",
          "subgraphIds": [],
          "attrs": [
            {
              "key": "[metadata] namespace",
              "value": ": transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.0: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.0.self_attn: transformers.models.llama.modeling_llama.LlamaAttention/model.layers.0.self_attn.k_proj: torch.nn.modules.linear.Linear/linear_1: aten.linear.default"
            },
            {
              "key": "[metadata] pkg.torch.onnx.class_hierarchy",
              "value": "['transformers.models.llama.modeling_llama.LlamaForCausalLM', 'transformers.models.llama.modeling_llama.LlamaModel', 'transformers.models.llama.modeling_llama.LlamaDecoderLayer', 'transformers.models.llama.modeling_llama.LlamaAttention', 'torch.nn.modules.linear.Linear', 'aten.linear.default']"
            },
            {
              "key": "[metadata] pkg.torch.onnx.fx_node",
              "value": "%linear_1 : [num_users=1] = call_function[target=torch.ops.aten.linear.default](args = (%mul_173, %p_model_layers_0_self_attn_k_proj_weight), kwargs = {})"
            },
            {
              "key": "[metadata] pkg.torch.onnx.name_scopes",
              "value": "['', 'model', 'model.layers.0', 'model.layers.0.self_attn', 'model.layers.0.self_attn.k_proj', 'linear_1']"
            },
            {
              "key": "[metadata] pkg.torch.onnx.stack_trace",
              "value": "File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 688, in forward\n    outputs: BaseModelOutputWithPast = self.model(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 453, in forward\n    layer_outputs = decoder_layer(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 308, in forward\n    hidden_states, self_attn_weights = self.self_attn(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 243, in forward\n    key_states = self.k_proj(hidden_states).view(hidden_shape).transpose(1, 2)\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/torch/nn/modules/linear.py\", line 125, in forward\n    return F.linear(input, self.weight, self.bias)"
            }
          ],
          "incomingEdges": [
            {
              "sourceNodeId": "node_Mul_338",
              "sourceNodeOutputId": "0",
              "targetNodeInputId": "0"
            },
            {
              "sourceNodeId": "node_Transpose_350",
              "sourceNodeOutputId": "0",
              "targetNodeInputId": "1"
            }
          ],
          "outputsMetadata": [
            {
              "id": "0",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "linear_1"
                },
                {
                  "key": "tensor_shape",
                  "value": "FLOAT[s34,s16,2048]"
                },
                {
                  "key": "param_name",
                  "value": "Y"
                }
              ]
            }
          ],
          "inputsMetadata": [
            {
              "id": "0",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "mul_173"
                },
                {
                  "key": "tensor_shape",
                  "value": "FLOAT[s34,s16,2048]"
                },
                {
                  "key": "param_name",
                  "value": "A"
                }
              ]
            },
            {
              "id": "1",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "val_261"
                },
                {
                  "key": "tensor_shape",
                  "value": "FLOAT[2048,2048]"
                },
                {
                  "key": "param_name",
                  "value": "B"
                }
              ]
            }
          ],
          "style": null,
          "config": null
        },
        {
          "id": "node_Concat_356",
          "label": "Concat",
          "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.0: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.0.self_attn: transformers.models.llama.modeling_llama.LlamaAttention/view_2: aten.view.default",
          "subgraphIds": [],
          "attrs": [
            {
              "key": "axis",
              "value": "0"
            },
            {
              "key": "[metadata] namespace",
              "value": ": transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.0: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.0.self_attn: transformers.models.llama.modeling_llama.LlamaAttention/view_2: aten.view.default"
            },
            {
              "key": "[metadata] pkg.torch.onnx.class_hierarchy",
              "value": "['transformers.models.llama.modeling_llama.LlamaForCausalLM', 'transformers.models.llama.modeling_llama.LlamaModel', 'transformers.models.llama.modeling_llama.LlamaDecoderLayer', 'transformers.models.llama.modeling_llama.LlamaAttention', 'aten.view.default']"
            },
            {
              "key": "[metadata] pkg.torch.onnx.fx_node",
              "value": "%view_2 : [num_users=1] = call_function[target=torch.ops.aten.view.default](args = (%linear_1, [%sym_size_int_68, %sym_size_int_61, -1, 64]), kwargs = {})"
            },
            {
              "key": "[metadata] pkg.torch.onnx.name_scopes",
              "value": "['', 'model', 'model.layers.0', 'model.layers.0.self_attn', 'view_2']"
            },
            {
              "key": "[metadata] pkg.torch.onnx.stack_trace",
              "value": "File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 688, in forward\n    outputs: BaseModelOutputWithPast = self.model(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 453, in forward\n    layer_outputs = decoder_layer(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 308, in forward\n    hidden_states, self_attn_weights = self.self_attn(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 243, in forward\n    key_states = self.k_proj(hidden_states).view(hidden_shape).transpose(1, 2)"
            }
          ],
          "incomingEdges": [
            {
              "sourceNodeId": "node_Shape_12",
              "sourceNodeOutputId": "0",
              "targetNodeInputId": "0"
            },
            {
              "sourceNodeId": "node_Shape_2",
              "sourceNodeOutputId": "0",
              "targetNodeInputId": "1"
            },
            {
              "sourceNodeId": "node_Constant_90",
              "sourceNodeOutputId": "0",
              "targetNodeInputId": "2"
            },
            {
              "sourceNodeId": "node_Constant_345",
              "sourceNodeOutputId": "0",
              "targetNodeInputId": "3"
            }
          ],
          "outputsMetadata": [
            {
              "id": "0",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "val_266"
                },
                {
                  "key": "tensor_shape",
                  "value": "INT64[4]"
                },
                {
                  "key": "param_name",
                  "value": "concat_result"
                }
              ]
            }
          ],
          "inputsMetadata": [
            {
              "id": "0",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "val_6"
                },
                {
                  "key": "tensor_shape",
                  "value": "INT64[1]"
                },
                {
                  "key": "param_name",
                  "value": "inputs"
                }
              ]
            },
            {
              "id": "1",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "val_1"
                },
                {
                  "key": "tensor_shape",
                  "value": "INT64[1]"
                },
                {
                  "key": "param_name",
                  "value": "inputs"
                }
              ]
            },
            {
              "id": "2",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "val_53"
                },
                {
                  "key": "tensor_shape",
                  "value": "INT64[1]"
                },
                {
                  "key": "param_name",
                  "value": "inputs"
                }
              ]
            },
            {
              "id": "3",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "val_258"
                },
                {
                  "key": "tensor_shape",
                  "value": "INT64[1]"
                },
                {
                  "key": "param_name",
                  "value": "inputs"
                }
              ]
            }
          ],
          "style": null,
          "config": null
        },
        {
          "id": "node_Reshape_358",
          "label": "Reshape",
          "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.0: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.0.self_attn: transformers.models.llama.modeling_llama.LlamaAttention/view_2: aten.view.default",
          "subgraphIds": [],
          "attrs": [
            {
              "key": "allowzero",
              "value": "1"
            },
            {
              "key": "[metadata] namespace",
              "value": ": transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.0: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.0.self_attn: transformers.models.llama.modeling_llama.LlamaAttention/view_2: aten.view.default"
            },
            {
              "key": "[metadata] pkg.torch.onnx.class_hierarchy",
              "value": "['transformers.models.llama.modeling_llama.LlamaForCausalLM', 'transformers.models.llama.modeling_llama.LlamaModel', 'transformers.models.llama.modeling_llama.LlamaDecoderLayer', 'transformers.models.llama.modeling_llama.LlamaAttention', 'aten.view.default']"
            },
            {
              "key": "[metadata] pkg.torch.onnx.fx_node",
              "value": "%view_2 : [num_users=1] = call_function[target=torch.ops.aten.view.default](args = (%linear_1, [%sym_size_int_68, %sym_size_int_61, -1, 64]), kwargs = {})"
            },
            {
              "key": "[metadata] pkg.torch.onnx.name_scopes",
              "value": "['', 'model', 'model.layers.0', 'model.layers.0.self_attn', 'view_2']"
            },
            {
              "key": "[metadata] pkg.torch.onnx.stack_trace",
              "value": "File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 688, in forward\n    outputs: BaseModelOutputWithPast = self.model(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 453, in forward\n    layer_outputs = decoder_layer(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 308, in forward\n    hidden_states, self_attn_weights = self.self_attn(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 243, in forward\n    key_states = self.k_proj(hidden_states).view(hidden_shape).transpose(1, 2)"
            }
          ],
          "incomingEdges": [
            {
              "sourceNodeId": "node_MatMul_351",
              "sourceNodeOutputId": "0",
              "targetNodeInputId": "0"
            },
            {
              "sourceNodeId": "node_Concat_356",
              "sourceNodeOutputId": "0",
              "targetNodeInputId": "1"
            }
          ],
          "outputsMetadata": [
            {
              "id": "0",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "view_2"
                },
                {
                  "key": "tensor_shape",
                  "value": "FLOAT[s34,s16,32,64]"
                },
                {
                  "key": "param_name",
                  "value": "reshaped"
                }
              ]
            }
          ],
          "inputsMetadata": [
            {
              "id": "0",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "linear_1"
                },
                {
                  "key": "tensor_shape",
                  "value": "FLOAT[s34,s16,2048]"
                },
                {
                  "key": "param_name",
                  "value": "data"
                }
              ]
            },
            {
              "id": "1",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "val_266"
                },
                {
                  "key": "tensor_shape",
                  "value": "INT64[4]"
                },
                {
                  "key": "param_name",
                  "value": "shape"
                }
              ]
            }
          ],
          "style": null,
          "config": null
        },
        {
          "id": "node_Transpose_359",
          "label": "Transpose",
          "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.0: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.0.self_attn: transformers.models.llama.modeling_llama.LlamaAttention/transpose_2: aten.transpose.int",
          "subgraphIds": [],
          "attrs": [
            {
              "key": "perm",
              "value": "[0, 2, 1, 3]"
            },
            {
              "key": "[metadata] namespace",
              "value": ": transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.0: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.0.self_attn: transformers.models.llama.modeling_llama.LlamaAttention/transpose_2: aten.transpose.int"
            },
            {
              "key": "[metadata] pkg.torch.onnx.class_hierarchy",
              "value": "['transformers.models.llama.modeling_llama.LlamaForCausalLM', 'transformers.models.llama.modeling_llama.LlamaModel', 'transformers.models.llama.modeling_llama.LlamaDecoderLayer', 'transformers.models.llama.modeling_llama.LlamaAttention', 'aten.transpose.int']"
            },
            {
              "key": "[metadata] pkg.torch.onnx.fx_node",
              "value": "%transpose_2 : [num_users=3] = call_function[target=torch.ops.aten.transpose.int](args = (%view_2, 1, 2), kwargs = {})"
            },
            {
              "key": "[metadata] pkg.torch.onnx.name_scopes",
              "value": "['', 'model', 'model.layers.0', 'model.layers.0.self_attn', 'transpose_2']"
            },
            {
              "key": "[metadata] pkg.torch.onnx.stack_trace",
              "value": "File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 688, in forward\n    outputs: BaseModelOutputWithPast = self.model(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 453, in forward\n    layer_outputs = decoder_layer(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 308, in forward\n    hidden_states, self_attn_weights = self.self_attn(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 243, in forward\n    key_states = self.k_proj(hidden_states).view(hidden_shape).transpose(1, 2)"
            }
          ],
          "incomingEdges": [
            {
              "sourceNodeId": "node_Reshape_358",
              "sourceNodeOutputId": "0",
              "targetNodeInputId": "0"
            }
          ],
          "outputsMetadata": [
            {
              "id": "0",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "transpose_2"
                },
                {
                  "key": "tensor_shape",
                  "value": "FLOAT[s34,32,s16,64]"
                },
                {
                  "key": "param_name",
                  "value": "transposed"
                }
              ]
            }
          ],
          "inputsMetadata": [
            {
              "id": "0",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "view_2"
                },
                {
                  "key": "tensor_shape",
                  "value": "FLOAT[s34,s16,32,64]"
                },
                {
                  "key": "param_name",
                  "value": "data"
                }
              ]
            }
          ],
          "style": null,
          "config": null
        },
        {
          "id": "node_Transpose_360",
          "label": "Transpose",
          "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.0: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.0.self_attn: transformers.models.llama.modeling_llama.LlamaAttention/model.layers.0.self_attn.v_proj: torch.nn.modules.linear.Linear/linear_2: aten.linear.default",
          "subgraphIds": [],
          "attrs": [
            {
              "key": "perm",
              "value": "[1, 0]"
            },
            {
              "key": "[metadata] namespace",
              "value": ": transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.0: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.0.self_attn: transformers.models.llama.modeling_llama.LlamaAttention/model.layers.0.self_attn.v_proj: torch.nn.modules.linear.Linear/linear_2: aten.linear.default"
            },
            {
              "key": "[metadata] pkg.torch.onnx.class_hierarchy",
              "value": "['transformers.models.llama.modeling_llama.LlamaForCausalLM', 'transformers.models.llama.modeling_llama.LlamaModel', 'transformers.models.llama.modeling_llama.LlamaDecoderLayer', 'transformers.models.llama.modeling_llama.LlamaAttention', 'torch.nn.modules.linear.Linear', 'aten.linear.default']"
            },
            {
              "key": "[metadata] pkg.torch.onnx.fx_node",
              "value": "%linear_2 : [num_users=1] = call_function[target=torch.ops.aten.linear.default](args = (%mul_173, %p_model_layers_0_self_attn_v_proj_weight), kwargs = {})"
            },
            {
              "key": "[metadata] pkg.torch.onnx.name_scopes",
              "value": "['', 'model', 'model.layers.0', 'model.layers.0.self_attn', 'model.layers.0.self_attn.v_proj', 'linear_2']"
            },
            {
              "key": "[metadata] pkg.torch.onnx.stack_trace",
              "value": "File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 688, in forward\n    outputs: BaseModelOutputWithPast = self.model(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 453, in forward\n    layer_outputs = decoder_layer(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 308, in forward\n    hidden_states, self_attn_weights = self.self_attn(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 244, in forward\n    value_states = self.v_proj(hidden_states).view(hidden_shape).transpose(1, 2)\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/torch/nn/modules/linear.py\", line 125, in forward\n    return F.linear(input, self.weight, self.bias)"
            }
          ],
          "incomingEdges": [
            {
              "sourceNodeId": "[value] model.layers.0.self_attn.v_proj.weight",
              "sourceNodeOutputId": "0",
              "targetNodeInputId": "0"
            }
          ],
          "outputsMetadata": [
            {
              "id": "0",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "val_268"
                },
                {
                  "key": "tensor_shape",
                  "value": "FLOAT[2048,2048]"
                },
                {
                  "key": "param_name",
                  "value": "transposed"
                }
              ]
            }
          ],
          "inputsMetadata": [
            {
              "id": "0",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "model.layers.0.self_attn.v_proj.weight"
                },
                {
                  "key": "tensor_shape",
                  "value": "FLOAT[2048,2048]"
                },
                {
                  "key": "param_name",
                  "value": "data"
                }
              ]
            }
          ],
          "style": null,
          "config": null
        },
        {
          "id": "node_MatMul_361",
          "label": "MatMul",
          "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.0: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.0.self_attn: transformers.models.llama.modeling_llama.LlamaAttention/model.layers.0.self_attn.v_proj: torch.nn.modules.linear.Linear/linear_2: aten.linear.default",
          "subgraphIds": [],
          "attrs": [
            {
              "key": "[metadata] namespace",
              "value": ": transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.0: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.0.self_attn: transformers.models.llama.modeling_llama.LlamaAttention/model.layers.0.self_attn.v_proj: torch.nn.modules.linear.Linear/linear_2: aten.linear.default"
            },
            {
              "key": "[metadata] pkg.torch.onnx.class_hierarchy",
              "value": "['transformers.models.llama.modeling_llama.LlamaForCausalLM', 'transformers.models.llama.modeling_llama.LlamaModel', 'transformers.models.llama.modeling_llama.LlamaDecoderLayer', 'transformers.models.llama.modeling_llama.LlamaAttention', 'torch.nn.modules.linear.Linear', 'aten.linear.default']"
            },
            {
              "key": "[metadata] pkg.torch.onnx.fx_node",
              "value": "%linear_2 : [num_users=1] = call_function[target=torch.ops.aten.linear.default](args = (%mul_173, %p_model_layers_0_self_attn_v_proj_weight), kwargs = {})"
            },
            {
              "key": "[metadata] pkg.torch.onnx.name_scopes",
              "value": "['', 'model', 'model.layers.0', 'model.layers.0.self_attn', 'model.layers.0.self_attn.v_proj', 'linear_2']"
            },
            {
              "key": "[metadata] pkg.torch.onnx.stack_trace",
              "value": "File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 688, in forward\n    outputs: BaseModelOutputWithPast = self.model(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 453, in forward\n    layer_outputs = decoder_layer(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 308, in forward\n    hidden_states, self_attn_weights = self.self_attn(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 244, in forward\n    value_states = self.v_proj(hidden_states).view(hidden_shape).transpose(1, 2)\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/torch/nn/modules/linear.py\", line 125, in forward\n    return F.linear(input, self.weight, self.bias)"
            }
          ],
          "incomingEdges": [
            {
              "sourceNodeId": "node_Mul_338",
              "sourceNodeOutputId": "0",
              "targetNodeInputId": "0"
            },
            {
              "sourceNodeId": "node_Transpose_360",
              "sourceNodeOutputId": "0",
              "targetNodeInputId": "1"
            }
          ],
          "outputsMetadata": [
            {
              "id": "0",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "linear_2"
                },
                {
                  "key": "tensor_shape",
                  "value": "FLOAT[s34,s16,2048]"
                },
                {
                  "key": "param_name",
                  "value": "Y"
                }
              ]
            }
          ],
          "inputsMetadata": [
            {
              "id": "0",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "mul_173"
                },
                {
                  "key": "tensor_shape",
                  "value": "FLOAT[s34,s16,2048]"
                },
                {
                  "key": "param_name",
                  "value": "A"
                }
              ]
            },
            {
              "id": "1",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "val_268"
                },
                {
                  "key": "tensor_shape",
                  "value": "FLOAT[2048,2048]"
                },
                {
                  "key": "param_name",
                  "value": "B"
                }
              ]
            }
          ],
          "style": null,
          "config": null
        },
        {
          "id": "node_Concat_366",
          "label": "Concat",
          "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.0: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.0.self_attn: transformers.models.llama.modeling_llama.LlamaAttention/view_3: aten.view.default",
          "subgraphIds": [],
          "attrs": [
            {
              "key": "axis",
              "value": "0"
            },
            {
              "key": "[metadata] namespace",
              "value": ": transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.0: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.0.self_attn: transformers.models.llama.modeling_llama.LlamaAttention/view_3: aten.view.default"
            },
            {
              "key": "[metadata] pkg.torch.onnx.class_hierarchy",
              "value": "['transformers.models.llama.modeling_llama.LlamaForCausalLM', 'transformers.models.llama.modeling_llama.LlamaModel', 'transformers.models.llama.modeling_llama.LlamaDecoderLayer', 'transformers.models.llama.modeling_llama.LlamaAttention', 'aten.view.default']"
            },
            {
              "key": "[metadata] pkg.torch.onnx.fx_node",
              "value": "%view_3 : [num_users=1] = call_function[target=torch.ops.aten.view.default](args = (%linear_2, [%sym_size_int_68, %sym_size_int_61, -1, 64]), kwargs = {})"
            },
            {
              "key": "[metadata] pkg.torch.onnx.name_scopes",
              "value": "['', 'model', 'model.layers.0', 'model.layers.0.self_attn', 'view_3']"
            },
            {
              "key": "[metadata] pkg.torch.onnx.stack_trace",
              "value": "File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 688, in forward\n    outputs: BaseModelOutputWithPast = self.model(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 453, in forward\n    layer_outputs = decoder_layer(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 308, in forward\n    hidden_states, self_attn_weights = self.self_attn(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 244, in forward\n    value_states = self.v_proj(hidden_states).view(hidden_shape).transpose(1, 2)"
            }
          ],
          "incomingEdges": [
            {
              "sourceNodeId": "node_Shape_12",
              "sourceNodeOutputId": "0",
              "targetNodeInputId": "0"
            },
            {
              "sourceNodeId": "node_Shape_2",
              "sourceNodeOutputId": "0",
              "targetNodeInputId": "1"
            },
            {
              "sourceNodeId": "node_Constant_90",
              "sourceNodeOutputId": "0",
              "targetNodeInputId": "2"
            },
            {
              "sourceNodeId": "node_Constant_345",
              "sourceNodeOutputId": "0",
              "targetNodeInputId": "3"
            }
          ],
          "outputsMetadata": [
            {
              "id": "0",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "val_273"
                },
                {
                  "key": "tensor_shape",
                  "value": "INT64[4]"
                },
                {
                  "key": "param_name",
                  "value": "concat_result"
                }
              ]
            }
          ],
          "inputsMetadata": [
            {
              "id": "0",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "val_6"
                },
                {
                  "key": "tensor_shape",
                  "value": "INT64[1]"
                },
                {
                  "key": "param_name",
                  "value": "inputs"
                }
              ]
            },
            {
              "id": "1",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "val_1"
                },
                {
                  "key": "tensor_shape",
                  "value": "INT64[1]"
                },
                {
                  "key": "param_name",
                  "value": "inputs"
                }
              ]
            },
            {
              "id": "2",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "val_53"
                },
                {
                  "key": "tensor_shape",
                  "value": "INT64[1]"
                },
                {
                  "key": "param_name",
                  "value": "inputs"
                }
              ]
            },
            {
              "id": "3",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "val_258"
                },
                {
                  "key": "tensor_shape",
                  "value": "INT64[1]"
                },
                {
                  "key": "param_name",
                  "value": "inputs"
                }
              ]
            }
          ],
          "style": null,
          "config": null
        },
        {
          "id": "node_Reshape_368",
          "label": "Reshape",
          "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.0: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.0.self_attn: transformers.models.llama.modeling_llama.LlamaAttention/view_3: aten.view.default",
          "subgraphIds": [],
          "attrs": [
            {
              "key": "allowzero",
              "value": "1"
            },
            {
              "key": "[metadata] namespace",
              "value": ": transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.0: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.0.self_attn: transformers.models.llama.modeling_llama.LlamaAttention/view_3: aten.view.default"
            },
            {
              "key": "[metadata] pkg.torch.onnx.class_hierarchy",
              "value": "['transformers.models.llama.modeling_llama.LlamaForCausalLM', 'transformers.models.llama.modeling_llama.LlamaModel', 'transformers.models.llama.modeling_llama.LlamaDecoderLayer', 'transformers.models.llama.modeling_llama.LlamaAttention', 'aten.view.default']"
            },
            {
              "key": "[metadata] pkg.torch.onnx.fx_node",
              "value": "%view_3 : [num_users=1] = call_function[target=torch.ops.aten.view.default](args = (%linear_2, [%sym_size_int_68, %sym_size_int_61, -1, 64]), kwargs = {})"
            },
            {
              "key": "[metadata] pkg.torch.onnx.name_scopes",
              "value": "['', 'model', 'model.layers.0', 'model.layers.0.self_attn', 'view_3']"
            },
            {
              "key": "[metadata] pkg.torch.onnx.stack_trace",
              "value": "File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 688, in forward\n    outputs: BaseModelOutputWithPast = self.model(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 453, in forward\n    layer_outputs = decoder_layer(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 308, in forward\n    hidden_states, self_attn_weights = self.self_attn(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 244, in forward\n    value_states = self.v_proj(hidden_states).view(hidden_shape).transpose(1, 2)"
            }
          ],
          "incomingEdges": [
            {
              "sourceNodeId": "node_MatMul_361",
              "sourceNodeOutputId": "0",
              "targetNodeInputId": "0"
            },
            {
              "sourceNodeId": "node_Concat_366",
              "sourceNodeOutputId": "0",
              "targetNodeInputId": "1"
            }
          ],
          "outputsMetadata": [
            {
              "id": "0",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "view_3"
                },
                {
                  "key": "tensor_shape",
                  "value": "FLOAT[s34,s16,32,64]"
                },
                {
                  "key": "param_name",
                  "value": "reshaped"
                }
              ]
            }
          ],
          "inputsMetadata": [
            {
              "id": "0",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "linear_2"
                },
                {
                  "key": "tensor_shape",
                  "value": "FLOAT[s34,s16,2048]"
                },
                {
                  "key": "param_name",
                  "value": "data"
                }
              ]
            },
            {
              "id": "1",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "val_273"
                },
                {
                  "key": "tensor_shape",
                  "value": "INT64[4]"
                },
                {
                  "key": "param_name",
                  "value": "shape"
                }
              ]
            }
          ],
          "style": null,
          "config": null
        },
        {
          "id": "node_Transpose_369",
          "label": "Transpose",
          "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.0: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.0.self_attn: transformers.models.llama.modeling_llama.LlamaAttention/transpose_3: aten.transpose.int",
          "subgraphIds": [],
          "attrs": [
            {
              "key": "perm",
              "value": "[0, 2, 1, 3]"
            },
            {
              "key": "[metadata] namespace",
              "value": ": transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.0: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.0.self_attn: transformers.models.llama.modeling_llama.LlamaAttention/transpose_3: aten.transpose.int"
            },
            {
              "key": "[metadata] pkg.torch.onnx.class_hierarchy",
              "value": "['transformers.models.llama.modeling_llama.LlamaForCausalLM', 'transformers.models.llama.modeling_llama.LlamaModel', 'transformers.models.llama.modeling_llama.LlamaDecoderLayer', 'transformers.models.llama.modeling_llama.LlamaAttention', 'aten.transpose.int']"
            },
            {
              "key": "[metadata] pkg.torch.onnx.fx_node",
              "value": "%transpose_3 : [num_users=1] = call_function[target=torch.ops.aten.transpose.int](args = (%view_3, 1, 2), kwargs = {})"
            },
            {
              "key": "[metadata] pkg.torch.onnx.name_scopes",
              "value": "['', 'model', 'model.layers.0', 'model.layers.0.self_attn', 'transpose_3']"
            },
            {
              "key": "[metadata] pkg.torch.onnx.stack_trace",
              "value": "File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 688, in forward\n    outputs: BaseModelOutputWithPast = self.model(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 453, in forward\n    layer_outputs = decoder_layer(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 308, in forward\n    hidden_states, self_attn_weights = self.self_attn(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 244, in forward\n    value_states = self.v_proj(hidden_states).view(hidden_shape).transpose(1, 2)"
            }
          ],
          "incomingEdges": [
            {
              "sourceNodeId": "node_Reshape_368",
              "sourceNodeOutputId": "0",
              "targetNodeInputId": "0"
            }
          ],
          "outputsMetadata": [
            {
              "id": "0",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "transpose_3"
                },
                {
                  "key": "tensor_shape",
                  "value": "FLOAT[s34,32,s16,64]"
                },
                {
                  "key": "param_name",
                  "value": "transposed"
                }
              ]
            }
          ],
          "inputsMetadata": [
            {
              "id": "0",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "view_3"
                },
                {
                  "key": "tensor_shape",
                  "value": "FLOAT[s34,s16,32,64]"
                },
                {
                  "key": "param_name",
                  "value": "data"
                }
              ]
            }
          ],
          "style": null,
          "config": null
        },
        {
          "id": "node_Unsqueeze_370",
          "label": "Unsqueeze",
          "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.0: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.0.self_attn: transformers.models.llama.modeling_llama.LlamaAttention/unsqueeze_10: aten.unsqueeze.default",
          "subgraphIds": [],
          "attrs": [
            {
              "key": "[metadata] namespace",
              "value": ": transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.0: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.0.self_attn: transformers.models.llama.modeling_llama.LlamaAttention/unsqueeze_10: aten.unsqueeze.default"
            },
            {
              "key": "[metadata] pkg.torch.onnx.class_hierarchy",
              "value": "['transformers.models.llama.modeling_llama.LlamaForCausalLM', 'transformers.models.llama.modeling_llama.LlamaModel', 'transformers.models.llama.modeling_llama.LlamaDecoderLayer', 'transformers.models.llama.modeling_llama.LlamaAttention', 'aten.unsqueeze.default']"
            },
            {
              "key": "[metadata] pkg.torch.onnx.fx_node",
              "value": "%unsqueeze_10 : [num_users=2] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%mul_149, 1), kwargs = {})"
            },
            {
              "key": "[metadata] pkg.torch.onnx.name_scopes",
              "value": "['', 'model', 'model.layers.0', 'model.layers.0.self_attn', 'unsqueeze_10']"
            },
            {
              "key": "[metadata] pkg.torch.onnx.stack_trace",
              "value": "File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 688, in forward\n    outputs: BaseModelOutputWithPast = self.model(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 453, in forward\n    layer_outputs = decoder_layer(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 308, in forward\n    hidden_states, self_attn_weights = self.self_attn(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 247, in forward\n    query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)"
            }
          ],
          "incomingEdges": [
            {
              "sourceNodeId": "node_Cos_324",
              "sourceNodeOutputId": "0",
              "targetNodeInputId": "0"
            },
            {
              "sourceNodeId": "node_Constant_60",
              "sourceNodeOutputId": "0",
              "targetNodeInputId": "1"
            }
          ],
          "outputsMetadata": [
            {
              "id": "0",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "unsqueeze_10"
                },
                {
                  "key": "tensor_shape",
                  "value": "FLOAT[1,1,s16,64]"
                },
                {
                  "key": "param_name",
                  "value": "expanded"
                }
              ]
            }
          ],
          "inputsMetadata": [
            {
              "id": "0",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "cos"
                },
                {
                  "key": "tensor_shape",
                  "value": "FLOAT[1,s16,64]"
                },
                {
                  "key": "param_name",
                  "value": "data"
                }
              ]
            },
            {
              "id": "1",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "val_26"
                },
                {
                  "key": "tensor_shape",
                  "value": "INT64[1]"
                },
                {
                  "key": "param_name",
                  "value": "axes"
                }
              ]
            }
          ],
          "style": null,
          "config": null
        },
        {
          "id": "node_Unsqueeze_371",
          "label": "Unsqueeze",
          "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.0: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.0.self_attn: transformers.models.llama.modeling_llama.LlamaAttention/unsqueeze_11: aten.unsqueeze.default",
          "subgraphIds": [],
          "attrs": [
            {
              "key": "[metadata] namespace",
              "value": ": transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.0: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.0.self_attn: transformers.models.llama.modeling_llama.LlamaAttention/unsqueeze_11: aten.unsqueeze.default"
            },
            {
              "key": "[metadata] pkg.torch.onnx.class_hierarchy",
              "value": "['transformers.models.llama.modeling_llama.LlamaForCausalLM', 'transformers.models.llama.modeling_llama.LlamaModel', 'transformers.models.llama.modeling_llama.LlamaDecoderLayer', 'transformers.models.llama.modeling_llama.LlamaAttention', 'aten.unsqueeze.default']"
            },
            {
              "key": "[metadata] pkg.torch.onnx.fx_node",
              "value": "%unsqueeze_11 : [num_users=2] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%mul_156, 1), kwargs = {})"
            },
            {
              "key": "[metadata] pkg.torch.onnx.name_scopes",
              "value": "['', 'model', 'model.layers.0', 'model.layers.0.self_attn', 'unsqueeze_11']"
            },
            {
              "key": "[metadata] pkg.torch.onnx.stack_trace",
              "value": "File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 688, in forward\n    outputs: BaseModelOutputWithPast = self.model(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 453, in forward\n    layer_outputs = decoder_layer(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 308, in forward\n    hidden_states, self_attn_weights = self.self_attn(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 247, in forward\n    query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)"
            }
          ],
          "incomingEdges": [
            {
              "sourceNodeId": "node_Sin_326",
              "sourceNodeOutputId": "0",
              "targetNodeInputId": "0"
            },
            {
              "sourceNodeId": "node_Constant_60",
              "sourceNodeOutputId": "0",
              "targetNodeInputId": "1"
            }
          ],
          "outputsMetadata": [
            {
              "id": "0",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "unsqueeze_11"
                },
                {
                  "key": "tensor_shape",
                  "value": "FLOAT[1,1,s16,64]"
                },
                {
                  "key": "param_name",
                  "value": "expanded"
                }
              ]
            }
          ],
          "inputsMetadata": [
            {
              "id": "0",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "sin"
                },
                {
                  "key": "tensor_shape",
                  "value": "FLOAT[1,s16,64]"
                },
                {
                  "key": "param_name",
                  "value": "data"
                }
              ]
            },
            {
              "id": "1",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "val_26"
                },
                {
                  "key": "tensor_shape",
                  "value": "INT64[1]"
                },
                {
                  "key": "param_name",
                  "value": "axes"
                }
              ]
            }
          ],
          "style": null,
          "config": null
        },
        {
          "id": "node_Mul_372",
          "label": "Mul",
          "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.0: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.0.self_attn: transformers.models.llama.modeling_llama.LlamaAttention/mul_212: aten.mul.Tensor",
          "subgraphIds": [],
          "attrs": [
            {
              "key": "[metadata] namespace",
              "value": ": transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.0: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.0.self_attn: transformers.models.llama.modeling_llama.LlamaAttention/mul_212: aten.mul.Tensor"
            },
            {
              "key": "[metadata] pkg.torch.onnx.class_hierarchy",
              "value": "['transformers.models.llama.modeling_llama.LlamaForCausalLM', 'transformers.models.llama.modeling_llama.LlamaModel', 'transformers.models.llama.modeling_llama.LlamaDecoderLayer', 'transformers.models.llama.modeling_llama.LlamaAttention', 'aten.mul.Tensor']"
            },
            {
              "key": "[metadata] pkg.torch.onnx.fx_node",
              "value": "%mul_212 : [num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%transpose_1, %unsqueeze_10), kwargs = {})"
            },
            {
              "key": "[metadata] pkg.torch.onnx.name_scopes",
              "value": "['', 'model', 'model.layers.0', 'model.layers.0.self_attn', 'mul_212']"
            },
            {
              "key": "[metadata] pkg.torch.onnx.stack_trace",
              "value": "File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 688, in forward\n    outputs: BaseModelOutputWithPast = self.model(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 453, in forward\n    layer_outputs = decoder_layer(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 308, in forward\n    hidden_states, self_attn_weights = self.self_attn(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 247, in forward\n    query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)"
            }
          ],
          "incomingEdges": [
            {
              "sourceNodeId": "node_Transpose_349",
              "sourceNodeOutputId": "0",
              "targetNodeInputId": "0"
            },
            {
              "sourceNodeId": "node_Unsqueeze_370",
              "sourceNodeOutputId": "0",
              "targetNodeInputId": "1"
            }
          ],
          "outputsMetadata": [
            {
              "id": "0",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "mul_212"
                },
                {
                  "key": "tensor_shape",
                  "value": "FLOAT[s34,32,s16,64]"
                },
                {
                  "key": "param_name",
                  "value": "C"
                }
              ]
            }
          ],
          "inputsMetadata": [
            {
              "id": "0",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "transpose_1"
                },
                {
                  "key": "tensor_shape",
                  "value": "FLOAT[s34,32,s16,64]"
                },
                {
                  "key": "param_name",
                  "value": "A"
                }
              ]
            },
            {
              "id": "1",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "unsqueeze_10"
                },
                {
                  "key": "tensor_shape",
                  "value": "FLOAT[1,1,s16,64]"
                },
                {
                  "key": "param_name",
                  "value": "B"
                }
              ]
            }
          ],
          "style": null,
          "config": null
        },
        {
          "id": "node_Constant_875",
          "label": "Constant",
          "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.0: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.0.self_attn: transformers.models.llama.modeling_llama.LlamaAttention/slice_26: aten.slice.Tensor",
          "subgraphIds": [],
          "attrs": [
            {
              "key": "__value",
              "value": "[0]"
            },
            {
              "key": "value",
              "value": "TensorProtoTensor<INT64,[1]>(array([0]), name='val_277')"
            }
          ],
          "incomingEdges": [],
          "outputsMetadata": [
            {
              "id": "0",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "val_277"
                },
                {
                  "key": "tensor_shape",
                  "value": "INT64[1]"
                },
                {
                  "key": "param_name",
                  "value": "output"
                }
              ]
            }
          ],
          "inputsMetadata": [],
          "style": null,
          "config": null
        },
        {
          "id": "node_Constant_878",
          "label": "Constant",
          "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.0: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.0.self_attn: transformers.models.llama.modeling_llama.LlamaAttention/slice_26: aten.slice.Tensor",
          "subgraphIds": [],
          "attrs": [
            {
              "key": "__value",
              "value": "[32]"
            },
            {
              "key": "value",
              "value": "TensorProtoTensor<INT64,[1]>(array([32]), name='val_281')"
            }
          ],
          "incomingEdges": [],
          "outputsMetadata": [
            {
              "id": "0",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "val_281"
                },
                {
                  "key": "tensor_shape",
                  "value": "INT64[1]"
                },
                {
                  "key": "param_name",
                  "value": "output"
                }
              ]
            }
          ],
          "inputsMetadata": [],
          "style": null,
          "config": null
        },
        {
          "id": "node_Constant_881",
          "label": "Constant",
          "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.0: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.0.self_attn: transformers.models.llama.modeling_llama.LlamaAttention/slice_26: aten.slice.Tensor",
          "subgraphIds": [],
          "attrs": [
            {
              "key": "__value",
              "value": "[3]"
            },
            {
              "key": "value",
              "value": "TensorProtoTensor<INT64,[1]>(array([3]), name='val_284')"
            }
          ],
          "incomingEdges": [],
          "outputsMetadata": [
            {
              "id": "0",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "val_284"
                },
                {
                  "key": "tensor_shape",
                  "value": "INT64[1]"
                },
                {
                  "key": "param_name",
                  "value": "output"
                }
              ]
            }
          ],
          "inputsMetadata": [],
          "style": null,
          "config": null
        },
        {
          "id": "node_Constant_383",
          "label": "Constant",
          "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.0: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.0.self_attn: transformers.models.llama.modeling_llama.LlamaAttention/slice_26: aten.slice.Tensor",
          "subgraphIds": [],
          "attrs": [
            {
              "key": "__value",
              "value": "[1]"
            },
            {
              "key": "value_ints",
              "value": "[1]"
            },
            {
              "key": "[metadata] namespace",
              "value": ": transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.0: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.0.self_attn: transformers.models.llama.modeling_llama.LlamaAttention/slice_26: aten.slice.Tensor"
            },
            {
              "key": "[metadata] pkg.torch.onnx.class_hierarchy",
              "value": "['transformers.models.llama.modeling_llama.LlamaForCausalLM', 'transformers.models.llama.modeling_llama.LlamaModel', 'transformers.models.llama.modeling_llama.LlamaDecoderLayer', 'transformers.models.llama.modeling_llama.LlamaAttention', 'aten.slice.Tensor']"
            },
            {
              "key": "[metadata] pkg.torch.onnx.fx_node",
              "value": "%slice_26 : [num_users=1] = call_function[target=torch.ops.aten.slice.Tensor](args = (%transpose_1, 3, 0, 32), kwargs = {})"
            },
            {
              "key": "[metadata] pkg.torch.onnx.name_scopes",
              "value": "['', 'model', 'model.layers.0', 'model.layers.0.self_attn', 'slice_26']"
            },
            {
              "key": "[metadata] pkg.torch.onnx.stack_trace",
              "value": "File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 688, in forward\n    outputs: BaseModelOutputWithPast = self.model(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 453, in forward\n    layer_outputs = decoder_layer(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 308, in forward\n    hidden_states, self_attn_weights = self.self_attn(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 247, in forward\n    query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)"
            }
          ],
          "incomingEdges": [],
          "outputsMetadata": [
            {
              "id": "0",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "val_285"
                },
                {
                  "key": "tensor_shape",
                  "value": "INT64[1]"
                },
                {
                  "key": "param_name",
                  "value": "output"
                }
              ]
            }
          ],
          "inputsMetadata": [],
          "style": null,
          "config": null
        },
        {
          "id": "node_Slice_384",
          "label": "Slice",
          "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.0: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.0.self_attn: transformers.models.llama.modeling_llama.LlamaAttention/slice_26: aten.slice.Tensor",
          "subgraphIds": [],
          "attrs": [
            {
              "key": "[metadata] namespace",
              "value": ": transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.0: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.0.self_attn: transformers.models.llama.modeling_llama.LlamaAttention/slice_26: aten.slice.Tensor"
            },
            {
              "key": "[metadata] pkg.torch.onnx.class_hierarchy",
              "value": "['transformers.models.llama.modeling_llama.LlamaForCausalLM', 'transformers.models.llama.modeling_llama.LlamaModel', 'transformers.models.llama.modeling_llama.LlamaDecoderLayer', 'transformers.models.llama.modeling_llama.LlamaAttention', 'aten.slice.Tensor']"
            },
            {
              "key": "[metadata] pkg.torch.onnx.fx_node",
              "value": "%slice_26 : [num_users=1] = call_function[target=torch.ops.aten.slice.Tensor](args = (%transpose_1, 3, 0, 32), kwargs = {})"
            },
            {
              "key": "[metadata] pkg.torch.onnx.name_scopes",
              "value": "['', 'model', 'model.layers.0', 'model.layers.0.self_attn', 'slice_26']"
            },
            {
              "key": "[metadata] pkg.torch.onnx.stack_trace",
              "value": "File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 688, in forward\n    outputs: BaseModelOutputWithPast = self.model(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 453, in forward\n    layer_outputs = decoder_layer(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 308, in forward\n    hidden_states, self_attn_weights = self.self_attn(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 247, in forward\n    query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)"
            }
          ],
          "incomingEdges": [
            {
              "sourceNodeId": "node_Transpose_349",
              "sourceNodeOutputId": "0",
              "targetNodeInputId": "0"
            },
            {
              "sourceNodeId": "node_Constant_875",
              "sourceNodeOutputId": "0",
              "targetNodeInputId": "1"
            },
            {
              "sourceNodeId": "node_Constant_878",
              "sourceNodeOutputId": "0",
              "targetNodeInputId": "2"
            },
            {
              "sourceNodeId": "node_Constant_881",
              "sourceNodeOutputId": "0",
              "targetNodeInputId": "3"
            },
            {
              "sourceNodeId": "node_Constant_383",
              "sourceNodeOutputId": "0",
              "targetNodeInputId": "4"
            }
          ],
          "outputsMetadata": [
            {
              "id": "0",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "slice_26"
                },
                {
                  "key": "tensor_shape",
                  "value": "FLOAT[s34,32,s16,32]"
                },
                {
                  "key": "param_name",
                  "value": "output"
                }
              ]
            }
          ],
          "inputsMetadata": [
            {
              "id": "0",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "transpose_1"
                },
                {
                  "key": "tensor_shape",
                  "value": "FLOAT[s34,32,s16,64]"
                },
                {
                  "key": "param_name",
                  "value": "data"
                }
              ]
            },
            {
              "id": "1",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "val_277"
                },
                {
                  "key": "tensor_shape",
                  "value": "INT64[1]"
                },
                {
                  "key": "param_name",
                  "value": "starts"
                }
              ]
            },
            {
              "id": "2",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "val_281"
                },
                {
                  "key": "tensor_shape",
                  "value": "INT64[1]"
                },
                {
                  "key": "param_name",
                  "value": "ends"
                }
              ]
            },
            {
              "id": "3",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "val_284"
                },
                {
                  "key": "tensor_shape",
                  "value": "INT64[1]"
                },
                {
                  "key": "param_name",
                  "value": "axes"
                }
              ]
            },
            {
              "id": "4",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "val_285"
                },
                {
                  "key": "tensor_shape",
                  "value": "INT64[1]"
                },
                {
                  "key": "param_name",
                  "value": "steps"
                }
              ]
            }
          ],
          "style": null,
          "config": null
        },
        {
          "id": "node_Constant_884",
          "label": "Constant",
          "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.0: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.0.self_attn: transformers.models.llama.modeling_llama.LlamaAttention/slice_27: aten.slice.Tensor",
          "subgraphIds": [],
          "attrs": [
            {
              "key": "__value",
              "value": "[32]"
            },
            {
              "key": "value",
              "value": "TensorProtoTensor<INT64,[1]>(array([32]), name='val_288')"
            }
          ],
          "incomingEdges": [],
          "outputsMetadata": [
            {
              "id": "0",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "val_288"
                },
                {
                  "key": "tensor_shape",
                  "value": "INT64[1]"
                },
                {
                  "key": "param_name",
                  "value": "output"
                }
              ]
            }
          ],
          "inputsMetadata": [],
          "style": null,
          "config": null
        },
        {
          "id": "node_Constant_887",
          "label": "Constant",
          "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.0: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.0.self_attn: transformers.models.llama.modeling_llama.LlamaAttention/slice_27: aten.slice.Tensor",
          "subgraphIds": [],
          "attrs": [
            {
              "key": "__value",
              "value": "[9223372036854775807]"
            },
            {
              "key": "value",
              "value": "TensorProtoTensor<INT64,[1]>(array([9223372036854775807]), name='val_291')"
            }
          ],
          "incomingEdges": [],
          "outputsMetadata": [
            {
              "id": "0",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "val_291"
                },
                {
                  "key": "tensor_shape",
                  "value": "INT64[1]"
                },
                {
                  "key": "param_name",
                  "value": "output"
                }
              ]
            }
          ],
          "inputsMetadata": [],
          "style": null,
          "config": null
        },
        {
          "id": "node_Constant_890",
          "label": "Constant",
          "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.0: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.0.self_attn: transformers.models.llama.modeling_llama.LlamaAttention/slice_27: aten.slice.Tensor",
          "subgraphIds": [],
          "attrs": [
            {
              "key": "__value",
              "value": "[3]"
            },
            {
              "key": "value",
              "value": "TensorProtoTensor<INT64,[1]>(array([3]), name='val_294')"
            }
          ],
          "incomingEdges": [],
          "outputsMetadata": [
            {
              "id": "0",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "val_294"
                },
                {
                  "key": "tensor_shape",
                  "value": "INT64[1]"
                },
                {
                  "key": "param_name",
                  "value": "output"
                }
              ]
            }
          ],
          "inputsMetadata": [],
          "style": null,
          "config": null
        },
        {
          "id": "node_Constant_394",
          "label": "Constant",
          "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.0: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.0.self_attn: transformers.models.llama.modeling_llama.LlamaAttention/slice_27: aten.slice.Tensor",
          "subgraphIds": [],
          "attrs": [
            {
              "key": "__value",
              "value": "[1]"
            },
            {
              "key": "value_ints",
              "value": "[1]"
            },
            {
              "key": "[metadata] namespace",
              "value": ": transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.0: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.0.self_attn: transformers.models.llama.modeling_llama.LlamaAttention/slice_27: aten.slice.Tensor"
            },
            {
              "key": "[metadata] pkg.torch.onnx.class_hierarchy",
              "value": "['transformers.models.llama.modeling_llama.LlamaForCausalLM', 'transformers.models.llama.modeling_llama.LlamaModel', 'transformers.models.llama.modeling_llama.LlamaDecoderLayer', 'transformers.models.llama.modeling_llama.LlamaAttention', 'aten.slice.Tensor']"
            },
            {
              "key": "[metadata] pkg.torch.onnx.fx_node",
              "value": "%slice_27 : [num_users=1] = call_function[target=torch.ops.aten.slice.Tensor](args = (%transpose_1, 3, 32, 9223372036854775807), kwargs = {})"
            },
            {
              "key": "[metadata] pkg.torch.onnx.name_scopes",
              "value": "['', 'model', 'model.layers.0', 'model.layers.0.self_attn', 'slice_27']"
            },
            {
              "key": "[metadata] pkg.torch.onnx.stack_trace",
              "value": "File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 688, in forward\n    outputs: BaseModelOutputWithPast = self.model(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 453, in forward\n    layer_outputs = decoder_layer(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 308, in forward\n    hidden_states, self_attn_weights = self.self_attn(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 247, in forward\n    query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)"
            }
          ],
          "incomingEdges": [],
          "outputsMetadata": [
            {
              "id": "0",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "val_295"
                },
                {
                  "key": "tensor_shape",
                  "value": "INT64[1]"
                },
                {
                  "key": "param_name",
                  "value": "output"
                }
              ]
            }
          ],
          "inputsMetadata": [],
          "style": null,
          "config": null
        },
        {
          "id": "node_Slice_395",
          "label": "Slice",
          "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.0: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.0.self_attn: transformers.models.llama.modeling_llama.LlamaAttention/slice_27: aten.slice.Tensor",
          "subgraphIds": [],
          "attrs": [
            {
              "key": "[metadata] namespace",
              "value": ": transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.0: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.0.self_attn: transformers.models.llama.modeling_llama.LlamaAttention/slice_27: aten.slice.Tensor"
            },
            {
              "key": "[metadata] pkg.torch.onnx.class_hierarchy",
              "value": "['transformers.models.llama.modeling_llama.LlamaForCausalLM', 'transformers.models.llama.modeling_llama.LlamaModel', 'transformers.models.llama.modeling_llama.LlamaDecoderLayer', 'transformers.models.llama.modeling_llama.LlamaAttention', 'aten.slice.Tensor']"
            },
            {
              "key": "[metadata] pkg.torch.onnx.fx_node",
              "value": "%slice_27 : [num_users=1] = call_function[target=torch.ops.aten.slice.Tensor](args = (%transpose_1, 3, 32, 9223372036854775807), kwargs = {})"
            },
            {
              "key": "[metadata] pkg.torch.onnx.name_scopes",
              "value": "['', 'model', 'model.layers.0', 'model.layers.0.self_attn', 'slice_27']"
            },
            {
              "key": "[metadata] pkg.torch.onnx.stack_trace",
              "value": "File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 688, in forward\n    outputs: BaseModelOutputWithPast = self.model(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 453, in forward\n    layer_outputs = decoder_layer(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 308, in forward\n    hidden_states, self_attn_weights = self.self_attn(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 247, in forward\n    query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)"
            }
          ],
          "incomingEdges": [
            {
              "sourceNodeId": "node_Transpose_349",
              "sourceNodeOutputId": "0",
              "targetNodeInputId": "0"
            },
            {
              "sourceNodeId": "node_Constant_884",
              "sourceNodeOutputId": "0",
              "targetNodeInputId": "1"
            },
            {
              "sourceNodeId": "node_Constant_887",
              "sourceNodeOutputId": "0",
              "targetNodeInputId": "2"
            },
            {
              "sourceNodeId": "node_Constant_890",
              "sourceNodeOutputId": "0",
              "targetNodeInputId": "3"
            },
            {
              "sourceNodeId": "node_Constant_394",
              "sourceNodeOutputId": "0",
              "targetNodeInputId": "4"
            }
          ],
          "outputsMetadata": [
            {
              "id": "0",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "slice_27"
                },
                {
                  "key": "tensor_shape",
                  "value": "FLOAT[s34,32,s16,32]"
                },
                {
                  "key": "param_name",
                  "value": "output"
                }
              ]
            }
          ],
          "inputsMetadata": [
            {
              "id": "0",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "transpose_1"
                },
                {
                  "key": "tensor_shape",
                  "value": "FLOAT[s34,32,s16,64]"
                },
                {
                  "key": "param_name",
                  "value": "data"
                }
              ]
            },
            {
              "id": "1",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "val_288"
                },
                {
                  "key": "tensor_shape",
                  "value": "INT64[1]"
                },
                {
                  "key": "param_name",
                  "value": "starts"
                }
              ]
            },
            {
              "id": "2",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "val_291"
                },
                {
                  "key": "tensor_shape",
                  "value": "INT64[1]"
                },
                {
                  "key": "param_name",
                  "value": "ends"
                }
              ]
            },
            {
              "id": "3",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "val_294"
                },
                {
                  "key": "tensor_shape",
                  "value": "INT64[1]"
                },
                {
                  "key": "param_name",
                  "value": "axes"
                }
              ]
            },
            {
              "id": "4",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "val_295"
                },
                {
                  "key": "tensor_shape",
                  "value": "INT64[1]"
                },
                {
                  "key": "param_name",
                  "value": "steps"
                }
              ]
            }
          ],
          "style": null,
          "config": null
        },
        {
          "id": "node_Neg_396",
          "label": "Neg",
          "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.0: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.0.self_attn: transformers.models.llama.modeling_llama.LlamaAttention/neg: aten.neg.default",
          "subgraphIds": [],
          "attrs": [
            {
              "key": "[metadata] namespace",
              "value": ": transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.0: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.0.self_attn: transformers.models.llama.modeling_llama.LlamaAttention/neg: aten.neg.default"
            },
            {
              "key": "[metadata] pkg.torch.onnx.class_hierarchy",
              "value": "['transformers.models.llama.modeling_llama.LlamaForCausalLM', 'transformers.models.llama.modeling_llama.LlamaModel', 'transformers.models.llama.modeling_llama.LlamaDecoderLayer', 'transformers.models.llama.modeling_llama.LlamaAttention', 'aten.neg.default']"
            },
            {
              "key": "[metadata] pkg.torch.onnx.fx_node",
              "value": "%neg : [num_users=1] = call_function[target=torch.ops.aten.neg.default](args = (%slice_27,), kwargs = {})"
            },
            {
              "key": "[metadata] pkg.torch.onnx.name_scopes",
              "value": "['', 'model', 'model.layers.0', 'model.layers.0.self_attn', 'neg']"
            },
            {
              "key": "[metadata] pkg.torch.onnx.stack_trace",
              "value": "File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 688, in forward\n    outputs: BaseModelOutputWithPast = self.model(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 453, in forward\n    layer_outputs = decoder_layer(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 308, in forward\n    hidden_states, self_attn_weights = self.self_attn(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 247, in forward\n    query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)"
            }
          ],
          "incomingEdges": [
            {
              "sourceNodeId": "node_Slice_395",
              "sourceNodeOutputId": "0",
              "targetNodeInputId": "0"
            }
          ],
          "outputsMetadata": [
            {
              "id": "0",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "neg"
                },
                {
                  "key": "tensor_shape",
                  "value": "FLOAT[s34,32,s16,32]"
                },
                {
                  "key": "param_name",
                  "value": "Y"
                }
              ]
            }
          ],
          "inputsMetadata": [
            {
              "id": "0",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "slice_27"
                },
                {
                  "key": "tensor_shape",
                  "value": "FLOAT[s34,32,s16,32]"
                },
                {
                  "key": "param_name",
                  "value": "X"
                }
              ]
            }
          ],
          "style": null,
          "config": null
        },
        {
          "id": "node_Concat_397",
          "label": "Concat",
          "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.0: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.0.self_attn: transformers.models.llama.modeling_llama.LlamaAttention/cat_1: aten.cat.default",
          "subgraphIds": [],
          "attrs": [
            {
              "key": "axis",
              "value": "-1"
            },
            {
              "key": "[metadata] namespace",
              "value": ": transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.0: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.0.self_attn: transformers.models.llama.modeling_llama.LlamaAttention/cat_1: aten.cat.default"
            },
            {
              "key": "[metadata] pkg.torch.onnx.class_hierarchy",
              "value": "['transformers.models.llama.modeling_llama.LlamaForCausalLM', 'transformers.models.llama.modeling_llama.LlamaModel', 'transformers.models.llama.modeling_llama.LlamaDecoderLayer', 'transformers.models.llama.modeling_llama.LlamaAttention', 'aten.cat.default']"
            },
            {
              "key": "[metadata] pkg.torch.onnx.fx_node",
              "value": "%cat_1 : [num_users=1] = call_function[target=torch.ops.aten.cat.default](args = ([%neg, %slice_26], -1), kwargs = {})"
            },
            {
              "key": "[metadata] pkg.torch.onnx.name_scopes",
              "value": "['', 'model', 'model.layers.0', 'model.layers.0.self_attn', 'cat_1']"
            },
            {
              "key": "[metadata] pkg.torch.onnx.stack_trace",
              "value": "File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 688, in forward\n    outputs: BaseModelOutputWithPast = self.model(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 453, in forward\n    layer_outputs = decoder_layer(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 308, in forward\n    hidden_states, self_attn_weights = self.self_attn(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 247, in forward\n    query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)"
            }
          ],
          "incomingEdges": [
            {
              "sourceNodeId": "node_Neg_396",
              "sourceNodeOutputId": "0",
              "targetNodeInputId": "0"
            },
            {
              "sourceNodeId": "node_Slice_384",
              "sourceNodeOutputId": "0",
              "targetNodeInputId": "1"
            }
          ],
          "outputsMetadata": [
            {
              "id": "0",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "cat_1"
                },
                {
                  "key": "tensor_shape",
                  "value": "FLOAT[s34,32,s16,64]"
                },
                {
                  "key": "param_name",
                  "value": "concat_result"
                }
              ]
            }
          ],
          "inputsMetadata": [
            {
              "id": "0",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "neg"
                },
                {
                  "key": "tensor_shape",
                  "value": "FLOAT[s34,32,s16,32]"
                },
                {
                  "key": "param_name",
                  "value": "inputs"
                }
              ]
            },
            {
              "id": "1",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "slice_26"
                },
                {
                  "key": "tensor_shape",
                  "value": "FLOAT[s34,32,s16,32]"
                },
                {
                  "key": "param_name",
                  "value": "inputs"
                }
              ]
            }
          ],
          "style": null,
          "config": null
        },
        {
          "id": "node_Mul_398",
          "label": "Mul",
          "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.0: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.0.self_attn: transformers.models.llama.modeling_llama.LlamaAttention/mul_229: aten.mul.Tensor",
          "subgraphIds": [],
          "attrs": [
            {
              "key": "[metadata] namespace",
              "value": ": transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.0: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.0.self_attn: transformers.models.llama.modeling_llama.LlamaAttention/mul_229: aten.mul.Tensor"
            },
            {
              "key": "[metadata] pkg.torch.onnx.class_hierarchy",
              "value": "['transformers.models.llama.modeling_llama.LlamaForCausalLM', 'transformers.models.llama.modeling_llama.LlamaModel', 'transformers.models.llama.modeling_llama.LlamaDecoderLayer', 'transformers.models.llama.modeling_llama.LlamaAttention', 'aten.mul.Tensor']"
            },
            {
              "key": "[metadata] pkg.torch.onnx.fx_node",
              "value": "%mul_229 : [num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%cat_1, %unsqueeze_11), kwargs = {})"
            },
            {
              "key": "[metadata] pkg.torch.onnx.name_scopes",
              "value": "['', 'model', 'model.layers.0', 'model.layers.0.self_attn', 'mul_229']"
            },
            {
              "key": "[metadata] pkg.torch.onnx.stack_trace",
              "value": "File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 688, in forward\n    outputs: BaseModelOutputWithPast = self.model(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 453, in forward\n    layer_outputs = decoder_layer(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 308, in forward\n    hidden_states, self_attn_weights = self.self_attn(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 247, in forward\n    query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)"
            }
          ],
          "incomingEdges": [
            {
              "sourceNodeId": "node_Concat_397",
              "sourceNodeOutputId": "0",
              "targetNodeInputId": "0"
            },
            {
              "sourceNodeId": "node_Unsqueeze_371",
              "sourceNodeOutputId": "0",
              "targetNodeInputId": "1"
            }
          ],
          "outputsMetadata": [
            {
              "id": "0",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "mul_229"
                },
                {
                  "key": "tensor_shape",
                  "value": "FLOAT[s34,32,s16,64]"
                },
                {
                  "key": "param_name",
                  "value": "C"
                }
              ]
            }
          ],
          "inputsMetadata": [
            {
              "id": "0",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "cat_1"
                },
                {
                  "key": "tensor_shape",
                  "value": "FLOAT[s34,32,s16,64]"
                },
                {
                  "key": "param_name",
                  "value": "A"
                }
              ]
            },
            {
              "id": "1",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "unsqueeze_11"
                },
                {
                  "key": "tensor_shape",
                  "value": "FLOAT[1,1,s16,64]"
                },
                {
                  "key": "param_name",
                  "value": "B"
                }
              ]
            }
          ],
          "style": null,
          "config": null
        },
        {
          "id": "node_Add_399",
          "label": "Add",
          "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.0: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.0.self_attn: transformers.models.llama.modeling_llama.LlamaAttention/add_287: aten.add.Tensor",
          "subgraphIds": [],
          "attrs": [
            {
              "key": "[metadata] namespace",
              "value": ": transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.0: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.0.self_attn: transformers.models.llama.modeling_llama.LlamaAttention/add_287: aten.add.Tensor"
            },
            {
              "key": "[metadata] pkg.torch.onnx.class_hierarchy",
              "value": "['transformers.models.llama.modeling_llama.LlamaForCausalLM', 'transformers.models.llama.modeling_llama.LlamaModel', 'transformers.models.llama.modeling_llama.LlamaDecoderLayer', 'transformers.models.llama.modeling_llama.LlamaAttention', 'aten.add.Tensor']"
            },
            {
              "key": "[metadata] pkg.torch.onnx.fx_node",
              "value": "%add_287 : [num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%mul_212, %mul_229), kwargs = {})"
            },
            {
              "key": "[metadata] pkg.torch.onnx.name_scopes",
              "value": "['', 'model', 'model.layers.0', 'model.layers.0.self_attn', 'add_287']"
            },
            {
              "key": "[metadata] pkg.torch.onnx.stack_trace",
              "value": "File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 688, in forward\n    outputs: BaseModelOutputWithPast = self.model(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 453, in forward\n    layer_outputs = decoder_layer(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 308, in forward\n    hidden_states, self_attn_weights = self.self_attn(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 247, in forward\n    query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)"
            }
          ],
          "incomingEdges": [
            {
              "sourceNodeId": "node_Mul_372",
              "sourceNodeOutputId": "0",
              "targetNodeInputId": "0"
            },
            {
              "sourceNodeId": "node_Mul_398",
              "sourceNodeOutputId": "0",
              "targetNodeInputId": "1"
            }
          ],
          "outputsMetadata": [
            {
              "id": "0",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "add_287"
                },
                {
                  "key": "tensor_shape",
                  "value": "FLOAT[s34,32,s16,64]"
                },
                {
                  "key": "param_name",
                  "value": "C"
                }
              ]
            }
          ],
          "inputsMetadata": [
            {
              "id": "0",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "mul_212"
                },
                {
                  "key": "tensor_shape",
                  "value": "FLOAT[s34,32,s16,64]"
                },
                {
                  "key": "param_name",
                  "value": "A"
                }
              ]
            },
            {
              "id": "1",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "mul_229"
                },
                {
                  "key": "tensor_shape",
                  "value": "FLOAT[s34,32,s16,64]"
                },
                {
                  "key": "param_name",
                  "value": "B"
                }
              ]
            }
          ],
          "style": null,
          "config": null
        },
        {
          "id": "node_Mul_400",
          "label": "Mul",
          "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.0: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.0.self_attn: transformers.models.llama.modeling_llama.LlamaAttention/mul_237: aten.mul.Tensor",
          "subgraphIds": [],
          "attrs": [
            {
              "key": "[metadata] namespace",
              "value": ": transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.0: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.0.self_attn: transformers.models.llama.modeling_llama.LlamaAttention/mul_237: aten.mul.Tensor"
            },
            {
              "key": "[metadata] pkg.torch.onnx.class_hierarchy",
              "value": "['transformers.models.llama.modeling_llama.LlamaForCausalLM', 'transformers.models.llama.modeling_llama.LlamaModel', 'transformers.models.llama.modeling_llama.LlamaDecoderLayer', 'transformers.models.llama.modeling_llama.LlamaAttention', 'aten.mul.Tensor']"
            },
            {
              "key": "[metadata] pkg.torch.onnx.fx_node",
              "value": "%mul_237 : [num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%transpose_2, %unsqueeze_10), kwargs = {})"
            },
            {
              "key": "[metadata] pkg.torch.onnx.name_scopes",
              "value": "['', 'model', 'model.layers.0', 'model.layers.0.self_attn', 'mul_237']"
            },
            {
              "key": "[metadata] pkg.torch.onnx.stack_trace",
              "value": "File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 688, in forward\n    outputs: BaseModelOutputWithPast = self.model(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 453, in forward\n    layer_outputs = decoder_layer(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 308, in forward\n    hidden_states, self_attn_weights = self.self_attn(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 247, in forward\n    query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)"
            }
          ],
          "incomingEdges": [
            {
              "sourceNodeId": "node_Transpose_359",
              "sourceNodeOutputId": "0",
              "targetNodeInputId": "0"
            },
            {
              "sourceNodeId": "node_Unsqueeze_370",
              "sourceNodeOutputId": "0",
              "targetNodeInputId": "1"
            }
          ],
          "outputsMetadata": [
            {
              "id": "0",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "mul_237"
                },
                {
                  "key": "tensor_shape",
                  "value": "FLOAT[s34,32,s16,64]"
                },
                {
                  "key": "param_name",
                  "value": "C"
                }
              ]
            }
          ],
          "inputsMetadata": [
            {
              "id": "0",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "transpose_2"
                },
                {
                  "key": "tensor_shape",
                  "value": "FLOAT[s34,32,s16,64]"
                },
                {
                  "key": "param_name",
                  "value": "A"
                }
              ]
            },
            {
              "id": "1",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "unsqueeze_10"
                },
                {
                  "key": "tensor_shape",
                  "value": "FLOAT[1,1,s16,64]"
                },
                {
                  "key": "param_name",
                  "value": "B"
                }
              ]
            }
          ],
          "style": null,
          "config": null
        },
        {
          "id": "node_Constant_893",
          "label": "Constant",
          "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.0: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.0.self_attn: transformers.models.llama.modeling_llama.LlamaAttention/slice_28: aten.slice.Tensor",
          "subgraphIds": [],
          "attrs": [
            {
              "key": "__value",
              "value": "[0]"
            },
            {
              "key": "value",
              "value": "TensorProtoTensor<INT64,[1]>(array([0]), name='val_298')"
            }
          ],
          "incomingEdges": [],
          "outputsMetadata": [
            {
              "id": "0",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "val_298"
                },
                {
                  "key": "tensor_shape",
                  "value": "INT64[1]"
                },
                {
                  "key": "param_name",
                  "value": "output"
                }
              ]
            }
          ],
          "inputsMetadata": [],
          "style": null,
          "config": null
        },
        {
          "id": "node_Constant_896",
          "label": "Constant",
          "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.0: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.0.self_attn: transformers.models.llama.modeling_llama.LlamaAttention/slice_28: aten.slice.Tensor",
          "subgraphIds": [],
          "attrs": [
            {
              "key": "__value",
              "value": "[32]"
            },
            {
              "key": "value",
              "value": "TensorProtoTensor<INT64,[1]>(array([32]), name='val_301')"
            }
          ],
          "incomingEdges": [],
          "outputsMetadata": [
            {
              "id": "0",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "val_301"
                },
                {
                  "key": "tensor_shape",
                  "value": "INT64[1]"
                },
                {
                  "key": "param_name",
                  "value": "output"
                }
              ]
            }
          ],
          "inputsMetadata": [],
          "style": null,
          "config": null
        },
        {
          "id": "node_Constant_899",
          "label": "Constant",
          "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.0: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.0.self_attn: transformers.models.llama.modeling_llama.LlamaAttention/slice_28: aten.slice.Tensor",
          "subgraphIds": [],
          "attrs": [
            {
              "key": "__value",
              "value": "[3]"
            },
            {
              "key": "value",
              "value": "TensorProtoTensor<INT64,[1]>(array([3]), name='val_304')"
            }
          ],
          "incomingEdges": [],
          "outputsMetadata": [
            {
              "id": "0",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "val_304"
                },
                {
                  "key": "tensor_shape",
                  "value": "INT64[1]"
                },
                {
                  "key": "param_name",
                  "value": "output"
                }
              ]
            }
          ],
          "inputsMetadata": [],
          "style": null,
          "config": null
        },
        {
          "id": "node_Constant_410",
          "label": "Constant",
          "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.0: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.0.self_attn: transformers.models.llama.modeling_llama.LlamaAttention/slice_28: aten.slice.Tensor",
          "subgraphIds": [],
          "attrs": [
            {
              "key": "__value",
              "value": "[1]"
            },
            {
              "key": "value_ints",
              "value": "[1]"
            },
            {
              "key": "[metadata] namespace",
              "value": ": transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.0: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.0.self_attn: transformers.models.llama.modeling_llama.LlamaAttention/slice_28: aten.slice.Tensor"
            },
            {
              "key": "[metadata] pkg.torch.onnx.class_hierarchy",
              "value": "['transformers.models.llama.modeling_llama.LlamaForCausalLM', 'transformers.models.llama.modeling_llama.LlamaModel', 'transformers.models.llama.modeling_llama.LlamaDecoderLayer', 'transformers.models.llama.modeling_llama.LlamaAttention', 'aten.slice.Tensor']"
            },
            {
              "key": "[metadata] pkg.torch.onnx.fx_node",
              "value": "%slice_28 : [num_users=1] = call_function[target=torch.ops.aten.slice.Tensor](args = (%transpose_2, 3, 0, 32), kwargs = {})"
            },
            {
              "key": "[metadata] pkg.torch.onnx.name_scopes",
              "value": "['', 'model', 'model.layers.0', 'model.layers.0.self_attn', 'slice_28']"
            },
            {
              "key": "[metadata] pkg.torch.onnx.stack_trace",
              "value": "File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 688, in forward\n    outputs: BaseModelOutputWithPast = self.model(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 453, in forward\n    layer_outputs = decoder_layer(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 308, in forward\n    hidden_states, self_attn_weights = self.self_attn(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 247, in forward\n    query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)"
            }
          ],
          "incomingEdges": [],
          "outputsMetadata": [
            {
              "id": "0",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "val_305"
                },
                {
                  "key": "tensor_shape",
                  "value": "INT64[1]"
                },
                {
                  "key": "param_name",
                  "value": "output"
                }
              ]
            }
          ],
          "inputsMetadata": [],
          "style": null,
          "config": null
        },
        {
          "id": "node_Slice_411",
          "label": "Slice",
          "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.0: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.0.self_attn: transformers.models.llama.modeling_llama.LlamaAttention/slice_28: aten.slice.Tensor",
          "subgraphIds": [],
          "attrs": [
            {
              "key": "[metadata] namespace",
              "value": ": transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.0: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.0.self_attn: transformers.models.llama.modeling_llama.LlamaAttention/slice_28: aten.slice.Tensor"
            },
            {
              "key": "[metadata] pkg.torch.onnx.class_hierarchy",
              "value": "['transformers.models.llama.modeling_llama.LlamaForCausalLM', 'transformers.models.llama.modeling_llama.LlamaModel', 'transformers.models.llama.modeling_llama.LlamaDecoderLayer', 'transformers.models.llama.modeling_llama.LlamaAttention', 'aten.slice.Tensor']"
            },
            {
              "key": "[metadata] pkg.torch.onnx.fx_node",
              "value": "%slice_28 : [num_users=1] = call_function[target=torch.ops.aten.slice.Tensor](args = (%transpose_2, 3, 0, 32), kwargs = {})"
            },
            {
              "key": "[metadata] pkg.torch.onnx.name_scopes",
              "value": "['', 'model', 'model.layers.0', 'model.layers.0.self_attn', 'slice_28']"
            },
            {
              "key": "[metadata] pkg.torch.onnx.stack_trace",
              "value": "File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 688, in forward\n    outputs: BaseModelOutputWithPast = self.model(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 453, in forward\n    layer_outputs = decoder_layer(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 308, in forward\n    hidden_states, self_attn_weights = self.self_attn(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 247, in forward\n    query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)"
            }
          ],
          "incomingEdges": [
            {
              "sourceNodeId": "node_Transpose_359",
              "sourceNodeOutputId": "0",
              "targetNodeInputId": "0"
            },
            {
              "sourceNodeId": "node_Constant_893",
              "sourceNodeOutputId": "0",
              "targetNodeInputId": "1"
            },
            {
              "sourceNodeId": "node_Constant_896",
              "sourceNodeOutputId": "0",
              "targetNodeInputId": "2"
            },
            {
              "sourceNodeId": "node_Constant_899",
              "sourceNodeOutputId": "0",
              "targetNodeInputId": "3"
            },
            {
              "sourceNodeId": "node_Constant_410",
              "sourceNodeOutputId": "0",
              "targetNodeInputId": "4"
            }
          ],
          "outputsMetadata": [
            {
              "id": "0",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "slice_28"
                },
                {
                  "key": "tensor_shape",
                  "value": "FLOAT[s34,32,s16,32]"
                },
                {
                  "key": "param_name",
                  "value": "output"
                }
              ]
            }
          ],
          "inputsMetadata": [
            {
              "id": "0",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "transpose_2"
                },
                {
                  "key": "tensor_shape",
                  "value": "FLOAT[s34,32,s16,64]"
                },
                {
                  "key": "param_name",
                  "value": "data"
                }
              ]
            },
            {
              "id": "1",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "val_298"
                },
                {
                  "key": "tensor_shape",
                  "value": "INT64[1]"
                },
                {
                  "key": "param_name",
                  "value": "starts"
                }
              ]
            },
            {
              "id": "2",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "val_301"
                },
                {
                  "key": "tensor_shape",
                  "value": "INT64[1]"
                },
                {
                  "key": "param_name",
                  "value": "ends"
                }
              ]
            },
            {
              "id": "3",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "val_304"
                },
                {
                  "key": "tensor_shape",
                  "value": "INT64[1]"
                },
                {
                  "key": "param_name",
                  "value": "axes"
                }
              ]
            },
            {
              "id": "4",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "val_305"
                },
                {
                  "key": "tensor_shape",
                  "value": "INT64[1]"
                },
                {
                  "key": "param_name",
                  "value": "steps"
                }
              ]
            }
          ],
          "style": null,
          "config": null
        },
        {
          "id": "node_Constant_902",
          "label": "Constant",
          "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.0: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.0.self_attn: transformers.models.llama.modeling_llama.LlamaAttention/slice_29: aten.slice.Tensor",
          "subgraphIds": [],
          "attrs": [
            {
              "key": "__value",
              "value": "[32]"
            },
            {
              "key": "value",
              "value": "TensorProtoTensor<INT64,[1]>(array([32]), name='val_308')"
            }
          ],
          "incomingEdges": [],
          "outputsMetadata": [
            {
              "id": "0",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "val_308"
                },
                {
                  "key": "tensor_shape",
                  "value": "INT64[1]"
                },
                {
                  "key": "param_name",
                  "value": "output"
                }
              ]
            }
          ],
          "inputsMetadata": [],
          "style": null,
          "config": null
        },
        {
          "id": "node_Constant_905",
          "label": "Constant",
          "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.0: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.0.self_attn: transformers.models.llama.modeling_llama.LlamaAttention/slice_29: aten.slice.Tensor",
          "subgraphIds": [],
          "attrs": [
            {
              "key": "__value",
              "value": "[9223372036854775807]"
            },
            {
              "key": "value",
              "value": "TensorProtoTensor<INT64,[1]>(array([9223372036854775807]), name='val_311')"
            }
          ],
          "incomingEdges": [],
          "outputsMetadata": [
            {
              "id": "0",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "val_311"
                },
                {
                  "key": "tensor_shape",
                  "value": "INT64[1]"
                },
                {
                  "key": "param_name",
                  "value": "output"
                }
              ]
            }
          ],
          "inputsMetadata": [],
          "style": null,
          "config": null
        },
        {
          "id": "node_Constant_908",
          "label": "Constant",
          "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.0: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.0.self_attn: transformers.models.llama.modeling_llama.LlamaAttention/slice_29: aten.slice.Tensor",
          "subgraphIds": [],
          "attrs": [
            {
              "key": "__value",
              "value": "[3]"
            },
            {
              "key": "value",
              "value": "TensorProtoTensor<INT64,[1]>(array([3]), name='val_314')"
            }
          ],
          "incomingEdges": [],
          "outputsMetadata": [
            {
              "id": "0",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "val_314"
                },
                {
                  "key": "tensor_shape",
                  "value": "INT64[1]"
                },
                {
                  "key": "param_name",
                  "value": "output"
                }
              ]
            }
          ],
          "inputsMetadata": [],
          "style": null,
          "config": null
        },
        {
          "id": "node_Constant_421",
          "label": "Constant",
          "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.0: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.0.self_attn: transformers.models.llama.modeling_llama.LlamaAttention/slice_29: aten.slice.Tensor",
          "subgraphIds": [],
          "attrs": [
            {
              "key": "__value",
              "value": "[1]"
            },
            {
              "key": "value_ints",
              "value": "[1]"
            },
            {
              "key": "[metadata] namespace",
              "value": ": transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.0: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.0.self_attn: transformers.models.llama.modeling_llama.LlamaAttention/slice_29: aten.slice.Tensor"
            },
            {
              "key": "[metadata] pkg.torch.onnx.class_hierarchy",
              "value": "['transformers.models.llama.modeling_llama.LlamaForCausalLM', 'transformers.models.llama.modeling_llama.LlamaModel', 'transformers.models.llama.modeling_llama.LlamaDecoderLayer', 'transformers.models.llama.modeling_llama.LlamaAttention', 'aten.slice.Tensor']"
            },
            {
              "key": "[metadata] pkg.torch.onnx.fx_node",
              "value": "%slice_29 : [num_users=1] = call_function[target=torch.ops.aten.slice.Tensor](args = (%transpose_2, 3, 32, 9223372036854775807), kwargs = {})"
            },
            {
              "key": "[metadata] pkg.torch.onnx.name_scopes",
              "value": "['', 'model', 'model.layers.0', 'model.layers.0.self_attn', 'slice_29']"
            },
            {
              "key": "[metadata] pkg.torch.onnx.stack_trace",
              "value": "File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 688, in forward\n    outputs: BaseModelOutputWithPast = self.model(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 453, in forward\n    layer_outputs = decoder_layer(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 308, in forward\n    hidden_states, self_attn_weights = self.self_attn(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 247, in forward\n    query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)"
            }
          ],
          "incomingEdges": [],
          "outputsMetadata": [
            {
              "id": "0",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "val_315"
                },
                {
                  "key": "tensor_shape",
                  "value": "INT64[1]"
                },
                {
                  "key": "param_name",
                  "value": "output"
                }
              ]
            }
          ],
          "inputsMetadata": [],
          "style": null,
          "config": null
        },
        {
          "id": "node_Slice_422",
          "label": "Slice",
          "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.0: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.0.self_attn: transformers.models.llama.modeling_llama.LlamaAttention/slice_29: aten.slice.Tensor",
          "subgraphIds": [],
          "attrs": [
            {
              "key": "[metadata] namespace",
              "value": ": transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.0: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.0.self_attn: transformers.models.llama.modeling_llama.LlamaAttention/slice_29: aten.slice.Tensor"
            },
            {
              "key": "[metadata] pkg.torch.onnx.class_hierarchy",
              "value": "['transformers.models.llama.modeling_llama.LlamaForCausalLM', 'transformers.models.llama.modeling_llama.LlamaModel', 'transformers.models.llama.modeling_llama.LlamaDecoderLayer', 'transformers.models.llama.modeling_llama.LlamaAttention', 'aten.slice.Tensor']"
            },
            {
              "key": "[metadata] pkg.torch.onnx.fx_node",
              "value": "%slice_29 : [num_users=1] = call_function[target=torch.ops.aten.slice.Tensor](args = (%transpose_2, 3, 32, 9223372036854775807), kwargs = {})"
            },
            {
              "key": "[metadata] pkg.torch.onnx.name_scopes",
              "value": "['', 'model', 'model.layers.0', 'model.layers.0.self_attn', 'slice_29']"
            },
            {
              "key": "[metadata] pkg.torch.onnx.stack_trace",
              "value": "File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 688, in forward\n    outputs: BaseModelOutputWithPast = self.model(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 453, in forward\n    layer_outputs = decoder_layer(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 308, in forward\n    hidden_states, self_attn_weights = self.self_attn(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 247, in forward\n    query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)"
            }
          ],
          "incomingEdges": [
            {
              "sourceNodeId": "node_Transpose_359",
              "sourceNodeOutputId": "0",
              "targetNodeInputId": "0"
            },
            {
              "sourceNodeId": "node_Constant_902",
              "sourceNodeOutputId": "0",
              "targetNodeInputId": "1"
            },
            {
              "sourceNodeId": "node_Constant_905",
              "sourceNodeOutputId": "0",
              "targetNodeInputId": "2"
            },
            {
              "sourceNodeId": "node_Constant_908",
              "sourceNodeOutputId": "0",
              "targetNodeInputId": "3"
            },
            {
              "sourceNodeId": "node_Constant_421",
              "sourceNodeOutputId": "0",
              "targetNodeInputId": "4"
            }
          ],
          "outputsMetadata": [
            {
              "id": "0",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "slice_29"
                },
                {
                  "key": "tensor_shape",
                  "value": "FLOAT[s34,32,s16,32]"
                },
                {
                  "key": "param_name",
                  "value": "output"
                }
              ]
            }
          ],
          "inputsMetadata": [
            {
              "id": "0",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "transpose_2"
                },
                {
                  "key": "tensor_shape",
                  "value": "FLOAT[s34,32,s16,64]"
                },
                {
                  "key": "param_name",
                  "value": "data"
                }
              ]
            },
            {
              "id": "1",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "val_308"
                },
                {
                  "key": "tensor_shape",
                  "value": "INT64[1]"
                },
                {
                  "key": "param_name",
                  "value": "starts"
                }
              ]
            },
            {
              "id": "2",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "val_311"
                },
                {
                  "key": "tensor_shape",
                  "value": "INT64[1]"
                },
                {
                  "key": "param_name",
                  "value": "ends"
                }
              ]
            },
            {
              "id": "3",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "val_314"
                },
                {
                  "key": "tensor_shape",
                  "value": "INT64[1]"
                },
                {
                  "key": "param_name",
                  "value": "axes"
                }
              ]
            },
            {
              "id": "4",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "val_315"
                },
                {
                  "key": "tensor_shape",
                  "value": "INT64[1]"
                },
                {
                  "key": "param_name",
                  "value": "steps"
                }
              ]
            }
          ],
          "style": null,
          "config": null
        },
        {
          "id": "node_Neg_423",
          "label": "Neg",
          "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.0: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.0.self_attn: transformers.models.llama.modeling_llama.LlamaAttention/neg_1: aten.neg.default",
          "subgraphIds": [],
          "attrs": [
            {
              "key": "[metadata] namespace",
              "value": ": transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.0: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.0.self_attn: transformers.models.llama.modeling_llama.LlamaAttention/neg_1: aten.neg.default"
            },
            {
              "key": "[metadata] pkg.torch.onnx.class_hierarchy",
              "value": "['transformers.models.llama.modeling_llama.LlamaForCausalLM', 'transformers.models.llama.modeling_llama.LlamaModel', 'transformers.models.llama.modeling_llama.LlamaDecoderLayer', 'transformers.models.llama.modeling_llama.LlamaAttention', 'aten.neg.default']"
            },
            {
              "key": "[metadata] pkg.torch.onnx.fx_node",
              "value": "%neg_1 : [num_users=1] = call_function[target=torch.ops.aten.neg.default](args = (%slice_29,), kwargs = {})"
            },
            {
              "key": "[metadata] pkg.torch.onnx.name_scopes",
              "value": "['', 'model', 'model.layers.0', 'model.layers.0.self_attn', 'neg_1']"
            },
            {
              "key": "[metadata] pkg.torch.onnx.stack_trace",
              "value": "File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 688, in forward\n    outputs: BaseModelOutputWithPast = self.model(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 453, in forward\n    layer_outputs = decoder_layer(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 308, in forward\n    hidden_states, self_attn_weights = self.self_attn(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 247, in forward\n    query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)"
            }
          ],
          "incomingEdges": [
            {
              "sourceNodeId": "node_Slice_422",
              "sourceNodeOutputId": "0",
              "targetNodeInputId": "0"
            }
          ],
          "outputsMetadata": [
            {
              "id": "0",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "neg_1"
                },
                {
                  "key": "tensor_shape",
                  "value": "FLOAT[s34,32,s16,32]"
                },
                {
                  "key": "param_name",
                  "value": "Y"
                }
              ]
            }
          ],
          "inputsMetadata": [
            {
              "id": "0",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "slice_29"
                },
                {
                  "key": "tensor_shape",
                  "value": "FLOAT[s34,32,s16,32]"
                },
                {
                  "key": "param_name",
                  "value": "X"
                }
              ]
            }
          ],
          "style": null,
          "config": null
        },
        {
          "id": "node_Concat_424",
          "label": "Concat",
          "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.0: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.0.self_attn: transformers.models.llama.modeling_llama.LlamaAttention/cat_2: aten.cat.default",
          "subgraphIds": [],
          "attrs": [
            {
              "key": "axis",
              "value": "-1"
            },
            {
              "key": "[metadata] namespace",
              "value": ": transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.0: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.0.self_attn: transformers.models.llama.modeling_llama.LlamaAttention/cat_2: aten.cat.default"
            },
            {
              "key": "[metadata] pkg.torch.onnx.class_hierarchy",
              "value": "['transformers.models.llama.modeling_llama.LlamaForCausalLM', 'transformers.models.llama.modeling_llama.LlamaModel', 'transformers.models.llama.modeling_llama.LlamaDecoderLayer', 'transformers.models.llama.modeling_llama.LlamaAttention', 'aten.cat.default']"
            },
            {
              "key": "[metadata] pkg.torch.onnx.fx_node",
              "value": "%cat_2 : [num_users=1] = call_function[target=torch.ops.aten.cat.default](args = ([%neg_1, %slice_28], -1), kwargs = {})"
            },
            {
              "key": "[metadata] pkg.torch.onnx.name_scopes",
              "value": "['', 'model', 'model.layers.0', 'model.layers.0.self_attn', 'cat_2']"
            },
            {
              "key": "[metadata] pkg.torch.onnx.stack_trace",
              "value": "File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 688, in forward\n    outputs: BaseModelOutputWithPast = self.model(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 453, in forward\n    layer_outputs = decoder_layer(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 308, in forward\n    hidden_states, self_attn_weights = self.self_attn(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 247, in forward\n    query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)"
            }
          ],
          "incomingEdges": [
            {
              "sourceNodeId": "node_Neg_423",
              "sourceNodeOutputId": "0",
              "targetNodeInputId": "0"
            },
            {
              "sourceNodeId": "node_Slice_411",
              "sourceNodeOutputId": "0",
              "targetNodeInputId": "1"
            }
          ],
          "outputsMetadata": [
            {
              "id": "0",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "cat_2"
                },
                {
                  "key": "tensor_shape",
                  "value": "FLOAT[s34,32,s16,64]"
                },
                {
                  "key": "param_name",
                  "value": "concat_result"
                }
              ]
            }
          ],
          "inputsMetadata": [
            {
              "id": "0",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "neg_1"
                },
                {
                  "key": "tensor_shape",
                  "value": "FLOAT[s34,32,s16,32]"
                },
                {
                  "key": "param_name",
                  "value": "inputs"
                }
              ]
            },
            {
              "id": "1",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "slice_28"
                },
                {
                  "key": "tensor_shape",
                  "value": "FLOAT[s34,32,s16,32]"
                },
                {
                  "key": "param_name",
                  "value": "inputs"
                }
              ]
            }
          ],
          "style": null,
          "config": null
        },
        {
          "id": "node_Mul_425",
          "label": "Mul",
          "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.0: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.0.self_attn: transformers.models.llama.modeling_llama.LlamaAttention/mul_254: aten.mul.Tensor",
          "subgraphIds": [],
          "attrs": [
            {
              "key": "[metadata] namespace",
              "value": ": transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.0: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.0.self_attn: transformers.models.llama.modeling_llama.LlamaAttention/mul_254: aten.mul.Tensor"
            },
            {
              "key": "[metadata] pkg.torch.onnx.class_hierarchy",
              "value": "['transformers.models.llama.modeling_llama.LlamaForCausalLM', 'transformers.models.llama.modeling_llama.LlamaModel', 'transformers.models.llama.modeling_llama.LlamaDecoderLayer', 'transformers.models.llama.modeling_llama.LlamaAttention', 'aten.mul.Tensor']"
            },
            {
              "key": "[metadata] pkg.torch.onnx.fx_node",
              "value": "%mul_254 : [num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%cat_2, %unsqueeze_11), kwargs = {})"
            },
            {
              "key": "[metadata] pkg.torch.onnx.name_scopes",
              "value": "['', 'model', 'model.layers.0', 'model.layers.0.self_attn', 'mul_254']"
            },
            {
              "key": "[metadata] pkg.torch.onnx.stack_trace",
              "value": "File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 688, in forward\n    outputs: BaseModelOutputWithPast = self.model(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 453, in forward\n    layer_outputs = decoder_layer(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 308, in forward\n    hidden_states, self_attn_weights = self.self_attn(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 247, in forward\n    query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)"
            }
          ],
          "incomingEdges": [
            {
              "sourceNodeId": "node_Concat_424",
              "sourceNodeOutputId": "0",
              "targetNodeInputId": "0"
            },
            {
              "sourceNodeId": "node_Unsqueeze_371",
              "sourceNodeOutputId": "0",
              "targetNodeInputId": "1"
            }
          ],
          "outputsMetadata": [
            {
              "id": "0",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "mul_254"
                },
                {
                  "key": "tensor_shape",
                  "value": "FLOAT[s34,32,s16,64]"
                },
                {
                  "key": "param_name",
                  "value": "C"
                }
              ]
            }
          ],
          "inputsMetadata": [
            {
              "id": "0",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "cat_2"
                },
                {
                  "key": "tensor_shape",
                  "value": "FLOAT[s34,32,s16,64]"
                },
                {
                  "key": "param_name",
                  "value": "A"
                }
              ]
            },
            {
              "id": "1",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "unsqueeze_11"
                },
                {
                  "key": "tensor_shape",
                  "value": "FLOAT[1,1,s16,64]"
                },
                {
                  "key": "param_name",
                  "value": "B"
                }
              ]
            }
          ],
          "style": null,
          "config": null
        },
        {
          "id": "node_Add_426",
          "label": "Add",
          "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.0: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.0.self_attn: transformers.models.llama.modeling_llama.LlamaAttention/add_323: aten.add.Tensor",
          "subgraphIds": [],
          "attrs": [
            {
              "key": "[metadata] namespace",
              "value": ": transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.0: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.0.self_attn: transformers.models.llama.modeling_llama.LlamaAttention/add_323: aten.add.Tensor"
            },
            {
              "key": "[metadata] pkg.torch.onnx.class_hierarchy",
              "value": "['transformers.models.llama.modeling_llama.LlamaForCausalLM', 'transformers.models.llama.modeling_llama.LlamaModel', 'transformers.models.llama.modeling_llama.LlamaDecoderLayer', 'transformers.models.llama.modeling_llama.LlamaAttention', 'aten.add.Tensor']"
            },
            {
              "key": "[metadata] pkg.torch.onnx.fx_node",
              "value": "%add_323 : [num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%mul_237, %mul_254), kwargs = {})"
            },
            {
              "key": "[metadata] pkg.torch.onnx.name_scopes",
              "value": "['', 'model', 'model.layers.0', 'model.layers.0.self_attn', 'add_323']"
            },
            {
              "key": "[metadata] pkg.torch.onnx.stack_trace",
              "value": "File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 688, in forward\n    outputs: BaseModelOutputWithPast = self.model(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 453, in forward\n    layer_outputs = decoder_layer(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 308, in forward\n    hidden_states, self_attn_weights = self.self_attn(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 247, in forward\n    query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)"
            }
          ],
          "incomingEdges": [
            {
              "sourceNodeId": "node_Mul_400",
              "sourceNodeOutputId": "0",
              "targetNodeInputId": "0"
            },
            {
              "sourceNodeId": "node_Mul_425",
              "sourceNodeOutputId": "0",
              "targetNodeInputId": "1"
            }
          ],
          "outputsMetadata": [
            {
              "id": "0",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "add_323"
                },
                {
                  "key": "tensor_shape",
                  "value": "FLOAT[s34,32,s16,64]"
                },
                {
                  "key": "param_name",
                  "value": "C"
                }
              ]
            }
          ],
          "inputsMetadata": [
            {
              "id": "0",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "mul_237"
                },
                {
                  "key": "tensor_shape",
                  "value": "FLOAT[s34,32,s16,64]"
                },
                {
                  "key": "param_name",
                  "value": "A"
                }
              ]
            },
            {
              "id": "1",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "mul_254"
                },
                {
                  "key": "tensor_shape",
                  "value": "FLOAT[s34,32,s16,64]"
                },
                {
                  "key": "param_name",
                  "value": "B"
                }
              ]
            }
          ],
          "style": null,
          "config": null
        },
        {
          "id": "node_Concat_427",
          "label": "Concat",
          "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.0: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.0.self_attn: transformers.models.llama.modeling_llama.LlamaAttention/cat_3: aten.cat.default",
          "subgraphIds": [],
          "attrs": [
            {
              "key": "axis",
              "value": "-2"
            },
            {
              "key": "[metadata] namespace",
              "value": ": transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.0: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.0.self_attn: transformers.models.llama.modeling_llama.LlamaAttention/cat_3: aten.cat.default"
            },
            {
              "key": "[metadata] pkg.torch.onnx.class_hierarchy",
              "value": "['transformers.models.llama.modeling_llama.LlamaForCausalLM', 'transformers.models.llama.modeling_llama.LlamaModel', 'transformers.models.llama.modeling_llama.LlamaDecoderLayer', 'transformers.models.llama.modeling_llama.LlamaAttention', 'aten.cat.default']"
            },
            {
              "key": "[metadata] pkg.torch.onnx.fx_node",
              "value": "%cat_3 : [num_users=2] = call_function[target=torch.ops.aten.cat.default](args = ([%past_key_values_key_cache_0, %add_323], -2), kwargs = {})"
            },
            {
              "key": "[metadata] pkg.torch.onnx.name_scopes",
              "value": "['', 'model', 'model.layers.0', 'model.layers.0.self_attn', 'cat_3']"
            },
            {
              "key": "[metadata] pkg.torch.onnx.stack_trace",
              "value": "File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 688, in forward\n    outputs: BaseModelOutputWithPast = self.model(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 453, in forward\n    layer_outputs = decoder_layer(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 308, in forward\n    hidden_states, self_attn_weights = self.self_attn(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 252, in forward\n    key_states, value_states = past_key_value.update(key_states, value_states, self.layer_idx, cache_kwargs)"
            }
          ],
          "incomingEdges": [
            {
              "sourceNodeId": "[value] past_key_values_key_cache_0",
              "sourceNodeOutputId": "0",
              "targetNodeInputId": "0"
            },
            {
              "sourceNodeId": "node_Add_426",
              "sourceNodeOutputId": "0",
              "targetNodeInputId": "1"
            }
          ],
          "outputsMetadata": [
            {
              "id": "0",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "cat_3"
                },
                {
                  "key": "tensor_shape",
                  "value": "FLOAT[s34,32,s16 + s17,64]"
                },
                {
                  "key": "[metadata] pkg.torch.export.graph_signature.OutputSpec.kind",
                  "value": "USER_OUTPUT"
                },
                {
                  "key": "param_name",
                  "value": "concat_result"
                }
              ]
            }
          ],
          "inputsMetadata": [
            {
              "id": "0",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "past_key_values_key_cache_0"
                },
                {
                  "key": "tensor_shape",
                  "value": "FLOAT[s34,32,s17,64]"
                },
                {
                  "key": "[metadata] pkg.torch.export.graph_signature.InputSpec.kind",
                  "value": "USER_INPUT"
                },
                {
                  "key": "[metadata] pkg.torch.export.graph_signature.InputSpec.persistent",
                  "value": "None"
                },
                {
                  "key": "param_name",
                  "value": "inputs"
                }
              ]
            },
            {
              "id": "1",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "add_323"
                },
                {
                  "key": "tensor_shape",
                  "value": "FLOAT[s34,32,s16,64]"
                },
                {
                  "key": "param_name",
                  "value": "inputs"
                }
              ]
            }
          ],
          "style": null,
          "config": null
        },
        {
          "id": "node_Concat_428",
          "label": "Concat",
          "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.0: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.0.self_attn: transformers.models.llama.modeling_llama.LlamaAttention/cat_4: aten.cat.default",
          "subgraphIds": [],
          "attrs": [
            {
              "key": "axis",
              "value": "-2"
            },
            {
              "key": "[metadata] namespace",
              "value": ": transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.0: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.0.self_attn: transformers.models.llama.modeling_llama.LlamaAttention/cat_4: aten.cat.default"
            },
            {
              "key": "[metadata] pkg.torch.onnx.class_hierarchy",
              "value": "['transformers.models.llama.modeling_llama.LlamaForCausalLM', 'transformers.models.llama.modeling_llama.LlamaModel', 'transformers.models.llama.modeling_llama.LlamaDecoderLayer', 'transformers.models.llama.modeling_llama.LlamaAttention', 'aten.cat.default']"
            },
            {
              "key": "[metadata] pkg.torch.onnx.fx_node",
              "value": "%cat_4 : [num_users=2] = call_function[target=torch.ops.aten.cat.default](args = ([%past_key_values_value_cache_0, %transpose_3], -2), kwargs = {})"
            },
            {
              "key": "[metadata] pkg.torch.onnx.name_scopes",
              "value": "['', 'model', 'model.layers.0', 'model.layers.0.self_attn', 'cat_4']"
            },
            {
              "key": "[metadata] pkg.torch.onnx.stack_trace",
              "value": "File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 688, in forward\n    outputs: BaseModelOutputWithPast = self.model(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 453, in forward\n    layer_outputs = decoder_layer(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 308, in forward\n    hidden_states, self_attn_weights = self.self_attn(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 252, in forward\n    key_states, value_states = past_key_value.update(key_states, value_states, self.layer_idx, cache_kwargs)"
            }
          ],
          "incomingEdges": [
            {
              "sourceNodeId": "[value] past_key_values_value_cache_0",
              "sourceNodeOutputId": "0",
              "targetNodeInputId": "0"
            },
            {
              "sourceNodeId": "node_Transpose_369",
              "sourceNodeOutputId": "0",
              "targetNodeInputId": "1"
            }
          ],
          "outputsMetadata": [
            {
              "id": "0",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "cat_4"
                },
                {
                  "key": "tensor_shape",
                  "value": "FLOAT[s34,32,s16 + s17,64]"
                },
                {
                  "key": "[metadata] pkg.torch.export.graph_signature.OutputSpec.kind",
                  "value": "USER_OUTPUT"
                },
                {
                  "key": "param_name",
                  "value": "concat_result"
                }
              ]
            }
          ],
          "inputsMetadata": [
            {
              "id": "0",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "past_key_values_value_cache_0"
                },
                {
                  "key": "tensor_shape",
                  "value": "FLOAT[s34,32,s17,64]"
                },
                {
                  "key": "[metadata] pkg.torch.export.graph_signature.InputSpec.kind",
                  "value": "USER_INPUT"
                },
                {
                  "key": "[metadata] pkg.torch.export.graph_signature.InputSpec.persistent",
                  "value": "None"
                },
                {
                  "key": "param_name",
                  "value": "inputs"
                }
              ]
            },
            {
              "id": "1",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "transpose_3"
                },
                {
                  "key": "tensor_shape",
                  "value": "FLOAT[s34,32,s16,64]"
                },
                {
                  "key": "param_name",
                  "value": "inputs"
                }
              ]
            }
          ],
          "style": null,
          "config": null
        },
        {
          "id": "node_Transpose_429",
          "label": "Transpose",
          "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.0: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.0.self_attn: transformers.models.llama.modeling_llama.LlamaAttention/transpose_4: aten.transpose.int",
          "subgraphIds": [],
          "attrs": [
            {
              "key": "perm",
              "value": "[0, 1, 3, 2]"
            },
            {
              "key": "[metadata] namespace",
              "value": ": transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.0: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.0.self_attn: transformers.models.llama.modeling_llama.LlamaAttention/transpose_4: aten.transpose.int"
            },
            {
              "key": "[metadata] pkg.torch.onnx.class_hierarchy",
              "value": "['transformers.models.llama.modeling_llama.LlamaForCausalLM', 'transformers.models.llama.modeling_llama.LlamaModel', 'transformers.models.llama.modeling_llama.LlamaDecoderLayer', 'transformers.models.llama.modeling_llama.LlamaAttention', 'aten.transpose.int']"
            },
            {
              "key": "[metadata] pkg.torch.onnx.fx_node",
              "value": "%transpose_4 : [num_users=1] = call_function[target=torch.ops.aten.transpose.int](args = (%cat_3, 2, 3), kwargs = {})"
            },
            {
              "key": "[metadata] pkg.torch.onnx.name_scopes",
              "value": "['', 'model', 'model.layers.0', 'model.layers.0.self_attn', 'transpose_4']"
            },
            {
              "key": "[metadata] pkg.torch.onnx.stack_trace",
              "value": "File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 688, in forward\n    outputs: BaseModelOutputWithPast = self.model(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 453, in forward\n    layer_outputs = decoder_layer(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 308, in forward\n    hidden_states, self_attn_weights = self.self_attn(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 265, in forward\n    attn_output, attn_weights = attention_interface("
            }
          ],
          "incomingEdges": [
            {
              "sourceNodeId": "node_Concat_427",
              "sourceNodeOutputId": "0",
              "targetNodeInputId": "0"
            }
          ],
          "outputsMetadata": [
            {
              "id": "0",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "transpose_4"
                },
                {
                  "key": "tensor_shape",
                  "value": "FLOAT[s34,32,64,s16 + s17]"
                },
                {
                  "key": "param_name",
                  "value": "transposed"
                }
              ]
            }
          ],
          "inputsMetadata": [
            {
              "id": "0",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "cat_3"
                },
                {
                  "key": "tensor_shape",
                  "value": "FLOAT[s34,32,s16 + s17,64]"
                },
                {
                  "key": "[metadata] pkg.torch.export.graph_signature.OutputSpec.kind",
                  "value": "USER_OUTPUT"
                },
                {
                  "key": "param_name",
                  "value": "data"
                }
              ]
            }
          ],
          "style": null,
          "config": null
        },
        {
          "id": "node_MatMul_430",
          "label": "MatMul",
          "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.0: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.0.self_attn: transformers.models.llama.modeling_llama.LlamaAttention/matmul_1: aten.matmul.default",
          "subgraphIds": [],
          "attrs": [
            {
              "key": "[metadata] namespace",
              "value": ": transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.0: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.0.self_attn: transformers.models.llama.modeling_llama.LlamaAttention/matmul_1: aten.matmul.default"
            },
            {
              "key": "[metadata] pkg.torch.onnx.class_hierarchy",
              "value": "['transformers.models.llama.modeling_llama.LlamaForCausalLM', 'transformers.models.llama.modeling_llama.LlamaModel', 'transformers.models.llama.modeling_llama.LlamaDecoderLayer', 'transformers.models.llama.modeling_llama.LlamaAttention', 'aten.matmul.default']"
            },
            {
              "key": "[metadata] pkg.torch.onnx.fx_node",
              "value": "%matmul_1 : [num_users=1] = call_function[target=torch.ops.aten.matmul.default](args = (%add_287, %transpose_4), kwargs = {})"
            },
            {
              "key": "[metadata] pkg.torch.onnx.name_scopes",
              "value": "['', 'model', 'model.layers.0', 'model.layers.0.self_attn', 'matmul_1']"
            },
            {
              "key": "[metadata] pkg.torch.onnx.stack_trace",
              "value": "File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 688, in forward\n    outputs: BaseModelOutputWithPast = self.model(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 453, in forward\n    layer_outputs = decoder_layer(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 308, in forward\n    hidden_states, self_attn_weights = self.self_attn(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 265, in forward\n    attn_output, attn_weights = attention_interface("
            }
          ],
          "incomingEdges": [
            {
              "sourceNodeId": "node_Add_399",
              "sourceNodeOutputId": "0",
              "targetNodeInputId": "0"
            },
            {
              "sourceNodeId": "node_Transpose_429",
              "sourceNodeOutputId": "0",
              "targetNodeInputId": "1"
            }
          ],
          "outputsMetadata": [
            {
              "id": "0",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "matmul_1"
                },
                {
                  "key": "tensor_shape",
                  "value": "FLOAT[s34,32,s16,s16 + s17]"
                },
                {
                  "key": "param_name",
                  "value": "Y"
                }
              ]
            }
          ],
          "inputsMetadata": [
            {
              "id": "0",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "add_287"
                },
                {
                  "key": "tensor_shape",
                  "value": "FLOAT[s34,32,s16,64]"
                },
                {
                  "key": "param_name",
                  "value": "A"
                }
              ]
            },
            {
              "id": "1",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "transpose_4"
                },
                {
                  "key": "tensor_shape",
                  "value": "FLOAT[s34,32,64,s16 + s17]"
                },
                {
                  "key": "param_name",
                  "value": "B"
                }
              ]
            }
          ],
          "style": null,
          "config": null
        },
        {
          "id": "node_Constant_431",
          "label": "Constant",
          "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel",
          "subgraphIds": [],
          "attrs": [
            {
              "key": "__value",
              "value": "0.125"
            },
            {
              "key": "value",
              "value": "TensorProtoTensor<FLOAT,[]>(array(0.125, dtype=float32), name='')"
            },
            {
              "key": "[metadata] namespace",
              "value": ": transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.0: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.0.self_attn: transformers.models.llama.modeling_llama.LlamaAttention/mul_278: aten.mul.Tensor"
            },
            {
              "key": "[metadata] pkg.torch.onnx.class_hierarchy",
              "value": "['transformers.models.llama.modeling_llama.LlamaForCausalLM', 'transformers.models.llama.modeling_llama.LlamaModel', 'transformers.models.llama.modeling_llama.LlamaDecoderLayer', 'transformers.models.llama.modeling_llama.LlamaAttention', 'aten.mul.Tensor']"
            },
            {
              "key": "[metadata] pkg.torch.onnx.fx_node",
              "value": "%mul_278 : [num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%matmul_1, 0.125), kwargs = {})"
            },
            {
              "key": "[metadata] pkg.torch.onnx.name_scopes",
              "value": "['', 'model', 'model.layers.0', 'model.layers.0.self_attn', 'mul_278']"
            },
            {
              "key": "[metadata] pkg.torch.onnx.stack_trace",
              "value": "File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 688, in forward\n    outputs: BaseModelOutputWithPast = self.model(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 453, in forward\n    layer_outputs = decoder_layer(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 308, in forward\n    hidden_states, self_attn_weights = self.self_attn(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 265, in forward\n    attn_output, attn_weights = attention_interface("
            }
          ],
          "incomingEdges": [],
          "outputsMetadata": [
            {
              "id": "0",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "val_316"
                },
                {
                  "key": "tensor_shape",
                  "value": "FLOAT[]"
                },
                {
                  "key": "param_name",
                  "value": "output"
                }
              ]
            }
          ],
          "inputsMetadata": [],
          "style": null,
          "config": null
        },
        {
          "id": "node_Mul_432",
          "label": "Mul",
          "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.0: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.0.self_attn: transformers.models.llama.modeling_llama.LlamaAttention/mul_278: aten.mul.Tensor",
          "subgraphIds": [],
          "attrs": [
            {
              "key": "[metadata] namespace",
              "value": ": transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.0: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.0.self_attn: transformers.models.llama.modeling_llama.LlamaAttention/mul_278: aten.mul.Tensor"
            },
            {
              "key": "[metadata] pkg.torch.onnx.class_hierarchy",
              "value": "['transformers.models.llama.modeling_llama.LlamaForCausalLM', 'transformers.models.llama.modeling_llama.LlamaModel', 'transformers.models.llama.modeling_llama.LlamaDecoderLayer', 'transformers.models.llama.modeling_llama.LlamaAttention', 'aten.mul.Tensor']"
            },
            {
              "key": "[metadata] pkg.torch.onnx.fx_node",
              "value": "%mul_278 : [num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%matmul_1, 0.125), kwargs = {})"
            },
            {
              "key": "[metadata] pkg.torch.onnx.name_scopes",
              "value": "['', 'model', 'model.layers.0', 'model.layers.0.self_attn', 'mul_278']"
            },
            {
              "key": "[metadata] pkg.torch.onnx.stack_trace",
              "value": "File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 688, in forward\n    outputs: BaseModelOutputWithPast = self.model(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 453, in forward\n    layer_outputs = decoder_layer(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 308, in forward\n    hidden_states, self_attn_weights = self.self_attn(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 265, in forward\n    attn_output, attn_weights = attention_interface("
            }
          ],
          "incomingEdges": [
            {
              "sourceNodeId": "node_MatMul_430",
              "sourceNodeOutputId": "0",
              "targetNodeInputId": "0"
            },
            {
              "sourceNodeId": "node_Constant_431",
              "sourceNodeOutputId": "0",
              "targetNodeInputId": "1"
            }
          ],
          "outputsMetadata": [
            {
              "id": "0",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "mul_278"
                },
                {
                  "key": "tensor_shape",
                  "value": "FLOAT[s34,32,s16,s16 + s17]"
                },
                {
                  "key": "param_name",
                  "value": "C"
                }
              ]
            }
          ],
          "inputsMetadata": [
            {
              "id": "0",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "matmul_1"
                },
                {
                  "key": "tensor_shape",
                  "value": "FLOAT[s34,32,s16,s16 + s17]"
                },
                {
                  "key": "param_name",
                  "value": "A"
                }
              ]
            },
            {
              "id": "1",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "val_316"
                },
                {
                  "key": "tensor_shape",
                  "value": "FLOAT[]"
                },
                {
                  "key": "param_name",
                  "value": "B"
                }
              ]
            }
          ],
          "style": null,
          "config": null
        },
        {
          "id": "node_Constant_454",
          "label": "Constant",
          "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.0: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.0.self_attn: transformers.models.llama.modeling_llama.LlamaAttention/slice_37: aten.slice.Tensor",
          "subgraphIds": [],
          "attrs": [
            {
              "key": "__value",
              "value": "[0]"
            },
            {
              "key": "value_ints",
              "value": "[0]"
            },
            {
              "key": "[metadata] namespace",
              "value": ": transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.0: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.0.self_attn: transformers.models.llama.modeling_llama.LlamaAttention/slice_37: aten.slice.Tensor"
            },
            {
              "key": "[metadata] pkg.torch.onnx.class_hierarchy",
              "value": "['transformers.models.llama.modeling_llama.LlamaForCausalLM', 'transformers.models.llama.modeling_llama.LlamaModel', 'transformers.models.llama.modeling_llama.LlamaDecoderLayer', 'transformers.models.llama.modeling_llama.LlamaAttention', 'aten.slice.Tensor']"
            },
            {
              "key": "[metadata] pkg.torch.onnx.fx_node",
              "value": "%slice_37 : [num_users=1] = call_function[target=torch.ops.aten.slice.Tensor](args = (%slice_36, 3, None, %add_4), kwargs = {})"
            },
            {
              "key": "[metadata] pkg.torch.onnx.name_scopes",
              "value": "['', 'model', 'model.layers.0', 'model.layers.0.self_attn', 'slice_37']"
            },
            {
              "key": "[metadata] pkg.torch.onnx.stack_trace",
              "value": "File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 688, in forward\n    outputs: BaseModelOutputWithPast = self.model(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 453, in forward\n    layer_outputs = decoder_layer(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 308, in forward\n    hidden_states, self_attn_weights = self.self_attn(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 265, in forward\n    attn_output, attn_weights = attention_interface("
            }
          ],
          "incomingEdges": [],
          "outputsMetadata": [
            {
              "id": "0",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "val_335"
                },
                {
                  "key": "tensor_shape",
                  "value": "INT64[1]"
                },
                {
                  "key": "param_name",
                  "value": "output"
                }
              ]
            }
          ],
          "inputsMetadata": [],
          "style": null,
          "config": null
        },
        {
          "id": "node_Constant_456",
          "label": "Constant",
          "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.0: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.0.self_attn: transformers.models.llama.modeling_llama.LlamaAttention/slice_37: aten.slice.Tensor",
          "subgraphIds": [],
          "attrs": [
            {
              "key": "__value",
              "value": "[-1]"
            },
            {
              "key": "value_ints",
              "value": "[-1]"
            },
            {
              "key": "[metadata] namespace",
              "value": ": transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.0: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.0.self_attn: transformers.models.llama.modeling_llama.LlamaAttention/slice_37: aten.slice.Tensor"
            },
            {
              "key": "[metadata] pkg.torch.onnx.class_hierarchy",
              "value": "['transformers.models.llama.modeling_llama.LlamaForCausalLM', 'transformers.models.llama.modeling_llama.LlamaModel', 'transformers.models.llama.modeling_llama.LlamaDecoderLayer', 'transformers.models.llama.modeling_llama.LlamaAttention', 'aten.slice.Tensor']"
            },
            {
              "key": "[metadata] pkg.torch.onnx.fx_node",
              "value": "%slice_37 : [num_users=1] = call_function[target=torch.ops.aten.slice.Tensor](args = (%slice_36, 3, None, %add_4), kwargs = {})"
            },
            {
              "key": "[metadata] pkg.torch.onnx.name_scopes",
              "value": "['', 'model', 'model.layers.0', 'model.layers.0.self_attn', 'slice_37']"
            },
            {
              "key": "[metadata] pkg.torch.onnx.stack_trace",
              "value": "File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 688, in forward\n    outputs: BaseModelOutputWithPast = self.model(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 453, in forward\n    layer_outputs = decoder_layer(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 308, in forward\n    hidden_states, self_attn_weights = self.self_attn(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 265, in forward\n    attn_output, attn_weights = attention_interface("
            }
          ],
          "incomingEdges": [],
          "outputsMetadata": [
            {
              "id": "0",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "val_337"
                },
                {
                  "key": "tensor_shape",
                  "value": "INT64[1]"
                },
                {
                  "key": "param_name",
                  "value": "output"
                }
              ]
            }
          ],
          "inputsMetadata": [],
          "style": null,
          "config": null
        },
        {
          "id": "node_Reshape_457",
          "label": "Reshape",
          "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.0: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.0.self_attn: transformers.models.llama.modeling_llama.LlamaAttention/slice_37: aten.slice.Tensor",
          "subgraphIds": [],
          "attrs": [
            {
              "key": "allowzero",
              "value": "0"
            },
            {
              "key": "[metadata] namespace",
              "value": ": transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.0: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.0.self_attn: transformers.models.llama.modeling_llama.LlamaAttention/slice_37: aten.slice.Tensor"
            },
            {
              "key": "[metadata] pkg.torch.onnx.class_hierarchy",
              "value": "['transformers.models.llama.modeling_llama.LlamaForCausalLM', 'transformers.models.llama.modeling_llama.LlamaModel', 'transformers.models.llama.modeling_llama.LlamaDecoderLayer', 'transformers.models.llama.modeling_llama.LlamaAttention', 'aten.slice.Tensor']"
            },
            {
              "key": "[metadata] pkg.torch.onnx.fx_node",
              "value": "%slice_37 : [num_users=1] = call_function[target=torch.ops.aten.slice.Tensor](args = (%slice_36, 3, None, %add_4), kwargs = {})"
            },
            {
              "key": "[metadata] pkg.torch.onnx.name_scopes",
              "value": "['', 'model', 'model.layers.0', 'model.layers.0.self_attn', 'slice_37']"
            },
            {
              "key": "[metadata] pkg.torch.onnx.stack_trace",
              "value": "File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 688, in forward\n    outputs: BaseModelOutputWithPast = self.model(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 453, in forward\n    layer_outputs = decoder_layer(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 308, in forward\n    hidden_states, self_attn_weights = self.self_attn(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 265, in forward\n    attn_output, attn_weights = attention_interface("
            }
          ],
          "incomingEdges": [
            {
              "sourceNodeId": "node_Add_33",
              "sourceNodeOutputId": "0",
              "targetNodeInputId": "0"
            },
            {
              "sourceNodeId": "node_Constant_456",
              "sourceNodeOutputId": "0",
              "targetNodeInputId": "1"
            }
          ],
          "outputsMetadata": [
            {
              "id": "0",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "val_338"
                },
                {
                  "key": "tensor_shape",
                  "value": "INT64[1]"
                },
                {
                  "key": "param_name",
                  "value": "reshaped"
                }
              ]
            }
          ],
          "inputsMetadata": [
            {
              "id": "0",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "add_4"
                },
                {
                  "key": "tensor_shape",
                  "value": "INT64[]"
                },
                {
                  "key": "param_name",
                  "value": "data"
                }
              ]
            },
            {
              "id": "1",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "val_337"
                },
                {
                  "key": "tensor_shape",
                  "value": "INT64[1]"
                },
                {
                  "key": "param_name",
                  "value": "shape"
                }
              ]
            }
          ],
          "style": null,
          "config": null
        },
        {
          "id": "node_Constant_921",
          "label": "Constant",
          "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.0: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.0.self_attn: transformers.models.llama.modeling_llama.LlamaAttention/slice_37: aten.slice.Tensor",
          "subgraphIds": [],
          "attrs": [
            {
              "key": "__value",
              "value": "[3]"
            },
            {
              "key": "value",
              "value": "TensorProtoTensor<INT64,[1]>(array([3]), name='val_341')"
            }
          ],
          "incomingEdges": [],
          "outputsMetadata": [
            {
              "id": "0",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "val_341"
                },
                {
                  "key": "tensor_shape",
                  "value": "INT64[1]"
                },
                {
                  "key": "param_name",
                  "value": "output"
                }
              ]
            }
          ],
          "inputsMetadata": [],
          "style": null,
          "config": null
        },
        {
          "id": "node_Constant_461",
          "label": "Constant",
          "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.0: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.0.self_attn: transformers.models.llama.modeling_llama.LlamaAttention/slice_37: aten.slice.Tensor",
          "subgraphIds": [],
          "attrs": [
            {
              "key": "__value",
              "value": "[1]"
            },
            {
              "key": "value_ints",
              "value": "[1]"
            },
            {
              "key": "[metadata] namespace",
              "value": ": transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.0: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.0.self_attn: transformers.models.llama.modeling_llama.LlamaAttention/slice_37: aten.slice.Tensor"
            },
            {
              "key": "[metadata] pkg.torch.onnx.class_hierarchy",
              "value": "['transformers.models.llama.modeling_llama.LlamaForCausalLM', 'transformers.models.llama.modeling_llama.LlamaModel', 'transformers.models.llama.modeling_llama.LlamaDecoderLayer', 'transformers.models.llama.modeling_llama.LlamaAttention', 'aten.slice.Tensor']"
            },
            {
              "key": "[metadata] pkg.torch.onnx.fx_node",
              "value": "%slice_37 : [num_users=1] = call_function[target=torch.ops.aten.slice.Tensor](args = (%slice_36, 3, None, %add_4), kwargs = {})"
            },
            {
              "key": "[metadata] pkg.torch.onnx.name_scopes",
              "value": "['', 'model', 'model.layers.0', 'model.layers.0.self_attn', 'slice_37']"
            },
            {
              "key": "[metadata] pkg.torch.onnx.stack_trace",
              "value": "File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 688, in forward\n    outputs: BaseModelOutputWithPast = self.model(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 453, in forward\n    layer_outputs = decoder_layer(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 308, in forward\n    hidden_states, self_attn_weights = self.self_attn(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 265, in forward\n    attn_output, attn_weights = attention_interface("
            }
          ],
          "incomingEdges": [],
          "outputsMetadata": [
            {
              "id": "0",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "val_342"
                },
                {
                  "key": "tensor_shape",
                  "value": "INT64[1]"
                },
                {
                  "key": "param_name",
                  "value": "output"
                }
              ]
            }
          ],
          "inputsMetadata": [],
          "style": null,
          "config": null
        },
        {
          "id": "node_Slice_462",
          "label": "Slice",
          "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.0: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.0.self_attn: transformers.models.llama.modeling_llama.LlamaAttention/slice_37: aten.slice.Tensor",
          "subgraphIds": [],
          "attrs": [
            {
              "key": "[metadata] namespace",
              "value": ": transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.0: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.0.self_attn: transformers.models.llama.modeling_llama.LlamaAttention/slice_37: aten.slice.Tensor"
            },
            {
              "key": "[metadata] pkg.torch.onnx.class_hierarchy",
              "value": "['transformers.models.llama.modeling_llama.LlamaForCausalLM', 'transformers.models.llama.modeling_llama.LlamaModel', 'transformers.models.llama.modeling_llama.LlamaDecoderLayer', 'transformers.models.llama.modeling_llama.LlamaAttention', 'aten.slice.Tensor']"
            },
            {
              "key": "[metadata] pkg.torch.onnx.fx_node",
              "value": "%slice_37 : [num_users=1] = call_function[target=torch.ops.aten.slice.Tensor](args = (%slice_36, 3, None, %add_4), kwargs = {})"
            },
            {
              "key": "[metadata] pkg.torch.onnx.name_scopes",
              "value": "['', 'model', 'model.layers.0', 'model.layers.0.self_attn', 'slice_37']"
            },
            {
              "key": "[metadata] pkg.torch.onnx.stack_trace",
              "value": "File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 688, in forward\n    outputs: BaseModelOutputWithPast = self.model(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 453, in forward\n    layer_outputs = decoder_layer(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 308, in forward\n    hidden_states, self_attn_weights = self.self_attn(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 265, in forward\n    attn_output, attn_weights = attention_interface("
            }
          ],
          "incomingEdges": [
            {
              "sourceNodeId": "node_ScatterND_279",
              "sourceNodeOutputId": "0",
              "targetNodeInputId": "0"
            },
            {
              "sourceNodeId": "node_Constant_454",
              "sourceNodeOutputId": "0",
              "targetNodeInputId": "1"
            },
            {
              "sourceNodeId": "node_Reshape_457",
              "sourceNodeOutputId": "0",
              "targetNodeInputId": "2"
            },
            {
              "sourceNodeId": "node_Constant_921",
              "sourceNodeOutputId": "0",
              "targetNodeInputId": "3"
            },
            {
              "sourceNodeId": "node_Constant_461",
              "sourceNodeOutputId": "0",
              "targetNodeInputId": "4"
            }
          ],
          "outputsMetadata": [
            {
              "id": "0",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "slice_37"
                },
                {
                  "key": "tensor_shape",
                  "value": "FLOAT[s34,1,s16,s16 + s17]"
                },
                {
                  "key": "param_name",
                  "value": "output"
                }
              ]
            }
          ],
          "inputsMetadata": [
            {
              "id": "0",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "slice_scatter_2"
                },
                {
                  "key": "tensor_shape",
                  "value": "FLOAT[s34,1,s16,s16 + s17]"
                },
                {
                  "key": "param_name",
                  "value": "data"
                }
              ]
            },
            {
              "id": "1",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "val_335"
                },
                {
                  "key": "tensor_shape",
                  "value": "INT64[1]"
                },
                {
                  "key": "param_name",
                  "value": "starts"
                }
              ]
            },
            {
              "id": "2",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "val_338"
                },
                {
                  "key": "tensor_shape",
                  "value": "INT64[1]"
                },
                {
                  "key": "param_name",
                  "value": "ends"
                }
              ]
            },
            {
              "id": "3",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "val_341"
                },
                {
                  "key": "tensor_shape",
                  "value": "INT64[1]"
                },
                {
                  "key": "param_name",
                  "value": "axes"
                }
              ]
            },
            {
              "id": "4",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "val_342"
                },
                {
                  "key": "tensor_shape",
                  "value": "INT64[1]"
                },
                {
                  "key": "param_name",
                  "value": "steps"
                }
              ]
            }
          ],
          "style": null,
          "config": null
        },
        {
          "id": "node_Add_463",
          "label": "Add",
          "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.0: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.0.self_attn: transformers.models.llama.modeling_llama.LlamaAttention/add_374: aten.add.Tensor",
          "subgraphIds": [],
          "attrs": [
            {
              "key": "[metadata] namespace",
              "value": ": transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.0: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.0.self_attn: transformers.models.llama.modeling_llama.LlamaAttention/add_374: aten.add.Tensor"
            },
            {
              "key": "[metadata] pkg.torch.onnx.class_hierarchy",
              "value": "['transformers.models.llama.modeling_llama.LlamaForCausalLM', 'transformers.models.llama.modeling_llama.LlamaModel', 'transformers.models.llama.modeling_llama.LlamaDecoderLayer', 'transformers.models.llama.modeling_llama.LlamaAttention', 'aten.add.Tensor']"
            },
            {
              "key": "[metadata] pkg.torch.onnx.fx_node",
              "value": "%add_374 : [num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%mul_278, %slice_37), kwargs = {})"
            },
            {
              "key": "[metadata] pkg.torch.onnx.name_scopes",
              "value": "['', 'model', 'model.layers.0', 'model.layers.0.self_attn', 'add_374']"
            },
            {
              "key": "[metadata] pkg.torch.onnx.stack_trace",
              "value": "File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 688, in forward\n    outputs: BaseModelOutputWithPast = self.model(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 453, in forward\n    layer_outputs = decoder_layer(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 308, in forward\n    hidden_states, self_attn_weights = self.self_attn(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 265, in forward\n    attn_output, attn_weights = attention_interface("
            }
          ],
          "incomingEdges": [
            {
              "sourceNodeId": "node_Mul_432",
              "sourceNodeOutputId": "0",
              "targetNodeInputId": "0"
            },
            {
              "sourceNodeId": "node_Slice_462",
              "sourceNodeOutputId": "0",
              "targetNodeInputId": "1"
            }
          ],
          "outputsMetadata": [
            {
              "id": "0",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "add_374"
                },
                {
                  "key": "tensor_shape",
                  "value": "FLOAT[s34,32,s16,s16 + s17]"
                },
                {
                  "key": "param_name",
                  "value": "C"
                }
              ]
            }
          ],
          "inputsMetadata": [
            {
              "id": "0",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "mul_278"
                },
                {
                  "key": "tensor_shape",
                  "value": "FLOAT[s34,32,s16,s16 + s17]"
                },
                {
                  "key": "param_name",
                  "value": "A"
                }
              ]
            },
            {
              "id": "1",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "slice_37"
                },
                {
                  "key": "tensor_shape",
                  "value": "FLOAT[s34,1,s16,s16 + s17]"
                },
                {
                  "key": "param_name",
                  "value": "B"
                }
              ]
            }
          ],
          "style": null,
          "config": null
        },
        {
          "id": "node_Softmax_464",
          "label": "Softmax",
          "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.0: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.0.self_attn: transformers.models.llama.modeling_llama.LlamaAttention/softmax: aten.softmax.int",
          "subgraphIds": [],
          "attrs": [
            {
              "key": "axis",
              "value": "-1"
            },
            {
              "key": "[metadata] namespace",
              "value": ": transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.0: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.0.self_attn: transformers.models.llama.modeling_llama.LlamaAttention/softmax: aten.softmax.int"
            },
            {
              "key": "[metadata] pkg.torch.onnx.class_hierarchy",
              "value": "['transformers.models.llama.modeling_llama.LlamaForCausalLM', 'transformers.models.llama.modeling_llama.LlamaModel', 'transformers.models.llama.modeling_llama.LlamaDecoderLayer', 'transformers.models.llama.modeling_llama.LlamaAttention', 'aten.softmax.int']"
            },
            {
              "key": "[metadata] pkg.torch.onnx.fx_node",
              "value": "%softmax : [num_users=1] = call_function[target=torch.ops.aten.softmax.int](args = (%add_374, -1, torch.float32), kwargs = {})"
            },
            {
              "key": "[metadata] pkg.torch.onnx.name_scopes",
              "value": "['', 'model', 'model.layers.0', 'model.layers.0.self_attn', 'softmax']"
            },
            {
              "key": "[metadata] pkg.torch.onnx.stack_trace",
              "value": "File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 688, in forward\n    outputs: BaseModelOutputWithPast = self.model(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 453, in forward\n    layer_outputs = decoder_layer(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 308, in forward\n    hidden_states, self_attn_weights = self.self_attn(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 265, in forward\n    attn_output, attn_weights = attention_interface("
            }
          ],
          "incomingEdges": [
            {
              "sourceNodeId": "node_Add_463",
              "sourceNodeOutputId": "0",
              "targetNodeInputId": "0"
            }
          ],
          "outputsMetadata": [
            {
              "id": "0",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "val_343"
                },
                {
                  "key": "tensor_shape",
                  "value": "FLOAT[s34,32,s16,s16 + s17]"
                },
                {
                  "key": "param_name",
                  "value": "output"
                }
              ]
            }
          ],
          "inputsMetadata": [
            {
              "id": "0",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "add_374"
                },
                {
                  "key": "tensor_shape",
                  "value": "FLOAT[s34,32,s16,s16 + s17]"
                },
                {
                  "key": "param_name",
                  "value": "input"
                }
              ]
            }
          ],
          "style": null,
          "config": null
        },
        {
          "id": "node_MatMul_467",
          "label": "MatMul",
          "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.0: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.0.self_attn: transformers.models.llama.modeling_llama.LlamaAttention/matmul_2: aten.matmul.default",
          "subgraphIds": [],
          "attrs": [
            {
              "key": "[metadata] namespace",
              "value": ": transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.0: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.0.self_attn: transformers.models.llama.modeling_llama.LlamaAttention/matmul_2: aten.matmul.default"
            },
            {
              "key": "[metadata] pkg.torch.onnx.class_hierarchy",
              "value": "['transformers.models.llama.modeling_llama.LlamaForCausalLM', 'transformers.models.llama.modeling_llama.LlamaModel', 'transformers.models.llama.modeling_llama.LlamaDecoderLayer', 'transformers.models.llama.modeling_llama.LlamaAttention', 'aten.matmul.default']"
            },
            {
              "key": "[metadata] pkg.torch.onnx.fx_node",
              "value": "%matmul_2 : [num_users=1] = call_function[target=torch.ops.aten.matmul.default](args = (%clone_1, %cat_4), kwargs = {})"
            },
            {
              "key": "[metadata] pkg.torch.onnx.name_scopes",
              "value": "['', 'model', 'model.layers.0', 'model.layers.0.self_attn', 'matmul_2']"
            },
            {
              "key": "[metadata] pkg.torch.onnx.stack_trace",
              "value": "File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 688, in forward\n    outputs: BaseModelOutputWithPast = self.model(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 453, in forward\n    layer_outputs = decoder_layer(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 308, in forward\n    hidden_states, self_attn_weights = self.self_attn(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 265, in forward\n    attn_output, attn_weights = attention_interface("
            }
          ],
          "incomingEdges": [
            {
              "sourceNodeId": "node_Softmax_464",
              "sourceNodeOutputId": "0",
              "targetNodeInputId": "0"
            },
            {
              "sourceNodeId": "node_Concat_428",
              "sourceNodeOutputId": "0",
              "targetNodeInputId": "1"
            }
          ],
          "outputsMetadata": [
            {
              "id": "0",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "matmul_2"
                },
                {
                  "key": "tensor_shape",
                  "value": "FLOAT[s34,32,s16,64]"
                },
                {
                  "key": "param_name",
                  "value": "Y"
                }
              ]
            }
          ],
          "inputsMetadata": [
            {
              "id": "0",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "val_343"
                },
                {
                  "key": "tensor_shape",
                  "value": "FLOAT[s34,32,s16,s16 + s17]"
                },
                {
                  "key": "param_name",
                  "value": "A"
                }
              ]
            },
            {
              "id": "1",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "cat_4"
                },
                {
                  "key": "tensor_shape",
                  "value": "FLOAT[s34,32,s16 + s17,64]"
                },
                {
                  "key": "[metadata] pkg.torch.export.graph_signature.OutputSpec.kind",
                  "value": "USER_OUTPUT"
                },
                {
                  "key": "param_name",
                  "value": "B"
                }
              ]
            }
          ],
          "style": null,
          "config": null
        },
        {
          "id": "node_Transpose_468",
          "label": "Transpose",
          "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.0: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.0.self_attn: transformers.models.llama.modeling_llama.LlamaAttention/transpose_5: aten.transpose.int",
          "subgraphIds": [],
          "attrs": [
            {
              "key": "perm",
              "value": "[0, 2, 1, 3]"
            },
            {
              "key": "[metadata] namespace",
              "value": ": transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.0: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.0.self_attn: transformers.models.llama.modeling_llama.LlamaAttention/transpose_5: aten.transpose.int"
            },
            {
              "key": "[metadata] pkg.torch.onnx.class_hierarchy",
              "value": "['transformers.models.llama.modeling_llama.LlamaForCausalLM', 'transformers.models.llama.modeling_llama.LlamaModel', 'transformers.models.llama.modeling_llama.LlamaDecoderLayer', 'transformers.models.llama.modeling_llama.LlamaAttention', 'aten.transpose.int']"
            },
            {
              "key": "[metadata] pkg.torch.onnx.fx_node",
              "value": "%transpose_5 : [num_users=1] = call_function[target=torch.ops.aten.transpose.int](args = (%matmul_2, 1, 2), kwargs = {})"
            },
            {
              "key": "[metadata] pkg.torch.onnx.name_scopes",
              "value": "['', 'model', 'model.layers.0', 'model.layers.0.self_attn', 'transpose_5']"
            },
            {
              "key": "[metadata] pkg.torch.onnx.stack_trace",
              "value": "File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 688, in forward\n    outputs: BaseModelOutputWithPast = self.model(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 453, in forward\n    layer_outputs = decoder_layer(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 308, in forward\n    hidden_states, self_attn_weights = self.self_attn(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 265, in forward\n    attn_output, attn_weights = attention_interface("
            }
          ],
          "incomingEdges": [
            {
              "sourceNodeId": "node_MatMul_467",
              "sourceNodeOutputId": "0",
              "targetNodeInputId": "0"
            }
          ],
          "outputsMetadata": [
            {
              "id": "0",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "transpose_5"
                },
                {
                  "key": "tensor_shape",
                  "value": "FLOAT[s34,s16,32,64]"
                },
                {
                  "key": "param_name",
                  "value": "transposed"
                }
              ]
            }
          ],
          "inputsMetadata": [
            {
              "id": "0",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "matmul_2"
                },
                {
                  "key": "tensor_shape",
                  "value": "FLOAT[s34,32,s16,64]"
                },
                {
                  "key": "param_name",
                  "value": "data"
                }
              ]
            }
          ],
          "style": null,
          "config": null
        },
        {
          "id": "node_Concat_474",
          "label": "Concat",
          "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.0: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.0.self_attn: transformers.models.llama.modeling_llama.LlamaAttention/view_4: aten.view.default",
          "subgraphIds": [],
          "attrs": [
            {
              "key": "axis",
              "value": "0"
            },
            {
              "key": "[metadata] namespace",
              "value": ": transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.0: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.0.self_attn: transformers.models.llama.modeling_llama.LlamaAttention/view_4: aten.view.default"
            },
            {
              "key": "[metadata] pkg.torch.onnx.class_hierarchy",
              "value": "['transformers.models.llama.modeling_llama.LlamaForCausalLM', 'transformers.models.llama.modeling_llama.LlamaModel', 'transformers.models.llama.modeling_llama.LlamaDecoderLayer', 'transformers.models.llama.modeling_llama.LlamaAttention', 'aten.view.default']"
            },
            {
              "key": "[metadata] pkg.torch.onnx.fx_node",
              "value": "%view_4 : [num_users=1] = call_function[target=torch.ops.aten.view.default](args = (%clone_2, [%sym_size_int_68, %sym_size_int_61, -1]), kwargs = {})"
            },
            {
              "key": "[metadata] pkg.torch.onnx.name_scopes",
              "value": "['', 'model', 'model.layers.0', 'model.layers.0.self_attn', 'view_4']"
            },
            {
              "key": "[metadata] pkg.torch.onnx.stack_trace",
              "value": "File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 688, in forward\n    outputs: BaseModelOutputWithPast = self.model(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 453, in forward\n    layer_outputs = decoder_layer(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 308, in forward\n    hidden_states, self_attn_weights = self.self_attn(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 276, in forward\n    attn_output = attn_output.reshape(*input_shape, -1).contiguous()"
            }
          ],
          "incomingEdges": [
            {
              "sourceNodeId": "node_Shape_12",
              "sourceNodeOutputId": "0",
              "targetNodeInputId": "0"
            },
            {
              "sourceNodeId": "node_Shape_2",
              "sourceNodeOutputId": "0",
              "targetNodeInputId": "1"
            },
            {
              "sourceNodeId": "node_Constant_90",
              "sourceNodeOutputId": "0",
              "targetNodeInputId": "2"
            }
          ],
          "outputsMetadata": [
            {
              "id": "0",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "val_348"
                },
                {
                  "key": "tensor_shape",
                  "value": "INT64[3]"
                },
                {
                  "key": "param_name",
                  "value": "concat_result"
                }
              ]
            }
          ],
          "inputsMetadata": [
            {
              "id": "0",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "val_6"
                },
                {
                  "key": "tensor_shape",
                  "value": "INT64[1]"
                },
                {
                  "key": "param_name",
                  "value": "inputs"
                }
              ]
            },
            {
              "id": "1",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "val_1"
                },
                {
                  "key": "tensor_shape",
                  "value": "INT64[1]"
                },
                {
                  "key": "param_name",
                  "value": "inputs"
                }
              ]
            },
            {
              "id": "2",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "val_53"
                },
                {
                  "key": "tensor_shape",
                  "value": "INT64[1]"
                },
                {
                  "key": "param_name",
                  "value": "inputs"
                }
              ]
            }
          ],
          "style": null,
          "config": null
        },
        {
          "id": "node_Reshape_476",
          "label": "Reshape",
          "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.0: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.0.self_attn: transformers.models.llama.modeling_llama.LlamaAttention/view_4: aten.view.default",
          "subgraphIds": [],
          "attrs": [
            {
              "key": "allowzero",
              "value": "1"
            },
            {
              "key": "[metadata] namespace",
              "value": ": transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.0: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.0.self_attn: transformers.models.llama.modeling_llama.LlamaAttention/view_4: aten.view.default"
            },
            {
              "key": "[metadata] pkg.torch.onnx.class_hierarchy",
              "value": "['transformers.models.llama.modeling_llama.LlamaForCausalLM', 'transformers.models.llama.modeling_llama.LlamaModel', 'transformers.models.llama.modeling_llama.LlamaDecoderLayer', 'transformers.models.llama.modeling_llama.LlamaAttention', 'aten.view.default']"
            },
            {
              "key": "[metadata] pkg.torch.onnx.fx_node",
              "value": "%view_4 : [num_users=1] = call_function[target=torch.ops.aten.view.default](args = (%clone_2, [%sym_size_int_68, %sym_size_int_61, -1]), kwargs = {})"
            },
            {
              "key": "[metadata] pkg.torch.onnx.name_scopes",
              "value": "['', 'model', 'model.layers.0', 'model.layers.0.self_attn', 'view_4']"
            },
            {
              "key": "[metadata] pkg.torch.onnx.stack_trace",
              "value": "File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 688, in forward\n    outputs: BaseModelOutputWithPast = self.model(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 453, in forward\n    layer_outputs = decoder_layer(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 308, in forward\n    hidden_states, self_attn_weights = self.self_attn(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 276, in forward\n    attn_output = attn_output.reshape(*input_shape, -1).contiguous()"
            }
          ],
          "incomingEdges": [
            {
              "sourceNodeId": "node_Transpose_468",
              "sourceNodeOutputId": "0",
              "targetNodeInputId": "0"
            },
            {
              "sourceNodeId": "node_Concat_474",
              "sourceNodeOutputId": "0",
              "targetNodeInputId": "1"
            }
          ],
          "outputsMetadata": [
            {
              "id": "0",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "view_4"
                },
                {
                  "key": "tensor_shape",
                  "value": "FLOAT[s34,s16,2048]"
                },
                {
                  "key": "param_name",
                  "value": "reshaped"
                }
              ]
            }
          ],
          "inputsMetadata": [
            {
              "id": "0",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "transpose_5"
                },
                {
                  "key": "tensor_shape",
                  "value": "FLOAT[s34,s16,32,64]"
                },
                {
                  "key": "param_name",
                  "value": "data"
                }
              ]
            },
            {
              "id": "1",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "val_348"
                },
                {
                  "key": "tensor_shape",
                  "value": "INT64[3]"
                },
                {
                  "key": "param_name",
                  "value": "shape"
                }
              ]
            }
          ],
          "style": null,
          "config": null
        },
        {
          "id": "node_Transpose_477",
          "label": "Transpose",
          "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.0: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.0.self_attn: transformers.models.llama.modeling_llama.LlamaAttention/model.layers.0.self_attn.o_proj: torch.nn.modules.linear.Linear/linear_3: aten.linear.default",
          "subgraphIds": [],
          "attrs": [
            {
              "key": "perm",
              "value": "[1, 0]"
            },
            {
              "key": "[metadata] namespace",
              "value": ": transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.0: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.0.self_attn: transformers.models.llama.modeling_llama.LlamaAttention/model.layers.0.self_attn.o_proj: torch.nn.modules.linear.Linear/linear_3: aten.linear.default"
            },
            {
              "key": "[metadata] pkg.torch.onnx.class_hierarchy",
              "value": "['transformers.models.llama.modeling_llama.LlamaForCausalLM', 'transformers.models.llama.modeling_llama.LlamaModel', 'transformers.models.llama.modeling_llama.LlamaDecoderLayer', 'transformers.models.llama.modeling_llama.LlamaAttention', 'torch.nn.modules.linear.Linear', 'aten.linear.default']"
            },
            {
              "key": "[metadata] pkg.torch.onnx.fx_node",
              "value": "%linear_3 : [num_users=1] = call_function[target=torch.ops.aten.linear.default](args = (%view_4, %p_model_layers_0_self_attn_o_proj_weight), kwargs = {})"
            },
            {
              "key": "[metadata] pkg.torch.onnx.name_scopes",
              "value": "['', 'model', 'model.layers.0', 'model.layers.0.self_attn', 'model.layers.0.self_attn.o_proj', 'linear_3']"
            },
            {
              "key": "[metadata] pkg.torch.onnx.stack_trace",
              "value": "File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 688, in forward\n    outputs: BaseModelOutputWithPast = self.model(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 453, in forward\n    layer_outputs = decoder_layer(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 308, in forward\n    hidden_states, self_attn_weights = self.self_attn(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 277, in forward\n    attn_output = self.o_proj(attn_output)\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/torch/nn/modules/linear.py\", line 125, in forward\n    return F.linear(input, self.weight, self.bias)"
            }
          ],
          "incomingEdges": [
            {
              "sourceNodeId": "[value] model.layers.0.self_attn.o_proj.weight",
              "sourceNodeOutputId": "0",
              "targetNodeInputId": "0"
            }
          ],
          "outputsMetadata": [
            {
              "id": "0",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "val_350"
                },
                {
                  "key": "tensor_shape",
                  "value": "FLOAT[2048,2048]"
                },
                {
                  "key": "param_name",
                  "value": "transposed"
                }
              ]
            }
          ],
          "inputsMetadata": [
            {
              "id": "0",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "model.layers.0.self_attn.o_proj.weight"
                },
                {
                  "key": "tensor_shape",
                  "value": "FLOAT[2048,2048]"
                },
                {
                  "key": "param_name",
                  "value": "data"
                }
              ]
            }
          ],
          "style": null,
          "config": null
        },
        {
          "id": "node_MatMul_478",
          "label": "MatMul",
          "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.0: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.0.self_attn: transformers.models.llama.modeling_llama.LlamaAttention/model.layers.0.self_attn.o_proj: torch.nn.modules.linear.Linear/linear_3: aten.linear.default",
          "subgraphIds": [],
          "attrs": [
            {
              "key": "[metadata] namespace",
              "value": ": transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.0: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.0.self_attn: transformers.models.llama.modeling_llama.LlamaAttention/model.layers.0.self_attn.o_proj: torch.nn.modules.linear.Linear/linear_3: aten.linear.default"
            },
            {
              "key": "[metadata] pkg.torch.onnx.class_hierarchy",
              "value": "['transformers.models.llama.modeling_llama.LlamaForCausalLM', 'transformers.models.llama.modeling_llama.LlamaModel', 'transformers.models.llama.modeling_llama.LlamaDecoderLayer', 'transformers.models.llama.modeling_llama.LlamaAttention', 'torch.nn.modules.linear.Linear', 'aten.linear.default']"
            },
            {
              "key": "[metadata] pkg.torch.onnx.fx_node",
              "value": "%linear_3 : [num_users=1] = call_function[target=torch.ops.aten.linear.default](args = (%view_4, %p_model_layers_0_self_attn_o_proj_weight), kwargs = {})"
            },
            {
              "key": "[metadata] pkg.torch.onnx.name_scopes",
              "value": "['', 'model', 'model.layers.0', 'model.layers.0.self_attn', 'model.layers.0.self_attn.o_proj', 'linear_3']"
            },
            {
              "key": "[metadata] pkg.torch.onnx.stack_trace",
              "value": "File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 688, in forward\n    outputs: BaseModelOutputWithPast = self.model(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 453, in forward\n    layer_outputs = decoder_layer(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 308, in forward\n    hidden_states, self_attn_weights = self.self_attn(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 277, in forward\n    attn_output = self.o_proj(attn_output)\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/torch/nn/modules/linear.py\", line 125, in forward\n    return F.linear(input, self.weight, self.bias)"
            }
          ],
          "incomingEdges": [
            {
              "sourceNodeId": "node_Reshape_476",
              "sourceNodeOutputId": "0",
              "targetNodeInputId": "0"
            },
            {
              "sourceNodeId": "node_Transpose_477",
              "sourceNodeOutputId": "0",
              "targetNodeInputId": "1"
            }
          ],
          "outputsMetadata": [
            {
              "id": "0",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "linear_3"
                },
                {
                  "key": "tensor_shape",
                  "value": "FLOAT[s34,s16,2048]"
                },
                {
                  "key": "param_name",
                  "value": "Y"
                }
              ]
            }
          ],
          "inputsMetadata": [
            {
              "id": "0",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "view_4"
                },
                {
                  "key": "tensor_shape",
                  "value": "FLOAT[s34,s16,2048]"
                },
                {
                  "key": "param_name",
                  "value": "A"
                }
              ]
            },
            {
              "id": "1",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "val_350"
                },
                {
                  "key": "tensor_shape",
                  "value": "FLOAT[2048,2048]"
                },
                {
                  "key": "param_name",
                  "value": "B"
                }
              ]
            }
          ],
          "style": null,
          "config": null
        },
        {
          "id": "node_Add_479",
          "label": "Add",
          "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.0: transformers.models.llama.modeling_llama.LlamaDecoderLayer/add_413: aten.add.Tensor",
          "subgraphIds": [],
          "attrs": [
            {
              "key": "[metadata] namespace",
              "value": ": transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.0: transformers.models.llama.modeling_llama.LlamaDecoderLayer/add_413: aten.add.Tensor"
            },
            {
              "key": "[metadata] pkg.torch.onnx.class_hierarchy",
              "value": "['transformers.models.llama.modeling_llama.LlamaForCausalLM', 'transformers.models.llama.modeling_llama.LlamaModel', 'transformers.models.llama.modeling_llama.LlamaDecoderLayer', 'aten.add.Tensor']"
            },
            {
              "key": "[metadata] pkg.torch.onnx.fx_node",
              "value": "%add_413 : [num_users=3] = call_function[target=torch.ops.aten.add.Tensor](args = (%embedding, %linear_3), kwargs = {})"
            },
            {
              "key": "[metadata] pkg.torch.onnx.name_scopes",
              "value": "['', 'model', 'model.layers.0', 'add_413']"
            },
            {
              "key": "[metadata] pkg.torch.onnx.stack_trace",
              "value": "File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 688, in forward\n    outputs: BaseModelOutputWithPast = self.model(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 453, in forward\n    layer_outputs = decoder_layer(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 319, in forward\n    hidden_states = residual + hidden_states"
            }
          ],
          "incomingEdges": [
            {
              "sourceNodeId": "node_Gather_20",
              "sourceNodeOutputId": "0",
              "targetNodeInputId": "0"
            },
            {
              "sourceNodeId": "node_MatMul_478",
              "sourceNodeOutputId": "0",
              "targetNodeInputId": "1"
            }
          ],
          "outputsMetadata": [
            {
              "id": "0",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "add_413"
                },
                {
                  "key": "tensor_shape",
                  "value": "FLOAT[s34,s16,2048]"
                },
                {
                  "key": "param_name",
                  "value": "C"
                }
              ]
            }
          ],
          "inputsMetadata": [
            {
              "id": "0",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "embedding"
                },
                {
                  "key": "tensor_shape",
                  "value": "FLOAT[s34,s16,2048]"
                },
                {
                  "key": "param_name",
                  "value": "A"
                }
              ]
            },
            {
              "id": "1",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "linear_3"
                },
                {
                  "key": "tensor_shape",
                  "value": "FLOAT[s34,s16,2048]"
                },
                {
                  "key": "param_name",
                  "value": "B"
                }
              ]
            }
          ],
          "style": null,
          "config": null
        },
        {
          "id": "node_Constant_480",
          "label": "Constant",
          "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.0: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.0.post_attention_layernorm: transformers.models.llama.modeling_llama.LlamaRMSNorm/pow_2: aten.pow.Tensor_Scalar",
          "subgraphIds": [],
          "attrs": [
            {
              "key": "__value",
              "value": "2.0"
            },
            {
              "key": "value",
              "value": "TensorProtoTensor<FLOAT,[]>(array(2., dtype=float32), name='')"
            },
            {
              "key": "[metadata] namespace",
              "value": ": transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.0: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.0.post_attention_layernorm: transformers.models.llama.modeling_llama.LlamaRMSNorm/pow_2: aten.pow.Tensor_Scalar"
            },
            {
              "key": "[metadata] pkg.torch.onnx.class_hierarchy",
              "value": "['transformers.models.llama.modeling_llama.LlamaForCausalLM', 'transformers.models.llama.modeling_llama.LlamaModel', 'transformers.models.llama.modeling_llama.LlamaDecoderLayer', 'transformers.models.llama.modeling_llama.LlamaRMSNorm', 'aten.pow.Tensor_Scalar']"
            },
            {
              "key": "[metadata] pkg.torch.onnx.fx_node",
              "value": "%pow_2 : [num_users=1] = call_function[target=torch.ops.aten.pow.Tensor_Scalar](args = (%add_413, 2), kwargs = {})"
            },
            {
              "key": "[metadata] pkg.torch.onnx.name_scopes",
              "value": "['', 'model', 'model.layers.0', 'model.layers.0.post_attention_layernorm', 'pow_2']"
            },
            {
              "key": "[metadata] pkg.torch.onnx.stack_trace",
              "value": "File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 688, in forward\n    outputs: BaseModelOutputWithPast = self.model(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 453, in forward\n    layer_outputs = decoder_layer(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 323, in forward\n    hidden_states = self.post_attention_layernorm(hidden_states)\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 71, in forward\n    variance = hidden_states.pow(2).mean(-1, keepdim=True)"
            }
          ],
          "incomingEdges": [],
          "outputsMetadata": [
            {
              "id": "0",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "val_351"
                },
                {
                  "key": "tensor_shape",
                  "value": "FLOAT[]"
                },
                {
                  "key": "param_name",
                  "value": "output"
                }
              ]
            }
          ],
          "inputsMetadata": [],
          "style": null,
          "config": null
        },
        {
          "id": "node_Pow_481",
          "label": "Pow",
          "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.0: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.0.post_attention_layernorm: transformers.models.llama.modeling_llama.LlamaRMSNorm/pow_2: aten.pow.Tensor_Scalar",
          "subgraphIds": [],
          "attrs": [
            {
              "key": "[metadata] namespace",
              "value": ": transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.0: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.0.post_attention_layernorm: transformers.models.llama.modeling_llama.LlamaRMSNorm/pow_2: aten.pow.Tensor_Scalar"
            },
            {
              "key": "[metadata] pkg.torch.onnx.class_hierarchy",
              "value": "['transformers.models.llama.modeling_llama.LlamaForCausalLM', 'transformers.models.llama.modeling_llama.LlamaModel', 'transformers.models.llama.modeling_llama.LlamaDecoderLayer', 'transformers.models.llama.modeling_llama.LlamaRMSNorm', 'aten.pow.Tensor_Scalar']"
            },
            {
              "key": "[metadata] pkg.torch.onnx.fx_node",
              "value": "%pow_2 : [num_users=1] = call_function[target=torch.ops.aten.pow.Tensor_Scalar](args = (%add_413, 2), kwargs = {})"
            },
            {
              "key": "[metadata] pkg.torch.onnx.name_scopes",
              "value": "['', 'model', 'model.layers.0', 'model.layers.0.post_attention_layernorm', 'pow_2']"
            },
            {
              "key": "[metadata] pkg.torch.onnx.stack_trace",
              "value": "File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 688, in forward\n    outputs: BaseModelOutputWithPast = self.model(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 453, in forward\n    layer_outputs = decoder_layer(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 323, in forward\n    hidden_states = self.post_attention_layernorm(hidden_states)\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 71, in forward\n    variance = hidden_states.pow(2).mean(-1, keepdim=True)"
            }
          ],
          "incomingEdges": [
            {
              "sourceNodeId": "node_Add_479",
              "sourceNodeOutputId": "0",
              "targetNodeInputId": "0"
            },
            {
              "sourceNodeId": "node_Constant_480",
              "sourceNodeOutputId": "0",
              "targetNodeInputId": "1"
            }
          ],
          "outputsMetadata": [
            {
              "id": "0",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "pow_2"
                },
                {
                  "key": "tensor_shape",
                  "value": "FLOAT[s34,s16,2048]"
                },
                {
                  "key": "param_name",
                  "value": "Z"
                }
              ]
            }
          ],
          "inputsMetadata": [
            {
              "id": "0",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "add_413"
                },
                {
                  "key": "tensor_shape",
                  "value": "FLOAT[s34,s16,2048]"
                },
                {
                  "key": "param_name",
                  "value": "X"
                }
              ]
            },
            {
              "id": "1",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "val_351"
                },
                {
                  "key": "tensor_shape",
                  "value": "FLOAT[]"
                },
                {
                  "key": "param_name",
                  "value": "Y"
                }
              ]
            }
          ],
          "style": null,
          "config": null
        },
        {
          "id": "node_Constant_924",
          "label": "Constant",
          "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.0: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.0.post_attention_layernorm: transformers.models.llama.modeling_llama.LlamaRMSNorm/mean_1: aten.mean.dim",
          "subgraphIds": [],
          "attrs": [
            {
              "key": "__value",
              "value": "[-1]"
            },
            {
              "key": "value",
              "value": "TensorProtoTensor<INT64,[1]>(array([-1]), name='val_353')"
            }
          ],
          "incomingEdges": [],
          "outputsMetadata": [
            {
              "id": "0",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "val_353"
                },
                {
                  "key": "tensor_shape",
                  "value": "INT64[1]"
                },
                {
                  "key": "param_name",
                  "value": "output"
                }
              ]
            }
          ],
          "inputsMetadata": [],
          "style": null,
          "config": null
        },
        {
          "id": "node_ReduceMean_484",
          "label": "ReduceMean",
          "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.0: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.0.post_attention_layernorm: transformers.models.llama.modeling_llama.LlamaRMSNorm/mean_1: aten.mean.dim",
          "subgraphIds": [],
          "attrs": [
            {
              "key": "noop_with_empty_axes",
              "value": "0"
            },
            {
              "key": "keepdims",
              "value": "1"
            },
            {
              "key": "[metadata] namespace",
              "value": ": transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.0: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.0.post_attention_layernorm: transformers.models.llama.modeling_llama.LlamaRMSNorm/mean_1: aten.mean.dim"
            },
            {
              "key": "[metadata] pkg.torch.onnx.class_hierarchy",
              "value": "['transformers.models.llama.modeling_llama.LlamaForCausalLM', 'transformers.models.llama.modeling_llama.LlamaModel', 'transformers.models.llama.modeling_llama.LlamaDecoderLayer', 'transformers.models.llama.modeling_llama.LlamaRMSNorm', 'aten.mean.dim']"
            },
            {
              "key": "[metadata] pkg.torch.onnx.fx_node",
              "value": "%mean_1 : [num_users=1] = call_function[target=torch.ops.aten.mean.dim](args = (%pow_2, [-1], True), kwargs = {})"
            },
            {
              "key": "[metadata] pkg.torch.onnx.name_scopes",
              "value": "['', 'model', 'model.layers.0', 'model.layers.0.post_attention_layernorm', 'mean_1']"
            },
            {
              "key": "[metadata] pkg.torch.onnx.stack_trace",
              "value": "File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 688, in forward\n    outputs: BaseModelOutputWithPast = self.model(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 453, in forward\n    layer_outputs = decoder_layer(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 323, in forward\n    hidden_states = self.post_attention_layernorm(hidden_states)\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 71, in forward\n    variance = hidden_states.pow(2).mean(-1, keepdim=True)"
            }
          ],
          "incomingEdges": [
            {
              "sourceNodeId": "node_Pow_481",
              "sourceNodeOutputId": "0",
              "targetNodeInputId": "0"
            },
            {
              "sourceNodeId": "node_Constant_924",
              "sourceNodeOutputId": "0",
              "targetNodeInputId": "1"
            }
          ],
          "outputsMetadata": [
            {
              "id": "0",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "mean_1"
                },
                {
                  "key": "tensor_shape",
                  "value": "FLOAT[s34,s16,1]"
                },
                {
                  "key": "param_name",
                  "value": "reduced"
                }
              ]
            }
          ],
          "inputsMetadata": [
            {
              "id": "0",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "pow_2"
                },
                {
                  "key": "tensor_shape",
                  "value": "FLOAT[s34,s16,2048]"
                },
                {
                  "key": "param_name",
                  "value": "data"
                }
              ]
            },
            {
              "id": "1",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "val_353"
                },
                {
                  "key": "tensor_shape",
                  "value": "INT64[1]"
                },
                {
                  "key": "param_name",
                  "value": "axes"
                }
              ]
            }
          ],
          "style": null,
          "config": null
        },
        {
          "id": "node_Add_485",
          "label": "Add",
          "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.0: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.0.post_attention_layernorm: transformers.models.llama.modeling_llama.LlamaRMSNorm/add_426: aten.add.Tensor",
          "subgraphIds": [],
          "attrs": [
            {
              "key": "[metadata] namespace",
              "value": ": transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.0: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.0.post_attention_layernorm: transformers.models.llama.modeling_llama.LlamaRMSNorm/add_426: aten.add.Tensor"
            },
            {
              "key": "[metadata] pkg.torch.onnx.class_hierarchy",
              "value": "['transformers.models.llama.modeling_llama.LlamaForCausalLM', 'transformers.models.llama.modeling_llama.LlamaModel', 'transformers.models.llama.modeling_llama.LlamaDecoderLayer', 'transformers.models.llama.modeling_llama.LlamaRMSNorm', 'aten.add.Tensor']"
            },
            {
              "key": "[metadata] pkg.torch.onnx.fx_node",
              "value": "%add_426 : [num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%mean_1, 1e-05), kwargs = {})"
            },
            {
              "key": "[metadata] pkg.torch.onnx.name_scopes",
              "value": "['', 'model', 'model.layers.0', 'model.layers.0.post_attention_layernorm', 'add_426']"
            },
            {
              "key": "[metadata] pkg.torch.onnx.stack_trace",
              "value": "File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 688, in forward\n    outputs: BaseModelOutputWithPast = self.model(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 453, in forward\n    layer_outputs = decoder_layer(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 323, in forward\n    hidden_states = self.post_attention_layernorm(hidden_states)\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 72, in forward\n    hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)"
            }
          ],
          "incomingEdges": [
            {
              "sourceNodeId": "node_ReduceMean_484",
              "sourceNodeOutputId": "0",
              "targetNodeInputId": "0"
            },
            {
              "sourceNodeId": "node_Constant_333",
              "sourceNodeOutputId": "0",
              "targetNodeInputId": "1"
            }
          ],
          "outputsMetadata": [
            {
              "id": "0",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "add_426"
                },
                {
                  "key": "tensor_shape",
                  "value": "FLOAT[s34,s16,1]"
                },
                {
                  "key": "param_name",
                  "value": "C"
                }
              ]
            }
          ],
          "inputsMetadata": [
            {
              "id": "0",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "mean_1"
                },
                {
                  "key": "tensor_shape",
                  "value": "FLOAT[s34,s16,1]"
                },
                {
                  "key": "param_name",
                  "value": "A"
                }
              ]
            },
            {
              "id": "1",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "val_251"
                },
                {
                  "key": "tensor_shape",
                  "value": "FLOAT[]"
                },
                {
                  "key": "param_name",
                  "value": "B"
                }
              ]
            }
          ],
          "style": null,
          "config": null
        },
        {
          "id": "node_Sqrt_486",
          "label": "Sqrt",
          "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.0: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.0.post_attention_layernorm: transformers.models.llama.modeling_llama.LlamaRMSNorm/rsqrt_1: aten.rsqrt.default",
          "subgraphIds": [],
          "attrs": [
            {
              "key": "[metadata] namespace",
              "value": ": transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.0: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.0.post_attention_layernorm: transformers.models.llama.modeling_llama.LlamaRMSNorm/rsqrt_1: aten.rsqrt.default"
            },
            {
              "key": "[metadata] pkg.torch.onnx.class_hierarchy",
              "value": "['transformers.models.llama.modeling_llama.LlamaForCausalLM', 'transformers.models.llama.modeling_llama.LlamaModel', 'transformers.models.llama.modeling_llama.LlamaDecoderLayer', 'transformers.models.llama.modeling_llama.LlamaRMSNorm', 'aten.rsqrt.default']"
            },
            {
              "key": "[metadata] pkg.torch.onnx.fx_node",
              "value": "%rsqrt_1 : [num_users=1] = call_function[target=torch.ops.aten.rsqrt.default](args = (%add_426,), kwargs = {})"
            },
            {
              "key": "[metadata] pkg.torch.onnx.name_scopes",
              "value": "['', 'model', 'model.layers.0', 'model.layers.0.post_attention_layernorm', 'rsqrt_1']"
            },
            {
              "key": "[metadata] pkg.torch.onnx.stack_trace",
              "value": "File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 688, in forward\n    outputs: BaseModelOutputWithPast = self.model(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 453, in forward\n    layer_outputs = decoder_layer(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 323, in forward\n    hidden_states = self.post_attention_layernorm(hidden_states)\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 72, in forward\n    hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)"
            }
          ],
          "incomingEdges": [
            {
              "sourceNodeId": "node_Add_485",
              "sourceNodeOutputId": "0",
              "targetNodeInputId": "0"
            }
          ],
          "outputsMetadata": [
            {
              "id": "0",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "val_354"
                },
                {
                  "key": "tensor_shape",
                  "value": "FLOAT[s34,s16,1]"
                },
                {
                  "key": "param_name",
                  "value": "Y"
                }
              ]
            }
          ],
          "inputsMetadata": [
            {
              "id": "0",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "add_426"
                },
                {
                  "key": "tensor_shape",
                  "value": "FLOAT[s34,s16,1]"
                },
                {
                  "key": "param_name",
                  "value": "X"
                }
              ]
            }
          ],
          "style": null,
          "config": null
        },
        {
          "id": "node_Reciprocal_487",
          "label": "Reciprocal",
          "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.0: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.0.post_attention_layernorm: transformers.models.llama.modeling_llama.LlamaRMSNorm/rsqrt_1: aten.rsqrt.default",
          "subgraphIds": [],
          "attrs": [
            {
              "key": "[metadata] namespace",
              "value": ": transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.0: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.0.post_attention_layernorm: transformers.models.llama.modeling_llama.LlamaRMSNorm/rsqrt_1: aten.rsqrt.default"
            },
            {
              "key": "[metadata] pkg.torch.onnx.class_hierarchy",
              "value": "['transformers.models.llama.modeling_llama.LlamaForCausalLM', 'transformers.models.llama.modeling_llama.LlamaModel', 'transformers.models.llama.modeling_llama.LlamaDecoderLayer', 'transformers.models.llama.modeling_llama.LlamaRMSNorm', 'aten.rsqrt.default']"
            },
            {
              "key": "[metadata] pkg.torch.onnx.fx_node",
              "value": "%rsqrt_1 : [num_users=1] = call_function[target=torch.ops.aten.rsqrt.default](args = (%add_426,), kwargs = {})"
            },
            {
              "key": "[metadata] pkg.torch.onnx.name_scopes",
              "value": "['', 'model', 'model.layers.0', 'model.layers.0.post_attention_layernorm', 'rsqrt_1']"
            },
            {
              "key": "[metadata] pkg.torch.onnx.stack_trace",
              "value": "File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 688, in forward\n    outputs: BaseModelOutputWithPast = self.model(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 453, in forward\n    layer_outputs = decoder_layer(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 323, in forward\n    hidden_states = self.post_attention_layernorm(hidden_states)\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 72, in forward\n    hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)"
            }
          ],
          "incomingEdges": [
            {
              "sourceNodeId": "node_Sqrt_486",
              "sourceNodeOutputId": "0",
              "targetNodeInputId": "0"
            }
          ],
          "outputsMetadata": [
            {
              "id": "0",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "rsqrt_1"
                },
                {
                  "key": "tensor_shape",
                  "value": "FLOAT[s34,s16,1]"
                },
                {
                  "key": "param_name",
                  "value": "Y"
                }
              ]
            }
          ],
          "inputsMetadata": [
            {
              "id": "0",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "val_354"
                },
                {
                  "key": "tensor_shape",
                  "value": "FLOAT[s34,s16,1]"
                },
                {
                  "key": "param_name",
                  "value": "X"
                }
              ]
            }
          ],
          "style": null,
          "config": null
        },
        {
          "id": "node_Mul_488",
          "label": "Mul",
          "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.0: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.0.post_attention_layernorm: transformers.models.llama.modeling_llama.LlamaRMSNorm/mul_348: aten.mul.Tensor",
          "subgraphIds": [],
          "attrs": [
            {
              "key": "[metadata] namespace",
              "value": ": transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.0: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.0.post_attention_layernorm: transformers.models.llama.modeling_llama.LlamaRMSNorm/mul_348: aten.mul.Tensor"
            },
            {
              "key": "[metadata] pkg.torch.onnx.class_hierarchy",
              "value": "['transformers.models.llama.modeling_llama.LlamaForCausalLM', 'transformers.models.llama.modeling_llama.LlamaModel', 'transformers.models.llama.modeling_llama.LlamaDecoderLayer', 'transformers.models.llama.modeling_llama.LlamaRMSNorm', 'aten.mul.Tensor']"
            },
            {
              "key": "[metadata] pkg.torch.onnx.fx_node",
              "value": "%mul_348 : [num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%add_413, %rsqrt_1), kwargs = {})"
            },
            {
              "key": "[metadata] pkg.torch.onnx.name_scopes",
              "value": "['', 'model', 'model.layers.0', 'model.layers.0.post_attention_layernorm', 'mul_348']"
            },
            {
              "key": "[metadata] pkg.torch.onnx.stack_trace",
              "value": "File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 688, in forward\n    outputs: BaseModelOutputWithPast = self.model(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 453, in forward\n    layer_outputs = decoder_layer(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 323, in forward\n    hidden_states = self.post_attention_layernorm(hidden_states)\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 72, in forward\n    hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)"
            }
          ],
          "incomingEdges": [
            {
              "sourceNodeId": "node_Add_479",
              "sourceNodeOutputId": "0",
              "targetNodeInputId": "0"
            },
            {
              "sourceNodeId": "node_Reciprocal_487",
              "sourceNodeOutputId": "0",
              "targetNodeInputId": "1"
            }
          ],
          "outputsMetadata": [
            {
              "id": "0",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "mul_348"
                },
                {
                  "key": "tensor_shape",
                  "value": "FLOAT[s34,s16,2048]"
                },
                {
                  "key": "param_name",
                  "value": "C"
                }
              ]
            }
          ],
          "inputsMetadata": [
            {
              "id": "0",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "add_413"
                },
                {
                  "key": "tensor_shape",
                  "value": "FLOAT[s34,s16,2048]"
                },
                {
                  "key": "param_name",
                  "value": "A"
                }
              ]
            },
            {
              "id": "1",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "rsqrt_1"
                },
                {
                  "key": "tensor_shape",
                  "value": "FLOAT[s34,s16,1]"
                },
                {
                  "key": "param_name",
                  "value": "B"
                }
              ]
            }
          ],
          "style": null,
          "config": null
        },
        {
          "id": "node_Mul_489",
          "label": "Mul",
          "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.0: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.0.post_attention_layernorm: transformers.models.llama.modeling_llama.LlamaRMSNorm/mul_352: aten.mul.Tensor",
          "subgraphIds": [],
          "attrs": [
            {
              "key": "[metadata] namespace",
              "value": ": transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.0: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.0.post_attention_layernorm: transformers.models.llama.modeling_llama.LlamaRMSNorm/mul_352: aten.mul.Tensor"
            },
            {
              "key": "[metadata] pkg.torch.onnx.class_hierarchy",
              "value": "['transformers.models.llama.modeling_llama.LlamaForCausalLM', 'transformers.models.llama.modeling_llama.LlamaModel', 'transformers.models.llama.modeling_llama.LlamaDecoderLayer', 'transformers.models.llama.modeling_llama.LlamaRMSNorm', 'aten.mul.Tensor']"
            },
            {
              "key": "[metadata] pkg.torch.onnx.fx_node",
              "value": "%mul_352 : [num_users=2] = call_function[target=torch.ops.aten.mul.Tensor](args = (%p_model_layers_0_post_attention_layernorm_weight, %mul_348), kwargs = {})"
            },
            {
              "key": "[metadata] pkg.torch.onnx.name_scopes",
              "value": "['', 'model', 'model.layers.0', 'model.layers.0.post_attention_layernorm', 'mul_352']"
            },
            {
              "key": "[metadata] pkg.torch.onnx.stack_trace",
              "value": "File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 688, in forward\n    outputs: BaseModelOutputWithPast = self.model(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 453, in forward\n    layer_outputs = decoder_layer(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 323, in forward\n    hidden_states = self.post_attention_layernorm(hidden_states)\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 73, in forward\n    return self.weight * hidden_states.to(input_dtype)"
            }
          ],
          "incomingEdges": [
            {
              "sourceNodeId": "[value] model.layers.0.post_attention_layernorm.weight",
              "sourceNodeOutputId": "0",
              "targetNodeInputId": "0"
            },
            {
              "sourceNodeId": "node_Mul_488",
              "sourceNodeOutputId": "0",
              "targetNodeInputId": "1"
            }
          ],
          "outputsMetadata": [
            {
              "id": "0",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "mul_352"
                },
                {
                  "key": "tensor_shape",
                  "value": "FLOAT[s34,s16,2048]"
                },
                {
                  "key": "param_name",
                  "value": "C"
                }
              ]
            }
          ],
          "inputsMetadata": [
            {
              "id": "0",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "model.layers.0.post_attention_layernorm.weight"
                },
                {
                  "key": "tensor_shape",
                  "value": "FLOAT[2048]"
                },
                {
                  "key": "param_name",
                  "value": "A"
                }
              ]
            },
            {
              "id": "1",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "mul_348"
                },
                {
                  "key": "tensor_shape",
                  "value": "FLOAT[s34,s16,2048]"
                },
                {
                  "key": "param_name",
                  "value": "B"
                }
              ]
            }
          ],
          "style": null,
          "config": null
        },
        {
          "id": "node_Transpose_490",
          "label": "Transpose",
          "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.0: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.0.mlp: transformers.models.llama.modeling_llama.LlamaMLP/model.layers.0.mlp.gate_proj: torch.nn.modules.linear.Linear/linear_4: aten.linear.default",
          "subgraphIds": [],
          "attrs": [
            {
              "key": "perm",
              "value": "[1, 0]"
            },
            {
              "key": "[metadata] namespace",
              "value": ": transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.0: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.0.mlp: transformers.models.llama.modeling_llama.LlamaMLP/model.layers.0.mlp.gate_proj: torch.nn.modules.linear.Linear/linear_4: aten.linear.default"
            },
            {
              "key": "[metadata] pkg.torch.onnx.class_hierarchy",
              "value": "['transformers.models.llama.modeling_llama.LlamaForCausalLM', 'transformers.models.llama.modeling_llama.LlamaModel', 'transformers.models.llama.modeling_llama.LlamaDecoderLayer', 'transformers.models.llama.modeling_llama.LlamaMLP', 'torch.nn.modules.linear.Linear', 'aten.linear.default']"
            },
            {
              "key": "[metadata] pkg.torch.onnx.fx_node",
              "value": "%linear_4 : [num_users=1] = call_function[target=torch.ops.aten.linear.default](args = (%mul_352, %p_model_layers_0_mlp_gate_proj_weight), kwargs = {})"
            },
            {
              "key": "[metadata] pkg.torch.onnx.name_scopes",
              "value": "['', 'model', 'model.layers.0', 'model.layers.0.mlp', 'model.layers.0.mlp.gate_proj', 'linear_4']"
            },
            {
              "key": "[metadata] pkg.torch.onnx.stack_trace",
              "value": "File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 688, in forward\n    outputs: BaseModelOutputWithPast = self.model(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 453, in forward\n    layer_outputs = decoder_layer(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 324, in forward\n    hidden_states = self.mlp(hidden_states)\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 162, in forward\n    down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/torch/nn/modules/linear.py\", line 125, in forward\n    return F.linear(input, self.weight, self.bias)"
            }
          ],
          "incomingEdges": [
            {
              "sourceNodeId": "[value] model.layers.0.mlp.gate_proj.weight",
              "sourceNodeOutputId": "0",
              "targetNodeInputId": "0"
            }
          ],
          "outputsMetadata": [
            {
              "id": "0",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "val_355"
                },
                {
                  "key": "tensor_shape",
                  "value": "FLOAT[2048,8192]"
                },
                {
                  "key": "param_name",
                  "value": "transposed"
                }
              ]
            }
          ],
          "inputsMetadata": [
            {
              "id": "0",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "model.layers.0.mlp.gate_proj.weight"
                },
                {
                  "key": "tensor_shape",
                  "value": "FLOAT[8192,2048]"
                },
                {
                  "key": "param_name",
                  "value": "data"
                }
              ]
            }
          ],
          "style": null,
          "config": null
        },
        {
          "id": "node_MatMul_491",
          "label": "MatMul",
          "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.0: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.0.mlp: transformers.models.llama.modeling_llama.LlamaMLP/model.layers.0.mlp.gate_proj: torch.nn.modules.linear.Linear/linear_4: aten.linear.default",
          "subgraphIds": [],
          "attrs": [
            {
              "key": "[metadata] namespace",
              "value": ": transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.0: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.0.mlp: transformers.models.llama.modeling_llama.LlamaMLP/model.layers.0.mlp.gate_proj: torch.nn.modules.linear.Linear/linear_4: aten.linear.default"
            },
            {
              "key": "[metadata] pkg.torch.onnx.class_hierarchy",
              "value": "['transformers.models.llama.modeling_llama.LlamaForCausalLM', 'transformers.models.llama.modeling_llama.LlamaModel', 'transformers.models.llama.modeling_llama.LlamaDecoderLayer', 'transformers.models.llama.modeling_llama.LlamaMLP', 'torch.nn.modules.linear.Linear', 'aten.linear.default']"
            },
            {
              "key": "[metadata] pkg.torch.onnx.fx_node",
              "value": "%linear_4 : [num_users=1] = call_function[target=torch.ops.aten.linear.default](args = (%mul_352, %p_model_layers_0_mlp_gate_proj_weight), kwargs = {})"
            },
            {
              "key": "[metadata] pkg.torch.onnx.name_scopes",
              "value": "['', 'model', 'model.layers.0', 'model.layers.0.mlp', 'model.layers.0.mlp.gate_proj', 'linear_4']"
            },
            {
              "key": "[metadata] pkg.torch.onnx.stack_trace",
              "value": "File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 688, in forward\n    outputs: BaseModelOutputWithPast = self.model(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 453, in forward\n    layer_outputs = decoder_layer(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 324, in forward\n    hidden_states = self.mlp(hidden_states)\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 162, in forward\n    down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/torch/nn/modules/linear.py\", line 125, in forward\n    return F.linear(input, self.weight, self.bias)"
            }
          ],
          "incomingEdges": [
            {
              "sourceNodeId": "node_Mul_489",
              "sourceNodeOutputId": "0",
              "targetNodeInputId": "0"
            },
            {
              "sourceNodeId": "node_Transpose_490",
              "sourceNodeOutputId": "0",
              "targetNodeInputId": "1"
            }
          ],
          "outputsMetadata": [
            {
              "id": "0",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "linear_4"
                },
                {
                  "key": "tensor_shape",
                  "value": "FLOAT[s34,s16,8192]"
                },
                {
                  "key": "param_name",
                  "value": "Y"
                }
              ]
            }
          ],
          "inputsMetadata": [
            {
              "id": "0",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "mul_352"
                },
                {
                  "key": "tensor_shape",
                  "value": "FLOAT[s34,s16,2048]"
                },
                {
                  "key": "param_name",
                  "value": "A"
                }
              ]
            },
            {
              "id": "1",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "val_355"
                },
                {
                  "key": "tensor_shape",
                  "value": "FLOAT[2048,8192]"
                },
                {
                  "key": "param_name",
                  "value": "B"
                }
              ]
            }
          ],
          "style": null,
          "config": null
        },
        {
          "id": "node_Sigmoid_492",
          "label": "Sigmoid",
          "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.0: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.0.mlp: transformers.models.llama.modeling_llama.LlamaMLP/model.layers.0.mlp.act_fn: torch.nn.modules.activation.SiLU/silu: aten.silu.default",
          "subgraphIds": [],
          "attrs": [
            {
              "key": "[metadata] namespace",
              "value": ": transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.0: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.0.mlp: transformers.models.llama.modeling_llama.LlamaMLP/model.layers.0.mlp.act_fn: torch.nn.modules.activation.SiLU/silu: aten.silu.default"
            },
            {
              "key": "[metadata] pkg.torch.onnx.class_hierarchy",
              "value": "['transformers.models.llama.modeling_llama.LlamaForCausalLM', 'transformers.models.llama.modeling_llama.LlamaModel', 'transformers.models.llama.modeling_llama.LlamaDecoderLayer', 'transformers.models.llama.modeling_llama.LlamaMLP', 'torch.nn.modules.activation.SiLU', 'aten.silu.default']"
            },
            {
              "key": "[metadata] pkg.torch.onnx.fx_node",
              "value": "%silu : [num_users=1] = call_function[target=torch.ops.aten.silu.default](args = (%linear_4,), kwargs = {})"
            },
            {
              "key": "[metadata] pkg.torch.onnx.name_scopes",
              "value": "['', 'model', 'model.layers.0', 'model.layers.0.mlp', 'model.layers.0.mlp.act_fn', 'silu']"
            },
            {
              "key": "[metadata] pkg.torch.onnx.stack_trace",
              "value": "File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 688, in forward\n    outputs: BaseModelOutputWithPast = self.model(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 453, in forward\n    layer_outputs = decoder_layer(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 324, in forward\n    hidden_states = self.mlp(hidden_states)\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 162, in forward\n    down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/torch/nn/modules/activation.py\", line 434, in forward\n    return F.silu(input, inplace=self.inplace)"
            }
          ],
          "incomingEdges": [
            {
              "sourceNodeId": "node_MatMul_491",
              "sourceNodeOutputId": "0",
              "targetNodeInputId": "0"
            }
          ],
          "outputsMetadata": [
            {
              "id": "0",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "val_356"
                },
                {
                  "key": "tensor_shape",
                  "value": "FLOAT[s34,s16,8192]"
                },
                {
                  "key": "param_name",
                  "value": "Y"
                }
              ]
            }
          ],
          "inputsMetadata": [
            {
              "id": "0",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "linear_4"
                },
                {
                  "key": "tensor_shape",
                  "value": "FLOAT[s34,s16,8192]"
                },
                {
                  "key": "param_name",
                  "value": "X"
                }
              ]
            }
          ],
          "style": null,
          "config": null
        },
        {
          "id": "node_Mul_493",
          "label": "Mul",
          "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.0: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.0.mlp: transformers.models.llama.modeling_llama.LlamaMLP/model.layers.0.mlp.act_fn: torch.nn.modules.activation.SiLU/silu: aten.silu.default",
          "subgraphIds": [],
          "attrs": [
            {
              "key": "[metadata] namespace",
              "value": ": transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.0: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.0.mlp: transformers.models.llama.modeling_llama.LlamaMLP/model.layers.0.mlp.act_fn: torch.nn.modules.activation.SiLU/silu: aten.silu.default"
            },
            {
              "key": "[metadata] pkg.torch.onnx.class_hierarchy",
              "value": "['transformers.models.llama.modeling_llama.LlamaForCausalLM', 'transformers.models.llama.modeling_llama.LlamaModel', 'transformers.models.llama.modeling_llama.LlamaDecoderLayer', 'transformers.models.llama.modeling_llama.LlamaMLP', 'torch.nn.modules.activation.SiLU', 'aten.silu.default']"
            },
            {
              "key": "[metadata] pkg.torch.onnx.fx_node",
              "value": "%silu : [num_users=1] = call_function[target=torch.ops.aten.silu.default](args = (%linear_4,), kwargs = {})"
            },
            {
              "key": "[metadata] pkg.torch.onnx.name_scopes",
              "value": "['', 'model', 'model.layers.0', 'model.layers.0.mlp', 'model.layers.0.mlp.act_fn', 'silu']"
            },
            {
              "key": "[metadata] pkg.torch.onnx.stack_trace",
              "value": "File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 688, in forward\n    outputs: BaseModelOutputWithPast = self.model(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 453, in forward\n    layer_outputs = decoder_layer(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 324, in forward\n    hidden_states = self.mlp(hidden_states)\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 162, in forward\n    down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/torch/nn/modules/activation.py\", line 434, in forward\n    return F.silu(input, inplace=self.inplace)"
            }
          ],
          "incomingEdges": [
            {
              "sourceNodeId": "node_MatMul_491",
              "sourceNodeOutputId": "0",
              "targetNodeInputId": "0"
            },
            {
              "sourceNodeId": "node_Sigmoid_492",
              "sourceNodeOutputId": "0",
              "targetNodeInputId": "1"
            }
          ],
          "outputsMetadata": [
            {
              "id": "0",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "silu"
                },
                {
                  "key": "tensor_shape",
                  "value": "FLOAT[s34,s16,8192]"
                },
                {
                  "key": "param_name",
                  "value": "C"
                }
              ]
            }
          ],
          "inputsMetadata": [
            {
              "id": "0",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "linear_4"
                },
                {
                  "key": "tensor_shape",
                  "value": "FLOAT[s34,s16,8192]"
                },
                {
                  "key": "param_name",
                  "value": "A"
                }
              ]
            },
            {
              "id": "1",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "val_356"
                },
                {
                  "key": "tensor_shape",
                  "value": "FLOAT[s34,s16,8192]"
                },
                {
                  "key": "param_name",
                  "value": "B"
                }
              ]
            }
          ],
          "style": null,
          "config": null
        },
        {
          "id": "node_Transpose_494",
          "label": "Transpose",
          "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.0: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.0.mlp: transformers.models.llama.modeling_llama.LlamaMLP/model.layers.0.mlp.up_proj: torch.nn.modules.linear.Linear/linear_5: aten.linear.default",
          "subgraphIds": [],
          "attrs": [
            {
              "key": "perm",
              "value": "[1, 0]"
            },
            {
              "key": "[metadata] namespace",
              "value": ": transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.0: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.0.mlp: transformers.models.llama.modeling_llama.LlamaMLP/model.layers.0.mlp.up_proj: torch.nn.modules.linear.Linear/linear_5: aten.linear.default"
            },
            {
              "key": "[metadata] pkg.torch.onnx.class_hierarchy",
              "value": "['transformers.models.llama.modeling_llama.LlamaForCausalLM', 'transformers.models.llama.modeling_llama.LlamaModel', 'transformers.models.llama.modeling_llama.LlamaDecoderLayer', 'transformers.models.llama.modeling_llama.LlamaMLP', 'torch.nn.modules.linear.Linear', 'aten.linear.default']"
            },
            {
              "key": "[metadata] pkg.torch.onnx.fx_node",
              "value": "%linear_5 : [num_users=1] = call_function[target=torch.ops.aten.linear.default](args = (%mul_352, %p_model_layers_0_mlp_up_proj_weight), kwargs = {})"
            },
            {
              "key": "[metadata] pkg.torch.onnx.name_scopes",
              "value": "['', 'model', 'model.layers.0', 'model.layers.0.mlp', 'model.layers.0.mlp.up_proj', 'linear_5']"
            },
            {
              "key": "[metadata] pkg.torch.onnx.stack_trace",
              "value": "File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 688, in forward\n    outputs: BaseModelOutputWithPast = self.model(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 453, in forward\n    layer_outputs = decoder_layer(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 324, in forward\n    hidden_states = self.mlp(hidden_states)\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 162, in forward\n    down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/torch/nn/modules/linear.py\", line 125, in forward\n    return F.linear(input, self.weight, self.bias)"
            }
          ],
          "incomingEdges": [
            {
              "sourceNodeId": "[value] model.layers.0.mlp.up_proj.weight",
              "sourceNodeOutputId": "0",
              "targetNodeInputId": "0"
            }
          ],
          "outputsMetadata": [
            {
              "id": "0",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "val_357"
                },
                {
                  "key": "tensor_shape",
                  "value": "FLOAT[2048,8192]"
                },
                {
                  "key": "param_name",
                  "value": "transposed"
                }
              ]
            }
          ],
          "inputsMetadata": [
            {
              "id": "0",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "model.layers.0.mlp.up_proj.weight"
                },
                {
                  "key": "tensor_shape",
                  "value": "FLOAT[8192,2048]"
                },
                {
                  "key": "param_name",
                  "value": "data"
                }
              ]
            }
          ],
          "style": null,
          "config": null
        },
        {
          "id": "node_MatMul_495",
          "label": "MatMul",
          "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.0: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.0.mlp: transformers.models.llama.modeling_llama.LlamaMLP/model.layers.0.mlp.up_proj: torch.nn.modules.linear.Linear/linear_5: aten.linear.default",
          "subgraphIds": [],
          "attrs": [
            {
              "key": "[metadata] namespace",
              "value": ": transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.0: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.0.mlp: transformers.models.llama.modeling_llama.LlamaMLP/model.layers.0.mlp.up_proj: torch.nn.modules.linear.Linear/linear_5: aten.linear.default"
            },
            {
              "key": "[metadata] pkg.torch.onnx.class_hierarchy",
              "value": "['transformers.models.llama.modeling_llama.LlamaForCausalLM', 'transformers.models.llama.modeling_llama.LlamaModel', 'transformers.models.llama.modeling_llama.LlamaDecoderLayer', 'transformers.models.llama.modeling_llama.LlamaMLP', 'torch.nn.modules.linear.Linear', 'aten.linear.default']"
            },
            {
              "key": "[metadata] pkg.torch.onnx.fx_node",
              "value": "%linear_5 : [num_users=1] = call_function[target=torch.ops.aten.linear.default](args = (%mul_352, %p_model_layers_0_mlp_up_proj_weight), kwargs = {})"
            },
            {
              "key": "[metadata] pkg.torch.onnx.name_scopes",
              "value": "['', 'model', 'model.layers.0', 'model.layers.0.mlp', 'model.layers.0.mlp.up_proj', 'linear_5']"
            },
            {
              "key": "[metadata] pkg.torch.onnx.stack_trace",
              "value": "File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 688, in forward\n    outputs: BaseModelOutputWithPast = self.model(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 453, in forward\n    layer_outputs = decoder_layer(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 324, in forward\n    hidden_states = self.mlp(hidden_states)\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 162, in forward\n    down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/torch/nn/modules/linear.py\", line 125, in forward\n    return F.linear(input, self.weight, self.bias)"
            }
          ],
          "incomingEdges": [
            {
              "sourceNodeId": "node_Mul_489",
              "sourceNodeOutputId": "0",
              "targetNodeInputId": "0"
            },
            {
              "sourceNodeId": "node_Transpose_494",
              "sourceNodeOutputId": "0",
              "targetNodeInputId": "1"
            }
          ],
          "outputsMetadata": [
            {
              "id": "0",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "linear_5"
                },
                {
                  "key": "tensor_shape",
                  "value": "FLOAT[s34,s16,8192]"
                },
                {
                  "key": "param_name",
                  "value": "Y"
                }
              ]
            }
          ],
          "inputsMetadata": [
            {
              "id": "0",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "mul_352"
                },
                {
                  "key": "tensor_shape",
                  "value": "FLOAT[s34,s16,2048]"
                },
                {
                  "key": "param_name",
                  "value": "A"
                }
              ]
            },
            {
              "id": "1",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "val_357"
                },
                {
                  "key": "tensor_shape",
                  "value": "FLOAT[2048,8192]"
                },
                {
                  "key": "param_name",
                  "value": "B"
                }
              ]
            }
          ],
          "style": null,
          "config": null
        },
        {
          "id": "node_Mul_496",
          "label": "Mul",
          "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.0: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.0.mlp: transformers.models.llama.modeling_llama.LlamaMLP/mul_365: aten.mul.Tensor",
          "subgraphIds": [],
          "attrs": [
            {
              "key": "[metadata] namespace",
              "value": ": transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.0: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.0.mlp: transformers.models.llama.modeling_llama.LlamaMLP/mul_365: aten.mul.Tensor"
            },
            {
              "key": "[metadata] pkg.torch.onnx.class_hierarchy",
              "value": "['transformers.models.llama.modeling_llama.LlamaForCausalLM', 'transformers.models.llama.modeling_llama.LlamaModel', 'transformers.models.llama.modeling_llama.LlamaDecoderLayer', 'transformers.models.llama.modeling_llama.LlamaMLP', 'aten.mul.Tensor']"
            },
            {
              "key": "[metadata] pkg.torch.onnx.fx_node",
              "value": "%mul_365 : [num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%silu, %linear_5), kwargs = {})"
            },
            {
              "key": "[metadata] pkg.torch.onnx.name_scopes",
              "value": "['', 'model', 'model.layers.0', 'model.layers.0.mlp', 'mul_365']"
            },
            {
              "key": "[metadata] pkg.torch.onnx.stack_trace",
              "value": "File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 688, in forward\n    outputs: BaseModelOutputWithPast = self.model(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 453, in forward\n    layer_outputs = decoder_layer(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 324, in forward\n    hidden_states = self.mlp(hidden_states)\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 162, in forward\n    down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))"
            }
          ],
          "incomingEdges": [
            {
              "sourceNodeId": "node_Mul_493",
              "sourceNodeOutputId": "0",
              "targetNodeInputId": "0"
            },
            {
              "sourceNodeId": "node_MatMul_495",
              "sourceNodeOutputId": "0",
              "targetNodeInputId": "1"
            }
          ],
          "outputsMetadata": [
            {
              "id": "0",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "mul_365"
                },
                {
                  "key": "tensor_shape",
                  "value": "FLOAT[s34,s16,8192]"
                },
                {
                  "key": "param_name",
                  "value": "C"
                }
              ]
            }
          ],
          "inputsMetadata": [
            {
              "id": "0",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "silu"
                },
                {
                  "key": "tensor_shape",
                  "value": "FLOAT[s34,s16,8192]"
                },
                {
                  "key": "param_name",
                  "value": "A"
                }
              ]
            },
            {
              "id": "1",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "linear_5"
                },
                {
                  "key": "tensor_shape",
                  "value": "FLOAT[s34,s16,8192]"
                },
                {
                  "key": "param_name",
                  "value": "B"
                }
              ]
            }
          ],
          "style": null,
          "config": null
        },
        {
          "id": "node_Transpose_497",
          "label": "Transpose",
          "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.0: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.0.mlp: transformers.models.llama.modeling_llama.LlamaMLP/model.layers.0.mlp.down_proj: torch.nn.modules.linear.Linear/linear_6: aten.linear.default",
          "subgraphIds": [],
          "attrs": [
            {
              "key": "perm",
              "value": "[1, 0]"
            },
            {
              "key": "[metadata] namespace",
              "value": ": transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.0: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.0.mlp: transformers.models.llama.modeling_llama.LlamaMLP/model.layers.0.mlp.down_proj: torch.nn.modules.linear.Linear/linear_6: aten.linear.default"
            },
            {
              "key": "[metadata] pkg.torch.onnx.class_hierarchy",
              "value": "['transformers.models.llama.modeling_llama.LlamaForCausalLM', 'transformers.models.llama.modeling_llama.LlamaModel', 'transformers.models.llama.modeling_llama.LlamaDecoderLayer', 'transformers.models.llama.modeling_llama.LlamaMLP', 'torch.nn.modules.linear.Linear', 'aten.linear.default']"
            },
            {
              "key": "[metadata] pkg.torch.onnx.fx_node",
              "value": "%linear_6 : [num_users=1] = call_function[target=torch.ops.aten.linear.default](args = (%mul_365, %p_model_layers_0_mlp_down_proj_weight), kwargs = {})"
            },
            {
              "key": "[metadata] pkg.torch.onnx.name_scopes",
              "value": "['', 'model', 'model.layers.0', 'model.layers.0.mlp', 'model.layers.0.mlp.down_proj', 'linear_6']"
            },
            {
              "key": "[metadata] pkg.torch.onnx.stack_trace",
              "value": "File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 688, in forward\n    outputs: BaseModelOutputWithPast = self.model(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 453, in forward\n    layer_outputs = decoder_layer(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 324, in forward\n    hidden_states = self.mlp(hidden_states)\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 162, in forward\n    down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/torch/nn/modules/linear.py\", line 125, in forward\n    return F.linear(input, self.weight, self.bias)"
            }
          ],
          "incomingEdges": [
            {
              "sourceNodeId": "[value] model.layers.0.mlp.down_proj.weight",
              "sourceNodeOutputId": "0",
              "targetNodeInputId": "0"
            }
          ],
          "outputsMetadata": [
            {
              "id": "0",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "val_358"
                },
                {
                  "key": "tensor_shape",
                  "value": "FLOAT[8192,2048]"
                },
                {
                  "key": "param_name",
                  "value": "transposed"
                }
              ]
            }
          ],
          "inputsMetadata": [
            {
              "id": "0",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "model.layers.0.mlp.down_proj.weight"
                },
                {
                  "key": "tensor_shape",
                  "value": "FLOAT[2048,8192]"
                },
                {
                  "key": "param_name",
                  "value": "data"
                }
              ]
            }
          ],
          "style": null,
          "config": null
        },
        {
          "id": "node_MatMul_498",
          "label": "MatMul",
          "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.0: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.0.mlp: transformers.models.llama.modeling_llama.LlamaMLP/model.layers.0.mlp.down_proj: torch.nn.modules.linear.Linear/linear_6: aten.linear.default",
          "subgraphIds": [],
          "attrs": [
            {
              "key": "[metadata] namespace",
              "value": ": transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.0: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.0.mlp: transformers.models.llama.modeling_llama.LlamaMLP/model.layers.0.mlp.down_proj: torch.nn.modules.linear.Linear/linear_6: aten.linear.default"
            },
            {
              "key": "[metadata] pkg.torch.onnx.class_hierarchy",
              "value": "['transformers.models.llama.modeling_llama.LlamaForCausalLM', 'transformers.models.llama.modeling_llama.LlamaModel', 'transformers.models.llama.modeling_llama.LlamaDecoderLayer', 'transformers.models.llama.modeling_llama.LlamaMLP', 'torch.nn.modules.linear.Linear', 'aten.linear.default']"
            },
            {
              "key": "[metadata] pkg.torch.onnx.fx_node",
              "value": "%linear_6 : [num_users=1] = call_function[target=torch.ops.aten.linear.default](args = (%mul_365, %p_model_layers_0_mlp_down_proj_weight), kwargs = {})"
            },
            {
              "key": "[metadata] pkg.torch.onnx.name_scopes",
              "value": "['', 'model', 'model.layers.0', 'model.layers.0.mlp', 'model.layers.0.mlp.down_proj', 'linear_6']"
            },
            {
              "key": "[metadata] pkg.torch.onnx.stack_trace",
              "value": "File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 688, in forward\n    outputs: BaseModelOutputWithPast = self.model(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 453, in forward\n    layer_outputs = decoder_layer(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 324, in forward\n    hidden_states = self.mlp(hidden_states)\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 162, in forward\n    down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/torch/nn/modules/linear.py\", line 125, in forward\n    return F.linear(input, self.weight, self.bias)"
            }
          ],
          "incomingEdges": [
            {
              "sourceNodeId": "node_Mul_496",
              "sourceNodeOutputId": "0",
              "targetNodeInputId": "0"
            },
            {
              "sourceNodeId": "node_Transpose_497",
              "sourceNodeOutputId": "0",
              "targetNodeInputId": "1"
            }
          ],
          "outputsMetadata": [
            {
              "id": "0",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "linear_6"
                },
                {
                  "key": "tensor_shape",
                  "value": "FLOAT[s34,s16,2048]"
                },
                {
                  "key": "param_name",
                  "value": "Y"
                }
              ]
            }
          ],
          "inputsMetadata": [
            {
              "id": "0",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "mul_365"
                },
                {
                  "key": "tensor_shape",
                  "value": "FLOAT[s34,s16,8192]"
                },
                {
                  "key": "param_name",
                  "value": "A"
                }
              ]
            },
            {
              "id": "1",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "val_358"
                },
                {
                  "key": "tensor_shape",
                  "value": "FLOAT[8192,2048]"
                },
                {
                  "key": "param_name",
                  "value": "B"
                }
              ]
            }
          ],
          "style": null,
          "config": null
        },
        {
          "id": "node_Add_499",
          "label": "Add",
          "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.0: transformers.models.llama.modeling_llama.LlamaDecoderLayer/add_463: aten.add.Tensor",
          "subgraphIds": [],
          "attrs": [
            {
              "key": "[metadata] namespace",
              "value": ": transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.0: transformers.models.llama.modeling_llama.LlamaDecoderLayer/add_463: aten.add.Tensor"
            },
            {
              "key": "[metadata] pkg.torch.onnx.class_hierarchy",
              "value": "['transformers.models.llama.modeling_llama.LlamaForCausalLM', 'transformers.models.llama.modeling_llama.LlamaModel', 'transformers.models.llama.modeling_llama.LlamaDecoderLayer', 'aten.add.Tensor']"
            },
            {
              "key": "[metadata] pkg.torch.onnx.fx_node",
              "value": "%add_463 : [num_users=3] = call_function[target=torch.ops.aten.add.Tensor](args = (%add_413, %linear_6), kwargs = {})"
            },
            {
              "key": "[metadata] pkg.torch.onnx.name_scopes",
              "value": "['', 'model', 'model.layers.0', 'add_463']"
            },
            {
              "key": "[metadata] pkg.torch.onnx.stack_trace",
              "value": "File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 688, in forward\n    outputs: BaseModelOutputWithPast = self.model(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 453, in forward\n    layer_outputs = decoder_layer(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 325, in forward\n    hidden_states = residual + hidden_states"
            }
          ],
          "incomingEdges": [
            {
              "sourceNodeId": "node_Add_479",
              "sourceNodeOutputId": "0",
              "targetNodeInputId": "0"
            },
            {
              "sourceNodeId": "node_MatMul_498",
              "sourceNodeOutputId": "0",
              "targetNodeInputId": "1"
            }
          ],
          "outputsMetadata": [
            {
              "id": "0",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "add_463"
                },
                {
                  "key": "tensor_shape",
                  "value": "FLOAT[s34,s16,2048]"
                },
                {
                  "key": "param_name",
                  "value": "C"
                }
              ]
            }
          ],
          "inputsMetadata": [
            {
              "id": "0",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "add_413"
                },
                {
                  "key": "tensor_shape",
                  "value": "FLOAT[s34,s16,2048]"
                },
                {
                  "key": "param_name",
                  "value": "A"
                }
              ]
            },
            {
              "id": "1",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "linear_6"
                },
                {
                  "key": "tensor_shape",
                  "value": "FLOAT[s34,s16,2048]"
                },
                {
                  "key": "param_name",
                  "value": "B"
                }
              ]
            }
          ],
          "style": null,
          "config": null
        },
        {
          "id": "node_Constant_500",
          "label": "Constant",
          "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.1: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.1.input_layernorm: transformers.models.llama.modeling_llama.LlamaRMSNorm/pow_3: aten.pow.Tensor_Scalar",
          "subgraphIds": [],
          "attrs": [
            {
              "key": "__value",
              "value": "2.0"
            },
            {
              "key": "value",
              "value": "TensorProtoTensor<FLOAT,[]>(array(2., dtype=float32), name='')"
            },
            {
              "key": "[metadata] namespace",
              "value": ": transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.1: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.1.input_layernorm: transformers.models.llama.modeling_llama.LlamaRMSNorm/pow_3: aten.pow.Tensor_Scalar"
            },
            {
              "key": "[metadata] pkg.torch.onnx.class_hierarchy",
              "value": "['transformers.models.llama.modeling_llama.LlamaForCausalLM', 'transformers.models.llama.modeling_llama.LlamaModel', 'transformers.models.llama.modeling_llama.LlamaDecoderLayer', 'transformers.models.llama.modeling_llama.LlamaRMSNorm', 'aten.pow.Tensor_Scalar']"
            },
            {
              "key": "[metadata] pkg.torch.onnx.fx_node",
              "value": "%pow_3 : [num_users=1] = call_function[target=torch.ops.aten.pow.Tensor_Scalar](args = (%add_463, 2), kwargs = {})"
            },
            {
              "key": "[metadata] pkg.torch.onnx.name_scopes",
              "value": "['', 'model', 'model.layers.1', 'model.layers.1.input_layernorm', 'pow_3']"
            },
            {
              "key": "[metadata] pkg.torch.onnx.stack_trace",
              "value": "File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 688, in forward\n    outputs: BaseModelOutputWithPast = self.model(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 453, in forward\n    layer_outputs = decoder_layer(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 305, in forward\n    hidden_states = self.input_layernorm(hidden_states)\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 71, in forward\n    variance = hidden_states.pow(2).mean(-1, keepdim=True)"
            }
          ],
          "incomingEdges": [],
          "outputsMetadata": [
            {
              "id": "0",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "val_359"
                },
                {
                  "key": "tensor_shape",
                  "value": "FLOAT[]"
                },
                {
                  "key": "param_name",
                  "value": "output"
                }
              ]
            }
          ],
          "inputsMetadata": [],
          "style": null,
          "config": null
        },
        {
          "id": "node_Pow_501",
          "label": "Pow",
          "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.1: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.1.input_layernorm: transformers.models.llama.modeling_llama.LlamaRMSNorm/pow_3: aten.pow.Tensor_Scalar",
          "subgraphIds": [],
          "attrs": [
            {
              "key": "[metadata] namespace",
              "value": ": transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.1: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.1.input_layernorm: transformers.models.llama.modeling_llama.LlamaRMSNorm/pow_3: aten.pow.Tensor_Scalar"
            },
            {
              "key": "[metadata] pkg.torch.onnx.class_hierarchy",
              "value": "['transformers.models.llama.modeling_llama.LlamaForCausalLM', 'transformers.models.llama.modeling_llama.LlamaModel', 'transformers.models.llama.modeling_llama.LlamaDecoderLayer', 'transformers.models.llama.modeling_llama.LlamaRMSNorm', 'aten.pow.Tensor_Scalar']"
            },
            {
              "key": "[metadata] pkg.torch.onnx.fx_node",
              "value": "%pow_3 : [num_users=1] = call_function[target=torch.ops.aten.pow.Tensor_Scalar](args = (%add_463, 2), kwargs = {})"
            },
            {
              "key": "[metadata] pkg.torch.onnx.name_scopes",
              "value": "['', 'model', 'model.layers.1', 'model.layers.1.input_layernorm', 'pow_3']"
            },
            {
              "key": "[metadata] pkg.torch.onnx.stack_trace",
              "value": "File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 688, in forward\n    outputs: BaseModelOutputWithPast = self.model(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 453, in forward\n    layer_outputs = decoder_layer(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 305, in forward\n    hidden_states = self.input_layernorm(hidden_states)\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 71, in forward\n    variance = hidden_states.pow(2).mean(-1, keepdim=True)"
            }
          ],
          "incomingEdges": [
            {
              "sourceNodeId": "node_Add_499",
              "sourceNodeOutputId": "0",
              "targetNodeInputId": "0"
            },
            {
              "sourceNodeId": "node_Constant_500",
              "sourceNodeOutputId": "0",
              "targetNodeInputId": "1"
            }
          ],
          "outputsMetadata": [
            {
              "id": "0",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "pow_3"
                },
                {
                  "key": "tensor_shape",
                  "value": "FLOAT[s34,s16,2048]"
                },
                {
                  "key": "param_name",
                  "value": "Z"
                }
              ]
            }
          ],
          "inputsMetadata": [
            {
              "id": "0",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "add_463"
                },
                {
                  "key": "tensor_shape",
                  "value": "FLOAT[s34,s16,2048]"
                },
                {
                  "key": "param_name",
                  "value": "X"
                }
              ]
            },
            {
              "id": "1",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "val_359"
                },
                {
                  "key": "tensor_shape",
                  "value": "FLOAT[]"
                },
                {
                  "key": "param_name",
                  "value": "Y"
                }
              ]
            }
          ],
          "style": null,
          "config": null
        },
        {
          "id": "node_Constant_925",
          "label": "Constant",
          "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.1: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.1.input_layernorm: transformers.models.llama.modeling_llama.LlamaRMSNorm/mean_2: aten.mean.dim",
          "subgraphIds": [],
          "attrs": [
            {
              "key": "__value",
              "value": "[-1]"
            },
            {
              "key": "value",
              "value": "TensorProtoTensor<INT64,[1]>(array([-1]), name='val_361')"
            }
          ],
          "incomingEdges": [],
          "outputsMetadata": [
            {
              "id": "0",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "val_361"
                },
                {
                  "key": "tensor_shape",
                  "value": "INT64[1]"
                },
                {
                  "key": "param_name",
                  "value": "output"
                }
              ]
            }
          ],
          "inputsMetadata": [],
          "style": null,
          "config": null
        },
        {
          "id": "node_ReduceMean_504",
          "label": "ReduceMean",
          "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.1: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.1.input_layernorm: transformers.models.llama.modeling_llama.LlamaRMSNorm/mean_2: aten.mean.dim",
          "subgraphIds": [],
          "attrs": [
            {
              "key": "noop_with_empty_axes",
              "value": "0"
            },
            {
              "key": "keepdims",
              "value": "1"
            },
            {
              "key": "[metadata] namespace",
              "value": ": transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.1: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.1.input_layernorm: transformers.models.llama.modeling_llama.LlamaRMSNorm/mean_2: aten.mean.dim"
            },
            {
              "key": "[metadata] pkg.torch.onnx.class_hierarchy",
              "value": "['transformers.models.llama.modeling_llama.LlamaForCausalLM', 'transformers.models.llama.modeling_llama.LlamaModel', 'transformers.models.llama.modeling_llama.LlamaDecoderLayer', 'transformers.models.llama.modeling_llama.LlamaRMSNorm', 'aten.mean.dim']"
            },
            {
              "key": "[metadata] pkg.torch.onnx.fx_node",
              "value": "%mean_2 : [num_users=1] = call_function[target=torch.ops.aten.mean.dim](args = (%pow_3, [-1], True), kwargs = {})"
            },
            {
              "key": "[metadata] pkg.torch.onnx.name_scopes",
              "value": "['', 'model', 'model.layers.1', 'model.layers.1.input_layernorm', 'mean_2']"
            },
            {
              "key": "[metadata] pkg.torch.onnx.stack_trace",
              "value": "File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 688, in forward\n    outputs: BaseModelOutputWithPast = self.model(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 453, in forward\n    layer_outputs = decoder_layer(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 305, in forward\n    hidden_states = self.input_layernorm(hidden_states)\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 71, in forward\n    variance = hidden_states.pow(2).mean(-1, keepdim=True)"
            }
          ],
          "incomingEdges": [
            {
              "sourceNodeId": "node_Pow_501",
              "sourceNodeOutputId": "0",
              "targetNodeInputId": "0"
            },
            {
              "sourceNodeId": "node_Constant_925",
              "sourceNodeOutputId": "0",
              "targetNodeInputId": "1"
            }
          ],
          "outputsMetadata": [
            {
              "id": "0",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "mean_2"
                },
                {
                  "key": "tensor_shape",
                  "value": "FLOAT[s34,s16,1]"
                },
                {
                  "key": "param_name",
                  "value": "reduced"
                }
              ]
            }
          ],
          "inputsMetadata": [
            {
              "id": "0",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "pow_3"
                },
                {
                  "key": "tensor_shape",
                  "value": "FLOAT[s34,s16,2048]"
                },
                {
                  "key": "param_name",
                  "value": "data"
                }
              ]
            },
            {
              "id": "1",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "val_361"
                },
                {
                  "key": "tensor_shape",
                  "value": "INT64[1]"
                },
                {
                  "key": "param_name",
                  "value": "axes"
                }
              ]
            }
          ],
          "style": null,
          "config": null
        },
        {
          "id": "node_Add_505",
          "label": "Add",
          "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.1: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.1.input_layernorm: transformers.models.llama.modeling_llama.LlamaRMSNorm/add_476: aten.add.Tensor",
          "subgraphIds": [],
          "attrs": [
            {
              "key": "[metadata] namespace",
              "value": ": transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.1: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.1.input_layernorm: transformers.models.llama.modeling_llama.LlamaRMSNorm/add_476: aten.add.Tensor"
            },
            {
              "key": "[metadata] pkg.torch.onnx.class_hierarchy",
              "value": "['transformers.models.llama.modeling_llama.LlamaForCausalLM', 'transformers.models.llama.modeling_llama.LlamaModel', 'transformers.models.llama.modeling_llama.LlamaDecoderLayer', 'transformers.models.llama.modeling_llama.LlamaRMSNorm', 'aten.add.Tensor']"
            },
            {
              "key": "[metadata] pkg.torch.onnx.fx_node",
              "value": "%add_476 : [num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%mean_2, 1e-05), kwargs = {})"
            },
            {
              "key": "[metadata] pkg.torch.onnx.name_scopes",
              "value": "['', 'model', 'model.layers.1', 'model.layers.1.input_layernorm', 'add_476']"
            },
            {
              "key": "[metadata] pkg.torch.onnx.stack_trace",
              "value": "File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 688, in forward\n    outputs: BaseModelOutputWithPast = self.model(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 453, in forward\n    layer_outputs = decoder_layer(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 305, in forward\n    hidden_states = self.input_layernorm(hidden_states)\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 72, in forward\n    hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)"
            }
          ],
          "incomingEdges": [
            {
              "sourceNodeId": "node_ReduceMean_504",
              "sourceNodeOutputId": "0",
              "targetNodeInputId": "0"
            },
            {
              "sourceNodeId": "node_Constant_333",
              "sourceNodeOutputId": "0",
              "targetNodeInputId": "1"
            }
          ],
          "outputsMetadata": [
            {
              "id": "0",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "add_476"
                },
                {
                  "key": "tensor_shape",
                  "value": "FLOAT[s34,s16,1]"
                },
                {
                  "key": "param_name",
                  "value": "C"
                }
              ]
            }
          ],
          "inputsMetadata": [
            {
              "id": "0",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "mean_2"
                },
                {
                  "key": "tensor_shape",
                  "value": "FLOAT[s34,s16,1]"
                },
                {
                  "key": "param_name",
                  "value": "A"
                }
              ]
            },
            {
              "id": "1",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "val_251"
                },
                {
                  "key": "tensor_shape",
                  "value": "FLOAT[]"
                },
                {
                  "key": "param_name",
                  "value": "B"
                }
              ]
            }
          ],
          "style": null,
          "config": null
        },
        {
          "id": "node_Sqrt_506",
          "label": "Sqrt",
          "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.1: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.1.input_layernorm: transformers.models.llama.modeling_llama.LlamaRMSNorm/rsqrt_2: aten.rsqrt.default",
          "subgraphIds": [],
          "attrs": [
            {
              "key": "[metadata] namespace",
              "value": ": transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.1: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.1.input_layernorm: transformers.models.llama.modeling_llama.LlamaRMSNorm/rsqrt_2: aten.rsqrt.default"
            },
            {
              "key": "[metadata] pkg.torch.onnx.class_hierarchy",
              "value": "['transformers.models.llama.modeling_llama.LlamaForCausalLM', 'transformers.models.llama.modeling_llama.LlamaModel', 'transformers.models.llama.modeling_llama.LlamaDecoderLayer', 'transformers.models.llama.modeling_llama.LlamaRMSNorm', 'aten.rsqrt.default']"
            },
            {
              "key": "[metadata] pkg.torch.onnx.fx_node",
              "value": "%rsqrt_2 : [num_users=1] = call_function[target=torch.ops.aten.rsqrt.default](args = (%add_476,), kwargs = {})"
            },
            {
              "key": "[metadata] pkg.torch.onnx.name_scopes",
              "value": "['', 'model', 'model.layers.1', 'model.layers.1.input_layernorm', 'rsqrt_2']"
            },
            {
              "key": "[metadata] pkg.torch.onnx.stack_trace",
              "value": "File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 688, in forward\n    outputs: BaseModelOutputWithPast = self.model(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 453, in forward\n    layer_outputs = decoder_layer(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 305, in forward\n    hidden_states = self.input_layernorm(hidden_states)\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 72, in forward\n    hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)"
            }
          ],
          "incomingEdges": [
            {
              "sourceNodeId": "node_Add_505",
              "sourceNodeOutputId": "0",
              "targetNodeInputId": "0"
            }
          ],
          "outputsMetadata": [
            {
              "id": "0",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "val_362"
                },
                {
                  "key": "tensor_shape",
                  "value": "FLOAT[s34,s16,1]"
                },
                {
                  "key": "param_name",
                  "value": "Y"
                }
              ]
            }
          ],
          "inputsMetadata": [
            {
              "id": "0",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "add_476"
                },
                {
                  "key": "tensor_shape",
                  "value": "FLOAT[s34,s16,1]"
                },
                {
                  "key": "param_name",
                  "value": "X"
                }
              ]
            }
          ],
          "style": null,
          "config": null
        },
        {
          "id": "node_Reciprocal_507",
          "label": "Reciprocal",
          "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.1: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.1.input_layernorm: transformers.models.llama.modeling_llama.LlamaRMSNorm/rsqrt_2: aten.rsqrt.default",
          "subgraphIds": [],
          "attrs": [
            {
              "key": "[metadata] namespace",
              "value": ": transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.1: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.1.input_layernorm: transformers.models.llama.modeling_llama.LlamaRMSNorm/rsqrt_2: aten.rsqrt.default"
            },
            {
              "key": "[metadata] pkg.torch.onnx.class_hierarchy",
              "value": "['transformers.models.llama.modeling_llama.LlamaForCausalLM', 'transformers.models.llama.modeling_llama.LlamaModel', 'transformers.models.llama.modeling_llama.LlamaDecoderLayer', 'transformers.models.llama.modeling_llama.LlamaRMSNorm', 'aten.rsqrt.default']"
            },
            {
              "key": "[metadata] pkg.torch.onnx.fx_node",
              "value": "%rsqrt_2 : [num_users=1] = call_function[target=torch.ops.aten.rsqrt.default](args = (%add_476,), kwargs = {})"
            },
            {
              "key": "[metadata] pkg.torch.onnx.name_scopes",
              "value": "['', 'model', 'model.layers.1', 'model.layers.1.input_layernorm', 'rsqrt_2']"
            },
            {
              "key": "[metadata] pkg.torch.onnx.stack_trace",
              "value": "File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 688, in forward\n    outputs: BaseModelOutputWithPast = self.model(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 453, in forward\n    layer_outputs = decoder_layer(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 305, in forward\n    hidden_states = self.input_layernorm(hidden_states)\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 72, in forward\n    hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)"
            }
          ],
          "incomingEdges": [
            {
              "sourceNodeId": "node_Sqrt_506",
              "sourceNodeOutputId": "0",
              "targetNodeInputId": "0"
            }
          ],
          "outputsMetadata": [
            {
              "id": "0",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "rsqrt_2"
                },
                {
                  "key": "tensor_shape",
                  "value": "FLOAT[s34,s16,1]"
                },
                {
                  "key": "param_name",
                  "value": "Y"
                }
              ]
            }
          ],
          "inputsMetadata": [
            {
              "id": "0",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "val_362"
                },
                {
                  "key": "tensor_shape",
                  "value": "FLOAT[s34,s16,1]"
                },
                {
                  "key": "param_name",
                  "value": "X"
                }
              ]
            }
          ],
          "style": null,
          "config": null
        },
        {
          "id": "node_Mul_508",
          "label": "Mul",
          "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.1: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.1.input_layernorm: transformers.models.llama.modeling_llama.LlamaRMSNorm/mul_384: aten.mul.Tensor",
          "subgraphIds": [],
          "attrs": [
            {
              "key": "[metadata] namespace",
              "value": ": transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.1: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.1.input_layernorm: transformers.models.llama.modeling_llama.LlamaRMSNorm/mul_384: aten.mul.Tensor"
            },
            {
              "key": "[metadata] pkg.torch.onnx.class_hierarchy",
              "value": "['transformers.models.llama.modeling_llama.LlamaForCausalLM', 'transformers.models.llama.modeling_llama.LlamaModel', 'transformers.models.llama.modeling_llama.LlamaDecoderLayer', 'transformers.models.llama.modeling_llama.LlamaRMSNorm', 'aten.mul.Tensor']"
            },
            {
              "key": "[metadata] pkg.torch.onnx.fx_node",
              "value": "%mul_384 : [num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%add_463, %rsqrt_2), kwargs = {})"
            },
            {
              "key": "[metadata] pkg.torch.onnx.name_scopes",
              "value": "['', 'model', 'model.layers.1', 'model.layers.1.input_layernorm', 'mul_384']"
            },
            {
              "key": "[metadata] pkg.torch.onnx.stack_trace",
              "value": "File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 688, in forward\n    outputs: BaseModelOutputWithPast = self.model(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 453, in forward\n    layer_outputs = decoder_layer(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 305, in forward\n    hidden_states = self.input_layernorm(hidden_states)\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 72, in forward\n    hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)"
            }
          ],
          "incomingEdges": [
            {
              "sourceNodeId": "node_Add_499",
              "sourceNodeOutputId": "0",
              "targetNodeInputId": "0"
            },
            {
              "sourceNodeId": "node_Reciprocal_507",
              "sourceNodeOutputId": "0",
              "targetNodeInputId": "1"
            }
          ],
          "outputsMetadata": [
            {
              "id": "0",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "mul_384"
                },
                {
                  "key": "tensor_shape",
                  "value": "FLOAT[s34,s16,2048]"
                },
                {
                  "key": "param_name",
                  "value": "C"
                }
              ]
            }
          ],
          "inputsMetadata": [
            {
              "id": "0",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "add_463"
                },
                {
                  "key": "tensor_shape",
                  "value": "FLOAT[s34,s16,2048]"
                },
                {
                  "key": "param_name",
                  "value": "A"
                }
              ]
            },
            {
              "id": "1",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "rsqrt_2"
                },
                {
                  "key": "tensor_shape",
                  "value": "FLOAT[s34,s16,1]"
                },
                {
                  "key": "param_name",
                  "value": "B"
                }
              ]
            }
          ],
          "style": null,
          "config": null
        },
        {
          "id": "node_Mul_509",
          "label": "Mul",
          "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.1: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.1.input_layernorm: transformers.models.llama.modeling_llama.LlamaRMSNorm/mul_388: aten.mul.Tensor",
          "subgraphIds": [],
          "attrs": [
            {
              "key": "[metadata] namespace",
              "value": ": transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.1: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.1.input_layernorm: transformers.models.llama.modeling_llama.LlamaRMSNorm/mul_388: aten.mul.Tensor"
            },
            {
              "key": "[metadata] pkg.torch.onnx.class_hierarchy",
              "value": "['transformers.models.llama.modeling_llama.LlamaForCausalLM', 'transformers.models.llama.modeling_llama.LlamaModel', 'transformers.models.llama.modeling_llama.LlamaDecoderLayer', 'transformers.models.llama.modeling_llama.LlamaRMSNorm', 'aten.mul.Tensor']"
            },
            {
              "key": "[metadata] pkg.torch.onnx.fx_node",
              "value": "%mul_388 : [num_users=3] = call_function[target=torch.ops.aten.mul.Tensor](args = (%p_model_layers_1_input_layernorm_weight, %mul_384), kwargs = {})"
            },
            {
              "key": "[metadata] pkg.torch.onnx.name_scopes",
              "value": "['', 'model', 'model.layers.1', 'model.layers.1.input_layernorm', 'mul_388']"
            },
            {
              "key": "[metadata] pkg.torch.onnx.stack_trace",
              "value": "File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 688, in forward\n    outputs: BaseModelOutputWithPast = self.model(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 453, in forward\n    layer_outputs = decoder_layer(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 305, in forward\n    hidden_states = self.input_layernorm(hidden_states)\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 73, in forward\n    return self.weight * hidden_states.to(input_dtype)"
            }
          ],
          "incomingEdges": [
            {
              "sourceNodeId": "[value] model.layers.1.input_layernorm.weight",
              "sourceNodeOutputId": "0",
              "targetNodeInputId": "0"
            },
            {
              "sourceNodeId": "node_Mul_508",
              "sourceNodeOutputId": "0",
              "targetNodeInputId": "1"
            }
          ],
          "outputsMetadata": [
            {
              "id": "0",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "mul_388"
                },
                {
                  "key": "tensor_shape",
                  "value": "FLOAT[s34,s16,2048]"
                },
                {
                  "key": "param_name",
                  "value": "C"
                }
              ]
            }
          ],
          "inputsMetadata": [
            {
              "id": "0",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "model.layers.1.input_layernorm.weight"
                },
                {
                  "key": "tensor_shape",
                  "value": "FLOAT[2048]"
                },
                {
                  "key": "param_name",
                  "value": "A"
                }
              ]
            },
            {
              "id": "1",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "mul_384"
                },
                {
                  "key": "tensor_shape",
                  "value": "FLOAT[s34,s16,2048]"
                },
                {
                  "key": "param_name",
                  "value": "B"
                }
              ]
            }
          ],
          "style": null,
          "config": null
        },
        {
          "id": "node_Transpose_510",
          "label": "Transpose",
          "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.1: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.1.self_attn: transformers.models.llama.modeling_llama.LlamaAttention/model.layers.1.self_attn.q_proj: torch.nn.modules.linear.Linear/linear_7: aten.linear.default",
          "subgraphIds": [],
          "attrs": [
            {
              "key": "perm",
              "value": "[1, 0]"
            },
            {
              "key": "[metadata] namespace",
              "value": ": transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.1: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.1.self_attn: transformers.models.llama.modeling_llama.LlamaAttention/model.layers.1.self_attn.q_proj: torch.nn.modules.linear.Linear/linear_7: aten.linear.default"
            },
            {
              "key": "[metadata] pkg.torch.onnx.class_hierarchy",
              "value": "['transformers.models.llama.modeling_llama.LlamaForCausalLM', 'transformers.models.llama.modeling_llama.LlamaModel', 'transformers.models.llama.modeling_llama.LlamaDecoderLayer', 'transformers.models.llama.modeling_llama.LlamaAttention', 'torch.nn.modules.linear.Linear', 'aten.linear.default']"
            },
            {
              "key": "[metadata] pkg.torch.onnx.fx_node",
              "value": "%linear_7 : [num_users=1] = call_function[target=torch.ops.aten.linear.default](args = (%mul_388, %p_model_layers_1_self_attn_q_proj_weight), kwargs = {})"
            },
            {
              "key": "[metadata] pkg.torch.onnx.name_scopes",
              "value": "['', 'model', 'model.layers.1', 'model.layers.1.self_attn', 'model.layers.1.self_attn.q_proj', 'linear_7']"
            },
            {
              "key": "[metadata] pkg.torch.onnx.stack_trace",
              "value": "File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 688, in forward\n    outputs: BaseModelOutputWithPast = self.model(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 453, in forward\n    layer_outputs = decoder_layer(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 308, in forward\n    hidden_states, self_attn_weights = self.self_attn(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 242, in forward\n    query_states = self.q_proj(hidden_states).view(hidden_shape).transpose(1, 2)\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/torch/nn/modules/linear.py\", line 125, in forward\n    return F.linear(input, self.weight, self.bias)"
            }
          ],
          "incomingEdges": [
            {
              "sourceNodeId": "[value] model.layers.1.self_attn.q_proj.weight",
              "sourceNodeOutputId": "0",
              "targetNodeInputId": "0"
            }
          ],
          "outputsMetadata": [
            {
              "id": "0",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "val_363"
                },
                {
                  "key": "tensor_shape",
                  "value": "FLOAT[2048,2048]"
                },
                {
                  "key": "param_name",
                  "value": "transposed"
                }
              ]
            }
          ],
          "inputsMetadata": [
            {
              "id": "0",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "model.layers.1.self_attn.q_proj.weight"
                },
                {
                  "key": "tensor_shape",
                  "value": "FLOAT[2048,2048]"
                },
                {
                  "key": "param_name",
                  "value": "data"
                }
              ]
            }
          ],
          "style": null,
          "config": null
        },
        {
          "id": "node_MatMul_511",
          "label": "MatMul",
          "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.1: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.1.self_attn: transformers.models.llama.modeling_llama.LlamaAttention/model.layers.1.self_attn.q_proj: torch.nn.modules.linear.Linear/linear_7: aten.linear.default",
          "subgraphIds": [],
          "attrs": [
            {
              "key": "[metadata] namespace",
              "value": ": transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.1: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.1.self_attn: transformers.models.llama.modeling_llama.LlamaAttention/model.layers.1.self_attn.q_proj: torch.nn.modules.linear.Linear/linear_7: aten.linear.default"
            },
            {
              "key": "[metadata] pkg.torch.onnx.class_hierarchy",
              "value": "['transformers.models.llama.modeling_llama.LlamaForCausalLM', 'transformers.models.llama.modeling_llama.LlamaModel', 'transformers.models.llama.modeling_llama.LlamaDecoderLayer', 'transformers.models.llama.modeling_llama.LlamaAttention', 'torch.nn.modules.linear.Linear', 'aten.linear.default']"
            },
            {
              "key": "[metadata] pkg.torch.onnx.fx_node",
              "value": "%linear_7 : [num_users=1] = call_function[target=torch.ops.aten.linear.default](args = (%mul_388, %p_model_layers_1_self_attn_q_proj_weight), kwargs = {})"
            },
            {
              "key": "[metadata] pkg.torch.onnx.name_scopes",
              "value": "['', 'model', 'model.layers.1', 'model.layers.1.self_attn', 'model.layers.1.self_attn.q_proj', 'linear_7']"
            },
            {
              "key": "[metadata] pkg.torch.onnx.stack_trace",
              "value": "File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 688, in forward\n    outputs: BaseModelOutputWithPast = self.model(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 453, in forward\n    layer_outputs = decoder_layer(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 308, in forward\n    hidden_states, self_attn_weights = self.self_attn(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 242, in forward\n    query_states = self.q_proj(hidden_states).view(hidden_shape).transpose(1, 2)\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/torch/nn/modules/linear.py\", line 125, in forward\n    return F.linear(input, self.weight, self.bias)"
            }
          ],
          "incomingEdges": [
            {
              "sourceNodeId": "node_Mul_509",
              "sourceNodeOutputId": "0",
              "targetNodeInputId": "0"
            },
            {
              "sourceNodeId": "node_Transpose_510",
              "sourceNodeOutputId": "0",
              "targetNodeInputId": "1"
            }
          ],
          "outputsMetadata": [
            {
              "id": "0",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "linear_7"
                },
                {
                  "key": "tensor_shape",
                  "value": "FLOAT[s34,s16,2048]"
                },
                {
                  "key": "param_name",
                  "value": "Y"
                }
              ]
            }
          ],
          "inputsMetadata": [
            {
              "id": "0",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "mul_388"
                },
                {
                  "key": "tensor_shape",
                  "value": "FLOAT[s34,s16,2048]"
                },
                {
                  "key": "param_name",
                  "value": "A"
                }
              ]
            },
            {
              "id": "1",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "val_363"
                },
                {
                  "key": "tensor_shape",
                  "value": "FLOAT[2048,2048]"
                },
                {
                  "key": "param_name",
                  "value": "B"
                }
              ]
            }
          ],
          "style": null,
          "config": null
        },
        {
          "id": "node_Concat_516",
          "label": "Concat",
          "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.1: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.1.self_attn: transformers.models.llama.modeling_llama.LlamaAttention/view_5: aten.view.default",
          "subgraphIds": [],
          "attrs": [
            {
              "key": "axis",
              "value": "0"
            },
            {
              "key": "[metadata] namespace",
              "value": ": transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.1: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.1.self_attn: transformers.models.llama.modeling_llama.LlamaAttention/view_5: aten.view.default"
            },
            {
              "key": "[metadata] pkg.torch.onnx.class_hierarchy",
              "value": "['transformers.models.llama.modeling_llama.LlamaForCausalLM', 'transformers.models.llama.modeling_llama.LlamaModel', 'transformers.models.llama.modeling_llama.LlamaDecoderLayer', 'transformers.models.llama.modeling_llama.LlamaAttention', 'aten.view.default']"
            },
            {
              "key": "[metadata] pkg.torch.onnx.fx_node",
              "value": "%view_5 : [num_users=1] = call_function[target=torch.ops.aten.view.default](args = (%linear_7, [%sym_size_int_68, %sym_size_int_61, -1, 64]), kwargs = {})"
            },
            {
              "key": "[metadata] pkg.torch.onnx.name_scopes",
              "value": "['', 'model', 'model.layers.1', 'model.layers.1.self_attn', 'view_5']"
            },
            {
              "key": "[metadata] pkg.torch.onnx.stack_trace",
              "value": "File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 688, in forward\n    outputs: BaseModelOutputWithPast = self.model(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 453, in forward\n    layer_outputs = decoder_layer(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 308, in forward\n    hidden_states, self_attn_weights = self.self_attn(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 242, in forward\n    query_states = self.q_proj(hidden_states).view(hidden_shape).transpose(1, 2)"
            }
          ],
          "incomingEdges": [
            {
              "sourceNodeId": "node_Shape_12",
              "sourceNodeOutputId": "0",
              "targetNodeInputId": "0"
            },
            {
              "sourceNodeId": "node_Shape_2",
              "sourceNodeOutputId": "0",
              "targetNodeInputId": "1"
            },
            {
              "sourceNodeId": "node_Constant_90",
              "sourceNodeOutputId": "0",
              "targetNodeInputId": "2"
            },
            {
              "sourceNodeId": "node_Constant_345",
              "sourceNodeOutputId": "0",
              "targetNodeInputId": "3"
            }
          ],
          "outputsMetadata": [
            {
              "id": "0",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "val_368"
                },
                {
                  "key": "tensor_shape",
                  "value": "INT64[4]"
                },
                {
                  "key": "param_name",
                  "value": "concat_result"
                }
              ]
            }
          ],
          "inputsMetadata": [
            {
              "id": "0",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "val_6"
                },
                {
                  "key": "tensor_shape",
                  "value": "INT64[1]"
                },
                {
                  "key": "param_name",
                  "value": "inputs"
                }
              ]
            },
            {
              "id": "1",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "val_1"
                },
                {
                  "key": "tensor_shape",
                  "value": "INT64[1]"
                },
                {
                  "key": "param_name",
                  "value": "inputs"
                }
              ]
            },
            {
              "id": "2",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "val_53"
                },
                {
                  "key": "tensor_shape",
                  "value": "INT64[1]"
                },
                {
                  "key": "param_name",
                  "value": "inputs"
                }
              ]
            },
            {
              "id": "3",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "val_258"
                },
                {
                  "key": "tensor_shape",
                  "value": "INT64[1]"
                },
                {
                  "key": "param_name",
                  "value": "inputs"
                }
              ]
            }
          ],
          "style": null,
          "config": null
        },
        {
          "id": "node_Reshape_518",
          "label": "Reshape",
          "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.1: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.1.self_attn: transformers.models.llama.modeling_llama.LlamaAttention/view_5: aten.view.default",
          "subgraphIds": [],
          "attrs": [
            {
              "key": "allowzero",
              "value": "1"
            },
            {
              "key": "[metadata] namespace",
              "value": ": transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.1: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.1.self_attn: transformers.models.llama.modeling_llama.LlamaAttention/view_5: aten.view.default"
            },
            {
              "key": "[metadata] pkg.torch.onnx.class_hierarchy",
              "value": "['transformers.models.llama.modeling_llama.LlamaForCausalLM', 'transformers.models.llama.modeling_llama.LlamaModel', 'transformers.models.llama.modeling_llama.LlamaDecoderLayer', 'transformers.models.llama.modeling_llama.LlamaAttention', 'aten.view.default']"
            },
            {
              "key": "[metadata] pkg.torch.onnx.fx_node",
              "value": "%view_5 : [num_users=1] = call_function[target=torch.ops.aten.view.default](args = (%linear_7, [%sym_size_int_68, %sym_size_int_61, -1, 64]), kwargs = {})"
            },
            {
              "key": "[metadata] pkg.torch.onnx.name_scopes",
              "value": "['', 'model', 'model.layers.1', 'model.layers.1.self_attn', 'view_5']"
            },
            {
              "key": "[metadata] pkg.torch.onnx.stack_trace",
              "value": "File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 688, in forward\n    outputs: BaseModelOutputWithPast = self.model(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 453, in forward\n    layer_outputs = decoder_layer(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 308, in forward\n    hidden_states, self_attn_weights = self.self_attn(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 242, in forward\n    query_states = self.q_proj(hidden_states).view(hidden_shape).transpose(1, 2)"
            }
          ],
          "incomingEdges": [
            {
              "sourceNodeId": "node_MatMul_511",
              "sourceNodeOutputId": "0",
              "targetNodeInputId": "0"
            },
            {
              "sourceNodeId": "node_Concat_516",
              "sourceNodeOutputId": "0",
              "targetNodeInputId": "1"
            }
          ],
          "outputsMetadata": [
            {
              "id": "0",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "view_5"
                },
                {
                  "key": "tensor_shape",
                  "value": "FLOAT[s34,s16,32,64]"
                },
                {
                  "key": "param_name",
                  "value": "reshaped"
                }
              ]
            }
          ],
          "inputsMetadata": [
            {
              "id": "0",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "linear_7"
                },
                {
                  "key": "tensor_shape",
                  "value": "FLOAT[s34,s16,2048]"
                },
                {
                  "key": "param_name",
                  "value": "data"
                }
              ]
            },
            {
              "id": "1",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "val_368"
                },
                {
                  "key": "tensor_shape",
                  "value": "INT64[4]"
                },
                {
                  "key": "param_name",
                  "value": "shape"
                }
              ]
            }
          ],
          "style": null,
          "config": null
        },
        {
          "id": "node_Transpose_519",
          "label": "Transpose",
          "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.1: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.1.self_attn: transformers.models.llama.modeling_llama.LlamaAttention/transpose_6: aten.transpose.int",
          "subgraphIds": [],
          "attrs": [
            {
              "key": "perm",
              "value": "[0, 2, 1, 3]"
            },
            {
              "key": "[metadata] namespace",
              "value": ": transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.1: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.1.self_attn: transformers.models.llama.modeling_llama.LlamaAttention/transpose_6: aten.transpose.int"
            },
            {
              "key": "[metadata] pkg.torch.onnx.class_hierarchy",
              "value": "['transformers.models.llama.modeling_llama.LlamaForCausalLM', 'transformers.models.llama.modeling_llama.LlamaModel', 'transformers.models.llama.modeling_llama.LlamaDecoderLayer', 'transformers.models.llama.modeling_llama.LlamaAttention', 'aten.transpose.int']"
            },
            {
              "key": "[metadata] pkg.torch.onnx.fx_node",
              "value": "%transpose_6 : [num_users=3] = call_function[target=torch.ops.aten.transpose.int](args = (%view_5, 1, 2), kwargs = {})"
            },
            {
              "key": "[metadata] pkg.torch.onnx.name_scopes",
              "value": "['', 'model', 'model.layers.1', 'model.layers.1.self_attn', 'transpose_6']"
            },
            {
              "key": "[metadata] pkg.torch.onnx.stack_trace",
              "value": "File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 688, in forward\n    outputs: BaseModelOutputWithPast = self.model(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 453, in forward\n    layer_outputs = decoder_layer(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 308, in forward\n    hidden_states, self_attn_weights = self.self_attn(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 242, in forward\n    query_states = self.q_proj(hidden_states).view(hidden_shape).transpose(1, 2)"
            }
          ],
          "incomingEdges": [
            {
              "sourceNodeId": "node_Reshape_518",
              "sourceNodeOutputId": "0",
              "targetNodeInputId": "0"
            }
          ],
          "outputsMetadata": [
            {
              "id": "0",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "transpose_6"
                },
                {
                  "key": "tensor_shape",
                  "value": "FLOAT[s34,32,s16,64]"
                },
                {
                  "key": "param_name",
                  "value": "transposed"
                }
              ]
            }
          ],
          "inputsMetadata": [
            {
              "id": "0",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "view_5"
                },
                {
                  "key": "tensor_shape",
                  "value": "FLOAT[s34,s16,32,64]"
                },
                {
                  "key": "param_name",
                  "value": "data"
                }
              ]
            }
          ],
          "style": null,
          "config": null
        },
        {
          "id": "node_Transpose_520",
          "label": "Transpose",
          "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.1: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.1.self_attn: transformers.models.llama.modeling_llama.LlamaAttention/model.layers.1.self_attn.k_proj: torch.nn.modules.linear.Linear/linear_8: aten.linear.default",
          "subgraphIds": [],
          "attrs": [
            {
              "key": "perm",
              "value": "[1, 0]"
            },
            {
              "key": "[metadata] namespace",
              "value": ": transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.1: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.1.self_attn: transformers.models.llama.modeling_llama.LlamaAttention/model.layers.1.self_attn.k_proj: torch.nn.modules.linear.Linear/linear_8: aten.linear.default"
            },
            {
              "key": "[metadata] pkg.torch.onnx.class_hierarchy",
              "value": "['transformers.models.llama.modeling_llama.LlamaForCausalLM', 'transformers.models.llama.modeling_llama.LlamaModel', 'transformers.models.llama.modeling_llama.LlamaDecoderLayer', 'transformers.models.llama.modeling_llama.LlamaAttention', 'torch.nn.modules.linear.Linear', 'aten.linear.default']"
            },
            {
              "key": "[metadata] pkg.torch.onnx.fx_node",
              "value": "%linear_8 : [num_users=1] = call_function[target=torch.ops.aten.linear.default](args = (%mul_388, %p_model_layers_1_self_attn_k_proj_weight), kwargs = {})"
            },
            {
              "key": "[metadata] pkg.torch.onnx.name_scopes",
              "value": "['', 'model', 'model.layers.1', 'model.layers.1.self_attn', 'model.layers.1.self_attn.k_proj', 'linear_8']"
            },
            {
              "key": "[metadata] pkg.torch.onnx.stack_trace",
              "value": "File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 688, in forward\n    outputs: BaseModelOutputWithPast = self.model(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 453, in forward\n    layer_outputs = decoder_layer(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 308, in forward\n    hidden_states, self_attn_weights = self.self_attn(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 243, in forward\n    key_states = self.k_proj(hidden_states).view(hidden_shape).transpose(1, 2)\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/torch/nn/modules/linear.py\", line 125, in forward\n    return F.linear(input, self.weight, self.bias)"
            }
          ],
          "incomingEdges": [
            {
              "sourceNodeId": "[value] model.layers.1.self_attn.k_proj.weight",
              "sourceNodeOutputId": "0",
              "targetNodeInputId": "0"
            }
          ],
          "outputsMetadata": [
            {
              "id": "0",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "val_370"
                },
                {
                  "key": "tensor_shape",
                  "value": "FLOAT[2048,2048]"
                },
                {
                  "key": "param_name",
                  "value": "transposed"
                }
              ]
            }
          ],
          "inputsMetadata": [
            {
              "id": "0",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "model.layers.1.self_attn.k_proj.weight"
                },
                {
                  "key": "tensor_shape",
                  "value": "FLOAT[2048,2048]"
                },
                {
                  "key": "param_name",
                  "value": "data"
                }
              ]
            }
          ],
          "style": null,
          "config": null
        },
        {
          "id": "node_MatMul_521",
          "label": "MatMul",
          "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.1: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.1.self_attn: transformers.models.llama.modeling_llama.LlamaAttention/model.layers.1.self_attn.k_proj: torch.nn.modules.linear.Linear/linear_8: aten.linear.default",
          "subgraphIds": [],
          "attrs": [
            {
              "key": "[metadata] namespace",
              "value": ": transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.1: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.1.self_attn: transformers.models.llama.modeling_llama.LlamaAttention/model.layers.1.self_attn.k_proj: torch.nn.modules.linear.Linear/linear_8: aten.linear.default"
            },
            {
              "key": "[metadata] pkg.torch.onnx.class_hierarchy",
              "value": "['transformers.models.llama.modeling_llama.LlamaForCausalLM', 'transformers.models.llama.modeling_llama.LlamaModel', 'transformers.models.llama.modeling_llama.LlamaDecoderLayer', 'transformers.models.llama.modeling_llama.LlamaAttention', 'torch.nn.modules.linear.Linear', 'aten.linear.default']"
            },
            {
              "key": "[metadata] pkg.torch.onnx.fx_node",
              "value": "%linear_8 : [num_users=1] = call_function[target=torch.ops.aten.linear.default](args = (%mul_388, %p_model_layers_1_self_attn_k_proj_weight), kwargs = {})"
            },
            {
              "key": "[metadata] pkg.torch.onnx.name_scopes",
              "value": "['', 'model', 'model.layers.1', 'model.layers.1.self_attn', 'model.layers.1.self_attn.k_proj', 'linear_8']"
            },
            {
              "key": "[metadata] pkg.torch.onnx.stack_trace",
              "value": "File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 688, in forward\n    outputs: BaseModelOutputWithPast = self.model(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 453, in forward\n    layer_outputs = decoder_layer(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 308, in forward\n    hidden_states, self_attn_weights = self.self_attn(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 243, in forward\n    key_states = self.k_proj(hidden_states).view(hidden_shape).transpose(1, 2)\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/torch/nn/modules/linear.py\", line 125, in forward\n    return F.linear(input, self.weight, self.bias)"
            }
          ],
          "incomingEdges": [
            {
              "sourceNodeId": "node_Mul_509",
              "sourceNodeOutputId": "0",
              "targetNodeInputId": "0"
            },
            {
              "sourceNodeId": "node_Transpose_520",
              "sourceNodeOutputId": "0",
              "targetNodeInputId": "1"
            }
          ],
          "outputsMetadata": [
            {
              "id": "0",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "linear_8"
                },
                {
                  "key": "tensor_shape",
                  "value": "FLOAT[s34,s16,2048]"
                },
                {
                  "key": "param_name",
                  "value": "Y"
                }
              ]
            }
          ],
          "inputsMetadata": [
            {
              "id": "0",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "mul_388"
                },
                {
                  "key": "tensor_shape",
                  "value": "FLOAT[s34,s16,2048]"
                },
                {
                  "key": "param_name",
                  "value": "A"
                }
              ]
            },
            {
              "id": "1",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "val_370"
                },
                {
                  "key": "tensor_shape",
                  "value": "FLOAT[2048,2048]"
                },
                {
                  "key": "param_name",
                  "value": "B"
                }
              ]
            }
          ],
          "style": null,
          "config": null
        },
        {
          "id": "node_Concat_526",
          "label": "Concat",
          "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.1: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.1.self_attn: transformers.models.llama.modeling_llama.LlamaAttention/view_6: aten.view.default",
          "subgraphIds": [],
          "attrs": [
            {
              "key": "axis",
              "value": "0"
            },
            {
              "key": "[metadata] namespace",
              "value": ": transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.1: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.1.self_attn: transformers.models.llama.modeling_llama.LlamaAttention/view_6: aten.view.default"
            },
            {
              "key": "[metadata] pkg.torch.onnx.class_hierarchy",
              "value": "['transformers.models.llama.modeling_llama.LlamaForCausalLM', 'transformers.models.llama.modeling_llama.LlamaModel', 'transformers.models.llama.modeling_llama.LlamaDecoderLayer', 'transformers.models.llama.modeling_llama.LlamaAttention', 'aten.view.default']"
            },
            {
              "key": "[metadata] pkg.torch.onnx.fx_node",
              "value": "%view_6 : [num_users=1] = call_function[target=torch.ops.aten.view.default](args = (%linear_8, [%sym_size_int_68, %sym_size_int_61, -1, 64]), kwargs = {})"
            },
            {
              "key": "[metadata] pkg.torch.onnx.name_scopes",
              "value": "['', 'model', 'model.layers.1', 'model.layers.1.self_attn', 'view_6']"
            },
            {
              "key": "[metadata] pkg.torch.onnx.stack_trace",
              "value": "File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 688, in forward\n    outputs: BaseModelOutputWithPast = self.model(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 453, in forward\n    layer_outputs = decoder_layer(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 308, in forward\n    hidden_states, self_attn_weights = self.self_attn(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 243, in forward\n    key_states = self.k_proj(hidden_states).view(hidden_shape).transpose(1, 2)"
            }
          ],
          "incomingEdges": [
            {
              "sourceNodeId": "node_Shape_12",
              "sourceNodeOutputId": "0",
              "targetNodeInputId": "0"
            },
            {
              "sourceNodeId": "node_Shape_2",
              "sourceNodeOutputId": "0",
              "targetNodeInputId": "1"
            },
            {
              "sourceNodeId": "node_Constant_90",
              "sourceNodeOutputId": "0",
              "targetNodeInputId": "2"
            },
            {
              "sourceNodeId": "node_Constant_345",
              "sourceNodeOutputId": "0",
              "targetNodeInputId": "3"
            }
          ],
          "outputsMetadata": [
            {
              "id": "0",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "val_375"
                },
                {
                  "key": "tensor_shape",
                  "value": "INT64[4]"
                },
                {
                  "key": "param_name",
                  "value": "concat_result"
                }
              ]
            }
          ],
          "inputsMetadata": [
            {
              "id": "0",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "val_6"
                },
                {
                  "key": "tensor_shape",
                  "value": "INT64[1]"
                },
                {
                  "key": "param_name",
                  "value": "inputs"
                }
              ]
            },
            {
              "id": "1",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "val_1"
                },
                {
                  "key": "tensor_shape",
                  "value": "INT64[1]"
                },
                {
                  "key": "param_name",
                  "value": "inputs"
                }
              ]
            },
            {
              "id": "2",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "val_53"
                },
                {
                  "key": "tensor_shape",
                  "value": "INT64[1]"
                },
                {
                  "key": "param_name",
                  "value": "inputs"
                }
              ]
            },
            {
              "id": "3",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "val_258"
                },
                {
                  "key": "tensor_shape",
                  "value": "INT64[1]"
                },
                {
                  "key": "param_name",
                  "value": "inputs"
                }
              ]
            }
          ],
          "style": null,
          "config": null
        },
        {
          "id": "node_Reshape_528",
          "label": "Reshape",
          "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.1: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.1.self_attn: transformers.models.llama.modeling_llama.LlamaAttention/view_6: aten.view.default",
          "subgraphIds": [],
          "attrs": [
            {
              "key": "allowzero",
              "value": "1"
            },
            {
              "key": "[metadata] namespace",
              "value": ": transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.1: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.1.self_attn: transformers.models.llama.modeling_llama.LlamaAttention/view_6: aten.view.default"
            },
            {
              "key": "[metadata] pkg.torch.onnx.class_hierarchy",
              "value": "['transformers.models.llama.modeling_llama.LlamaForCausalLM', 'transformers.models.llama.modeling_llama.LlamaModel', 'transformers.models.llama.modeling_llama.LlamaDecoderLayer', 'transformers.models.llama.modeling_llama.LlamaAttention', 'aten.view.default']"
            },
            {
              "key": "[metadata] pkg.torch.onnx.fx_node",
              "value": "%view_6 : [num_users=1] = call_function[target=torch.ops.aten.view.default](args = (%linear_8, [%sym_size_int_68, %sym_size_int_61, -1, 64]), kwargs = {})"
            },
            {
              "key": "[metadata] pkg.torch.onnx.name_scopes",
              "value": "['', 'model', 'model.layers.1', 'model.layers.1.self_attn', 'view_6']"
            },
            {
              "key": "[metadata] pkg.torch.onnx.stack_trace",
              "value": "File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 688, in forward\n    outputs: BaseModelOutputWithPast = self.model(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 453, in forward\n    layer_outputs = decoder_layer(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 308, in forward\n    hidden_states, self_attn_weights = self.self_attn(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 243, in forward\n    key_states = self.k_proj(hidden_states).view(hidden_shape).transpose(1, 2)"
            }
          ],
          "incomingEdges": [
            {
              "sourceNodeId": "node_MatMul_521",
              "sourceNodeOutputId": "0",
              "targetNodeInputId": "0"
            },
            {
              "sourceNodeId": "node_Concat_526",
              "sourceNodeOutputId": "0",
              "targetNodeInputId": "1"
            }
          ],
          "outputsMetadata": [
            {
              "id": "0",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "view_6"
                },
                {
                  "key": "tensor_shape",
                  "value": "FLOAT[s34,s16,32,64]"
                },
                {
                  "key": "param_name",
                  "value": "reshaped"
                }
              ]
            }
          ],
          "inputsMetadata": [
            {
              "id": "0",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "linear_8"
                },
                {
                  "key": "tensor_shape",
                  "value": "FLOAT[s34,s16,2048]"
                },
                {
                  "key": "param_name",
                  "value": "data"
                }
              ]
            },
            {
              "id": "1",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "val_375"
                },
                {
                  "key": "tensor_shape",
                  "value": "INT64[4]"
                },
                {
                  "key": "param_name",
                  "value": "shape"
                }
              ]
            }
          ],
          "style": null,
          "config": null
        },
        {
          "id": "node_Transpose_529",
          "label": "Transpose",
          "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.1: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.1.self_attn: transformers.models.llama.modeling_llama.LlamaAttention/transpose_7: aten.transpose.int",
          "subgraphIds": [],
          "attrs": [
            {
              "key": "perm",
              "value": "[0, 2, 1, 3]"
            },
            {
              "key": "[metadata] namespace",
              "value": ": transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.1: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.1.self_attn: transformers.models.llama.modeling_llama.LlamaAttention/transpose_7: aten.transpose.int"
            },
            {
              "key": "[metadata] pkg.torch.onnx.class_hierarchy",
              "value": "['transformers.models.llama.modeling_llama.LlamaForCausalLM', 'transformers.models.llama.modeling_llama.LlamaModel', 'transformers.models.llama.modeling_llama.LlamaDecoderLayer', 'transformers.models.llama.modeling_llama.LlamaAttention', 'aten.transpose.int']"
            },
            {
              "key": "[metadata] pkg.torch.onnx.fx_node",
              "value": "%transpose_7 : [num_users=3] = call_function[target=torch.ops.aten.transpose.int](args = (%view_6, 1, 2), kwargs = {})"
            },
            {
              "key": "[metadata] pkg.torch.onnx.name_scopes",
              "value": "['', 'model', 'model.layers.1', 'model.layers.1.self_attn', 'transpose_7']"
            },
            {
              "key": "[metadata] pkg.torch.onnx.stack_trace",
              "value": "File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 688, in forward\n    outputs: BaseModelOutputWithPast = self.model(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 453, in forward\n    layer_outputs = decoder_layer(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 308, in forward\n    hidden_states, self_attn_weights = self.self_attn(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 243, in forward\n    key_states = self.k_proj(hidden_states).view(hidden_shape).transpose(1, 2)"
            }
          ],
          "incomingEdges": [
            {
              "sourceNodeId": "node_Reshape_528",
              "sourceNodeOutputId": "0",
              "targetNodeInputId": "0"
            }
          ],
          "outputsMetadata": [
            {
              "id": "0",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "transpose_7"
                },
                {
                  "key": "tensor_shape",
                  "value": "FLOAT[s34,32,s16,64]"
                },
                {
                  "key": "param_name",
                  "value": "transposed"
                }
              ]
            }
          ],
          "inputsMetadata": [
            {
              "id": "0",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "view_6"
                },
                {
                  "key": "tensor_shape",
                  "value": "FLOAT[s34,s16,32,64]"
                },
                {
                  "key": "param_name",
                  "value": "data"
                }
              ]
            }
          ],
          "style": null,
          "config": null
        },
        {
          "id": "node_Transpose_530",
          "label": "Transpose",
          "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.1: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.1.self_attn: transformers.models.llama.modeling_llama.LlamaAttention/model.layers.1.self_attn.v_proj: torch.nn.modules.linear.Linear/linear_9: aten.linear.default",
          "subgraphIds": [],
          "attrs": [
            {
              "key": "perm",
              "value": "[1, 0]"
            },
            {
              "key": "[metadata] namespace",
              "value": ": transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.1: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.1.self_attn: transformers.models.llama.modeling_llama.LlamaAttention/model.layers.1.self_attn.v_proj: torch.nn.modules.linear.Linear/linear_9: aten.linear.default"
            },
            {
              "key": "[metadata] pkg.torch.onnx.class_hierarchy",
              "value": "['transformers.models.llama.modeling_llama.LlamaForCausalLM', 'transformers.models.llama.modeling_llama.LlamaModel', 'transformers.models.llama.modeling_llama.LlamaDecoderLayer', 'transformers.models.llama.modeling_llama.LlamaAttention', 'torch.nn.modules.linear.Linear', 'aten.linear.default']"
            },
            {
              "key": "[metadata] pkg.torch.onnx.fx_node",
              "value": "%linear_9 : [num_users=1] = call_function[target=torch.ops.aten.linear.default](args = (%mul_388, %p_model_layers_1_self_attn_v_proj_weight), kwargs = {})"
            },
            {
              "key": "[metadata] pkg.torch.onnx.name_scopes",
              "value": "['', 'model', 'model.layers.1', 'model.layers.1.self_attn', 'model.layers.1.self_attn.v_proj', 'linear_9']"
            },
            {
              "key": "[metadata] pkg.torch.onnx.stack_trace",
              "value": "File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 688, in forward\n    outputs: BaseModelOutputWithPast = self.model(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 453, in forward\n    layer_outputs = decoder_layer(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 308, in forward\n    hidden_states, self_attn_weights = self.self_attn(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 244, in forward\n    value_states = self.v_proj(hidden_states).view(hidden_shape).transpose(1, 2)\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/torch/nn/modules/linear.py\", line 125, in forward\n    return F.linear(input, self.weight, self.bias)"
            }
          ],
          "incomingEdges": [
            {
              "sourceNodeId": "[value] model.layers.1.self_attn.v_proj.weight",
              "sourceNodeOutputId": "0",
              "targetNodeInputId": "0"
            }
          ],
          "outputsMetadata": [
            {
              "id": "0",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "val_377"
                },
                {
                  "key": "tensor_shape",
                  "value": "FLOAT[2048,2048]"
                },
                {
                  "key": "param_name",
                  "value": "transposed"
                }
              ]
            }
          ],
          "inputsMetadata": [
            {
              "id": "0",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "model.layers.1.self_attn.v_proj.weight"
                },
                {
                  "key": "tensor_shape",
                  "value": "FLOAT[2048,2048]"
                },
                {
                  "key": "param_name",
                  "value": "data"
                }
              ]
            }
          ],
          "style": null,
          "config": null
        },
        {
          "id": "node_MatMul_531",
          "label": "MatMul",
          "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.1: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.1.self_attn: transformers.models.llama.modeling_llama.LlamaAttention/model.layers.1.self_attn.v_proj: torch.nn.modules.linear.Linear/linear_9: aten.linear.default",
          "subgraphIds": [],
          "attrs": [
            {
              "key": "[metadata] namespace",
              "value": ": transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.1: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.1.self_attn: transformers.models.llama.modeling_llama.LlamaAttention/model.layers.1.self_attn.v_proj: torch.nn.modules.linear.Linear/linear_9: aten.linear.default"
            },
            {
              "key": "[metadata] pkg.torch.onnx.class_hierarchy",
              "value": "['transformers.models.llama.modeling_llama.LlamaForCausalLM', 'transformers.models.llama.modeling_llama.LlamaModel', 'transformers.models.llama.modeling_llama.LlamaDecoderLayer', 'transformers.models.llama.modeling_llama.LlamaAttention', 'torch.nn.modules.linear.Linear', 'aten.linear.default']"
            },
            {
              "key": "[metadata] pkg.torch.onnx.fx_node",
              "value": "%linear_9 : [num_users=1] = call_function[target=torch.ops.aten.linear.default](args = (%mul_388, %p_model_layers_1_self_attn_v_proj_weight), kwargs = {})"
            },
            {
              "key": "[metadata] pkg.torch.onnx.name_scopes",
              "value": "['', 'model', 'model.layers.1', 'model.layers.1.self_attn', 'model.layers.1.self_attn.v_proj', 'linear_9']"
            },
            {
              "key": "[metadata] pkg.torch.onnx.stack_trace",
              "value": "File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 688, in forward\n    outputs: BaseModelOutputWithPast = self.model(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 453, in forward\n    layer_outputs = decoder_layer(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 308, in forward\n    hidden_states, self_attn_weights = self.self_attn(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 244, in forward\n    value_states = self.v_proj(hidden_states).view(hidden_shape).transpose(1, 2)\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/torch/nn/modules/linear.py\", line 125, in forward\n    return F.linear(input, self.weight, self.bias)"
            }
          ],
          "incomingEdges": [
            {
              "sourceNodeId": "node_Mul_509",
              "sourceNodeOutputId": "0",
              "targetNodeInputId": "0"
            },
            {
              "sourceNodeId": "node_Transpose_530",
              "sourceNodeOutputId": "0",
              "targetNodeInputId": "1"
            }
          ],
          "outputsMetadata": [
            {
              "id": "0",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "linear_9"
                },
                {
                  "key": "tensor_shape",
                  "value": "FLOAT[s34,s16,2048]"
                },
                {
                  "key": "param_name",
                  "value": "Y"
                }
              ]
            }
          ],
          "inputsMetadata": [
            {
              "id": "0",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "mul_388"
                },
                {
                  "key": "tensor_shape",
                  "value": "FLOAT[s34,s16,2048]"
                },
                {
                  "key": "param_name",
                  "value": "A"
                }
              ]
            },
            {
              "id": "1",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "val_377"
                },
                {
                  "key": "tensor_shape",
                  "value": "FLOAT[2048,2048]"
                },
                {
                  "key": "param_name",
                  "value": "B"
                }
              ]
            }
          ],
          "style": null,
          "config": null
        },
        {
          "id": "node_Concat_536",
          "label": "Concat",
          "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.1: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.1.self_attn: transformers.models.llama.modeling_llama.LlamaAttention/view_7: aten.view.default",
          "subgraphIds": [],
          "attrs": [
            {
              "key": "axis",
              "value": "0"
            },
            {
              "key": "[metadata] namespace",
              "value": ": transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.1: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.1.self_attn: transformers.models.llama.modeling_llama.LlamaAttention/view_7: aten.view.default"
            },
            {
              "key": "[metadata] pkg.torch.onnx.class_hierarchy",
              "value": "['transformers.models.llama.modeling_llama.LlamaForCausalLM', 'transformers.models.llama.modeling_llama.LlamaModel', 'transformers.models.llama.modeling_llama.LlamaDecoderLayer', 'transformers.models.llama.modeling_llama.LlamaAttention', 'aten.view.default']"
            },
            {
              "key": "[metadata] pkg.torch.onnx.fx_node",
              "value": "%view_7 : [num_users=1] = call_function[target=torch.ops.aten.view.default](args = (%linear_9, [%sym_size_int_68, %sym_size_int_61, -1, 64]), kwargs = {})"
            },
            {
              "key": "[metadata] pkg.torch.onnx.name_scopes",
              "value": "['', 'model', 'model.layers.1', 'model.layers.1.self_attn', 'view_7']"
            },
            {
              "key": "[metadata] pkg.torch.onnx.stack_trace",
              "value": "File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 688, in forward\n    outputs: BaseModelOutputWithPast = self.model(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 453, in forward\n    layer_outputs = decoder_layer(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 308, in forward\n    hidden_states, self_attn_weights = self.self_attn(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 244, in forward\n    value_states = self.v_proj(hidden_states).view(hidden_shape).transpose(1, 2)"
            }
          ],
          "incomingEdges": [
            {
              "sourceNodeId": "node_Shape_12",
              "sourceNodeOutputId": "0",
              "targetNodeInputId": "0"
            },
            {
              "sourceNodeId": "node_Shape_2",
              "sourceNodeOutputId": "0",
              "targetNodeInputId": "1"
            },
            {
              "sourceNodeId": "node_Constant_90",
              "sourceNodeOutputId": "0",
              "targetNodeInputId": "2"
            },
            {
              "sourceNodeId": "node_Constant_345",
              "sourceNodeOutputId": "0",
              "targetNodeInputId": "3"
            }
          ],
          "outputsMetadata": [
            {
              "id": "0",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "val_382"
                },
                {
                  "key": "tensor_shape",
                  "value": "INT64[4]"
                },
                {
                  "key": "param_name",
                  "value": "concat_result"
                }
              ]
            }
          ],
          "inputsMetadata": [
            {
              "id": "0",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "val_6"
                },
                {
                  "key": "tensor_shape",
                  "value": "INT64[1]"
                },
                {
                  "key": "param_name",
                  "value": "inputs"
                }
              ]
            },
            {
              "id": "1",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "val_1"
                },
                {
                  "key": "tensor_shape",
                  "value": "INT64[1]"
                },
                {
                  "key": "param_name",
                  "value": "inputs"
                }
              ]
            },
            {
              "id": "2",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "val_53"
                },
                {
                  "key": "tensor_shape",
                  "value": "INT64[1]"
                },
                {
                  "key": "param_name",
                  "value": "inputs"
                }
              ]
            },
            {
              "id": "3",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "val_258"
                },
                {
                  "key": "tensor_shape",
                  "value": "INT64[1]"
                },
                {
                  "key": "param_name",
                  "value": "inputs"
                }
              ]
            }
          ],
          "style": null,
          "config": null
        },
        {
          "id": "node_Reshape_538",
          "label": "Reshape",
          "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.1: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.1.self_attn: transformers.models.llama.modeling_llama.LlamaAttention/view_7: aten.view.default",
          "subgraphIds": [],
          "attrs": [
            {
              "key": "allowzero",
              "value": "1"
            },
            {
              "key": "[metadata] namespace",
              "value": ": transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.1: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.1.self_attn: transformers.models.llama.modeling_llama.LlamaAttention/view_7: aten.view.default"
            },
            {
              "key": "[metadata] pkg.torch.onnx.class_hierarchy",
              "value": "['transformers.models.llama.modeling_llama.LlamaForCausalLM', 'transformers.models.llama.modeling_llama.LlamaModel', 'transformers.models.llama.modeling_llama.LlamaDecoderLayer', 'transformers.models.llama.modeling_llama.LlamaAttention', 'aten.view.default']"
            },
            {
              "key": "[metadata] pkg.torch.onnx.fx_node",
              "value": "%view_7 : [num_users=1] = call_function[target=torch.ops.aten.view.default](args = (%linear_9, [%sym_size_int_68, %sym_size_int_61, -1, 64]), kwargs = {})"
            },
            {
              "key": "[metadata] pkg.torch.onnx.name_scopes",
              "value": "['', 'model', 'model.layers.1', 'model.layers.1.self_attn', 'view_7']"
            },
            {
              "key": "[metadata] pkg.torch.onnx.stack_trace",
              "value": "File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 688, in forward\n    outputs: BaseModelOutputWithPast = self.model(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 453, in forward\n    layer_outputs = decoder_layer(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 308, in forward\n    hidden_states, self_attn_weights = self.self_attn(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 244, in forward\n    value_states = self.v_proj(hidden_states).view(hidden_shape).transpose(1, 2)"
            }
          ],
          "incomingEdges": [
            {
              "sourceNodeId": "node_MatMul_531",
              "sourceNodeOutputId": "0",
              "targetNodeInputId": "0"
            },
            {
              "sourceNodeId": "node_Concat_536",
              "sourceNodeOutputId": "0",
              "targetNodeInputId": "1"
            }
          ],
          "outputsMetadata": [
            {
              "id": "0",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "view_7"
                },
                {
                  "key": "tensor_shape",
                  "value": "FLOAT[s34,s16,32,64]"
                },
                {
                  "key": "param_name",
                  "value": "reshaped"
                }
              ]
            }
          ],
          "inputsMetadata": [
            {
              "id": "0",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "linear_9"
                },
                {
                  "key": "tensor_shape",
                  "value": "FLOAT[s34,s16,2048]"
                },
                {
                  "key": "param_name",
                  "value": "data"
                }
              ]
            },
            {
              "id": "1",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "val_382"
                },
                {
                  "key": "tensor_shape",
                  "value": "INT64[4]"
                },
                {
                  "key": "param_name",
                  "value": "shape"
                }
              ]
            }
          ],
          "style": null,
          "config": null
        },
        {
          "id": "node_Transpose_539",
          "label": "Transpose",
          "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.1: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.1.self_attn: transformers.models.llama.modeling_llama.LlamaAttention/transpose_8: aten.transpose.int",
          "subgraphIds": [],
          "attrs": [
            {
              "key": "perm",
              "value": "[0, 2, 1, 3]"
            },
            {
              "key": "[metadata] namespace",
              "value": ": transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.1: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.1.self_attn: transformers.models.llama.modeling_llama.LlamaAttention/transpose_8: aten.transpose.int"
            },
            {
              "key": "[metadata] pkg.torch.onnx.class_hierarchy",
              "value": "['transformers.models.llama.modeling_llama.LlamaForCausalLM', 'transformers.models.llama.modeling_llama.LlamaModel', 'transformers.models.llama.modeling_llama.LlamaDecoderLayer', 'transformers.models.llama.modeling_llama.LlamaAttention', 'aten.transpose.int']"
            },
            {
              "key": "[metadata] pkg.torch.onnx.fx_node",
              "value": "%transpose_8 : [num_users=1] = call_function[target=torch.ops.aten.transpose.int](args = (%view_7, 1, 2), kwargs = {})"
            },
            {
              "key": "[metadata] pkg.torch.onnx.name_scopes",
              "value": "['', 'model', 'model.layers.1', 'model.layers.1.self_attn', 'transpose_8']"
            },
            {
              "key": "[metadata] pkg.torch.onnx.stack_trace",
              "value": "File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 688, in forward\n    outputs: BaseModelOutputWithPast = self.model(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 453, in forward\n    layer_outputs = decoder_layer(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 308, in forward\n    hidden_states, self_attn_weights = self.self_attn(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 244, in forward\n    value_states = self.v_proj(hidden_states).view(hidden_shape).transpose(1, 2)"
            }
          ],
          "incomingEdges": [
            {
              "sourceNodeId": "node_Reshape_538",
              "sourceNodeOutputId": "0",
              "targetNodeInputId": "0"
            }
          ],
          "outputsMetadata": [
            {
              "id": "0",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "transpose_8"
                },
                {
                  "key": "tensor_shape",
                  "value": "FLOAT[s34,32,s16,64]"
                },
                {
                  "key": "param_name",
                  "value": "transposed"
                }
              ]
            }
          ],
          "inputsMetadata": [
            {
              "id": "0",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "view_7"
                },
                {
                  "key": "tensor_shape",
                  "value": "FLOAT[s34,s16,32,64]"
                },
                {
                  "key": "param_name",
                  "value": "data"
                }
              ]
            }
          ],
          "style": null,
          "config": null
        },
        {
          "id": "node_Unsqueeze_540",
          "label": "Unsqueeze",
          "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.1: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.1.self_attn: transformers.models.llama.modeling_llama.LlamaAttention/unsqueeze_12: aten.unsqueeze.default",
          "subgraphIds": [],
          "attrs": [
            {
              "key": "[metadata] namespace",
              "value": ": transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.1: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.1.self_attn: transformers.models.llama.modeling_llama.LlamaAttention/unsqueeze_12: aten.unsqueeze.default"
            },
            {
              "key": "[metadata] pkg.torch.onnx.class_hierarchy",
              "value": "['transformers.models.llama.modeling_llama.LlamaForCausalLM', 'transformers.models.llama.modeling_llama.LlamaModel', 'transformers.models.llama.modeling_llama.LlamaDecoderLayer', 'transformers.models.llama.modeling_llama.LlamaAttention', 'aten.unsqueeze.default']"
            },
            {
              "key": "[metadata] pkg.torch.onnx.fx_node",
              "value": "%unsqueeze_12 : [num_users=2] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%mul_149, 1), kwargs = {})"
            },
            {
              "key": "[metadata] pkg.torch.onnx.name_scopes",
              "value": "['', 'model', 'model.layers.1', 'model.layers.1.self_attn', 'unsqueeze_12']"
            },
            {
              "key": "[metadata] pkg.torch.onnx.stack_trace",
              "value": "File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 688, in forward\n    outputs: BaseModelOutputWithPast = self.model(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 453, in forward\n    layer_outputs = decoder_layer(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 308, in forward\n    hidden_states, self_attn_weights = self.self_attn(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 247, in forward\n    query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)"
            }
          ],
          "incomingEdges": [
            {
              "sourceNodeId": "node_Cos_324",
              "sourceNodeOutputId": "0",
              "targetNodeInputId": "0"
            },
            {
              "sourceNodeId": "node_Constant_60",
              "sourceNodeOutputId": "0",
              "targetNodeInputId": "1"
            }
          ],
          "outputsMetadata": [
            {
              "id": "0",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "unsqueeze_12"
                },
                {
                  "key": "tensor_shape",
                  "value": "FLOAT[1,1,s16,64]"
                },
                {
                  "key": "param_name",
                  "value": "expanded"
                }
              ]
            }
          ],
          "inputsMetadata": [
            {
              "id": "0",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "cos"
                },
                {
                  "key": "tensor_shape",
                  "value": "FLOAT[1,s16,64]"
                },
                {
                  "key": "param_name",
                  "value": "data"
                }
              ]
            },
            {
              "id": "1",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "val_26"
                },
                {
                  "key": "tensor_shape",
                  "value": "INT64[1]"
                },
                {
                  "key": "param_name",
                  "value": "axes"
                }
              ]
            }
          ],
          "style": null,
          "config": null
        },
        {
          "id": "node_Unsqueeze_541",
          "label": "Unsqueeze",
          "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.1: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.1.self_attn: transformers.models.llama.modeling_llama.LlamaAttention/unsqueeze_13: aten.unsqueeze.default",
          "subgraphIds": [],
          "attrs": [
            {
              "key": "[metadata] namespace",
              "value": ": transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.1: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.1.self_attn: transformers.models.llama.modeling_llama.LlamaAttention/unsqueeze_13: aten.unsqueeze.default"
            },
            {
              "key": "[metadata] pkg.torch.onnx.class_hierarchy",
              "value": "['transformers.models.llama.modeling_llama.LlamaForCausalLM', 'transformers.models.llama.modeling_llama.LlamaModel', 'transformers.models.llama.modeling_llama.LlamaDecoderLayer', 'transformers.models.llama.modeling_llama.LlamaAttention', 'aten.unsqueeze.default']"
            },
            {
              "key": "[metadata] pkg.torch.onnx.fx_node",
              "value": "%unsqueeze_13 : [num_users=2] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%mul_156, 1), kwargs = {})"
            },
            {
              "key": "[metadata] pkg.torch.onnx.name_scopes",
              "value": "['', 'model', 'model.layers.1', 'model.layers.1.self_attn', 'unsqueeze_13']"
            },
            {
              "key": "[metadata] pkg.torch.onnx.stack_trace",
              "value": "File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 688, in forward\n    outputs: BaseModelOutputWithPast = self.model(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 453, in forward\n    layer_outputs = decoder_layer(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 308, in forward\n    hidden_states, self_attn_weights = self.self_attn(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 247, in forward\n    query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)"
            }
          ],
          "incomingEdges": [
            {
              "sourceNodeId": "node_Sin_326",
              "sourceNodeOutputId": "0",
              "targetNodeInputId": "0"
            },
            {
              "sourceNodeId": "node_Constant_60",
              "sourceNodeOutputId": "0",
              "targetNodeInputId": "1"
            }
          ],
          "outputsMetadata": [
            {
              "id": "0",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "unsqueeze_13"
                },
                {
                  "key": "tensor_shape",
                  "value": "FLOAT[1,1,s16,64]"
                },
                {
                  "key": "param_name",
                  "value": "expanded"
                }
              ]
            }
          ],
          "inputsMetadata": [
            {
              "id": "0",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "sin"
                },
                {
                  "key": "tensor_shape",
                  "value": "FLOAT[1,s16,64]"
                },
                {
                  "key": "param_name",
                  "value": "data"
                }
              ]
            },
            {
              "id": "1",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "val_26"
                },
                {
                  "key": "tensor_shape",
                  "value": "INT64[1]"
                },
                {
                  "key": "param_name",
                  "value": "axes"
                }
              ]
            }
          ],
          "style": null,
          "config": null
        },
        {
          "id": "node_Mul_542",
          "label": "Mul",
          "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.1: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.1.self_attn: transformers.models.llama.modeling_llama.LlamaAttention/mul_427: aten.mul.Tensor",
          "subgraphIds": [],
          "attrs": [
            {
              "key": "[metadata] namespace",
              "value": ": transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.1: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.1.self_attn: transformers.models.llama.modeling_llama.LlamaAttention/mul_427: aten.mul.Tensor"
            },
            {
              "key": "[metadata] pkg.torch.onnx.class_hierarchy",
              "value": "['transformers.models.llama.modeling_llama.LlamaForCausalLM', 'transformers.models.llama.modeling_llama.LlamaModel', 'transformers.models.llama.modeling_llama.LlamaDecoderLayer', 'transformers.models.llama.modeling_llama.LlamaAttention', 'aten.mul.Tensor']"
            },
            {
              "key": "[metadata] pkg.torch.onnx.fx_node",
              "value": "%mul_427 : [num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%transpose_6, %unsqueeze_12), kwargs = {})"
            },
            {
              "key": "[metadata] pkg.torch.onnx.name_scopes",
              "value": "['', 'model', 'model.layers.1', 'model.layers.1.self_attn', 'mul_427']"
            },
            {
              "key": "[metadata] pkg.torch.onnx.stack_trace",
              "value": "File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 688, in forward\n    outputs: BaseModelOutputWithPast = self.model(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 453, in forward\n    layer_outputs = decoder_layer(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 308, in forward\n    hidden_states, self_attn_weights = self.self_attn(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 247, in forward\n    query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)"
            }
          ],
          "incomingEdges": [
            {
              "sourceNodeId": "node_Transpose_519",
              "sourceNodeOutputId": "0",
              "targetNodeInputId": "0"
            },
            {
              "sourceNodeId": "node_Unsqueeze_540",
              "sourceNodeOutputId": "0",
              "targetNodeInputId": "1"
            }
          ],
          "outputsMetadata": [
            {
              "id": "0",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "mul_427"
                },
                {
                  "key": "tensor_shape",
                  "value": "FLOAT[s34,32,s16,64]"
                },
                {
                  "key": "param_name",
                  "value": "C"
                }
              ]
            }
          ],
          "inputsMetadata": [
            {
              "id": "0",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "transpose_6"
                },
                {
                  "key": "tensor_shape",
                  "value": "FLOAT[s34,32,s16,64]"
                },
                {
                  "key": "param_name",
                  "value": "A"
                }
              ]
            },
            {
              "id": "1",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "unsqueeze_12"
                },
                {
                  "key": "tensor_shape",
                  "value": "FLOAT[1,1,s16,64]"
                },
                {
                  "key": "param_name",
                  "value": "B"
                }
              ]
            }
          ],
          "style": null,
          "config": null
        },
        {
          "id": "node_Constant_931",
          "label": "Constant",
          "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.1: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.1.self_attn: transformers.models.llama.modeling_llama.LlamaAttention/slice_38: aten.slice.Tensor",
          "subgraphIds": [],
          "attrs": [
            {
              "key": "__value",
              "value": "[0]"
            },
            {
              "key": "value",
              "value": "TensorProtoTensor<INT64,[1]>(array([0]), name='val_386')"
            }
          ],
          "incomingEdges": [],
          "outputsMetadata": [
            {
              "id": "0",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "val_386"
                },
                {
                  "key": "tensor_shape",
                  "value": "INT64[1]"
                },
                {
                  "key": "param_name",
                  "value": "output"
                }
              ]
            }
          ],
          "inputsMetadata": [],
          "style": null,
          "config": null
        },
        {
          "id": "node_Constant_934",
          "label": "Constant",
          "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.1: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.1.self_attn: transformers.models.llama.modeling_llama.LlamaAttention/slice_38: aten.slice.Tensor",
          "subgraphIds": [],
          "attrs": [
            {
              "key": "__value",
              "value": "[32]"
            },
            {
              "key": "value",
              "value": "TensorProtoTensor<INT64,[1]>(array([32]), name='val_389')"
            }
          ],
          "incomingEdges": [],
          "outputsMetadata": [
            {
              "id": "0",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "val_389"
                },
                {
                  "key": "tensor_shape",
                  "value": "INT64[1]"
                },
                {
                  "key": "param_name",
                  "value": "output"
                }
              ]
            }
          ],
          "inputsMetadata": [],
          "style": null,
          "config": null
        },
        {
          "id": "node_Constant_937",
          "label": "Constant",
          "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.1: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.1.self_attn: transformers.models.llama.modeling_llama.LlamaAttention/slice_38: aten.slice.Tensor",
          "subgraphIds": [],
          "attrs": [
            {
              "key": "__value",
              "value": "[3]"
            },
            {
              "key": "value",
              "value": "TensorProtoTensor<INT64,[1]>(array([3]), name='val_392')"
            }
          ],
          "incomingEdges": [],
          "outputsMetadata": [
            {
              "id": "0",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "val_392"
                },
                {
                  "key": "tensor_shape",
                  "value": "INT64[1]"
                },
                {
                  "key": "param_name",
                  "value": "output"
                }
              ]
            }
          ],
          "inputsMetadata": [],
          "style": null,
          "config": null
        },
        {
          "id": "node_Constant_552",
          "label": "Constant",
          "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.1: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.1.self_attn: transformers.models.llama.modeling_llama.LlamaAttention/slice_38: aten.slice.Tensor",
          "subgraphIds": [],
          "attrs": [
            {
              "key": "__value",
              "value": "[1]"
            },
            {
              "key": "value_ints",
              "value": "[1]"
            },
            {
              "key": "[metadata] namespace",
              "value": ": transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.1: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.1.self_attn: transformers.models.llama.modeling_llama.LlamaAttention/slice_38: aten.slice.Tensor"
            },
            {
              "key": "[metadata] pkg.torch.onnx.class_hierarchy",
              "value": "['transformers.models.llama.modeling_llama.LlamaForCausalLM', 'transformers.models.llama.modeling_llama.LlamaModel', 'transformers.models.llama.modeling_llama.LlamaDecoderLayer', 'transformers.models.llama.modeling_llama.LlamaAttention', 'aten.slice.Tensor']"
            },
            {
              "key": "[metadata] pkg.torch.onnx.fx_node",
              "value": "%slice_38 : [num_users=1] = call_function[target=torch.ops.aten.slice.Tensor](args = (%transpose_6, 3, 0, 32), kwargs = {})"
            },
            {
              "key": "[metadata] pkg.torch.onnx.name_scopes",
              "value": "['', 'model', 'model.layers.1', 'model.layers.1.self_attn', 'slice_38']"
            },
            {
              "key": "[metadata] pkg.torch.onnx.stack_trace",
              "value": "File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 688, in forward\n    outputs: BaseModelOutputWithPast = self.model(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 453, in forward\n    layer_outputs = decoder_layer(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 308, in forward\n    hidden_states, self_attn_weights = self.self_attn(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 247, in forward\n    query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)"
            }
          ],
          "incomingEdges": [],
          "outputsMetadata": [
            {
              "id": "0",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "val_393"
                },
                {
                  "key": "tensor_shape",
                  "value": "INT64[1]"
                },
                {
                  "key": "param_name",
                  "value": "output"
                }
              ]
            }
          ],
          "inputsMetadata": [],
          "style": null,
          "config": null
        },
        {
          "id": "node_Slice_553",
          "label": "Slice",
          "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.1: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.1.self_attn: transformers.models.llama.modeling_llama.LlamaAttention/slice_38: aten.slice.Tensor",
          "subgraphIds": [],
          "attrs": [
            {
              "key": "[metadata] namespace",
              "value": ": transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.1: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.1.self_attn: transformers.models.llama.modeling_llama.LlamaAttention/slice_38: aten.slice.Tensor"
            },
            {
              "key": "[metadata] pkg.torch.onnx.class_hierarchy",
              "value": "['transformers.models.llama.modeling_llama.LlamaForCausalLM', 'transformers.models.llama.modeling_llama.LlamaModel', 'transformers.models.llama.modeling_llama.LlamaDecoderLayer', 'transformers.models.llama.modeling_llama.LlamaAttention', 'aten.slice.Tensor']"
            },
            {
              "key": "[metadata] pkg.torch.onnx.fx_node",
              "value": "%slice_38 : [num_users=1] = call_function[target=torch.ops.aten.slice.Tensor](args = (%transpose_6, 3, 0, 32), kwargs = {})"
            },
            {
              "key": "[metadata] pkg.torch.onnx.name_scopes",
              "value": "['', 'model', 'model.layers.1', 'model.layers.1.self_attn', 'slice_38']"
            },
            {
              "key": "[metadata] pkg.torch.onnx.stack_trace",
              "value": "File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 688, in forward\n    outputs: BaseModelOutputWithPast = self.model(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 453, in forward\n    layer_outputs = decoder_layer(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 308, in forward\n    hidden_states, self_attn_weights = self.self_attn(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 247, in forward\n    query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)"
            }
          ],
          "incomingEdges": [
            {
              "sourceNodeId": "node_Transpose_519",
              "sourceNodeOutputId": "0",
              "targetNodeInputId": "0"
            },
            {
              "sourceNodeId": "node_Constant_931",
              "sourceNodeOutputId": "0",
              "targetNodeInputId": "1"
            },
            {
              "sourceNodeId": "node_Constant_934",
              "sourceNodeOutputId": "0",
              "targetNodeInputId": "2"
            },
            {
              "sourceNodeId": "node_Constant_937",
              "sourceNodeOutputId": "0",
              "targetNodeInputId": "3"
            },
            {
              "sourceNodeId": "node_Constant_552",
              "sourceNodeOutputId": "0",
              "targetNodeInputId": "4"
            }
          ],
          "outputsMetadata": [
            {
              "id": "0",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "slice_38"
                },
                {
                  "key": "tensor_shape",
                  "value": "FLOAT[s34,32,s16,32]"
                },
                {
                  "key": "param_name",
                  "value": "output"
                }
              ]
            }
          ],
          "inputsMetadata": [
            {
              "id": "0",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "transpose_6"
                },
                {
                  "key": "tensor_shape",
                  "value": "FLOAT[s34,32,s16,64]"
                },
                {
                  "key": "param_name",
                  "value": "data"
                }
              ]
            },
            {
              "id": "1",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "val_386"
                },
                {
                  "key": "tensor_shape",
                  "value": "INT64[1]"
                },
                {
                  "key": "param_name",
                  "value": "starts"
                }
              ]
            },
            {
              "id": "2",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "val_389"
                },
                {
                  "key": "tensor_shape",
                  "value": "INT64[1]"
                },
                {
                  "key": "param_name",
                  "value": "ends"
                }
              ]
            },
            {
              "id": "3",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "val_392"
                },
                {
                  "key": "tensor_shape",
                  "value": "INT64[1]"
                },
                {
                  "key": "param_name",
                  "value": "axes"
                }
              ]
            },
            {
              "id": "4",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "val_393"
                },
                {
                  "key": "tensor_shape",
                  "value": "INT64[1]"
                },
                {
                  "key": "param_name",
                  "value": "steps"
                }
              ]
            }
          ],
          "style": null,
          "config": null
        },
        {
          "id": "node_Constant_940",
          "label": "Constant",
          "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.1: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.1.self_attn: transformers.models.llama.modeling_llama.LlamaAttention/slice_39: aten.slice.Tensor",
          "subgraphIds": [],
          "attrs": [
            {
              "key": "__value",
              "value": "[32]"
            },
            {
              "key": "value",
              "value": "TensorProtoTensor<INT64,[1]>(array([32]), name='val_396')"
            }
          ],
          "incomingEdges": [],
          "outputsMetadata": [
            {
              "id": "0",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "val_396"
                },
                {
                  "key": "tensor_shape",
                  "value": "INT64[1]"
                },
                {
                  "key": "param_name",
                  "value": "output"
                }
              ]
            }
          ],
          "inputsMetadata": [],
          "style": null,
          "config": null
        },
        {
          "id": "node_Constant_943",
          "label": "Constant",
          "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.1: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.1.self_attn: transformers.models.llama.modeling_llama.LlamaAttention/slice_39: aten.slice.Tensor",
          "subgraphIds": [],
          "attrs": [
            {
              "key": "__value",
              "value": "[9223372036854775807]"
            },
            {
              "key": "value",
              "value": "TensorProtoTensor<INT64,[1]>(array([9223372036854775807]), name='val_399')"
            }
          ],
          "incomingEdges": [],
          "outputsMetadata": [
            {
              "id": "0",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "val_399"
                },
                {
                  "key": "tensor_shape",
                  "value": "INT64[1]"
                },
                {
                  "key": "param_name",
                  "value": "output"
                }
              ]
            }
          ],
          "inputsMetadata": [],
          "style": null,
          "config": null
        },
        {
          "id": "node_Constant_946",
          "label": "Constant",
          "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.1: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.1.self_attn: transformers.models.llama.modeling_llama.LlamaAttention/slice_39: aten.slice.Tensor",
          "subgraphIds": [],
          "attrs": [
            {
              "key": "__value",
              "value": "[3]"
            },
            {
              "key": "value",
              "value": "TensorProtoTensor<INT64,[1]>(array([3]), name='val_402')"
            }
          ],
          "incomingEdges": [],
          "outputsMetadata": [
            {
              "id": "0",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "val_402"
                },
                {
                  "key": "tensor_shape",
                  "value": "INT64[1]"
                },
                {
                  "key": "param_name",
                  "value": "output"
                }
              ]
            }
          ],
          "inputsMetadata": [],
          "style": null,
          "config": null
        },
        {
          "id": "node_Constant_563",
          "label": "Constant",
          "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.1: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.1.self_attn: transformers.models.llama.modeling_llama.LlamaAttention/slice_39: aten.slice.Tensor",
          "subgraphIds": [],
          "attrs": [
            {
              "key": "__value",
              "value": "[1]"
            },
            {
              "key": "value_ints",
              "value": "[1]"
            },
            {
              "key": "[metadata] namespace",
              "value": ": transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.1: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.1.self_attn: transformers.models.llama.modeling_llama.LlamaAttention/slice_39: aten.slice.Tensor"
            },
            {
              "key": "[metadata] pkg.torch.onnx.class_hierarchy",
              "value": "['transformers.models.llama.modeling_llama.LlamaForCausalLM', 'transformers.models.llama.modeling_llama.LlamaModel', 'transformers.models.llama.modeling_llama.LlamaDecoderLayer', 'transformers.models.llama.modeling_llama.LlamaAttention', 'aten.slice.Tensor']"
            },
            {
              "key": "[metadata] pkg.torch.onnx.fx_node",
              "value": "%slice_39 : [num_users=1] = call_function[target=torch.ops.aten.slice.Tensor](args = (%transpose_6, 3, 32, 9223372036854775807), kwargs = {})"
            },
            {
              "key": "[metadata] pkg.torch.onnx.name_scopes",
              "value": "['', 'model', 'model.layers.1', 'model.layers.1.self_attn', 'slice_39']"
            },
            {
              "key": "[metadata] pkg.torch.onnx.stack_trace",
              "value": "File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 688, in forward\n    outputs: BaseModelOutputWithPast = self.model(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 453, in forward\n    layer_outputs = decoder_layer(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 308, in forward\n    hidden_states, self_attn_weights = self.self_attn(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 247, in forward\n    query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)"
            }
          ],
          "incomingEdges": [],
          "outputsMetadata": [
            {
              "id": "0",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "val_403"
                },
                {
                  "key": "tensor_shape",
                  "value": "INT64[1]"
                },
                {
                  "key": "param_name",
                  "value": "output"
                }
              ]
            }
          ],
          "inputsMetadata": [],
          "style": null,
          "config": null
        },
        {
          "id": "node_Slice_564",
          "label": "Slice",
          "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.1: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.1.self_attn: transformers.models.llama.modeling_llama.LlamaAttention/slice_39: aten.slice.Tensor",
          "subgraphIds": [],
          "attrs": [
            {
              "key": "[metadata] namespace",
              "value": ": transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.1: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.1.self_attn: transformers.models.llama.modeling_llama.LlamaAttention/slice_39: aten.slice.Tensor"
            },
            {
              "key": "[metadata] pkg.torch.onnx.class_hierarchy",
              "value": "['transformers.models.llama.modeling_llama.LlamaForCausalLM', 'transformers.models.llama.modeling_llama.LlamaModel', 'transformers.models.llama.modeling_llama.LlamaDecoderLayer', 'transformers.models.llama.modeling_llama.LlamaAttention', 'aten.slice.Tensor']"
            },
            {
              "key": "[metadata] pkg.torch.onnx.fx_node",
              "value": "%slice_39 : [num_users=1] = call_function[target=torch.ops.aten.slice.Tensor](args = (%transpose_6, 3, 32, 9223372036854775807), kwargs = {})"
            },
            {
              "key": "[metadata] pkg.torch.onnx.name_scopes",
              "value": "['', 'model', 'model.layers.1', 'model.layers.1.self_attn', 'slice_39']"
            },
            {
              "key": "[metadata] pkg.torch.onnx.stack_trace",
              "value": "File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 688, in forward\n    outputs: BaseModelOutputWithPast = self.model(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 453, in forward\n    layer_outputs = decoder_layer(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 308, in forward\n    hidden_states, self_attn_weights = self.self_attn(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 247, in forward\n    query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)"
            }
          ],
          "incomingEdges": [
            {
              "sourceNodeId": "node_Transpose_519",
              "sourceNodeOutputId": "0",
              "targetNodeInputId": "0"
            },
            {
              "sourceNodeId": "node_Constant_940",
              "sourceNodeOutputId": "0",
              "targetNodeInputId": "1"
            },
            {
              "sourceNodeId": "node_Constant_943",
              "sourceNodeOutputId": "0",
              "targetNodeInputId": "2"
            },
            {
              "sourceNodeId": "node_Constant_946",
              "sourceNodeOutputId": "0",
              "targetNodeInputId": "3"
            },
            {
              "sourceNodeId": "node_Constant_563",
              "sourceNodeOutputId": "0",
              "targetNodeInputId": "4"
            }
          ],
          "outputsMetadata": [
            {
              "id": "0",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "slice_39"
                },
                {
                  "key": "tensor_shape",
                  "value": "FLOAT[s34,32,s16,32]"
                },
                {
                  "key": "param_name",
                  "value": "output"
                }
              ]
            }
          ],
          "inputsMetadata": [
            {
              "id": "0",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "transpose_6"
                },
                {
                  "key": "tensor_shape",
                  "value": "FLOAT[s34,32,s16,64]"
                },
                {
                  "key": "param_name",
                  "value": "data"
                }
              ]
            },
            {
              "id": "1",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "val_396"
                },
                {
                  "key": "tensor_shape",
                  "value": "INT64[1]"
                },
                {
                  "key": "param_name",
                  "value": "starts"
                }
              ]
            },
            {
              "id": "2",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "val_399"
                },
                {
                  "key": "tensor_shape",
                  "value": "INT64[1]"
                },
                {
                  "key": "param_name",
                  "value": "ends"
                }
              ]
            },
            {
              "id": "3",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "val_402"
                },
                {
                  "key": "tensor_shape",
                  "value": "INT64[1]"
                },
                {
                  "key": "param_name",
                  "value": "axes"
                }
              ]
            },
            {
              "id": "4",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "val_403"
                },
                {
                  "key": "tensor_shape",
                  "value": "INT64[1]"
                },
                {
                  "key": "param_name",
                  "value": "steps"
                }
              ]
            }
          ],
          "style": null,
          "config": null
        },
        {
          "id": "node_Neg_565",
          "label": "Neg",
          "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.1: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.1.self_attn: transformers.models.llama.modeling_llama.LlamaAttention/neg_2: aten.neg.default",
          "subgraphIds": [],
          "attrs": [
            {
              "key": "[metadata] namespace",
              "value": ": transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.1: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.1.self_attn: transformers.models.llama.modeling_llama.LlamaAttention/neg_2: aten.neg.default"
            },
            {
              "key": "[metadata] pkg.torch.onnx.class_hierarchy",
              "value": "['transformers.models.llama.modeling_llama.LlamaForCausalLM', 'transformers.models.llama.modeling_llama.LlamaModel', 'transformers.models.llama.modeling_llama.LlamaDecoderLayer', 'transformers.models.llama.modeling_llama.LlamaAttention', 'aten.neg.default']"
            },
            {
              "key": "[metadata] pkg.torch.onnx.fx_node",
              "value": "%neg_2 : [num_users=1] = call_function[target=torch.ops.aten.neg.default](args = (%slice_39,), kwargs = {})"
            },
            {
              "key": "[metadata] pkg.torch.onnx.name_scopes",
              "value": "['', 'model', 'model.layers.1', 'model.layers.1.self_attn', 'neg_2']"
            },
            {
              "key": "[metadata] pkg.torch.onnx.stack_trace",
              "value": "File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 688, in forward\n    outputs: BaseModelOutputWithPast = self.model(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 453, in forward\n    layer_outputs = decoder_layer(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 308, in forward\n    hidden_states, self_attn_weights = self.self_attn(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 247, in forward\n    query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)"
            }
          ],
          "incomingEdges": [
            {
              "sourceNodeId": "node_Slice_564",
              "sourceNodeOutputId": "0",
              "targetNodeInputId": "0"
            }
          ],
          "outputsMetadata": [
            {
              "id": "0",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "neg_2"
                },
                {
                  "key": "tensor_shape",
                  "value": "FLOAT[s34,32,s16,32]"
                },
                {
                  "key": "param_name",
                  "value": "Y"
                }
              ]
            }
          ],
          "inputsMetadata": [
            {
              "id": "0",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "slice_39"
                },
                {
                  "key": "tensor_shape",
                  "value": "FLOAT[s34,32,s16,32]"
                },
                {
                  "key": "param_name",
                  "value": "X"
                }
              ]
            }
          ],
          "style": null,
          "config": null
        },
        {
          "id": "node_Concat_566",
          "label": "Concat",
          "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.1: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.1.self_attn: transformers.models.llama.modeling_llama.LlamaAttention/cat_5: aten.cat.default",
          "subgraphIds": [],
          "attrs": [
            {
              "key": "axis",
              "value": "-1"
            },
            {
              "key": "[metadata] namespace",
              "value": ": transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.1: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.1.self_attn: transformers.models.llama.modeling_llama.LlamaAttention/cat_5: aten.cat.default"
            },
            {
              "key": "[metadata] pkg.torch.onnx.class_hierarchy",
              "value": "['transformers.models.llama.modeling_llama.LlamaForCausalLM', 'transformers.models.llama.modeling_llama.LlamaModel', 'transformers.models.llama.modeling_llama.LlamaDecoderLayer', 'transformers.models.llama.modeling_llama.LlamaAttention', 'aten.cat.default']"
            },
            {
              "key": "[metadata] pkg.torch.onnx.fx_node",
              "value": "%cat_5 : [num_users=1] = call_function[target=torch.ops.aten.cat.default](args = ([%neg_2, %slice_38], -1), kwargs = {})"
            },
            {
              "key": "[metadata] pkg.torch.onnx.name_scopes",
              "value": "['', 'model', 'model.layers.1', 'model.layers.1.self_attn', 'cat_5']"
            },
            {
              "key": "[metadata] pkg.torch.onnx.stack_trace",
              "value": "File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 688, in forward\n    outputs: BaseModelOutputWithPast = self.model(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 453, in forward\n    layer_outputs = decoder_layer(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 308, in forward\n    hidden_states, self_attn_weights = self.self_attn(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 247, in forward\n    query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)"
            }
          ],
          "incomingEdges": [
            {
              "sourceNodeId": "node_Neg_565",
              "sourceNodeOutputId": "0",
              "targetNodeInputId": "0"
            },
            {
              "sourceNodeId": "node_Slice_553",
              "sourceNodeOutputId": "0",
              "targetNodeInputId": "1"
            }
          ],
          "outputsMetadata": [
            {
              "id": "0",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "cat_5"
                },
                {
                  "key": "tensor_shape",
                  "value": "FLOAT[s34,32,s16,64]"
                },
                {
                  "key": "param_name",
                  "value": "concat_result"
                }
              ]
            }
          ],
          "inputsMetadata": [
            {
              "id": "0",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "neg_2"
                },
                {
                  "key": "tensor_shape",
                  "value": "FLOAT[s34,32,s16,32]"
                },
                {
                  "key": "param_name",
                  "value": "inputs"
                }
              ]
            },
            {
              "id": "1",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "slice_38"
                },
                {
                  "key": "tensor_shape",
                  "value": "FLOAT[s34,32,s16,32]"
                },
                {
                  "key": "param_name",
                  "value": "inputs"
                }
              ]
            }
          ],
          "style": null,
          "config": null
        },
        {
          "id": "node_Mul_567",
          "label": "Mul",
          "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.1: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.1.self_attn: transformers.models.llama.modeling_llama.LlamaAttention/mul_444: aten.mul.Tensor",
          "subgraphIds": [],
          "attrs": [
            {
              "key": "[metadata] namespace",
              "value": ": transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.1: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.1.self_attn: transformers.models.llama.modeling_llama.LlamaAttention/mul_444: aten.mul.Tensor"
            },
            {
              "key": "[metadata] pkg.torch.onnx.class_hierarchy",
              "value": "['transformers.models.llama.modeling_llama.LlamaForCausalLM', 'transformers.models.llama.modeling_llama.LlamaModel', 'transformers.models.llama.modeling_llama.LlamaDecoderLayer', 'transformers.models.llama.modeling_llama.LlamaAttention', 'aten.mul.Tensor']"
            },
            {
              "key": "[metadata] pkg.torch.onnx.fx_node",
              "value": "%mul_444 : [num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%cat_5, %unsqueeze_13), kwargs = {})"
            },
            {
              "key": "[metadata] pkg.torch.onnx.name_scopes",
              "value": "['', 'model', 'model.layers.1', 'model.layers.1.self_attn', 'mul_444']"
            },
            {
              "key": "[metadata] pkg.torch.onnx.stack_trace",
              "value": "File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 688, in forward\n    outputs: BaseModelOutputWithPast = self.model(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 453, in forward\n    layer_outputs = decoder_layer(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 308, in forward\n    hidden_states, self_attn_weights = self.self_attn(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 247, in forward\n    query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)"
            }
          ],
          "incomingEdges": [
            {
              "sourceNodeId": "node_Concat_566",
              "sourceNodeOutputId": "0",
              "targetNodeInputId": "0"
            },
            {
              "sourceNodeId": "node_Unsqueeze_541",
              "sourceNodeOutputId": "0",
              "targetNodeInputId": "1"
            }
          ],
          "outputsMetadata": [
            {
              "id": "0",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "mul_444"
                },
                {
                  "key": "tensor_shape",
                  "value": "FLOAT[s34,32,s16,64]"
                },
                {
                  "key": "param_name",
                  "value": "C"
                }
              ]
            }
          ],
          "inputsMetadata": [
            {
              "id": "0",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "cat_5"
                },
                {
                  "key": "tensor_shape",
                  "value": "FLOAT[s34,32,s16,64]"
                },
                {
                  "key": "param_name",
                  "value": "A"
                }
              ]
            },
            {
              "id": "1",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "unsqueeze_13"
                },
                {
                  "key": "tensor_shape",
                  "value": "FLOAT[1,1,s16,64]"
                },
                {
                  "key": "param_name",
                  "value": "B"
                }
              ]
            }
          ],
          "style": null,
          "config": null
        },
        {
          "id": "node_Add_568",
          "label": "Add",
          "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.1: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.1.self_attn: transformers.models.llama.modeling_llama.LlamaAttention/add_571: aten.add.Tensor",
          "subgraphIds": [],
          "attrs": [
            {
              "key": "[metadata] namespace",
              "value": ": transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.1: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.1.self_attn: transformers.models.llama.modeling_llama.LlamaAttention/add_571: aten.add.Tensor"
            },
            {
              "key": "[metadata] pkg.torch.onnx.class_hierarchy",
              "value": "['transformers.models.llama.modeling_llama.LlamaForCausalLM', 'transformers.models.llama.modeling_llama.LlamaModel', 'transformers.models.llama.modeling_llama.LlamaDecoderLayer', 'transformers.models.llama.modeling_llama.LlamaAttention', 'aten.add.Tensor']"
            },
            {
              "key": "[metadata] pkg.torch.onnx.fx_node",
              "value": "%add_571 : [num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%mul_427, %mul_444), kwargs = {})"
            },
            {
              "key": "[metadata] pkg.torch.onnx.name_scopes",
              "value": "['', 'model', 'model.layers.1', 'model.layers.1.self_attn', 'add_571']"
            },
            {
              "key": "[metadata] pkg.torch.onnx.stack_trace",
              "value": "File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 688, in forward\n    outputs: BaseModelOutputWithPast = self.model(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 453, in forward\n    layer_outputs = decoder_layer(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 308, in forward\n    hidden_states, self_attn_weights = self.self_attn(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 247, in forward\n    query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)"
            }
          ],
          "incomingEdges": [
            {
              "sourceNodeId": "node_Mul_542",
              "sourceNodeOutputId": "0",
              "targetNodeInputId": "0"
            },
            {
              "sourceNodeId": "node_Mul_567",
              "sourceNodeOutputId": "0",
              "targetNodeInputId": "1"
            }
          ],
          "outputsMetadata": [
            {
              "id": "0",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "add_571"
                },
                {
                  "key": "tensor_shape",
                  "value": "FLOAT[s34,32,s16,64]"
                },
                {
                  "key": "param_name",
                  "value": "C"
                }
              ]
            }
          ],
          "inputsMetadata": [
            {
              "id": "0",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "mul_427"
                },
                {
                  "key": "tensor_shape",
                  "value": "FLOAT[s34,32,s16,64]"
                },
                {
                  "key": "param_name",
                  "value": "A"
                }
              ]
            },
            {
              "id": "1",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "mul_444"
                },
                {
                  "key": "tensor_shape",
                  "value": "FLOAT[s34,32,s16,64]"
                },
                {
                  "key": "param_name",
                  "value": "B"
                }
              ]
            }
          ],
          "style": null,
          "config": null
        },
        {
          "id": "node_Mul_569",
          "label": "Mul",
          "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.1: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.1.self_attn: transformers.models.llama.modeling_llama.LlamaAttention/mul_452: aten.mul.Tensor",
          "subgraphIds": [],
          "attrs": [
            {
              "key": "[metadata] namespace",
              "value": ": transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.1: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.1.self_attn: transformers.models.llama.modeling_llama.LlamaAttention/mul_452: aten.mul.Tensor"
            },
            {
              "key": "[metadata] pkg.torch.onnx.class_hierarchy",
              "value": "['transformers.models.llama.modeling_llama.LlamaForCausalLM', 'transformers.models.llama.modeling_llama.LlamaModel', 'transformers.models.llama.modeling_llama.LlamaDecoderLayer', 'transformers.models.llama.modeling_llama.LlamaAttention', 'aten.mul.Tensor']"
            },
            {
              "key": "[metadata] pkg.torch.onnx.fx_node",
              "value": "%mul_452 : [num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%transpose_7, %unsqueeze_12), kwargs = {})"
            },
            {
              "key": "[metadata] pkg.torch.onnx.name_scopes",
              "value": "['', 'model', 'model.layers.1', 'model.layers.1.self_attn', 'mul_452']"
            },
            {
              "key": "[metadata] pkg.torch.onnx.stack_trace",
              "value": "File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 688, in forward\n    outputs: BaseModelOutputWithPast = self.model(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 453, in forward\n    layer_outputs = decoder_layer(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 308, in forward\n    hidden_states, self_attn_weights = self.self_attn(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 247, in forward\n    query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)"
            }
          ],
          "incomingEdges": [
            {
              "sourceNodeId": "node_Transpose_529",
              "sourceNodeOutputId": "0",
              "targetNodeInputId": "0"
            },
            {
              "sourceNodeId": "node_Unsqueeze_540",
              "sourceNodeOutputId": "0",
              "targetNodeInputId": "1"
            }
          ],
          "outputsMetadata": [
            {
              "id": "0",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "mul_452"
                },
                {
                  "key": "tensor_shape",
                  "value": "FLOAT[s34,32,s16,64]"
                },
                {
                  "key": "param_name",
                  "value": "C"
                }
              ]
            }
          ],
          "inputsMetadata": [
            {
              "id": "0",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "transpose_7"
                },
                {
                  "key": "tensor_shape",
                  "value": "FLOAT[s34,32,s16,64]"
                },
                {
                  "key": "param_name",
                  "value": "A"
                }
              ]
            },
            {
              "id": "1",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "unsqueeze_12"
                },
                {
                  "key": "tensor_shape",
                  "value": "FLOAT[1,1,s16,64]"
                },
                {
                  "key": "param_name",
                  "value": "B"
                }
              ]
            }
          ],
          "style": null,
          "config": null
        },
        {
          "id": "node_Constant_949",
          "label": "Constant",
          "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.1: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.1.self_attn: transformers.models.llama.modeling_llama.LlamaAttention/slice_40: aten.slice.Tensor",
          "subgraphIds": [],
          "attrs": [
            {
              "key": "__value",
              "value": "[0]"
            },
            {
              "key": "value",
              "value": "TensorProtoTensor<INT64,[1]>(array([0]), name='val_406')"
            }
          ],
          "incomingEdges": [],
          "outputsMetadata": [
            {
              "id": "0",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "val_406"
                },
                {
                  "key": "tensor_shape",
                  "value": "INT64[1]"
                },
                {
                  "key": "param_name",
                  "value": "output"
                }
              ]
            }
          ],
          "inputsMetadata": [],
          "style": null,
          "config": null
        },
        {
          "id": "node_Constant_952",
          "label": "Constant",
          "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.1: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.1.self_attn: transformers.models.llama.modeling_llama.LlamaAttention/slice_40: aten.slice.Tensor",
          "subgraphIds": [],
          "attrs": [
            {
              "key": "__value",
              "value": "[32]"
            },
            {
              "key": "value",
              "value": "TensorProtoTensor<INT64,[1]>(array([32]), name='val_409')"
            }
          ],
          "incomingEdges": [],
          "outputsMetadata": [
            {
              "id": "0",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "val_409"
                },
                {
                  "key": "tensor_shape",
                  "value": "INT64[1]"
                },
                {
                  "key": "param_name",
                  "value": "output"
                }
              ]
            }
          ],
          "inputsMetadata": [],
          "style": null,
          "config": null
        },
        {
          "id": "node_Constant_955",
          "label": "Constant",
          "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.1: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.1.self_attn: transformers.models.llama.modeling_llama.LlamaAttention/slice_40: aten.slice.Tensor",
          "subgraphIds": [],
          "attrs": [
            {
              "key": "__value",
              "value": "[3]"
            },
            {
              "key": "value",
              "value": "TensorProtoTensor<INT64,[1]>(array([3]), name='val_412')"
            }
          ],
          "incomingEdges": [],
          "outputsMetadata": [
            {
              "id": "0",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "val_412"
                },
                {
                  "key": "tensor_shape",
                  "value": "INT64[1]"
                },
                {
                  "key": "param_name",
                  "value": "output"
                }
              ]
            }
          ],
          "inputsMetadata": [],
          "style": null,
          "config": null
        },
        {
          "id": "node_Constant_579",
          "label": "Constant",
          "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.1: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.1.self_attn: transformers.models.llama.modeling_llama.LlamaAttention/slice_40: aten.slice.Tensor",
          "subgraphIds": [],
          "attrs": [
            {
              "key": "__value",
              "value": "[1]"
            },
            {
              "key": "value_ints",
              "value": "[1]"
            },
            {
              "key": "[metadata] namespace",
              "value": ": transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.1: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.1.self_attn: transformers.models.llama.modeling_llama.LlamaAttention/slice_40: aten.slice.Tensor"
            },
            {
              "key": "[metadata] pkg.torch.onnx.class_hierarchy",
              "value": "['transformers.models.llama.modeling_llama.LlamaForCausalLM', 'transformers.models.llama.modeling_llama.LlamaModel', 'transformers.models.llama.modeling_llama.LlamaDecoderLayer', 'transformers.models.llama.modeling_llama.LlamaAttention', 'aten.slice.Tensor']"
            },
            {
              "key": "[metadata] pkg.torch.onnx.fx_node",
              "value": "%slice_40 : [num_users=1] = call_function[target=torch.ops.aten.slice.Tensor](args = (%transpose_7, 3, 0, 32), kwargs = {})"
            },
            {
              "key": "[metadata] pkg.torch.onnx.name_scopes",
              "value": "['', 'model', 'model.layers.1', 'model.layers.1.self_attn', 'slice_40']"
            },
            {
              "key": "[metadata] pkg.torch.onnx.stack_trace",
              "value": "File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 688, in forward\n    outputs: BaseModelOutputWithPast = self.model(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 453, in forward\n    layer_outputs = decoder_layer(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 308, in forward\n    hidden_states, self_attn_weights = self.self_attn(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 247, in forward\n    query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)"
            }
          ],
          "incomingEdges": [],
          "outputsMetadata": [
            {
              "id": "0",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "val_413"
                },
                {
                  "key": "tensor_shape",
                  "value": "INT64[1]"
                },
                {
                  "key": "param_name",
                  "value": "output"
                }
              ]
            }
          ],
          "inputsMetadata": [],
          "style": null,
          "config": null
        },
        {
          "id": "node_Slice_580",
          "label": "Slice",
          "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.1: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.1.self_attn: transformers.models.llama.modeling_llama.LlamaAttention/slice_40: aten.slice.Tensor",
          "subgraphIds": [],
          "attrs": [
            {
              "key": "[metadata] namespace",
              "value": ": transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.1: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.1.self_attn: transformers.models.llama.modeling_llama.LlamaAttention/slice_40: aten.slice.Tensor"
            },
            {
              "key": "[metadata] pkg.torch.onnx.class_hierarchy",
              "value": "['transformers.models.llama.modeling_llama.LlamaForCausalLM', 'transformers.models.llama.modeling_llama.LlamaModel', 'transformers.models.llama.modeling_llama.LlamaDecoderLayer', 'transformers.models.llama.modeling_llama.LlamaAttention', 'aten.slice.Tensor']"
            },
            {
              "key": "[metadata] pkg.torch.onnx.fx_node",
              "value": "%slice_40 : [num_users=1] = call_function[target=torch.ops.aten.slice.Tensor](args = (%transpose_7, 3, 0, 32), kwargs = {})"
            },
            {
              "key": "[metadata] pkg.torch.onnx.name_scopes",
              "value": "['', 'model', 'model.layers.1', 'model.layers.1.self_attn', 'slice_40']"
            },
            {
              "key": "[metadata] pkg.torch.onnx.stack_trace",
              "value": "File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 688, in forward\n    outputs: BaseModelOutputWithPast = self.model(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 453, in forward\n    layer_outputs = decoder_layer(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 308, in forward\n    hidden_states, self_attn_weights = self.self_attn(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 247, in forward\n    query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)"
            }
          ],
          "incomingEdges": [
            {
              "sourceNodeId": "node_Transpose_529",
              "sourceNodeOutputId": "0",
              "targetNodeInputId": "0"
            },
            {
              "sourceNodeId": "node_Constant_949",
              "sourceNodeOutputId": "0",
              "targetNodeInputId": "1"
            },
            {
              "sourceNodeId": "node_Constant_952",
              "sourceNodeOutputId": "0",
              "targetNodeInputId": "2"
            },
            {
              "sourceNodeId": "node_Constant_955",
              "sourceNodeOutputId": "0",
              "targetNodeInputId": "3"
            },
            {
              "sourceNodeId": "node_Constant_579",
              "sourceNodeOutputId": "0",
              "targetNodeInputId": "4"
            }
          ],
          "outputsMetadata": [
            {
              "id": "0",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "slice_40"
                },
                {
                  "key": "tensor_shape",
                  "value": "FLOAT[s34,32,s16,32]"
                },
                {
                  "key": "param_name",
                  "value": "output"
                }
              ]
            }
          ],
          "inputsMetadata": [
            {
              "id": "0",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "transpose_7"
                },
                {
                  "key": "tensor_shape",
                  "value": "FLOAT[s34,32,s16,64]"
                },
                {
                  "key": "param_name",
                  "value": "data"
                }
              ]
            },
            {
              "id": "1",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "val_406"
                },
                {
                  "key": "tensor_shape",
                  "value": "INT64[1]"
                },
                {
                  "key": "param_name",
                  "value": "starts"
                }
              ]
            },
            {
              "id": "2",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "val_409"
                },
                {
                  "key": "tensor_shape",
                  "value": "INT64[1]"
                },
                {
                  "key": "param_name",
                  "value": "ends"
                }
              ]
            },
            {
              "id": "3",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "val_412"
                },
                {
                  "key": "tensor_shape",
                  "value": "INT64[1]"
                },
                {
                  "key": "param_name",
                  "value": "axes"
                }
              ]
            },
            {
              "id": "4",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "val_413"
                },
                {
                  "key": "tensor_shape",
                  "value": "INT64[1]"
                },
                {
                  "key": "param_name",
                  "value": "steps"
                }
              ]
            }
          ],
          "style": null,
          "config": null
        },
        {
          "id": "node_Constant_958",
          "label": "Constant",
          "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.1: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.1.self_attn: transformers.models.llama.modeling_llama.LlamaAttention/slice_41: aten.slice.Tensor",
          "subgraphIds": [],
          "attrs": [
            {
              "key": "__value",
              "value": "[32]"
            },
            {
              "key": "value",
              "value": "TensorProtoTensor<INT64,[1]>(array([32]), name='val_416')"
            }
          ],
          "incomingEdges": [],
          "outputsMetadata": [
            {
              "id": "0",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "val_416"
                },
                {
                  "key": "tensor_shape",
                  "value": "INT64[1]"
                },
                {
                  "key": "param_name",
                  "value": "output"
                }
              ]
            }
          ],
          "inputsMetadata": [],
          "style": null,
          "config": null
        },
        {
          "id": "node_Constant_961",
          "label": "Constant",
          "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.1: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.1.self_attn: transformers.models.llama.modeling_llama.LlamaAttention/slice_41: aten.slice.Tensor",
          "subgraphIds": [],
          "attrs": [
            {
              "key": "__value",
              "value": "[9223372036854775807]"
            },
            {
              "key": "value",
              "value": "TensorProtoTensor<INT64,[1]>(array([9223372036854775807]), name='val_419')"
            }
          ],
          "incomingEdges": [],
          "outputsMetadata": [
            {
              "id": "0",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "val_419"
                },
                {
                  "key": "tensor_shape",
                  "value": "INT64[1]"
                },
                {
                  "key": "param_name",
                  "value": "output"
                }
              ]
            }
          ],
          "inputsMetadata": [],
          "style": null,
          "config": null
        },
        {
          "id": "node_Constant_964",
          "label": "Constant",
          "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.1: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.1.self_attn: transformers.models.llama.modeling_llama.LlamaAttention/slice_41: aten.slice.Tensor",
          "subgraphIds": [],
          "attrs": [
            {
              "key": "__value",
              "value": "[3]"
            },
            {
              "key": "value",
              "value": "TensorProtoTensor<INT64,[1]>(array([3]), name='val_422')"
            }
          ],
          "incomingEdges": [],
          "outputsMetadata": [
            {
              "id": "0",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "val_422"
                },
                {
                  "key": "tensor_shape",
                  "value": "INT64[1]"
                },
                {
                  "key": "param_name",
                  "value": "output"
                }
              ]
            }
          ],
          "inputsMetadata": [],
          "style": null,
          "config": null
        },
        {
          "id": "node_Constant_590",
          "label": "Constant",
          "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.1: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.1.self_attn: transformers.models.llama.modeling_llama.LlamaAttention/slice_41: aten.slice.Tensor",
          "subgraphIds": [],
          "attrs": [
            {
              "key": "__value",
              "value": "[1]"
            },
            {
              "key": "value_ints",
              "value": "[1]"
            },
            {
              "key": "[metadata] namespace",
              "value": ": transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.1: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.1.self_attn: transformers.models.llama.modeling_llama.LlamaAttention/slice_41: aten.slice.Tensor"
            },
            {
              "key": "[metadata] pkg.torch.onnx.class_hierarchy",
              "value": "['transformers.models.llama.modeling_llama.LlamaForCausalLM', 'transformers.models.llama.modeling_llama.LlamaModel', 'transformers.models.llama.modeling_llama.LlamaDecoderLayer', 'transformers.models.llama.modeling_llama.LlamaAttention', 'aten.slice.Tensor']"
            },
            {
              "key": "[metadata] pkg.torch.onnx.fx_node",
              "value": "%slice_41 : [num_users=1] = call_function[target=torch.ops.aten.slice.Tensor](args = (%transpose_7, 3, 32, 9223372036854775807), kwargs = {})"
            },
            {
              "key": "[metadata] pkg.torch.onnx.name_scopes",
              "value": "['', 'model', 'model.layers.1', 'model.layers.1.self_attn', 'slice_41']"
            },
            {
              "key": "[metadata] pkg.torch.onnx.stack_trace",
              "value": "File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 688, in forward\n    outputs: BaseModelOutputWithPast = self.model(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 453, in forward\n    layer_outputs = decoder_layer(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 308, in forward\n    hidden_states, self_attn_weights = self.self_attn(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 247, in forward\n    query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)"
            }
          ],
          "incomingEdges": [],
          "outputsMetadata": [
            {
              "id": "0",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "val_423"
                },
                {
                  "key": "tensor_shape",
                  "value": "INT64[1]"
                },
                {
                  "key": "param_name",
                  "value": "output"
                }
              ]
            }
          ],
          "inputsMetadata": [],
          "style": null,
          "config": null
        },
        {
          "id": "node_Slice_591",
          "label": "Slice",
          "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.1: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.1.self_attn: transformers.models.llama.modeling_llama.LlamaAttention/slice_41: aten.slice.Tensor",
          "subgraphIds": [],
          "attrs": [
            {
              "key": "[metadata] namespace",
              "value": ": transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.1: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.1.self_attn: transformers.models.llama.modeling_llama.LlamaAttention/slice_41: aten.slice.Tensor"
            },
            {
              "key": "[metadata] pkg.torch.onnx.class_hierarchy",
              "value": "['transformers.models.llama.modeling_llama.LlamaForCausalLM', 'transformers.models.llama.modeling_llama.LlamaModel', 'transformers.models.llama.modeling_llama.LlamaDecoderLayer', 'transformers.models.llama.modeling_llama.LlamaAttention', 'aten.slice.Tensor']"
            },
            {
              "key": "[metadata] pkg.torch.onnx.fx_node",
              "value": "%slice_41 : [num_users=1] = call_function[target=torch.ops.aten.slice.Tensor](args = (%transpose_7, 3, 32, 9223372036854775807), kwargs = {})"
            },
            {
              "key": "[metadata] pkg.torch.onnx.name_scopes",
              "value": "['', 'model', 'model.layers.1', 'model.layers.1.self_attn', 'slice_41']"
            },
            {
              "key": "[metadata] pkg.torch.onnx.stack_trace",
              "value": "File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 688, in forward\n    outputs: BaseModelOutputWithPast = self.model(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 453, in forward\n    layer_outputs = decoder_layer(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 308, in forward\n    hidden_states, self_attn_weights = self.self_attn(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 247, in forward\n    query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)"
            }
          ],
          "incomingEdges": [
            {
              "sourceNodeId": "node_Transpose_529",
              "sourceNodeOutputId": "0",
              "targetNodeInputId": "0"
            },
            {
              "sourceNodeId": "node_Constant_958",
              "sourceNodeOutputId": "0",
              "targetNodeInputId": "1"
            },
            {
              "sourceNodeId": "node_Constant_961",
              "sourceNodeOutputId": "0",
              "targetNodeInputId": "2"
            },
            {
              "sourceNodeId": "node_Constant_964",
              "sourceNodeOutputId": "0",
              "targetNodeInputId": "3"
            },
            {
              "sourceNodeId": "node_Constant_590",
              "sourceNodeOutputId": "0",
              "targetNodeInputId": "4"
            }
          ],
          "outputsMetadata": [
            {
              "id": "0",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "slice_41"
                },
                {
                  "key": "tensor_shape",
                  "value": "FLOAT[s34,32,s16,32]"
                },
                {
                  "key": "param_name",
                  "value": "output"
                }
              ]
            }
          ],
          "inputsMetadata": [
            {
              "id": "0",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "transpose_7"
                },
                {
                  "key": "tensor_shape",
                  "value": "FLOAT[s34,32,s16,64]"
                },
                {
                  "key": "param_name",
                  "value": "data"
                }
              ]
            },
            {
              "id": "1",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "val_416"
                },
                {
                  "key": "tensor_shape",
                  "value": "INT64[1]"
                },
                {
                  "key": "param_name",
                  "value": "starts"
                }
              ]
            },
            {
              "id": "2",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "val_419"
                },
                {
                  "key": "tensor_shape",
                  "value": "INT64[1]"
                },
                {
                  "key": "param_name",
                  "value": "ends"
                }
              ]
            },
            {
              "id": "3",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "val_422"
                },
                {
                  "key": "tensor_shape",
                  "value": "INT64[1]"
                },
                {
                  "key": "param_name",
                  "value": "axes"
                }
              ]
            },
            {
              "id": "4",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "val_423"
                },
                {
                  "key": "tensor_shape",
                  "value": "INT64[1]"
                },
                {
                  "key": "param_name",
                  "value": "steps"
                }
              ]
            }
          ],
          "style": null,
          "config": null
        },
        {
          "id": "node_Neg_592",
          "label": "Neg",
          "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.1: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.1.self_attn: transformers.models.llama.modeling_llama.LlamaAttention/neg_3: aten.neg.default",
          "subgraphIds": [],
          "attrs": [
            {
              "key": "[metadata] namespace",
              "value": ": transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.1: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.1.self_attn: transformers.models.llama.modeling_llama.LlamaAttention/neg_3: aten.neg.default"
            },
            {
              "key": "[metadata] pkg.torch.onnx.class_hierarchy",
              "value": "['transformers.models.llama.modeling_llama.LlamaForCausalLM', 'transformers.models.llama.modeling_llama.LlamaModel', 'transformers.models.llama.modeling_llama.LlamaDecoderLayer', 'transformers.models.llama.modeling_llama.LlamaAttention', 'aten.neg.default']"
            },
            {
              "key": "[metadata] pkg.torch.onnx.fx_node",
              "value": "%neg_3 : [num_users=1] = call_function[target=torch.ops.aten.neg.default](args = (%slice_41,), kwargs = {})"
            },
            {
              "key": "[metadata] pkg.torch.onnx.name_scopes",
              "value": "['', 'model', 'model.layers.1', 'model.layers.1.self_attn', 'neg_3']"
            },
            {
              "key": "[metadata] pkg.torch.onnx.stack_trace",
              "value": "File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 688, in forward\n    outputs: BaseModelOutputWithPast = self.model(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 453, in forward\n    layer_outputs = decoder_layer(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 308, in forward\n    hidden_states, self_attn_weights = self.self_attn(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 247, in forward\n    query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)"
            }
          ],
          "incomingEdges": [
            {
              "sourceNodeId": "node_Slice_591",
              "sourceNodeOutputId": "0",
              "targetNodeInputId": "0"
            }
          ],
          "outputsMetadata": [
            {
              "id": "0",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "neg_3"
                },
                {
                  "key": "tensor_shape",
                  "value": "FLOAT[s34,32,s16,32]"
                },
                {
                  "key": "param_name",
                  "value": "Y"
                }
              ]
            }
          ],
          "inputsMetadata": [
            {
              "id": "0",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "slice_41"
                },
                {
                  "key": "tensor_shape",
                  "value": "FLOAT[s34,32,s16,32]"
                },
                {
                  "key": "param_name",
                  "value": "X"
                }
              ]
            }
          ],
          "style": null,
          "config": null
        },
        {
          "id": "node_Concat_593",
          "label": "Concat",
          "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.1: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.1.self_attn: transformers.models.llama.modeling_llama.LlamaAttention/cat_6: aten.cat.default",
          "subgraphIds": [],
          "attrs": [
            {
              "key": "axis",
              "value": "-1"
            },
            {
              "key": "[metadata] namespace",
              "value": ": transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.1: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.1.self_attn: transformers.models.llama.modeling_llama.LlamaAttention/cat_6: aten.cat.default"
            },
            {
              "key": "[metadata] pkg.torch.onnx.class_hierarchy",
              "value": "['transformers.models.llama.modeling_llama.LlamaForCausalLM', 'transformers.models.llama.modeling_llama.LlamaModel', 'transformers.models.llama.modeling_llama.LlamaDecoderLayer', 'transformers.models.llama.modeling_llama.LlamaAttention', 'aten.cat.default']"
            },
            {
              "key": "[metadata] pkg.torch.onnx.fx_node",
              "value": "%cat_6 : [num_users=1] = call_function[target=torch.ops.aten.cat.default](args = ([%neg_3, %slice_40], -1), kwargs = {})"
            },
            {
              "key": "[metadata] pkg.torch.onnx.name_scopes",
              "value": "['', 'model', 'model.layers.1', 'model.layers.1.self_attn', 'cat_6']"
            },
            {
              "key": "[metadata] pkg.torch.onnx.stack_trace",
              "value": "File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 688, in forward\n    outputs: BaseModelOutputWithPast = self.model(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 453, in forward\n    layer_outputs = decoder_layer(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 308, in forward\n    hidden_states, self_attn_weights = self.self_attn(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 247, in forward\n    query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)"
            }
          ],
          "incomingEdges": [
            {
              "sourceNodeId": "node_Neg_592",
              "sourceNodeOutputId": "0",
              "targetNodeInputId": "0"
            },
            {
              "sourceNodeId": "node_Slice_580",
              "sourceNodeOutputId": "0",
              "targetNodeInputId": "1"
            }
          ],
          "outputsMetadata": [
            {
              "id": "0",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "cat_6"
                },
                {
                  "key": "tensor_shape",
                  "value": "FLOAT[s34,32,s16,64]"
                },
                {
                  "key": "param_name",
                  "value": "concat_result"
                }
              ]
            }
          ],
          "inputsMetadata": [
            {
              "id": "0",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "neg_3"
                },
                {
                  "key": "tensor_shape",
                  "value": "FLOAT[s34,32,s16,32]"
                },
                {
                  "key": "param_name",
                  "value": "inputs"
                }
              ]
            },
            {
              "id": "1",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "slice_40"
                },
                {
                  "key": "tensor_shape",
                  "value": "FLOAT[s34,32,s16,32]"
                },
                {
                  "key": "param_name",
                  "value": "inputs"
                }
              ]
            }
          ],
          "style": null,
          "config": null
        },
        {
          "id": "node_Mul_594",
          "label": "Mul",
          "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.1: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.1.self_attn: transformers.models.llama.modeling_llama.LlamaAttention/mul_469: aten.mul.Tensor",
          "subgraphIds": [],
          "attrs": [
            {
              "key": "[metadata] namespace",
              "value": ": transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.1: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.1.self_attn: transformers.models.llama.modeling_llama.LlamaAttention/mul_469: aten.mul.Tensor"
            },
            {
              "key": "[metadata] pkg.torch.onnx.class_hierarchy",
              "value": "['transformers.models.llama.modeling_llama.LlamaForCausalLM', 'transformers.models.llama.modeling_llama.LlamaModel', 'transformers.models.llama.modeling_llama.LlamaDecoderLayer', 'transformers.models.llama.modeling_llama.LlamaAttention', 'aten.mul.Tensor']"
            },
            {
              "key": "[metadata] pkg.torch.onnx.fx_node",
              "value": "%mul_469 : [num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%cat_6, %unsqueeze_13), kwargs = {})"
            },
            {
              "key": "[metadata] pkg.torch.onnx.name_scopes",
              "value": "['', 'model', 'model.layers.1', 'model.layers.1.self_attn', 'mul_469']"
            },
            {
              "key": "[metadata] pkg.torch.onnx.stack_trace",
              "value": "File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 688, in forward\n    outputs: BaseModelOutputWithPast = self.model(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 453, in forward\n    layer_outputs = decoder_layer(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 308, in forward\n    hidden_states, self_attn_weights = self.self_attn(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 247, in forward\n    query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)"
            }
          ],
          "incomingEdges": [
            {
              "sourceNodeId": "node_Concat_593",
              "sourceNodeOutputId": "0",
              "targetNodeInputId": "0"
            },
            {
              "sourceNodeId": "node_Unsqueeze_541",
              "sourceNodeOutputId": "0",
              "targetNodeInputId": "1"
            }
          ],
          "outputsMetadata": [
            {
              "id": "0",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "mul_469"
                },
                {
                  "key": "tensor_shape",
                  "value": "FLOAT[s34,32,s16,64]"
                },
                {
                  "key": "param_name",
                  "value": "C"
                }
              ]
            }
          ],
          "inputsMetadata": [
            {
              "id": "0",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "cat_6"
                },
                {
                  "key": "tensor_shape",
                  "value": "FLOAT[s34,32,s16,64]"
                },
                {
                  "key": "param_name",
                  "value": "A"
                }
              ]
            },
            {
              "id": "1",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "unsqueeze_13"
                },
                {
                  "key": "tensor_shape",
                  "value": "FLOAT[1,1,s16,64]"
                },
                {
                  "key": "param_name",
                  "value": "B"
                }
              ]
            }
          ],
          "style": null,
          "config": null
        },
        {
          "id": "node_Add_595",
          "label": "Add",
          "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.1: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.1.self_attn: transformers.models.llama.modeling_llama.LlamaAttention/add_607: aten.add.Tensor",
          "subgraphIds": [],
          "attrs": [
            {
              "key": "[metadata] namespace",
              "value": ": transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.1: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.1.self_attn: transformers.models.llama.modeling_llama.LlamaAttention/add_607: aten.add.Tensor"
            },
            {
              "key": "[metadata] pkg.torch.onnx.class_hierarchy",
              "value": "['transformers.models.llama.modeling_llama.LlamaForCausalLM', 'transformers.models.llama.modeling_llama.LlamaModel', 'transformers.models.llama.modeling_llama.LlamaDecoderLayer', 'transformers.models.llama.modeling_llama.LlamaAttention', 'aten.add.Tensor']"
            },
            {
              "key": "[metadata] pkg.torch.onnx.fx_node",
              "value": "%add_607 : [num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%mul_452, %mul_469), kwargs = {})"
            },
            {
              "key": "[metadata] pkg.torch.onnx.name_scopes",
              "value": "['', 'model', 'model.layers.1', 'model.layers.1.self_attn', 'add_607']"
            },
            {
              "key": "[metadata] pkg.torch.onnx.stack_trace",
              "value": "File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 688, in forward\n    outputs: BaseModelOutputWithPast = self.model(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 453, in forward\n    layer_outputs = decoder_layer(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 308, in forward\n    hidden_states, self_attn_weights = self.self_attn(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 247, in forward\n    query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)"
            }
          ],
          "incomingEdges": [
            {
              "sourceNodeId": "node_Mul_569",
              "sourceNodeOutputId": "0",
              "targetNodeInputId": "0"
            },
            {
              "sourceNodeId": "node_Mul_594",
              "sourceNodeOutputId": "0",
              "targetNodeInputId": "1"
            }
          ],
          "outputsMetadata": [
            {
              "id": "0",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "add_607"
                },
                {
                  "key": "tensor_shape",
                  "value": "FLOAT[s34,32,s16,64]"
                },
                {
                  "key": "param_name",
                  "value": "C"
                }
              ]
            }
          ],
          "inputsMetadata": [
            {
              "id": "0",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "mul_452"
                },
                {
                  "key": "tensor_shape",
                  "value": "FLOAT[s34,32,s16,64]"
                },
                {
                  "key": "param_name",
                  "value": "A"
                }
              ]
            },
            {
              "id": "1",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "mul_469"
                },
                {
                  "key": "tensor_shape",
                  "value": "FLOAT[s34,32,s16,64]"
                },
                {
                  "key": "param_name",
                  "value": "B"
                }
              ]
            }
          ],
          "style": null,
          "config": null
        },
        {
          "id": "node_Concat_596",
          "label": "Concat",
          "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.1: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.1.self_attn: transformers.models.llama.modeling_llama.LlamaAttention/cat_7: aten.cat.default",
          "subgraphIds": [],
          "attrs": [
            {
              "key": "axis",
              "value": "-2"
            },
            {
              "key": "[metadata] namespace",
              "value": ": transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.1: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.1.self_attn: transformers.models.llama.modeling_llama.LlamaAttention/cat_7: aten.cat.default"
            },
            {
              "key": "[metadata] pkg.torch.onnx.class_hierarchy",
              "value": "['transformers.models.llama.modeling_llama.LlamaForCausalLM', 'transformers.models.llama.modeling_llama.LlamaModel', 'transformers.models.llama.modeling_llama.LlamaDecoderLayer', 'transformers.models.llama.modeling_llama.LlamaAttention', 'aten.cat.default']"
            },
            {
              "key": "[metadata] pkg.torch.onnx.fx_node",
              "value": "%cat_7 : [num_users=2] = call_function[target=torch.ops.aten.cat.default](args = ([%past_key_values_key_cache_1, %add_607], -2), kwargs = {})"
            },
            {
              "key": "[metadata] pkg.torch.onnx.name_scopes",
              "value": "['', 'model', 'model.layers.1', 'model.layers.1.self_attn', 'cat_7']"
            },
            {
              "key": "[metadata] pkg.torch.onnx.stack_trace",
              "value": "File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 688, in forward\n    outputs: BaseModelOutputWithPast = self.model(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 453, in forward\n    layer_outputs = decoder_layer(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 308, in forward\n    hidden_states, self_attn_weights = self.self_attn(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 252, in forward\n    key_states, value_states = past_key_value.update(key_states, value_states, self.layer_idx, cache_kwargs)"
            }
          ],
          "incomingEdges": [
            {
              "sourceNodeId": "[value] past_key_values_key_cache_1",
              "sourceNodeOutputId": "0",
              "targetNodeInputId": "0"
            },
            {
              "sourceNodeId": "node_Add_595",
              "sourceNodeOutputId": "0",
              "targetNodeInputId": "1"
            }
          ],
          "outputsMetadata": [
            {
              "id": "0",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "cat_7"
                },
                {
                  "key": "tensor_shape",
                  "value": "FLOAT[s34,32,s16 + s17,64]"
                },
                {
                  "key": "[metadata] pkg.torch.export.graph_signature.OutputSpec.kind",
                  "value": "USER_OUTPUT"
                },
                {
                  "key": "param_name",
                  "value": "concat_result"
                }
              ]
            }
          ],
          "inputsMetadata": [
            {
              "id": "0",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "past_key_values_key_cache_1"
                },
                {
                  "key": "tensor_shape",
                  "value": "FLOAT[s34,32,s17,64]"
                },
                {
                  "key": "[metadata] pkg.torch.export.graph_signature.InputSpec.kind",
                  "value": "USER_INPUT"
                },
                {
                  "key": "[metadata] pkg.torch.export.graph_signature.InputSpec.persistent",
                  "value": "None"
                },
                {
                  "key": "param_name",
                  "value": "inputs"
                }
              ]
            },
            {
              "id": "1",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "add_607"
                },
                {
                  "key": "tensor_shape",
                  "value": "FLOAT[s34,32,s16,64]"
                },
                {
                  "key": "param_name",
                  "value": "inputs"
                }
              ]
            }
          ],
          "style": null,
          "config": null
        },
        {
          "id": "node_Concat_597",
          "label": "Concat",
          "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.1: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.1.self_attn: transformers.models.llama.modeling_llama.LlamaAttention/cat_8: aten.cat.default",
          "subgraphIds": [],
          "attrs": [
            {
              "key": "axis",
              "value": "-2"
            },
            {
              "key": "[metadata] namespace",
              "value": ": transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.1: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.1.self_attn: transformers.models.llama.modeling_llama.LlamaAttention/cat_8: aten.cat.default"
            },
            {
              "key": "[metadata] pkg.torch.onnx.class_hierarchy",
              "value": "['transformers.models.llama.modeling_llama.LlamaForCausalLM', 'transformers.models.llama.modeling_llama.LlamaModel', 'transformers.models.llama.modeling_llama.LlamaDecoderLayer', 'transformers.models.llama.modeling_llama.LlamaAttention', 'aten.cat.default']"
            },
            {
              "key": "[metadata] pkg.torch.onnx.fx_node",
              "value": "%cat_8 : [num_users=2] = call_function[target=torch.ops.aten.cat.default](args = ([%past_key_values_value_cache_1, %transpose_8], -2), kwargs = {})"
            },
            {
              "key": "[metadata] pkg.torch.onnx.name_scopes",
              "value": "['', 'model', 'model.layers.1', 'model.layers.1.self_attn', 'cat_8']"
            },
            {
              "key": "[metadata] pkg.torch.onnx.stack_trace",
              "value": "File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 688, in forward\n    outputs: BaseModelOutputWithPast = self.model(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 453, in forward\n    layer_outputs = decoder_layer(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 308, in forward\n    hidden_states, self_attn_weights = self.self_attn(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 252, in forward\n    key_states, value_states = past_key_value.update(key_states, value_states, self.layer_idx, cache_kwargs)"
            }
          ],
          "incomingEdges": [
            {
              "sourceNodeId": "[value] past_key_values_value_cache_1",
              "sourceNodeOutputId": "0",
              "targetNodeInputId": "0"
            },
            {
              "sourceNodeId": "node_Transpose_539",
              "sourceNodeOutputId": "0",
              "targetNodeInputId": "1"
            }
          ],
          "outputsMetadata": [
            {
              "id": "0",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "cat_8"
                },
                {
                  "key": "tensor_shape",
                  "value": "FLOAT[s34,32,s16 + s17,64]"
                },
                {
                  "key": "[metadata] pkg.torch.export.graph_signature.OutputSpec.kind",
                  "value": "USER_OUTPUT"
                },
                {
                  "key": "param_name",
                  "value": "concat_result"
                }
              ]
            }
          ],
          "inputsMetadata": [
            {
              "id": "0",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "past_key_values_value_cache_1"
                },
                {
                  "key": "tensor_shape",
                  "value": "FLOAT[s34,32,s17,64]"
                },
                {
                  "key": "[metadata] pkg.torch.export.graph_signature.InputSpec.kind",
                  "value": "USER_INPUT"
                },
                {
                  "key": "[metadata] pkg.torch.export.graph_signature.InputSpec.persistent",
                  "value": "None"
                },
                {
                  "key": "param_name",
                  "value": "inputs"
                }
              ]
            },
            {
              "id": "1",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "transpose_8"
                },
                {
                  "key": "tensor_shape",
                  "value": "FLOAT[s34,32,s16,64]"
                },
                {
                  "key": "param_name",
                  "value": "inputs"
                }
              ]
            }
          ],
          "style": null,
          "config": null
        },
        {
          "id": "node_Transpose_598",
          "label": "Transpose",
          "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.1: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.1.self_attn: transformers.models.llama.modeling_llama.LlamaAttention/transpose_9: aten.transpose.int",
          "subgraphIds": [],
          "attrs": [
            {
              "key": "perm",
              "value": "[0, 1, 3, 2]"
            },
            {
              "key": "[metadata] namespace",
              "value": ": transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.1: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.1.self_attn: transformers.models.llama.modeling_llama.LlamaAttention/transpose_9: aten.transpose.int"
            },
            {
              "key": "[metadata] pkg.torch.onnx.class_hierarchy",
              "value": "['transformers.models.llama.modeling_llama.LlamaForCausalLM', 'transformers.models.llama.modeling_llama.LlamaModel', 'transformers.models.llama.modeling_llama.LlamaDecoderLayer', 'transformers.models.llama.modeling_llama.LlamaAttention', 'aten.transpose.int']"
            },
            {
              "key": "[metadata] pkg.torch.onnx.fx_node",
              "value": "%transpose_9 : [num_users=1] = call_function[target=torch.ops.aten.transpose.int](args = (%cat_7, 2, 3), kwargs = {})"
            },
            {
              "key": "[metadata] pkg.torch.onnx.name_scopes",
              "value": "['', 'model', 'model.layers.1', 'model.layers.1.self_attn', 'transpose_9']"
            },
            {
              "key": "[metadata] pkg.torch.onnx.stack_trace",
              "value": "File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 688, in forward\n    outputs: BaseModelOutputWithPast = self.model(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 453, in forward\n    layer_outputs = decoder_layer(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 308, in forward\n    hidden_states, self_attn_weights = self.self_attn(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 265, in forward\n    attn_output, attn_weights = attention_interface("
            }
          ],
          "incomingEdges": [
            {
              "sourceNodeId": "node_Concat_596",
              "sourceNodeOutputId": "0",
              "targetNodeInputId": "0"
            }
          ],
          "outputsMetadata": [
            {
              "id": "0",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "transpose_9"
                },
                {
                  "key": "tensor_shape",
                  "value": "FLOAT[s34,32,64,s16 + s17]"
                },
                {
                  "key": "param_name",
                  "value": "transposed"
                }
              ]
            }
          ],
          "inputsMetadata": [
            {
              "id": "0",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "cat_7"
                },
                {
                  "key": "tensor_shape",
                  "value": "FLOAT[s34,32,s16 + s17,64]"
                },
                {
                  "key": "[metadata] pkg.torch.export.graph_signature.OutputSpec.kind",
                  "value": "USER_OUTPUT"
                },
                {
                  "key": "param_name",
                  "value": "data"
                }
              ]
            }
          ],
          "style": null,
          "config": null
        },
        {
          "id": "node_MatMul_599",
          "label": "MatMul",
          "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.1: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.1.self_attn: transformers.models.llama.modeling_llama.LlamaAttention/matmul_3: aten.matmul.default",
          "subgraphIds": [],
          "attrs": [
            {
              "key": "[metadata] namespace",
              "value": ": transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.1: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.1.self_attn: transformers.models.llama.modeling_llama.LlamaAttention/matmul_3: aten.matmul.default"
            },
            {
              "key": "[metadata] pkg.torch.onnx.class_hierarchy",
              "value": "['transformers.models.llama.modeling_llama.LlamaForCausalLM', 'transformers.models.llama.modeling_llama.LlamaModel', 'transformers.models.llama.modeling_llama.LlamaDecoderLayer', 'transformers.models.llama.modeling_llama.LlamaAttention', 'aten.matmul.default']"
            },
            {
              "key": "[metadata] pkg.torch.onnx.fx_node",
              "value": "%matmul_3 : [num_users=1] = call_function[target=torch.ops.aten.matmul.default](args = (%add_571, %transpose_9), kwargs = {})"
            },
            {
              "key": "[metadata] pkg.torch.onnx.name_scopes",
              "value": "['', 'model', 'model.layers.1', 'model.layers.1.self_attn', 'matmul_3']"
            },
            {
              "key": "[metadata] pkg.torch.onnx.stack_trace",
              "value": "File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 688, in forward\n    outputs: BaseModelOutputWithPast = self.model(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 453, in forward\n    layer_outputs = decoder_layer(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 308, in forward\n    hidden_states, self_attn_weights = self.self_attn(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 265, in forward\n    attn_output, attn_weights = attention_interface("
            }
          ],
          "incomingEdges": [
            {
              "sourceNodeId": "node_Add_568",
              "sourceNodeOutputId": "0",
              "targetNodeInputId": "0"
            },
            {
              "sourceNodeId": "node_Transpose_598",
              "sourceNodeOutputId": "0",
              "targetNodeInputId": "1"
            }
          ],
          "outputsMetadata": [
            {
              "id": "0",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "matmul_3"
                },
                {
                  "key": "tensor_shape",
                  "value": "FLOAT[s34,32,s16,s16 + s17]"
                },
                {
                  "key": "param_name",
                  "value": "Y"
                }
              ]
            }
          ],
          "inputsMetadata": [
            {
              "id": "0",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "add_571"
                },
                {
                  "key": "tensor_shape",
                  "value": "FLOAT[s34,32,s16,64]"
                },
                {
                  "key": "param_name",
                  "value": "A"
                }
              ]
            },
            {
              "id": "1",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "transpose_9"
                },
                {
                  "key": "tensor_shape",
                  "value": "FLOAT[s34,32,64,s16 + s17]"
                },
                {
                  "key": "param_name",
                  "value": "B"
                }
              ]
            }
          ],
          "style": null,
          "config": null
        },
        {
          "id": "node_Mul_600",
          "label": "Mul",
          "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.1: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.1.self_attn: transformers.models.llama.modeling_llama.LlamaAttention/mul_493: aten.mul.Tensor",
          "subgraphIds": [],
          "attrs": [
            {
              "key": "[metadata] namespace",
              "value": ": transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.1: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.1.self_attn: transformers.models.llama.modeling_llama.LlamaAttention/mul_493: aten.mul.Tensor"
            },
            {
              "key": "[metadata] pkg.torch.onnx.class_hierarchy",
              "value": "['transformers.models.llama.modeling_llama.LlamaForCausalLM', 'transformers.models.llama.modeling_llama.LlamaModel', 'transformers.models.llama.modeling_llama.LlamaDecoderLayer', 'transformers.models.llama.modeling_llama.LlamaAttention', 'aten.mul.Tensor']"
            },
            {
              "key": "[metadata] pkg.torch.onnx.fx_node",
              "value": "%mul_493 : [num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%matmul_3, 0.125), kwargs = {})"
            },
            {
              "key": "[metadata] pkg.torch.onnx.name_scopes",
              "value": "['', 'model', 'model.layers.1', 'model.layers.1.self_attn', 'mul_493']"
            },
            {
              "key": "[metadata] pkg.torch.onnx.stack_trace",
              "value": "File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 688, in forward\n    outputs: BaseModelOutputWithPast = self.model(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 453, in forward\n    layer_outputs = decoder_layer(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 308, in forward\n    hidden_states, self_attn_weights = self.self_attn(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 265, in forward\n    attn_output, attn_weights = attention_interface("
            }
          ],
          "incomingEdges": [
            {
              "sourceNodeId": "node_MatMul_599",
              "sourceNodeOutputId": "0",
              "targetNodeInputId": "0"
            },
            {
              "sourceNodeId": "node_Constant_431",
              "sourceNodeOutputId": "0",
              "targetNodeInputId": "1"
            }
          ],
          "outputsMetadata": [
            {
              "id": "0",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "mul_493"
                },
                {
                  "key": "tensor_shape",
                  "value": "FLOAT[s34,32,s16,s16 + s17]"
                },
                {
                  "key": "param_name",
                  "value": "C"
                }
              ]
            }
          ],
          "inputsMetadata": [
            {
              "id": "0",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "matmul_3"
                },
                {
                  "key": "tensor_shape",
                  "value": "FLOAT[s34,32,s16,s16 + s17]"
                },
                {
                  "key": "param_name",
                  "value": "A"
                }
              ]
            },
            {
              "id": "1",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "val_316"
                },
                {
                  "key": "tensor_shape",
                  "value": "FLOAT[]"
                },
                {
                  "key": "param_name",
                  "value": "B"
                }
              ]
            }
          ],
          "style": null,
          "config": null
        },
        {
          "id": "node_Constant_622",
          "label": "Constant",
          "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.1: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.1.self_attn: transformers.models.llama.modeling_llama.LlamaAttention/slice_49: aten.slice.Tensor",
          "subgraphIds": [],
          "attrs": [
            {
              "key": "__value",
              "value": "[0]"
            },
            {
              "key": "value_ints",
              "value": "[0]"
            },
            {
              "key": "[metadata] namespace",
              "value": ": transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.1: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.1.self_attn: transformers.models.llama.modeling_llama.LlamaAttention/slice_49: aten.slice.Tensor"
            },
            {
              "key": "[metadata] pkg.torch.onnx.class_hierarchy",
              "value": "['transformers.models.llama.modeling_llama.LlamaForCausalLM', 'transformers.models.llama.modeling_llama.LlamaModel', 'transformers.models.llama.modeling_llama.LlamaDecoderLayer', 'transformers.models.llama.modeling_llama.LlamaAttention', 'aten.slice.Tensor']"
            },
            {
              "key": "[metadata] pkg.torch.onnx.fx_node",
              "value": "%slice_49 : [num_users=1] = call_function[target=torch.ops.aten.slice.Tensor](args = (%slice_48, 3, None, %add_4), kwargs = {})"
            },
            {
              "key": "[metadata] pkg.torch.onnx.name_scopes",
              "value": "['', 'model', 'model.layers.1', 'model.layers.1.self_attn', 'slice_49']"
            },
            {
              "key": "[metadata] pkg.torch.onnx.stack_trace",
              "value": "File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 688, in forward\n    outputs: BaseModelOutputWithPast = self.model(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 453, in forward\n    layer_outputs = decoder_layer(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 308, in forward\n    hidden_states, self_attn_weights = self.self_attn(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 265, in forward\n    attn_output, attn_weights = attention_interface("
            }
          ],
          "incomingEdges": [],
          "outputsMetadata": [
            {
              "id": "0",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "val_442"
                },
                {
                  "key": "tensor_shape",
                  "value": "INT64[1]"
                },
                {
                  "key": "param_name",
                  "value": "output"
                }
              ]
            }
          ],
          "inputsMetadata": [],
          "style": null,
          "config": null
        },
        {
          "id": "node_Constant_624",
          "label": "Constant",
          "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.1: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.1.self_attn: transformers.models.llama.modeling_llama.LlamaAttention/slice_49: aten.slice.Tensor",
          "subgraphIds": [],
          "attrs": [
            {
              "key": "__value",
              "value": "[-1]"
            },
            {
              "key": "value_ints",
              "value": "[-1]"
            },
            {
              "key": "[metadata] namespace",
              "value": ": transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.1: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.1.self_attn: transformers.models.llama.modeling_llama.LlamaAttention/slice_49: aten.slice.Tensor"
            },
            {
              "key": "[metadata] pkg.torch.onnx.class_hierarchy",
              "value": "['transformers.models.llama.modeling_llama.LlamaForCausalLM', 'transformers.models.llama.modeling_llama.LlamaModel', 'transformers.models.llama.modeling_llama.LlamaDecoderLayer', 'transformers.models.llama.modeling_llama.LlamaAttention', 'aten.slice.Tensor']"
            },
            {
              "key": "[metadata] pkg.torch.onnx.fx_node",
              "value": "%slice_49 : [num_users=1] = call_function[target=torch.ops.aten.slice.Tensor](args = (%slice_48, 3, None, %add_4), kwargs = {})"
            },
            {
              "key": "[metadata] pkg.torch.onnx.name_scopes",
              "value": "['', 'model', 'model.layers.1', 'model.layers.1.self_attn', 'slice_49']"
            },
            {
              "key": "[metadata] pkg.torch.onnx.stack_trace",
              "value": "File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 688, in forward\n    outputs: BaseModelOutputWithPast = self.model(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 453, in forward\n    layer_outputs = decoder_layer(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 308, in forward\n    hidden_states, self_attn_weights = self.self_attn(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 265, in forward\n    attn_output, attn_weights = attention_interface("
            }
          ],
          "incomingEdges": [],
          "outputsMetadata": [
            {
              "id": "0",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "val_444"
                },
                {
                  "key": "tensor_shape",
                  "value": "INT64[1]"
                },
                {
                  "key": "param_name",
                  "value": "output"
                }
              ]
            }
          ],
          "inputsMetadata": [],
          "style": null,
          "config": null
        },
        {
          "id": "node_Reshape_625",
          "label": "Reshape",
          "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.1: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.1.self_attn: transformers.models.llama.modeling_llama.LlamaAttention/slice_49: aten.slice.Tensor",
          "subgraphIds": [],
          "attrs": [
            {
              "key": "allowzero",
              "value": "0"
            },
            {
              "key": "[metadata] namespace",
              "value": ": transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.1: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.1.self_attn: transformers.models.llama.modeling_llama.LlamaAttention/slice_49: aten.slice.Tensor"
            },
            {
              "key": "[metadata] pkg.torch.onnx.class_hierarchy",
              "value": "['transformers.models.llama.modeling_llama.LlamaForCausalLM', 'transformers.models.llama.modeling_llama.LlamaModel', 'transformers.models.llama.modeling_llama.LlamaDecoderLayer', 'transformers.models.llama.modeling_llama.LlamaAttention', 'aten.slice.Tensor']"
            },
            {
              "key": "[metadata] pkg.torch.onnx.fx_node",
              "value": "%slice_49 : [num_users=1] = call_function[target=torch.ops.aten.slice.Tensor](args = (%slice_48, 3, None, %add_4), kwargs = {})"
            },
            {
              "key": "[metadata] pkg.torch.onnx.name_scopes",
              "value": "['', 'model', 'model.layers.1', 'model.layers.1.self_attn', 'slice_49']"
            },
            {
              "key": "[metadata] pkg.torch.onnx.stack_trace",
              "value": "File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 688, in forward\n    outputs: BaseModelOutputWithPast = self.model(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 453, in forward\n    layer_outputs = decoder_layer(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 308, in forward\n    hidden_states, self_attn_weights = self.self_attn(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 265, in forward\n    attn_output, attn_weights = attention_interface("
            }
          ],
          "incomingEdges": [
            {
              "sourceNodeId": "node_Add_33",
              "sourceNodeOutputId": "0",
              "targetNodeInputId": "0"
            },
            {
              "sourceNodeId": "node_Constant_624",
              "sourceNodeOutputId": "0",
              "targetNodeInputId": "1"
            }
          ],
          "outputsMetadata": [
            {
              "id": "0",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "val_445"
                },
                {
                  "key": "tensor_shape",
                  "value": "INT64[1]"
                },
                {
                  "key": "param_name",
                  "value": "reshaped"
                }
              ]
            }
          ],
          "inputsMetadata": [
            {
              "id": "0",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "add_4"
                },
                {
                  "key": "tensor_shape",
                  "value": "INT64[]"
                },
                {
                  "key": "param_name",
                  "value": "data"
                }
              ]
            },
            {
              "id": "1",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "val_444"
                },
                {
                  "key": "tensor_shape",
                  "value": "INT64[1]"
                },
                {
                  "key": "param_name",
                  "value": "shape"
                }
              ]
            }
          ],
          "style": null,
          "config": null
        },
        {
          "id": "node_Constant_977",
          "label": "Constant",
          "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.1: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.1.self_attn: transformers.models.llama.modeling_llama.LlamaAttention/slice_49: aten.slice.Tensor",
          "subgraphIds": [],
          "attrs": [
            {
              "key": "__value",
              "value": "[3]"
            },
            {
              "key": "value",
              "value": "TensorProtoTensor<INT64,[1]>(array([3]), name='val_448')"
            }
          ],
          "incomingEdges": [],
          "outputsMetadata": [
            {
              "id": "0",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "val_448"
                },
                {
                  "key": "tensor_shape",
                  "value": "INT64[1]"
                },
                {
                  "key": "param_name",
                  "value": "output"
                }
              ]
            }
          ],
          "inputsMetadata": [],
          "style": null,
          "config": null
        },
        {
          "id": "node_Constant_629",
          "label": "Constant",
          "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.1: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.1.self_attn: transformers.models.llama.modeling_llama.LlamaAttention/slice_49: aten.slice.Tensor",
          "subgraphIds": [],
          "attrs": [
            {
              "key": "__value",
              "value": "[1]"
            },
            {
              "key": "value_ints",
              "value": "[1]"
            },
            {
              "key": "[metadata] namespace",
              "value": ": transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.1: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.1.self_attn: transformers.models.llama.modeling_llama.LlamaAttention/slice_49: aten.slice.Tensor"
            },
            {
              "key": "[metadata] pkg.torch.onnx.class_hierarchy",
              "value": "['transformers.models.llama.modeling_llama.LlamaForCausalLM', 'transformers.models.llama.modeling_llama.LlamaModel', 'transformers.models.llama.modeling_llama.LlamaDecoderLayer', 'transformers.models.llama.modeling_llama.LlamaAttention', 'aten.slice.Tensor']"
            },
            {
              "key": "[metadata] pkg.torch.onnx.fx_node",
              "value": "%slice_49 : [num_users=1] = call_function[target=torch.ops.aten.slice.Tensor](args = (%slice_48, 3, None, %add_4), kwargs = {})"
            },
            {
              "key": "[metadata] pkg.torch.onnx.name_scopes",
              "value": "['', 'model', 'model.layers.1', 'model.layers.1.self_attn', 'slice_49']"
            },
            {
              "key": "[metadata] pkg.torch.onnx.stack_trace",
              "value": "File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 688, in forward\n    outputs: BaseModelOutputWithPast = self.model(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 453, in forward\n    layer_outputs = decoder_layer(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 308, in forward\n    hidden_states, self_attn_weights = self.self_attn(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 265, in forward\n    attn_output, attn_weights = attention_interface("
            }
          ],
          "incomingEdges": [],
          "outputsMetadata": [
            {
              "id": "0",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "val_449"
                },
                {
                  "key": "tensor_shape",
                  "value": "INT64[1]"
                },
                {
                  "key": "param_name",
                  "value": "output"
                }
              ]
            }
          ],
          "inputsMetadata": [],
          "style": null,
          "config": null
        },
        {
          "id": "node_Slice_630",
          "label": "Slice",
          "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.1: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.1.self_attn: transformers.models.llama.modeling_llama.LlamaAttention/slice_49: aten.slice.Tensor",
          "subgraphIds": [],
          "attrs": [
            {
              "key": "[metadata] namespace",
              "value": ": transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.1: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.1.self_attn: transformers.models.llama.modeling_llama.LlamaAttention/slice_49: aten.slice.Tensor"
            },
            {
              "key": "[metadata] pkg.torch.onnx.class_hierarchy",
              "value": "['transformers.models.llama.modeling_llama.LlamaForCausalLM', 'transformers.models.llama.modeling_llama.LlamaModel', 'transformers.models.llama.modeling_llama.LlamaDecoderLayer', 'transformers.models.llama.modeling_llama.LlamaAttention', 'aten.slice.Tensor']"
            },
            {
              "key": "[metadata] pkg.torch.onnx.fx_node",
              "value": "%slice_49 : [num_users=1] = call_function[target=torch.ops.aten.slice.Tensor](args = (%slice_48, 3, None, %add_4), kwargs = {})"
            },
            {
              "key": "[metadata] pkg.torch.onnx.name_scopes",
              "value": "['', 'model', 'model.layers.1', 'model.layers.1.self_attn', 'slice_49']"
            },
            {
              "key": "[metadata] pkg.torch.onnx.stack_trace",
              "value": "File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 688, in forward\n    outputs: BaseModelOutputWithPast = self.model(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 453, in forward\n    layer_outputs = decoder_layer(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 308, in forward\n    hidden_states, self_attn_weights = self.self_attn(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 265, in forward\n    attn_output, attn_weights = attention_interface("
            }
          ],
          "incomingEdges": [
            {
              "sourceNodeId": "node_ScatterND_279",
              "sourceNodeOutputId": "0",
              "targetNodeInputId": "0"
            },
            {
              "sourceNodeId": "node_Constant_622",
              "sourceNodeOutputId": "0",
              "targetNodeInputId": "1"
            },
            {
              "sourceNodeId": "node_Reshape_625",
              "sourceNodeOutputId": "0",
              "targetNodeInputId": "2"
            },
            {
              "sourceNodeId": "node_Constant_977",
              "sourceNodeOutputId": "0",
              "targetNodeInputId": "3"
            },
            {
              "sourceNodeId": "node_Constant_629",
              "sourceNodeOutputId": "0",
              "targetNodeInputId": "4"
            }
          ],
          "outputsMetadata": [
            {
              "id": "0",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "slice_49"
                },
                {
                  "key": "tensor_shape",
                  "value": "FLOAT[s34,1,s16,s16 + s17]"
                },
                {
                  "key": "param_name",
                  "value": "output"
                }
              ]
            }
          ],
          "inputsMetadata": [
            {
              "id": "0",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "slice_scatter_2"
                },
                {
                  "key": "tensor_shape",
                  "value": "FLOAT[s34,1,s16,s16 + s17]"
                },
                {
                  "key": "param_name",
                  "value": "data"
                }
              ]
            },
            {
              "id": "1",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "val_442"
                },
                {
                  "key": "tensor_shape",
                  "value": "INT64[1]"
                },
                {
                  "key": "param_name",
                  "value": "starts"
                }
              ]
            },
            {
              "id": "2",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "val_445"
                },
                {
                  "key": "tensor_shape",
                  "value": "INT64[1]"
                },
                {
                  "key": "param_name",
                  "value": "ends"
                }
              ]
            },
            {
              "id": "3",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "val_448"
                },
                {
                  "key": "tensor_shape",
                  "value": "INT64[1]"
                },
                {
                  "key": "param_name",
                  "value": "axes"
                }
              ]
            },
            {
              "id": "4",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "val_449"
                },
                {
                  "key": "tensor_shape",
                  "value": "INT64[1]"
                },
                {
                  "key": "param_name",
                  "value": "steps"
                }
              ]
            }
          ],
          "style": null,
          "config": null
        },
        {
          "id": "node_Add_631",
          "label": "Add",
          "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.1: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.1.self_attn: transformers.models.llama.modeling_llama.LlamaAttention/add_658: aten.add.Tensor",
          "subgraphIds": [],
          "attrs": [
            {
              "key": "[metadata] namespace",
              "value": ": transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.1: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.1.self_attn: transformers.models.llama.modeling_llama.LlamaAttention/add_658: aten.add.Tensor"
            },
            {
              "key": "[metadata] pkg.torch.onnx.class_hierarchy",
              "value": "['transformers.models.llama.modeling_llama.LlamaForCausalLM', 'transformers.models.llama.modeling_llama.LlamaModel', 'transformers.models.llama.modeling_llama.LlamaDecoderLayer', 'transformers.models.llama.modeling_llama.LlamaAttention', 'aten.add.Tensor']"
            },
            {
              "key": "[metadata] pkg.torch.onnx.fx_node",
              "value": "%add_658 : [num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%mul_493, %slice_49), kwargs = {})"
            },
            {
              "key": "[metadata] pkg.torch.onnx.name_scopes",
              "value": "['', 'model', 'model.layers.1', 'model.layers.1.self_attn', 'add_658']"
            },
            {
              "key": "[metadata] pkg.torch.onnx.stack_trace",
              "value": "File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 688, in forward\n    outputs: BaseModelOutputWithPast = self.model(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 453, in forward\n    layer_outputs = decoder_layer(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 308, in forward\n    hidden_states, self_attn_weights = self.self_attn(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 265, in forward\n    attn_output, attn_weights = attention_interface("
            }
          ],
          "incomingEdges": [
            {
              "sourceNodeId": "node_Mul_600",
              "sourceNodeOutputId": "0",
              "targetNodeInputId": "0"
            },
            {
              "sourceNodeId": "node_Slice_630",
              "sourceNodeOutputId": "0",
              "targetNodeInputId": "1"
            }
          ],
          "outputsMetadata": [
            {
              "id": "0",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "add_658"
                },
                {
                  "key": "tensor_shape",
                  "value": "FLOAT[s34,32,s16,s16 + s17]"
                },
                {
                  "key": "param_name",
                  "value": "C"
                }
              ]
            }
          ],
          "inputsMetadata": [
            {
              "id": "0",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "mul_493"
                },
                {
                  "key": "tensor_shape",
                  "value": "FLOAT[s34,32,s16,s16 + s17]"
                },
                {
                  "key": "param_name",
                  "value": "A"
                }
              ]
            },
            {
              "id": "1",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "slice_49"
                },
                {
                  "key": "tensor_shape",
                  "value": "FLOAT[s34,1,s16,s16 + s17]"
                },
                {
                  "key": "param_name",
                  "value": "B"
                }
              ]
            }
          ],
          "style": null,
          "config": null
        },
        {
          "id": "node_Softmax_632",
          "label": "Softmax",
          "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.1: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.1.self_attn: transformers.models.llama.modeling_llama.LlamaAttention/softmax_1: aten.softmax.int",
          "subgraphIds": [],
          "attrs": [
            {
              "key": "axis",
              "value": "-1"
            },
            {
              "key": "[metadata] namespace",
              "value": ": transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.1: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.1.self_attn: transformers.models.llama.modeling_llama.LlamaAttention/softmax_1: aten.softmax.int"
            },
            {
              "key": "[metadata] pkg.torch.onnx.class_hierarchy",
              "value": "['transformers.models.llama.modeling_llama.LlamaForCausalLM', 'transformers.models.llama.modeling_llama.LlamaModel', 'transformers.models.llama.modeling_llama.LlamaDecoderLayer', 'transformers.models.llama.modeling_llama.LlamaAttention', 'aten.softmax.int']"
            },
            {
              "key": "[metadata] pkg.torch.onnx.fx_node",
              "value": "%softmax_1 : [num_users=1] = call_function[target=torch.ops.aten.softmax.int](args = (%add_658, -1, torch.float32), kwargs = {})"
            },
            {
              "key": "[metadata] pkg.torch.onnx.name_scopes",
              "value": "['', 'model', 'model.layers.1', 'model.layers.1.self_attn', 'softmax_1']"
            },
            {
              "key": "[metadata] pkg.torch.onnx.stack_trace",
              "value": "File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 688, in forward\n    outputs: BaseModelOutputWithPast = self.model(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 453, in forward\n    layer_outputs = decoder_layer(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 308, in forward\n    hidden_states, self_attn_weights = self.self_attn(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 265, in forward\n    attn_output, attn_weights = attention_interface("
            }
          ],
          "incomingEdges": [
            {
              "sourceNodeId": "node_Add_631",
              "sourceNodeOutputId": "0",
              "targetNodeInputId": "0"
            }
          ],
          "outputsMetadata": [
            {
              "id": "0",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "val_450"
                },
                {
                  "key": "tensor_shape",
                  "value": "FLOAT[s34,32,s16,s16 + s17]"
                },
                {
                  "key": "param_name",
                  "value": "output"
                }
              ]
            }
          ],
          "inputsMetadata": [
            {
              "id": "0",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "add_658"
                },
                {
                  "key": "tensor_shape",
                  "value": "FLOAT[s34,32,s16,s16 + s17]"
                },
                {
                  "key": "param_name",
                  "value": "input"
                }
              ]
            }
          ],
          "style": null,
          "config": null
        },
        {
          "id": "node_MatMul_635",
          "label": "MatMul",
          "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.1: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.1.self_attn: transformers.models.llama.modeling_llama.LlamaAttention/matmul_4: aten.matmul.default",
          "subgraphIds": [],
          "attrs": [
            {
              "key": "[metadata] namespace",
              "value": ": transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.1: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.1.self_attn: transformers.models.llama.modeling_llama.LlamaAttention/matmul_4: aten.matmul.default"
            },
            {
              "key": "[metadata] pkg.torch.onnx.class_hierarchy",
              "value": "['transformers.models.llama.modeling_llama.LlamaForCausalLM', 'transformers.models.llama.modeling_llama.LlamaModel', 'transformers.models.llama.modeling_llama.LlamaDecoderLayer', 'transformers.models.llama.modeling_llama.LlamaAttention', 'aten.matmul.default']"
            },
            {
              "key": "[metadata] pkg.torch.onnx.fx_node",
              "value": "%matmul_4 : [num_users=1] = call_function[target=torch.ops.aten.matmul.default](args = (%clone_3, %cat_8), kwargs = {})"
            },
            {
              "key": "[metadata] pkg.torch.onnx.name_scopes",
              "value": "['', 'model', 'model.layers.1', 'model.layers.1.self_attn', 'matmul_4']"
            },
            {
              "key": "[metadata] pkg.torch.onnx.stack_trace",
              "value": "File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 688, in forward\n    outputs: BaseModelOutputWithPast = self.model(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 453, in forward\n    layer_outputs = decoder_layer(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 308, in forward\n    hidden_states, self_attn_weights = self.self_attn(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 265, in forward\n    attn_output, attn_weights = attention_interface("
            }
          ],
          "incomingEdges": [
            {
              "sourceNodeId": "node_Softmax_632",
              "sourceNodeOutputId": "0",
              "targetNodeInputId": "0"
            },
            {
              "sourceNodeId": "node_Concat_597",
              "sourceNodeOutputId": "0",
              "targetNodeInputId": "1"
            }
          ],
          "outputsMetadata": [
            {
              "id": "0",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "matmul_4"
                },
                {
                  "key": "tensor_shape",
                  "value": "FLOAT[s34,32,s16,64]"
                },
                {
                  "key": "param_name",
                  "value": "Y"
                }
              ]
            }
          ],
          "inputsMetadata": [
            {
              "id": "0",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "val_450"
                },
                {
                  "key": "tensor_shape",
                  "value": "FLOAT[s34,32,s16,s16 + s17]"
                },
                {
                  "key": "param_name",
                  "value": "A"
                }
              ]
            },
            {
              "id": "1",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "cat_8"
                },
                {
                  "key": "tensor_shape",
                  "value": "FLOAT[s34,32,s16 + s17,64]"
                },
                {
                  "key": "[metadata] pkg.torch.export.graph_signature.OutputSpec.kind",
                  "value": "USER_OUTPUT"
                },
                {
                  "key": "param_name",
                  "value": "B"
                }
              ]
            }
          ],
          "style": null,
          "config": null
        },
        {
          "id": "node_Transpose_636",
          "label": "Transpose",
          "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.1: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.1.self_attn: transformers.models.llama.modeling_llama.LlamaAttention/transpose_10: aten.transpose.int",
          "subgraphIds": [],
          "attrs": [
            {
              "key": "perm",
              "value": "[0, 2, 1, 3]"
            },
            {
              "key": "[metadata] namespace",
              "value": ": transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.1: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.1.self_attn: transformers.models.llama.modeling_llama.LlamaAttention/transpose_10: aten.transpose.int"
            },
            {
              "key": "[metadata] pkg.torch.onnx.class_hierarchy",
              "value": "['transformers.models.llama.modeling_llama.LlamaForCausalLM', 'transformers.models.llama.modeling_llama.LlamaModel', 'transformers.models.llama.modeling_llama.LlamaDecoderLayer', 'transformers.models.llama.modeling_llama.LlamaAttention', 'aten.transpose.int']"
            },
            {
              "key": "[metadata] pkg.torch.onnx.fx_node",
              "value": "%transpose_10 : [num_users=1] = call_function[target=torch.ops.aten.transpose.int](args = (%matmul_4, 1, 2), kwargs = {})"
            },
            {
              "key": "[metadata] pkg.torch.onnx.name_scopes",
              "value": "['', 'model', 'model.layers.1', 'model.layers.1.self_attn', 'transpose_10']"
            },
            {
              "key": "[metadata] pkg.torch.onnx.stack_trace",
              "value": "File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 688, in forward\n    outputs: BaseModelOutputWithPast = self.model(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 453, in forward\n    layer_outputs = decoder_layer(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 308, in forward\n    hidden_states, self_attn_weights = self.self_attn(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 265, in forward\n    attn_output, attn_weights = attention_interface("
            }
          ],
          "incomingEdges": [
            {
              "sourceNodeId": "node_MatMul_635",
              "sourceNodeOutputId": "0",
              "targetNodeInputId": "0"
            }
          ],
          "outputsMetadata": [
            {
              "id": "0",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "transpose_10"
                },
                {
                  "key": "tensor_shape",
                  "value": "FLOAT[s34,s16,32,64]"
                },
                {
                  "key": "param_name",
                  "value": "transposed"
                }
              ]
            }
          ],
          "inputsMetadata": [
            {
              "id": "0",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "matmul_4"
                },
                {
                  "key": "tensor_shape",
                  "value": "FLOAT[s34,32,s16,64]"
                },
                {
                  "key": "param_name",
                  "value": "data"
                }
              ]
            }
          ],
          "style": null,
          "config": null
        },
        {
          "id": "node_Concat_642",
          "label": "Concat",
          "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.1: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.1.self_attn: transformers.models.llama.modeling_llama.LlamaAttention/view_8: aten.view.default",
          "subgraphIds": [],
          "attrs": [
            {
              "key": "axis",
              "value": "0"
            },
            {
              "key": "[metadata] namespace",
              "value": ": transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.1: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.1.self_attn: transformers.models.llama.modeling_llama.LlamaAttention/view_8: aten.view.default"
            },
            {
              "key": "[metadata] pkg.torch.onnx.class_hierarchy",
              "value": "['transformers.models.llama.modeling_llama.LlamaForCausalLM', 'transformers.models.llama.modeling_llama.LlamaModel', 'transformers.models.llama.modeling_llama.LlamaDecoderLayer', 'transformers.models.llama.modeling_llama.LlamaAttention', 'aten.view.default']"
            },
            {
              "key": "[metadata] pkg.torch.onnx.fx_node",
              "value": "%view_8 : [num_users=1] = call_function[target=torch.ops.aten.view.default](args = (%clone_4, [%sym_size_int_68, %sym_size_int_61, -1]), kwargs = {})"
            },
            {
              "key": "[metadata] pkg.torch.onnx.name_scopes",
              "value": "['', 'model', 'model.layers.1', 'model.layers.1.self_attn', 'view_8']"
            },
            {
              "key": "[metadata] pkg.torch.onnx.stack_trace",
              "value": "File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 688, in forward\n    outputs: BaseModelOutputWithPast = self.model(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 453, in forward\n    layer_outputs = decoder_layer(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 308, in forward\n    hidden_states, self_attn_weights = self.self_attn(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 276, in forward\n    attn_output = attn_output.reshape(*input_shape, -1).contiguous()"
            }
          ],
          "incomingEdges": [
            {
              "sourceNodeId": "node_Shape_12",
              "sourceNodeOutputId": "0",
              "targetNodeInputId": "0"
            },
            {
              "sourceNodeId": "node_Shape_2",
              "sourceNodeOutputId": "0",
              "targetNodeInputId": "1"
            },
            {
              "sourceNodeId": "node_Constant_90",
              "sourceNodeOutputId": "0",
              "targetNodeInputId": "2"
            }
          ],
          "outputsMetadata": [
            {
              "id": "0",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "val_455"
                },
                {
                  "key": "tensor_shape",
                  "value": "INT64[3]"
                },
                {
                  "key": "param_name",
                  "value": "concat_result"
                }
              ]
            }
          ],
          "inputsMetadata": [
            {
              "id": "0",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "val_6"
                },
                {
                  "key": "tensor_shape",
                  "value": "INT64[1]"
                },
                {
                  "key": "param_name",
                  "value": "inputs"
                }
              ]
            },
            {
              "id": "1",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "val_1"
                },
                {
                  "key": "tensor_shape",
                  "value": "INT64[1]"
                },
                {
                  "key": "param_name",
                  "value": "inputs"
                }
              ]
            },
            {
              "id": "2",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "val_53"
                },
                {
                  "key": "tensor_shape",
                  "value": "INT64[1]"
                },
                {
                  "key": "param_name",
                  "value": "inputs"
                }
              ]
            }
          ],
          "style": null,
          "config": null
        },
        {
          "id": "node_Reshape_644",
          "label": "Reshape",
          "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.1: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.1.self_attn: transformers.models.llama.modeling_llama.LlamaAttention/view_8: aten.view.default",
          "subgraphIds": [],
          "attrs": [
            {
              "key": "allowzero",
              "value": "1"
            },
            {
              "key": "[metadata] namespace",
              "value": ": transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.1: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.1.self_attn: transformers.models.llama.modeling_llama.LlamaAttention/view_8: aten.view.default"
            },
            {
              "key": "[metadata] pkg.torch.onnx.class_hierarchy",
              "value": "['transformers.models.llama.modeling_llama.LlamaForCausalLM', 'transformers.models.llama.modeling_llama.LlamaModel', 'transformers.models.llama.modeling_llama.LlamaDecoderLayer', 'transformers.models.llama.modeling_llama.LlamaAttention', 'aten.view.default']"
            },
            {
              "key": "[metadata] pkg.torch.onnx.fx_node",
              "value": "%view_8 : [num_users=1] = call_function[target=torch.ops.aten.view.default](args = (%clone_4, [%sym_size_int_68, %sym_size_int_61, -1]), kwargs = {})"
            },
            {
              "key": "[metadata] pkg.torch.onnx.name_scopes",
              "value": "['', 'model', 'model.layers.1', 'model.layers.1.self_attn', 'view_8']"
            },
            {
              "key": "[metadata] pkg.torch.onnx.stack_trace",
              "value": "File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 688, in forward\n    outputs: BaseModelOutputWithPast = self.model(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 453, in forward\n    layer_outputs = decoder_layer(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 308, in forward\n    hidden_states, self_attn_weights = self.self_attn(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 276, in forward\n    attn_output = attn_output.reshape(*input_shape, -1).contiguous()"
            }
          ],
          "incomingEdges": [
            {
              "sourceNodeId": "node_Transpose_636",
              "sourceNodeOutputId": "0",
              "targetNodeInputId": "0"
            },
            {
              "sourceNodeId": "node_Concat_642",
              "sourceNodeOutputId": "0",
              "targetNodeInputId": "1"
            }
          ],
          "outputsMetadata": [
            {
              "id": "0",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "view_8"
                },
                {
                  "key": "tensor_shape",
                  "value": "FLOAT[s34,s16,2048]"
                },
                {
                  "key": "param_name",
                  "value": "reshaped"
                }
              ]
            }
          ],
          "inputsMetadata": [
            {
              "id": "0",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "transpose_10"
                },
                {
                  "key": "tensor_shape",
                  "value": "FLOAT[s34,s16,32,64]"
                },
                {
                  "key": "param_name",
                  "value": "data"
                }
              ]
            },
            {
              "id": "1",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "val_455"
                },
                {
                  "key": "tensor_shape",
                  "value": "INT64[3]"
                },
                {
                  "key": "param_name",
                  "value": "shape"
                }
              ]
            }
          ],
          "style": null,
          "config": null
        },
        {
          "id": "node_Transpose_645",
          "label": "Transpose",
          "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.1: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.1.self_attn: transformers.models.llama.modeling_llama.LlamaAttention/model.layers.1.self_attn.o_proj: torch.nn.modules.linear.Linear/linear_10: aten.linear.default",
          "subgraphIds": [],
          "attrs": [
            {
              "key": "perm",
              "value": "[1, 0]"
            },
            {
              "key": "[metadata] namespace",
              "value": ": transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.1: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.1.self_attn: transformers.models.llama.modeling_llama.LlamaAttention/model.layers.1.self_attn.o_proj: torch.nn.modules.linear.Linear/linear_10: aten.linear.default"
            },
            {
              "key": "[metadata] pkg.torch.onnx.class_hierarchy",
              "value": "['transformers.models.llama.modeling_llama.LlamaForCausalLM', 'transformers.models.llama.modeling_llama.LlamaModel', 'transformers.models.llama.modeling_llama.LlamaDecoderLayer', 'transformers.models.llama.modeling_llama.LlamaAttention', 'torch.nn.modules.linear.Linear', 'aten.linear.default']"
            },
            {
              "key": "[metadata] pkg.torch.onnx.fx_node",
              "value": "%linear_10 : [num_users=1] = call_function[target=torch.ops.aten.linear.default](args = (%view_8, %p_model_layers_1_self_attn_o_proj_weight), kwargs = {})"
            },
            {
              "key": "[metadata] pkg.torch.onnx.name_scopes",
              "value": "['', 'model', 'model.layers.1', 'model.layers.1.self_attn', 'model.layers.1.self_attn.o_proj', 'linear_10']"
            },
            {
              "key": "[metadata] pkg.torch.onnx.stack_trace",
              "value": "File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 688, in forward\n    outputs: BaseModelOutputWithPast = self.model(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 453, in forward\n    layer_outputs = decoder_layer(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 308, in forward\n    hidden_states, self_attn_weights = self.self_attn(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 277, in forward\n    attn_output = self.o_proj(attn_output)\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/torch/nn/modules/linear.py\", line 125, in forward\n    return F.linear(input, self.weight, self.bias)"
            }
          ],
          "incomingEdges": [
            {
              "sourceNodeId": "[value] model.layers.1.self_attn.o_proj.weight",
              "sourceNodeOutputId": "0",
              "targetNodeInputId": "0"
            }
          ],
          "outputsMetadata": [
            {
              "id": "0",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "val_457"
                },
                {
                  "key": "tensor_shape",
                  "value": "FLOAT[2048,2048]"
                },
                {
                  "key": "param_name",
                  "value": "transposed"
                }
              ]
            }
          ],
          "inputsMetadata": [
            {
              "id": "0",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "model.layers.1.self_attn.o_proj.weight"
                },
                {
                  "key": "tensor_shape",
                  "value": "FLOAT[2048,2048]"
                },
                {
                  "key": "param_name",
                  "value": "data"
                }
              ]
            }
          ],
          "style": null,
          "config": null
        },
        {
          "id": "node_MatMul_646",
          "label": "MatMul",
          "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.1: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.1.self_attn: transformers.models.llama.modeling_llama.LlamaAttention/model.layers.1.self_attn.o_proj: torch.nn.modules.linear.Linear/linear_10: aten.linear.default",
          "subgraphIds": [],
          "attrs": [
            {
              "key": "[metadata] namespace",
              "value": ": transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.1: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.1.self_attn: transformers.models.llama.modeling_llama.LlamaAttention/model.layers.1.self_attn.o_proj: torch.nn.modules.linear.Linear/linear_10: aten.linear.default"
            },
            {
              "key": "[metadata] pkg.torch.onnx.class_hierarchy",
              "value": "['transformers.models.llama.modeling_llama.LlamaForCausalLM', 'transformers.models.llama.modeling_llama.LlamaModel', 'transformers.models.llama.modeling_llama.LlamaDecoderLayer', 'transformers.models.llama.modeling_llama.LlamaAttention', 'torch.nn.modules.linear.Linear', 'aten.linear.default']"
            },
            {
              "key": "[metadata] pkg.torch.onnx.fx_node",
              "value": "%linear_10 : [num_users=1] = call_function[target=torch.ops.aten.linear.default](args = (%view_8, %p_model_layers_1_self_attn_o_proj_weight), kwargs = {})"
            },
            {
              "key": "[metadata] pkg.torch.onnx.name_scopes",
              "value": "['', 'model', 'model.layers.1', 'model.layers.1.self_attn', 'model.layers.1.self_attn.o_proj', 'linear_10']"
            },
            {
              "key": "[metadata] pkg.torch.onnx.stack_trace",
              "value": "File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 688, in forward\n    outputs: BaseModelOutputWithPast = self.model(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 453, in forward\n    layer_outputs = decoder_layer(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 308, in forward\n    hidden_states, self_attn_weights = self.self_attn(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 277, in forward\n    attn_output = self.o_proj(attn_output)\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/torch/nn/modules/linear.py\", line 125, in forward\n    return F.linear(input, self.weight, self.bias)"
            }
          ],
          "incomingEdges": [
            {
              "sourceNodeId": "node_Reshape_644",
              "sourceNodeOutputId": "0",
              "targetNodeInputId": "0"
            },
            {
              "sourceNodeId": "node_Transpose_645",
              "sourceNodeOutputId": "0",
              "targetNodeInputId": "1"
            }
          ],
          "outputsMetadata": [
            {
              "id": "0",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "linear_10"
                },
                {
                  "key": "tensor_shape",
                  "value": "FLOAT[s34,s16,2048]"
                },
                {
                  "key": "param_name",
                  "value": "Y"
                }
              ]
            }
          ],
          "inputsMetadata": [
            {
              "id": "0",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "view_8"
                },
                {
                  "key": "tensor_shape",
                  "value": "FLOAT[s34,s16,2048]"
                },
                {
                  "key": "param_name",
                  "value": "A"
                }
              ]
            },
            {
              "id": "1",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "val_457"
                },
                {
                  "key": "tensor_shape",
                  "value": "FLOAT[2048,2048]"
                },
                {
                  "key": "param_name",
                  "value": "B"
                }
              ]
            }
          ],
          "style": null,
          "config": null
        },
        {
          "id": "node_Add_647",
          "label": "Add",
          "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.1: transformers.models.llama.modeling_llama.LlamaDecoderLayer/add_697: aten.add.Tensor",
          "subgraphIds": [],
          "attrs": [
            {
              "key": "[metadata] namespace",
              "value": ": transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.1: transformers.models.llama.modeling_llama.LlamaDecoderLayer/add_697: aten.add.Tensor"
            },
            {
              "key": "[metadata] pkg.torch.onnx.class_hierarchy",
              "value": "['transformers.models.llama.modeling_llama.LlamaForCausalLM', 'transformers.models.llama.modeling_llama.LlamaModel', 'transformers.models.llama.modeling_llama.LlamaDecoderLayer', 'aten.add.Tensor']"
            },
            {
              "key": "[metadata] pkg.torch.onnx.fx_node",
              "value": "%add_697 : [num_users=3] = call_function[target=torch.ops.aten.add.Tensor](args = (%add_463, %linear_10), kwargs = {})"
            },
            {
              "key": "[metadata] pkg.torch.onnx.name_scopes",
              "value": "['', 'model', 'model.layers.1', 'add_697']"
            },
            {
              "key": "[metadata] pkg.torch.onnx.stack_trace",
              "value": "File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 688, in forward\n    outputs: BaseModelOutputWithPast = self.model(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 453, in forward\n    layer_outputs = decoder_layer(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 319, in forward\n    hidden_states = residual + hidden_states"
            }
          ],
          "incomingEdges": [
            {
              "sourceNodeId": "node_Add_499",
              "sourceNodeOutputId": "0",
              "targetNodeInputId": "0"
            },
            {
              "sourceNodeId": "node_MatMul_646",
              "sourceNodeOutputId": "0",
              "targetNodeInputId": "1"
            }
          ],
          "outputsMetadata": [
            {
              "id": "0",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "add_697"
                },
                {
                  "key": "tensor_shape",
                  "value": "FLOAT[s34,s16,2048]"
                },
                {
                  "key": "param_name",
                  "value": "C"
                }
              ]
            }
          ],
          "inputsMetadata": [
            {
              "id": "0",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "add_463"
                },
                {
                  "key": "tensor_shape",
                  "value": "FLOAT[s34,s16,2048]"
                },
                {
                  "key": "param_name",
                  "value": "A"
                }
              ]
            },
            {
              "id": "1",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "linear_10"
                },
                {
                  "key": "tensor_shape",
                  "value": "FLOAT[s34,s16,2048]"
                },
                {
                  "key": "param_name",
                  "value": "B"
                }
              ]
            }
          ],
          "style": null,
          "config": null
        },
        {
          "id": "node_Constant_648",
          "label": "Constant",
          "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.1: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.1.post_attention_layernorm: transformers.models.llama.modeling_llama.LlamaRMSNorm/pow_4: aten.pow.Tensor_Scalar",
          "subgraphIds": [],
          "attrs": [
            {
              "key": "__value",
              "value": "2.0"
            },
            {
              "key": "value",
              "value": "TensorProtoTensor<FLOAT,[]>(array(2., dtype=float32), name='')"
            },
            {
              "key": "[metadata] namespace",
              "value": ": transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.1: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.1.post_attention_layernorm: transformers.models.llama.modeling_llama.LlamaRMSNorm/pow_4: aten.pow.Tensor_Scalar"
            },
            {
              "key": "[metadata] pkg.torch.onnx.class_hierarchy",
              "value": "['transformers.models.llama.modeling_llama.LlamaForCausalLM', 'transformers.models.llama.modeling_llama.LlamaModel', 'transformers.models.llama.modeling_llama.LlamaDecoderLayer', 'transformers.models.llama.modeling_llama.LlamaRMSNorm', 'aten.pow.Tensor_Scalar']"
            },
            {
              "key": "[metadata] pkg.torch.onnx.fx_node",
              "value": "%pow_4 : [num_users=1] = call_function[target=torch.ops.aten.pow.Tensor_Scalar](args = (%add_697, 2), kwargs = {})"
            },
            {
              "key": "[metadata] pkg.torch.onnx.name_scopes",
              "value": "['', 'model', 'model.layers.1', 'model.layers.1.post_attention_layernorm', 'pow_4']"
            },
            {
              "key": "[metadata] pkg.torch.onnx.stack_trace",
              "value": "File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 688, in forward\n    outputs: BaseModelOutputWithPast = self.model(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 453, in forward\n    layer_outputs = decoder_layer(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 323, in forward\n    hidden_states = self.post_attention_layernorm(hidden_states)\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 71, in forward\n    variance = hidden_states.pow(2).mean(-1, keepdim=True)"
            }
          ],
          "incomingEdges": [],
          "outputsMetadata": [
            {
              "id": "0",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "val_458"
                },
                {
                  "key": "tensor_shape",
                  "value": "FLOAT[]"
                },
                {
                  "key": "param_name",
                  "value": "output"
                }
              ]
            }
          ],
          "inputsMetadata": [],
          "style": null,
          "config": null
        },
        {
          "id": "node_Pow_649",
          "label": "Pow",
          "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.1: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.1.post_attention_layernorm: transformers.models.llama.modeling_llama.LlamaRMSNorm/pow_4: aten.pow.Tensor_Scalar",
          "subgraphIds": [],
          "attrs": [
            {
              "key": "[metadata] namespace",
              "value": ": transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.1: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.1.post_attention_layernorm: transformers.models.llama.modeling_llama.LlamaRMSNorm/pow_4: aten.pow.Tensor_Scalar"
            },
            {
              "key": "[metadata] pkg.torch.onnx.class_hierarchy",
              "value": "['transformers.models.llama.modeling_llama.LlamaForCausalLM', 'transformers.models.llama.modeling_llama.LlamaModel', 'transformers.models.llama.modeling_llama.LlamaDecoderLayer', 'transformers.models.llama.modeling_llama.LlamaRMSNorm', 'aten.pow.Tensor_Scalar']"
            },
            {
              "key": "[metadata] pkg.torch.onnx.fx_node",
              "value": "%pow_4 : [num_users=1] = call_function[target=torch.ops.aten.pow.Tensor_Scalar](args = (%add_697, 2), kwargs = {})"
            },
            {
              "key": "[metadata] pkg.torch.onnx.name_scopes",
              "value": "['', 'model', 'model.layers.1', 'model.layers.1.post_attention_layernorm', 'pow_4']"
            },
            {
              "key": "[metadata] pkg.torch.onnx.stack_trace",
              "value": "File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 688, in forward\n    outputs: BaseModelOutputWithPast = self.model(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 453, in forward\n    layer_outputs = decoder_layer(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 323, in forward\n    hidden_states = self.post_attention_layernorm(hidden_states)\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 71, in forward\n    variance = hidden_states.pow(2).mean(-1, keepdim=True)"
            }
          ],
          "incomingEdges": [
            {
              "sourceNodeId": "node_Add_647",
              "sourceNodeOutputId": "0",
              "targetNodeInputId": "0"
            },
            {
              "sourceNodeId": "node_Constant_648",
              "sourceNodeOutputId": "0",
              "targetNodeInputId": "1"
            }
          ],
          "outputsMetadata": [
            {
              "id": "0",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "pow_4"
                },
                {
                  "key": "tensor_shape",
                  "value": "FLOAT[s34,s16,2048]"
                },
                {
                  "key": "param_name",
                  "value": "Z"
                }
              ]
            }
          ],
          "inputsMetadata": [
            {
              "id": "0",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "add_697"
                },
                {
                  "key": "tensor_shape",
                  "value": "FLOAT[s34,s16,2048]"
                },
                {
                  "key": "param_name",
                  "value": "X"
                }
              ]
            },
            {
              "id": "1",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "val_458"
                },
                {
                  "key": "tensor_shape",
                  "value": "FLOAT[]"
                },
                {
                  "key": "param_name",
                  "value": "Y"
                }
              ]
            }
          ],
          "style": null,
          "config": null
        },
        {
          "id": "node_Constant_980",
          "label": "Constant",
          "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.1: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.1.post_attention_layernorm: transformers.models.llama.modeling_llama.LlamaRMSNorm/mean_3: aten.mean.dim",
          "subgraphIds": [],
          "attrs": [
            {
              "key": "__value",
              "value": "[-1]"
            },
            {
              "key": "value",
              "value": "TensorProtoTensor<INT64,[1]>(array([-1]), name='val_460')"
            }
          ],
          "incomingEdges": [],
          "outputsMetadata": [
            {
              "id": "0",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "val_460"
                },
                {
                  "key": "tensor_shape",
                  "value": "INT64[1]"
                },
                {
                  "key": "param_name",
                  "value": "output"
                }
              ]
            }
          ],
          "inputsMetadata": [],
          "style": null,
          "config": null
        },
        {
          "id": "node_ReduceMean_652",
          "label": "ReduceMean",
          "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.1: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.1.post_attention_layernorm: transformers.models.llama.modeling_llama.LlamaRMSNorm/mean_3: aten.mean.dim",
          "subgraphIds": [],
          "attrs": [
            {
              "key": "noop_with_empty_axes",
              "value": "0"
            },
            {
              "key": "keepdims",
              "value": "1"
            },
            {
              "key": "[metadata] namespace",
              "value": ": transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.1: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.1.post_attention_layernorm: transformers.models.llama.modeling_llama.LlamaRMSNorm/mean_3: aten.mean.dim"
            },
            {
              "key": "[metadata] pkg.torch.onnx.class_hierarchy",
              "value": "['transformers.models.llama.modeling_llama.LlamaForCausalLM', 'transformers.models.llama.modeling_llama.LlamaModel', 'transformers.models.llama.modeling_llama.LlamaDecoderLayer', 'transformers.models.llama.modeling_llama.LlamaRMSNorm', 'aten.mean.dim']"
            },
            {
              "key": "[metadata] pkg.torch.onnx.fx_node",
              "value": "%mean_3 : [num_users=1] = call_function[target=torch.ops.aten.mean.dim](args = (%pow_4, [-1], True), kwargs = {})"
            },
            {
              "key": "[metadata] pkg.torch.onnx.name_scopes",
              "value": "['', 'model', 'model.layers.1', 'model.layers.1.post_attention_layernorm', 'mean_3']"
            },
            {
              "key": "[metadata] pkg.torch.onnx.stack_trace",
              "value": "File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 688, in forward\n    outputs: BaseModelOutputWithPast = self.model(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 453, in forward\n    layer_outputs = decoder_layer(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 323, in forward\n    hidden_states = self.post_attention_layernorm(hidden_states)\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 71, in forward\n    variance = hidden_states.pow(2).mean(-1, keepdim=True)"
            }
          ],
          "incomingEdges": [
            {
              "sourceNodeId": "node_Pow_649",
              "sourceNodeOutputId": "0",
              "targetNodeInputId": "0"
            },
            {
              "sourceNodeId": "node_Constant_980",
              "sourceNodeOutputId": "0",
              "targetNodeInputId": "1"
            }
          ],
          "outputsMetadata": [
            {
              "id": "0",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "mean_3"
                },
                {
                  "key": "tensor_shape",
                  "value": "FLOAT[s34,s16,1]"
                },
                {
                  "key": "param_name",
                  "value": "reduced"
                }
              ]
            }
          ],
          "inputsMetadata": [
            {
              "id": "0",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "pow_4"
                },
                {
                  "key": "tensor_shape",
                  "value": "FLOAT[s34,s16,2048]"
                },
                {
                  "key": "param_name",
                  "value": "data"
                }
              ]
            },
            {
              "id": "1",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "val_460"
                },
                {
                  "key": "tensor_shape",
                  "value": "INT64[1]"
                },
                {
                  "key": "param_name",
                  "value": "axes"
                }
              ]
            }
          ],
          "style": null,
          "config": null
        },
        {
          "id": "node_Add_653",
          "label": "Add",
          "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.1: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.1.post_attention_layernorm: transformers.models.llama.modeling_llama.LlamaRMSNorm/add_710: aten.add.Tensor",
          "subgraphIds": [],
          "attrs": [
            {
              "key": "[metadata] namespace",
              "value": ": transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.1: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.1.post_attention_layernorm: transformers.models.llama.modeling_llama.LlamaRMSNorm/add_710: aten.add.Tensor"
            },
            {
              "key": "[metadata] pkg.torch.onnx.class_hierarchy",
              "value": "['transformers.models.llama.modeling_llama.LlamaForCausalLM', 'transformers.models.llama.modeling_llama.LlamaModel', 'transformers.models.llama.modeling_llama.LlamaDecoderLayer', 'transformers.models.llama.modeling_llama.LlamaRMSNorm', 'aten.add.Tensor']"
            },
            {
              "key": "[metadata] pkg.torch.onnx.fx_node",
              "value": "%add_710 : [num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%mean_3, 1e-05), kwargs = {})"
            },
            {
              "key": "[metadata] pkg.torch.onnx.name_scopes",
              "value": "['', 'model', 'model.layers.1', 'model.layers.1.post_attention_layernorm', 'add_710']"
            },
            {
              "key": "[metadata] pkg.torch.onnx.stack_trace",
              "value": "File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 688, in forward\n    outputs: BaseModelOutputWithPast = self.model(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 453, in forward\n    layer_outputs = decoder_layer(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 323, in forward\n    hidden_states = self.post_attention_layernorm(hidden_states)\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 72, in forward\n    hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)"
            }
          ],
          "incomingEdges": [
            {
              "sourceNodeId": "node_ReduceMean_652",
              "sourceNodeOutputId": "0",
              "targetNodeInputId": "0"
            },
            {
              "sourceNodeId": "node_Constant_333",
              "sourceNodeOutputId": "0",
              "targetNodeInputId": "1"
            }
          ],
          "outputsMetadata": [
            {
              "id": "0",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "add_710"
                },
                {
                  "key": "tensor_shape",
                  "value": "FLOAT[s34,s16,1]"
                },
                {
                  "key": "param_name",
                  "value": "C"
                }
              ]
            }
          ],
          "inputsMetadata": [
            {
              "id": "0",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "mean_3"
                },
                {
                  "key": "tensor_shape",
                  "value": "FLOAT[s34,s16,1]"
                },
                {
                  "key": "param_name",
                  "value": "A"
                }
              ]
            },
            {
              "id": "1",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "val_251"
                },
                {
                  "key": "tensor_shape",
                  "value": "FLOAT[]"
                },
                {
                  "key": "param_name",
                  "value": "B"
                }
              ]
            }
          ],
          "style": null,
          "config": null
        },
        {
          "id": "node_Sqrt_654",
          "label": "Sqrt",
          "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.1: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.1.post_attention_layernorm: transformers.models.llama.modeling_llama.LlamaRMSNorm/rsqrt_3: aten.rsqrt.default",
          "subgraphIds": [],
          "attrs": [
            {
              "key": "[metadata] namespace",
              "value": ": transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.1: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.1.post_attention_layernorm: transformers.models.llama.modeling_llama.LlamaRMSNorm/rsqrt_3: aten.rsqrt.default"
            },
            {
              "key": "[metadata] pkg.torch.onnx.class_hierarchy",
              "value": "['transformers.models.llama.modeling_llama.LlamaForCausalLM', 'transformers.models.llama.modeling_llama.LlamaModel', 'transformers.models.llama.modeling_llama.LlamaDecoderLayer', 'transformers.models.llama.modeling_llama.LlamaRMSNorm', 'aten.rsqrt.default']"
            },
            {
              "key": "[metadata] pkg.torch.onnx.fx_node",
              "value": "%rsqrt_3 : [num_users=1] = call_function[target=torch.ops.aten.rsqrt.default](args = (%add_710,), kwargs = {})"
            },
            {
              "key": "[metadata] pkg.torch.onnx.name_scopes",
              "value": "['', 'model', 'model.layers.1', 'model.layers.1.post_attention_layernorm', 'rsqrt_3']"
            },
            {
              "key": "[metadata] pkg.torch.onnx.stack_trace",
              "value": "File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 688, in forward\n    outputs: BaseModelOutputWithPast = self.model(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 453, in forward\n    layer_outputs = decoder_layer(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 323, in forward\n    hidden_states = self.post_attention_layernorm(hidden_states)\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 72, in forward\n    hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)"
            }
          ],
          "incomingEdges": [
            {
              "sourceNodeId": "node_Add_653",
              "sourceNodeOutputId": "0",
              "targetNodeInputId": "0"
            }
          ],
          "outputsMetadata": [
            {
              "id": "0",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "val_461"
                },
                {
                  "key": "tensor_shape",
                  "value": "FLOAT[s34,s16,1]"
                },
                {
                  "key": "param_name",
                  "value": "Y"
                }
              ]
            }
          ],
          "inputsMetadata": [
            {
              "id": "0",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "add_710"
                },
                {
                  "key": "tensor_shape",
                  "value": "FLOAT[s34,s16,1]"
                },
                {
                  "key": "param_name",
                  "value": "X"
                }
              ]
            }
          ],
          "style": null,
          "config": null
        },
        {
          "id": "node_Reciprocal_655",
          "label": "Reciprocal",
          "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.1: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.1.post_attention_layernorm: transformers.models.llama.modeling_llama.LlamaRMSNorm/rsqrt_3: aten.rsqrt.default",
          "subgraphIds": [],
          "attrs": [
            {
              "key": "[metadata] namespace",
              "value": ": transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.1: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.1.post_attention_layernorm: transformers.models.llama.modeling_llama.LlamaRMSNorm/rsqrt_3: aten.rsqrt.default"
            },
            {
              "key": "[metadata] pkg.torch.onnx.class_hierarchy",
              "value": "['transformers.models.llama.modeling_llama.LlamaForCausalLM', 'transformers.models.llama.modeling_llama.LlamaModel', 'transformers.models.llama.modeling_llama.LlamaDecoderLayer', 'transformers.models.llama.modeling_llama.LlamaRMSNorm', 'aten.rsqrt.default']"
            },
            {
              "key": "[metadata] pkg.torch.onnx.fx_node",
              "value": "%rsqrt_3 : [num_users=1] = call_function[target=torch.ops.aten.rsqrt.default](args = (%add_710,), kwargs = {})"
            },
            {
              "key": "[metadata] pkg.torch.onnx.name_scopes",
              "value": "['', 'model', 'model.layers.1', 'model.layers.1.post_attention_layernorm', 'rsqrt_3']"
            },
            {
              "key": "[metadata] pkg.torch.onnx.stack_trace",
              "value": "File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 688, in forward\n    outputs: BaseModelOutputWithPast = self.model(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 453, in forward\n    layer_outputs = decoder_layer(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 323, in forward\n    hidden_states = self.post_attention_layernorm(hidden_states)\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 72, in forward\n    hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)"
            }
          ],
          "incomingEdges": [
            {
              "sourceNodeId": "node_Sqrt_654",
              "sourceNodeOutputId": "0",
              "targetNodeInputId": "0"
            }
          ],
          "outputsMetadata": [
            {
              "id": "0",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "rsqrt_3"
                },
                {
                  "key": "tensor_shape",
                  "value": "FLOAT[s34,s16,1]"
                },
                {
                  "key": "param_name",
                  "value": "Y"
                }
              ]
            }
          ],
          "inputsMetadata": [
            {
              "id": "0",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "val_461"
                },
                {
                  "key": "tensor_shape",
                  "value": "FLOAT[s34,s16,1]"
                },
                {
                  "key": "param_name",
                  "value": "X"
                }
              ]
            }
          ],
          "style": null,
          "config": null
        },
        {
          "id": "node_Mul_656",
          "label": "Mul",
          "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.1: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.1.post_attention_layernorm: transformers.models.llama.modeling_llama.LlamaRMSNorm/mul_563: aten.mul.Tensor",
          "subgraphIds": [],
          "attrs": [
            {
              "key": "[metadata] namespace",
              "value": ": transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.1: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.1.post_attention_layernorm: transformers.models.llama.modeling_llama.LlamaRMSNorm/mul_563: aten.mul.Tensor"
            },
            {
              "key": "[metadata] pkg.torch.onnx.class_hierarchy",
              "value": "['transformers.models.llama.modeling_llama.LlamaForCausalLM', 'transformers.models.llama.modeling_llama.LlamaModel', 'transformers.models.llama.modeling_llama.LlamaDecoderLayer', 'transformers.models.llama.modeling_llama.LlamaRMSNorm', 'aten.mul.Tensor']"
            },
            {
              "key": "[metadata] pkg.torch.onnx.fx_node",
              "value": "%mul_563 : [num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%add_697, %rsqrt_3), kwargs = {})"
            },
            {
              "key": "[metadata] pkg.torch.onnx.name_scopes",
              "value": "['', 'model', 'model.layers.1', 'model.layers.1.post_attention_layernorm', 'mul_563']"
            },
            {
              "key": "[metadata] pkg.torch.onnx.stack_trace",
              "value": "File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 688, in forward\n    outputs: BaseModelOutputWithPast = self.model(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 453, in forward\n    layer_outputs = decoder_layer(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 323, in forward\n    hidden_states = self.post_attention_layernorm(hidden_states)\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 72, in forward\n    hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)"
            }
          ],
          "incomingEdges": [
            {
              "sourceNodeId": "node_Add_647",
              "sourceNodeOutputId": "0",
              "targetNodeInputId": "0"
            },
            {
              "sourceNodeId": "node_Reciprocal_655",
              "sourceNodeOutputId": "0",
              "targetNodeInputId": "1"
            }
          ],
          "outputsMetadata": [
            {
              "id": "0",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "mul_563"
                },
                {
                  "key": "tensor_shape",
                  "value": "FLOAT[s34,s16,2048]"
                },
                {
                  "key": "param_name",
                  "value": "C"
                }
              ]
            }
          ],
          "inputsMetadata": [
            {
              "id": "0",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "add_697"
                },
                {
                  "key": "tensor_shape",
                  "value": "FLOAT[s34,s16,2048]"
                },
                {
                  "key": "param_name",
                  "value": "A"
                }
              ]
            },
            {
              "id": "1",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "rsqrt_3"
                },
                {
                  "key": "tensor_shape",
                  "value": "FLOAT[s34,s16,1]"
                },
                {
                  "key": "param_name",
                  "value": "B"
                }
              ]
            }
          ],
          "style": null,
          "config": null
        },
        {
          "id": "node_Mul_657",
          "label": "Mul",
          "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.1: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.1.post_attention_layernorm: transformers.models.llama.modeling_llama.LlamaRMSNorm/mul_567: aten.mul.Tensor",
          "subgraphIds": [],
          "attrs": [
            {
              "key": "[metadata] namespace",
              "value": ": transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.1: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.1.post_attention_layernorm: transformers.models.llama.modeling_llama.LlamaRMSNorm/mul_567: aten.mul.Tensor"
            },
            {
              "key": "[metadata] pkg.torch.onnx.class_hierarchy",
              "value": "['transformers.models.llama.modeling_llama.LlamaForCausalLM', 'transformers.models.llama.modeling_llama.LlamaModel', 'transformers.models.llama.modeling_llama.LlamaDecoderLayer', 'transformers.models.llama.modeling_llama.LlamaRMSNorm', 'aten.mul.Tensor']"
            },
            {
              "key": "[metadata] pkg.torch.onnx.fx_node",
              "value": "%mul_567 : [num_users=2] = call_function[target=torch.ops.aten.mul.Tensor](args = (%p_model_layers_1_post_attention_layernorm_weight, %mul_563), kwargs = {})"
            },
            {
              "key": "[metadata] pkg.torch.onnx.name_scopes",
              "value": "['', 'model', 'model.layers.1', 'model.layers.1.post_attention_layernorm', 'mul_567']"
            },
            {
              "key": "[metadata] pkg.torch.onnx.stack_trace",
              "value": "File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 688, in forward\n    outputs: BaseModelOutputWithPast = self.model(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 453, in forward\n    layer_outputs = decoder_layer(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 323, in forward\n    hidden_states = self.post_attention_layernorm(hidden_states)\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 73, in forward\n    return self.weight * hidden_states.to(input_dtype)"
            }
          ],
          "incomingEdges": [
            {
              "sourceNodeId": "[value] model.layers.1.post_attention_layernorm.weight",
              "sourceNodeOutputId": "0",
              "targetNodeInputId": "0"
            },
            {
              "sourceNodeId": "node_Mul_656",
              "sourceNodeOutputId": "0",
              "targetNodeInputId": "1"
            }
          ],
          "outputsMetadata": [
            {
              "id": "0",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "mul_567"
                },
                {
                  "key": "tensor_shape",
                  "value": "FLOAT[s34,s16,2048]"
                },
                {
                  "key": "param_name",
                  "value": "C"
                }
              ]
            }
          ],
          "inputsMetadata": [
            {
              "id": "0",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "model.layers.1.post_attention_layernorm.weight"
                },
                {
                  "key": "tensor_shape",
                  "value": "FLOAT[2048]"
                },
                {
                  "key": "param_name",
                  "value": "A"
                }
              ]
            },
            {
              "id": "1",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "mul_563"
                },
                {
                  "key": "tensor_shape",
                  "value": "FLOAT[s34,s16,2048]"
                },
                {
                  "key": "param_name",
                  "value": "B"
                }
              ]
            }
          ],
          "style": null,
          "config": null
        },
        {
          "id": "node_Transpose_658",
          "label": "Transpose",
          "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.1: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.1.mlp: transformers.models.llama.modeling_llama.LlamaMLP/model.layers.1.mlp.gate_proj: torch.nn.modules.linear.Linear/linear_11: aten.linear.default",
          "subgraphIds": [],
          "attrs": [
            {
              "key": "perm",
              "value": "[1, 0]"
            },
            {
              "key": "[metadata] namespace",
              "value": ": transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.1: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.1.mlp: transformers.models.llama.modeling_llama.LlamaMLP/model.layers.1.mlp.gate_proj: torch.nn.modules.linear.Linear/linear_11: aten.linear.default"
            },
            {
              "key": "[metadata] pkg.torch.onnx.class_hierarchy",
              "value": "['transformers.models.llama.modeling_llama.LlamaForCausalLM', 'transformers.models.llama.modeling_llama.LlamaModel', 'transformers.models.llama.modeling_llama.LlamaDecoderLayer', 'transformers.models.llama.modeling_llama.LlamaMLP', 'torch.nn.modules.linear.Linear', 'aten.linear.default']"
            },
            {
              "key": "[metadata] pkg.torch.onnx.fx_node",
              "value": "%linear_11 : [num_users=1] = call_function[target=torch.ops.aten.linear.default](args = (%mul_567, %p_model_layers_1_mlp_gate_proj_weight), kwargs = {})"
            },
            {
              "key": "[metadata] pkg.torch.onnx.name_scopes",
              "value": "['', 'model', 'model.layers.1', 'model.layers.1.mlp', 'model.layers.1.mlp.gate_proj', 'linear_11']"
            },
            {
              "key": "[metadata] pkg.torch.onnx.stack_trace",
              "value": "File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 688, in forward\n    outputs: BaseModelOutputWithPast = self.model(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 453, in forward\n    layer_outputs = decoder_layer(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 324, in forward\n    hidden_states = self.mlp(hidden_states)\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 162, in forward\n    down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/torch/nn/modules/linear.py\", line 125, in forward\n    return F.linear(input, self.weight, self.bias)"
            }
          ],
          "incomingEdges": [
            {
              "sourceNodeId": "[value] model.layers.1.mlp.gate_proj.weight",
              "sourceNodeOutputId": "0",
              "targetNodeInputId": "0"
            }
          ],
          "outputsMetadata": [
            {
              "id": "0",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "val_462"
                },
                {
                  "key": "tensor_shape",
                  "value": "FLOAT[2048,8192]"
                },
                {
                  "key": "param_name",
                  "value": "transposed"
                }
              ]
            }
          ],
          "inputsMetadata": [
            {
              "id": "0",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "model.layers.1.mlp.gate_proj.weight"
                },
                {
                  "key": "tensor_shape",
                  "value": "FLOAT[8192,2048]"
                },
                {
                  "key": "param_name",
                  "value": "data"
                }
              ]
            }
          ],
          "style": null,
          "config": null
        },
        {
          "id": "node_MatMul_659",
          "label": "MatMul",
          "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.1: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.1.mlp: transformers.models.llama.modeling_llama.LlamaMLP/model.layers.1.mlp.gate_proj: torch.nn.modules.linear.Linear/linear_11: aten.linear.default",
          "subgraphIds": [],
          "attrs": [
            {
              "key": "[metadata] namespace",
              "value": ": transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.1: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.1.mlp: transformers.models.llama.modeling_llama.LlamaMLP/model.layers.1.mlp.gate_proj: torch.nn.modules.linear.Linear/linear_11: aten.linear.default"
            },
            {
              "key": "[metadata] pkg.torch.onnx.class_hierarchy",
              "value": "['transformers.models.llama.modeling_llama.LlamaForCausalLM', 'transformers.models.llama.modeling_llama.LlamaModel', 'transformers.models.llama.modeling_llama.LlamaDecoderLayer', 'transformers.models.llama.modeling_llama.LlamaMLP', 'torch.nn.modules.linear.Linear', 'aten.linear.default']"
            },
            {
              "key": "[metadata] pkg.torch.onnx.fx_node",
              "value": "%linear_11 : [num_users=1] = call_function[target=torch.ops.aten.linear.default](args = (%mul_567, %p_model_layers_1_mlp_gate_proj_weight), kwargs = {})"
            },
            {
              "key": "[metadata] pkg.torch.onnx.name_scopes",
              "value": "['', 'model', 'model.layers.1', 'model.layers.1.mlp', 'model.layers.1.mlp.gate_proj', 'linear_11']"
            },
            {
              "key": "[metadata] pkg.torch.onnx.stack_trace",
              "value": "File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 688, in forward\n    outputs: BaseModelOutputWithPast = self.model(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 453, in forward\n    layer_outputs = decoder_layer(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 324, in forward\n    hidden_states = self.mlp(hidden_states)\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 162, in forward\n    down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/torch/nn/modules/linear.py\", line 125, in forward\n    return F.linear(input, self.weight, self.bias)"
            }
          ],
          "incomingEdges": [
            {
              "sourceNodeId": "node_Mul_657",
              "sourceNodeOutputId": "0",
              "targetNodeInputId": "0"
            },
            {
              "sourceNodeId": "node_Transpose_658",
              "sourceNodeOutputId": "0",
              "targetNodeInputId": "1"
            }
          ],
          "outputsMetadata": [
            {
              "id": "0",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "linear_11"
                },
                {
                  "key": "tensor_shape",
                  "value": "FLOAT[s34,s16,8192]"
                },
                {
                  "key": "param_name",
                  "value": "Y"
                }
              ]
            }
          ],
          "inputsMetadata": [
            {
              "id": "0",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "mul_567"
                },
                {
                  "key": "tensor_shape",
                  "value": "FLOAT[s34,s16,2048]"
                },
                {
                  "key": "param_name",
                  "value": "A"
                }
              ]
            },
            {
              "id": "1",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "val_462"
                },
                {
                  "key": "tensor_shape",
                  "value": "FLOAT[2048,8192]"
                },
                {
                  "key": "param_name",
                  "value": "B"
                }
              ]
            }
          ],
          "style": null,
          "config": null
        },
        {
          "id": "node_Sigmoid_660",
          "label": "Sigmoid",
          "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.1: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.1.mlp: transformers.models.llama.modeling_llama.LlamaMLP/model.layers.1.mlp.act_fn: torch.nn.modules.activation.SiLU/silu_1: aten.silu.default",
          "subgraphIds": [],
          "attrs": [
            {
              "key": "[metadata] namespace",
              "value": ": transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.1: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.1.mlp: transformers.models.llama.modeling_llama.LlamaMLP/model.layers.1.mlp.act_fn: torch.nn.modules.activation.SiLU/silu_1: aten.silu.default"
            },
            {
              "key": "[metadata] pkg.torch.onnx.class_hierarchy",
              "value": "['transformers.models.llama.modeling_llama.LlamaForCausalLM', 'transformers.models.llama.modeling_llama.LlamaModel', 'transformers.models.llama.modeling_llama.LlamaDecoderLayer', 'transformers.models.llama.modeling_llama.LlamaMLP', 'torch.nn.modules.activation.SiLU', 'aten.silu.default']"
            },
            {
              "key": "[metadata] pkg.torch.onnx.fx_node",
              "value": "%silu_1 : [num_users=1] = call_function[target=torch.ops.aten.silu.default](args = (%linear_11,), kwargs = {})"
            },
            {
              "key": "[metadata] pkg.torch.onnx.name_scopes",
              "value": "['', 'model', 'model.layers.1', 'model.layers.1.mlp', 'model.layers.1.mlp.act_fn', 'silu_1']"
            },
            {
              "key": "[metadata] pkg.torch.onnx.stack_trace",
              "value": "File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 688, in forward\n    outputs: BaseModelOutputWithPast = self.model(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 453, in forward\n    layer_outputs = decoder_layer(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 324, in forward\n    hidden_states = self.mlp(hidden_states)\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 162, in forward\n    down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/torch/nn/modules/activation.py\", line 434, in forward\n    return F.silu(input, inplace=self.inplace)"
            }
          ],
          "incomingEdges": [
            {
              "sourceNodeId": "node_MatMul_659",
              "sourceNodeOutputId": "0",
              "targetNodeInputId": "0"
            }
          ],
          "outputsMetadata": [
            {
              "id": "0",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "val_463"
                },
                {
                  "key": "tensor_shape",
                  "value": "FLOAT[s34,s16,8192]"
                },
                {
                  "key": "param_name",
                  "value": "Y"
                }
              ]
            }
          ],
          "inputsMetadata": [
            {
              "id": "0",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "linear_11"
                },
                {
                  "key": "tensor_shape",
                  "value": "FLOAT[s34,s16,8192]"
                },
                {
                  "key": "param_name",
                  "value": "X"
                }
              ]
            }
          ],
          "style": null,
          "config": null
        },
        {
          "id": "node_Mul_661",
          "label": "Mul",
          "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.1: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.1.mlp: transformers.models.llama.modeling_llama.LlamaMLP/model.layers.1.mlp.act_fn: torch.nn.modules.activation.SiLU/silu_1: aten.silu.default",
          "subgraphIds": [],
          "attrs": [
            {
              "key": "[metadata] namespace",
              "value": ": transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.1: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.1.mlp: transformers.models.llama.modeling_llama.LlamaMLP/model.layers.1.mlp.act_fn: torch.nn.modules.activation.SiLU/silu_1: aten.silu.default"
            },
            {
              "key": "[metadata] pkg.torch.onnx.class_hierarchy",
              "value": "['transformers.models.llama.modeling_llama.LlamaForCausalLM', 'transformers.models.llama.modeling_llama.LlamaModel', 'transformers.models.llama.modeling_llama.LlamaDecoderLayer', 'transformers.models.llama.modeling_llama.LlamaMLP', 'torch.nn.modules.activation.SiLU', 'aten.silu.default']"
            },
            {
              "key": "[metadata] pkg.torch.onnx.fx_node",
              "value": "%silu_1 : [num_users=1] = call_function[target=torch.ops.aten.silu.default](args = (%linear_11,), kwargs = {})"
            },
            {
              "key": "[metadata] pkg.torch.onnx.name_scopes",
              "value": "['', 'model', 'model.layers.1', 'model.layers.1.mlp', 'model.layers.1.mlp.act_fn', 'silu_1']"
            },
            {
              "key": "[metadata] pkg.torch.onnx.stack_trace",
              "value": "File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 688, in forward\n    outputs: BaseModelOutputWithPast = self.model(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 453, in forward\n    layer_outputs = decoder_layer(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 324, in forward\n    hidden_states = self.mlp(hidden_states)\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 162, in forward\n    down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/torch/nn/modules/activation.py\", line 434, in forward\n    return F.silu(input, inplace=self.inplace)"
            }
          ],
          "incomingEdges": [
            {
              "sourceNodeId": "node_MatMul_659",
              "sourceNodeOutputId": "0",
              "targetNodeInputId": "0"
            },
            {
              "sourceNodeId": "node_Sigmoid_660",
              "sourceNodeOutputId": "0",
              "targetNodeInputId": "1"
            }
          ],
          "outputsMetadata": [
            {
              "id": "0",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "silu_1"
                },
                {
                  "key": "tensor_shape",
                  "value": "FLOAT[s34,s16,8192]"
                },
                {
                  "key": "param_name",
                  "value": "C"
                }
              ]
            }
          ],
          "inputsMetadata": [
            {
              "id": "0",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "linear_11"
                },
                {
                  "key": "tensor_shape",
                  "value": "FLOAT[s34,s16,8192]"
                },
                {
                  "key": "param_name",
                  "value": "A"
                }
              ]
            },
            {
              "id": "1",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "val_463"
                },
                {
                  "key": "tensor_shape",
                  "value": "FLOAT[s34,s16,8192]"
                },
                {
                  "key": "param_name",
                  "value": "B"
                }
              ]
            }
          ],
          "style": null,
          "config": null
        },
        {
          "id": "node_Transpose_662",
          "label": "Transpose",
          "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.1: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.1.mlp: transformers.models.llama.modeling_llama.LlamaMLP/model.layers.1.mlp.up_proj: torch.nn.modules.linear.Linear/linear_12: aten.linear.default",
          "subgraphIds": [],
          "attrs": [
            {
              "key": "perm",
              "value": "[1, 0]"
            },
            {
              "key": "[metadata] namespace",
              "value": ": transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.1: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.1.mlp: transformers.models.llama.modeling_llama.LlamaMLP/model.layers.1.mlp.up_proj: torch.nn.modules.linear.Linear/linear_12: aten.linear.default"
            },
            {
              "key": "[metadata] pkg.torch.onnx.class_hierarchy",
              "value": "['transformers.models.llama.modeling_llama.LlamaForCausalLM', 'transformers.models.llama.modeling_llama.LlamaModel', 'transformers.models.llama.modeling_llama.LlamaDecoderLayer', 'transformers.models.llama.modeling_llama.LlamaMLP', 'torch.nn.modules.linear.Linear', 'aten.linear.default']"
            },
            {
              "key": "[metadata] pkg.torch.onnx.fx_node",
              "value": "%linear_12 : [num_users=1] = call_function[target=torch.ops.aten.linear.default](args = (%mul_567, %p_model_layers_1_mlp_up_proj_weight), kwargs = {})"
            },
            {
              "key": "[metadata] pkg.torch.onnx.name_scopes",
              "value": "['', 'model', 'model.layers.1', 'model.layers.1.mlp', 'model.layers.1.mlp.up_proj', 'linear_12']"
            },
            {
              "key": "[metadata] pkg.torch.onnx.stack_trace",
              "value": "File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 688, in forward\n    outputs: BaseModelOutputWithPast = self.model(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 453, in forward\n    layer_outputs = decoder_layer(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 324, in forward\n    hidden_states = self.mlp(hidden_states)\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 162, in forward\n    down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/torch/nn/modules/linear.py\", line 125, in forward\n    return F.linear(input, self.weight, self.bias)"
            }
          ],
          "incomingEdges": [
            {
              "sourceNodeId": "[value] model.layers.1.mlp.up_proj.weight",
              "sourceNodeOutputId": "0",
              "targetNodeInputId": "0"
            }
          ],
          "outputsMetadata": [
            {
              "id": "0",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "val_464"
                },
                {
                  "key": "tensor_shape",
                  "value": "FLOAT[2048,8192]"
                },
                {
                  "key": "param_name",
                  "value": "transposed"
                }
              ]
            }
          ],
          "inputsMetadata": [
            {
              "id": "0",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "model.layers.1.mlp.up_proj.weight"
                },
                {
                  "key": "tensor_shape",
                  "value": "FLOAT[8192,2048]"
                },
                {
                  "key": "param_name",
                  "value": "data"
                }
              ]
            }
          ],
          "style": null,
          "config": null
        },
        {
          "id": "node_MatMul_663",
          "label": "MatMul",
          "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.1: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.1.mlp: transformers.models.llama.modeling_llama.LlamaMLP/model.layers.1.mlp.up_proj: torch.nn.modules.linear.Linear/linear_12: aten.linear.default",
          "subgraphIds": [],
          "attrs": [
            {
              "key": "[metadata] namespace",
              "value": ": transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.1: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.1.mlp: transformers.models.llama.modeling_llama.LlamaMLP/model.layers.1.mlp.up_proj: torch.nn.modules.linear.Linear/linear_12: aten.linear.default"
            },
            {
              "key": "[metadata] pkg.torch.onnx.class_hierarchy",
              "value": "['transformers.models.llama.modeling_llama.LlamaForCausalLM', 'transformers.models.llama.modeling_llama.LlamaModel', 'transformers.models.llama.modeling_llama.LlamaDecoderLayer', 'transformers.models.llama.modeling_llama.LlamaMLP', 'torch.nn.modules.linear.Linear', 'aten.linear.default']"
            },
            {
              "key": "[metadata] pkg.torch.onnx.fx_node",
              "value": "%linear_12 : [num_users=1] = call_function[target=torch.ops.aten.linear.default](args = (%mul_567, %p_model_layers_1_mlp_up_proj_weight), kwargs = {})"
            },
            {
              "key": "[metadata] pkg.torch.onnx.name_scopes",
              "value": "['', 'model', 'model.layers.1', 'model.layers.1.mlp', 'model.layers.1.mlp.up_proj', 'linear_12']"
            },
            {
              "key": "[metadata] pkg.torch.onnx.stack_trace",
              "value": "File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 688, in forward\n    outputs: BaseModelOutputWithPast = self.model(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 453, in forward\n    layer_outputs = decoder_layer(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 324, in forward\n    hidden_states = self.mlp(hidden_states)\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 162, in forward\n    down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/torch/nn/modules/linear.py\", line 125, in forward\n    return F.linear(input, self.weight, self.bias)"
            }
          ],
          "incomingEdges": [
            {
              "sourceNodeId": "node_Mul_657",
              "sourceNodeOutputId": "0",
              "targetNodeInputId": "0"
            },
            {
              "sourceNodeId": "node_Transpose_662",
              "sourceNodeOutputId": "0",
              "targetNodeInputId": "1"
            }
          ],
          "outputsMetadata": [
            {
              "id": "0",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "linear_12"
                },
                {
                  "key": "tensor_shape",
                  "value": "FLOAT[s34,s16,8192]"
                },
                {
                  "key": "param_name",
                  "value": "Y"
                }
              ]
            }
          ],
          "inputsMetadata": [
            {
              "id": "0",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "mul_567"
                },
                {
                  "key": "tensor_shape",
                  "value": "FLOAT[s34,s16,2048]"
                },
                {
                  "key": "param_name",
                  "value": "A"
                }
              ]
            },
            {
              "id": "1",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "val_464"
                },
                {
                  "key": "tensor_shape",
                  "value": "FLOAT[2048,8192]"
                },
                {
                  "key": "param_name",
                  "value": "B"
                }
              ]
            }
          ],
          "style": null,
          "config": null
        },
        {
          "id": "node_Mul_664",
          "label": "Mul",
          "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.1: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.1.mlp: transformers.models.llama.modeling_llama.LlamaMLP/mul_580: aten.mul.Tensor",
          "subgraphIds": [],
          "attrs": [
            {
              "key": "[metadata] namespace",
              "value": ": transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.1: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.1.mlp: transformers.models.llama.modeling_llama.LlamaMLP/mul_580: aten.mul.Tensor"
            },
            {
              "key": "[metadata] pkg.torch.onnx.class_hierarchy",
              "value": "['transformers.models.llama.modeling_llama.LlamaForCausalLM', 'transformers.models.llama.modeling_llama.LlamaModel', 'transformers.models.llama.modeling_llama.LlamaDecoderLayer', 'transformers.models.llama.modeling_llama.LlamaMLP', 'aten.mul.Tensor']"
            },
            {
              "key": "[metadata] pkg.torch.onnx.fx_node",
              "value": "%mul_580 : [num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%silu_1, %linear_12), kwargs = {})"
            },
            {
              "key": "[metadata] pkg.torch.onnx.name_scopes",
              "value": "['', 'model', 'model.layers.1', 'model.layers.1.mlp', 'mul_580']"
            },
            {
              "key": "[metadata] pkg.torch.onnx.stack_trace",
              "value": "File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 688, in forward\n    outputs: BaseModelOutputWithPast = self.model(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 453, in forward\n    layer_outputs = decoder_layer(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 324, in forward\n    hidden_states = self.mlp(hidden_states)\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 162, in forward\n    down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))"
            }
          ],
          "incomingEdges": [
            {
              "sourceNodeId": "node_Mul_661",
              "sourceNodeOutputId": "0",
              "targetNodeInputId": "0"
            },
            {
              "sourceNodeId": "node_MatMul_663",
              "sourceNodeOutputId": "0",
              "targetNodeInputId": "1"
            }
          ],
          "outputsMetadata": [
            {
              "id": "0",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "mul_580"
                },
                {
                  "key": "tensor_shape",
                  "value": "FLOAT[s34,s16,8192]"
                },
                {
                  "key": "param_name",
                  "value": "C"
                }
              ]
            }
          ],
          "inputsMetadata": [
            {
              "id": "0",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "silu_1"
                },
                {
                  "key": "tensor_shape",
                  "value": "FLOAT[s34,s16,8192]"
                },
                {
                  "key": "param_name",
                  "value": "A"
                }
              ]
            },
            {
              "id": "1",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "linear_12"
                },
                {
                  "key": "tensor_shape",
                  "value": "FLOAT[s34,s16,8192]"
                },
                {
                  "key": "param_name",
                  "value": "B"
                }
              ]
            }
          ],
          "style": null,
          "config": null
        },
        {
          "id": "node_Transpose_665",
          "label": "Transpose",
          "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.1: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.1.mlp: transformers.models.llama.modeling_llama.LlamaMLP/model.layers.1.mlp.down_proj: torch.nn.modules.linear.Linear/linear_13: aten.linear.default",
          "subgraphIds": [],
          "attrs": [
            {
              "key": "perm",
              "value": "[1, 0]"
            },
            {
              "key": "[metadata] namespace",
              "value": ": transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.1: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.1.mlp: transformers.models.llama.modeling_llama.LlamaMLP/model.layers.1.mlp.down_proj: torch.nn.modules.linear.Linear/linear_13: aten.linear.default"
            },
            {
              "key": "[metadata] pkg.torch.onnx.class_hierarchy",
              "value": "['transformers.models.llama.modeling_llama.LlamaForCausalLM', 'transformers.models.llama.modeling_llama.LlamaModel', 'transformers.models.llama.modeling_llama.LlamaDecoderLayer', 'transformers.models.llama.modeling_llama.LlamaMLP', 'torch.nn.modules.linear.Linear', 'aten.linear.default']"
            },
            {
              "key": "[metadata] pkg.torch.onnx.fx_node",
              "value": "%linear_13 : [num_users=1] = call_function[target=torch.ops.aten.linear.default](args = (%mul_580, %p_model_layers_1_mlp_down_proj_weight), kwargs = {})"
            },
            {
              "key": "[metadata] pkg.torch.onnx.name_scopes",
              "value": "['', 'model', 'model.layers.1', 'model.layers.1.mlp', 'model.layers.1.mlp.down_proj', 'linear_13']"
            },
            {
              "key": "[metadata] pkg.torch.onnx.stack_trace",
              "value": "File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 688, in forward\n    outputs: BaseModelOutputWithPast = self.model(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 453, in forward\n    layer_outputs = decoder_layer(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 324, in forward\n    hidden_states = self.mlp(hidden_states)\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 162, in forward\n    down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/torch/nn/modules/linear.py\", line 125, in forward\n    return F.linear(input, self.weight, self.bias)"
            }
          ],
          "incomingEdges": [
            {
              "sourceNodeId": "[value] model.layers.1.mlp.down_proj.weight",
              "sourceNodeOutputId": "0",
              "targetNodeInputId": "0"
            }
          ],
          "outputsMetadata": [
            {
              "id": "0",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "val_465"
                },
                {
                  "key": "tensor_shape",
                  "value": "FLOAT[8192,2048]"
                },
                {
                  "key": "param_name",
                  "value": "transposed"
                }
              ]
            }
          ],
          "inputsMetadata": [
            {
              "id": "0",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "model.layers.1.mlp.down_proj.weight"
                },
                {
                  "key": "tensor_shape",
                  "value": "FLOAT[2048,8192]"
                },
                {
                  "key": "param_name",
                  "value": "data"
                }
              ]
            }
          ],
          "style": null,
          "config": null
        },
        {
          "id": "node_MatMul_666",
          "label": "MatMul",
          "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.1: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.1.mlp: transformers.models.llama.modeling_llama.LlamaMLP/model.layers.1.mlp.down_proj: torch.nn.modules.linear.Linear/linear_13: aten.linear.default",
          "subgraphIds": [],
          "attrs": [
            {
              "key": "[metadata] namespace",
              "value": ": transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.1: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.1.mlp: transformers.models.llama.modeling_llama.LlamaMLP/model.layers.1.mlp.down_proj: torch.nn.modules.linear.Linear/linear_13: aten.linear.default"
            },
            {
              "key": "[metadata] pkg.torch.onnx.class_hierarchy",
              "value": "['transformers.models.llama.modeling_llama.LlamaForCausalLM', 'transformers.models.llama.modeling_llama.LlamaModel', 'transformers.models.llama.modeling_llama.LlamaDecoderLayer', 'transformers.models.llama.modeling_llama.LlamaMLP', 'torch.nn.modules.linear.Linear', 'aten.linear.default']"
            },
            {
              "key": "[metadata] pkg.torch.onnx.fx_node",
              "value": "%linear_13 : [num_users=1] = call_function[target=torch.ops.aten.linear.default](args = (%mul_580, %p_model_layers_1_mlp_down_proj_weight), kwargs = {})"
            },
            {
              "key": "[metadata] pkg.torch.onnx.name_scopes",
              "value": "['', 'model', 'model.layers.1', 'model.layers.1.mlp', 'model.layers.1.mlp.down_proj', 'linear_13']"
            },
            {
              "key": "[metadata] pkg.torch.onnx.stack_trace",
              "value": "File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 688, in forward\n    outputs: BaseModelOutputWithPast = self.model(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 453, in forward\n    layer_outputs = decoder_layer(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 324, in forward\n    hidden_states = self.mlp(hidden_states)\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 162, in forward\n    down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/torch/nn/modules/linear.py\", line 125, in forward\n    return F.linear(input, self.weight, self.bias)"
            }
          ],
          "incomingEdges": [
            {
              "sourceNodeId": "node_Mul_664",
              "sourceNodeOutputId": "0",
              "targetNodeInputId": "0"
            },
            {
              "sourceNodeId": "node_Transpose_665",
              "sourceNodeOutputId": "0",
              "targetNodeInputId": "1"
            }
          ],
          "outputsMetadata": [
            {
              "id": "0",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "linear_13"
                },
                {
                  "key": "tensor_shape",
                  "value": "FLOAT[s34,s16,2048]"
                },
                {
                  "key": "param_name",
                  "value": "Y"
                }
              ]
            }
          ],
          "inputsMetadata": [
            {
              "id": "0",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "mul_580"
                },
                {
                  "key": "tensor_shape",
                  "value": "FLOAT[s34,s16,8192]"
                },
                {
                  "key": "param_name",
                  "value": "A"
                }
              ]
            },
            {
              "id": "1",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "val_465"
                },
                {
                  "key": "tensor_shape",
                  "value": "FLOAT[8192,2048]"
                },
                {
                  "key": "param_name",
                  "value": "B"
                }
              ]
            }
          ],
          "style": null,
          "config": null
        },
        {
          "id": "node_Add_667",
          "label": "Add",
          "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.1: transformers.models.llama.modeling_llama.LlamaDecoderLayer/add_747: aten.add.Tensor",
          "subgraphIds": [],
          "attrs": [
            {
              "key": "[metadata] namespace",
              "value": ": transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.1: transformers.models.llama.modeling_llama.LlamaDecoderLayer/add_747: aten.add.Tensor"
            },
            {
              "key": "[metadata] pkg.torch.onnx.class_hierarchy",
              "value": "['transformers.models.llama.modeling_llama.LlamaForCausalLM', 'transformers.models.llama.modeling_llama.LlamaModel', 'transformers.models.llama.modeling_llama.LlamaDecoderLayer', 'aten.add.Tensor']"
            },
            {
              "key": "[metadata] pkg.torch.onnx.fx_node",
              "value": "%add_747 : [num_users=2] = call_function[target=torch.ops.aten.add.Tensor](args = (%add_697, %linear_13), kwargs = {})"
            },
            {
              "key": "[metadata] pkg.torch.onnx.name_scopes",
              "value": "['', 'model', 'model.layers.1', 'add_747']"
            },
            {
              "key": "[metadata] pkg.torch.onnx.stack_trace",
              "value": "File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 688, in forward\n    outputs: BaseModelOutputWithPast = self.model(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 453, in forward\n    layer_outputs = decoder_layer(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 325, in forward\n    hidden_states = residual + hidden_states"
            }
          ],
          "incomingEdges": [
            {
              "sourceNodeId": "node_Add_647",
              "sourceNodeOutputId": "0",
              "targetNodeInputId": "0"
            },
            {
              "sourceNodeId": "node_MatMul_666",
              "sourceNodeOutputId": "0",
              "targetNodeInputId": "1"
            }
          ],
          "outputsMetadata": [
            {
              "id": "0",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "add_747"
                },
                {
                  "key": "tensor_shape",
                  "value": "FLOAT[s34,s16,2048]"
                },
                {
                  "key": "param_name",
                  "value": "C"
                }
              ]
            }
          ],
          "inputsMetadata": [
            {
              "id": "0",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "add_697"
                },
                {
                  "key": "tensor_shape",
                  "value": "FLOAT[s34,s16,2048]"
                },
                {
                  "key": "param_name",
                  "value": "A"
                }
              ]
            },
            {
              "id": "1",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "linear_13"
                },
                {
                  "key": "tensor_shape",
                  "value": "FLOAT[s34,s16,2048]"
                },
                {
                  "key": "param_name",
                  "value": "B"
                }
              ]
            }
          ],
          "style": null,
          "config": null
        },
        {
          "id": "node_Constant_668",
          "label": "Constant",
          "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.norm: transformers.models.llama.modeling_llama.LlamaRMSNorm/pow_5: aten.pow.Tensor_Scalar",
          "subgraphIds": [],
          "attrs": [
            {
              "key": "__value",
              "value": "2.0"
            },
            {
              "key": "value",
              "value": "TensorProtoTensor<FLOAT,[]>(array(2., dtype=float32), name='')"
            },
            {
              "key": "[metadata] namespace",
              "value": ": transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.norm: transformers.models.llama.modeling_llama.LlamaRMSNorm/pow_5: aten.pow.Tensor_Scalar"
            },
            {
              "key": "[metadata] pkg.torch.onnx.class_hierarchy",
              "value": "['transformers.models.llama.modeling_llama.LlamaForCausalLM', 'transformers.models.llama.modeling_llama.LlamaModel', 'transformers.models.llama.modeling_llama.LlamaRMSNorm', 'aten.pow.Tensor_Scalar']"
            },
            {
              "key": "[metadata] pkg.torch.onnx.fx_node",
              "value": "%pow_5 : [num_users=1] = call_function[target=torch.ops.aten.pow.Tensor_Scalar](args = (%add_747, 2), kwargs = {})"
            },
            {
              "key": "[metadata] pkg.torch.onnx.name_scopes",
              "value": "['', 'model', 'model.norm', 'pow_5']"
            },
            {
              "key": "[metadata] pkg.torch.onnx.stack_trace",
              "value": "File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 688, in forward\n    outputs: BaseModelOutputWithPast = self.model(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 470, in forward\n    hidden_states = self.norm(hidden_states)\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 71, in forward\n    variance = hidden_states.pow(2).mean(-1, keepdim=True)"
            }
          ],
          "incomingEdges": [],
          "outputsMetadata": [
            {
              "id": "0",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "val_466"
                },
                {
                  "key": "tensor_shape",
                  "value": "FLOAT[]"
                },
                {
                  "key": "param_name",
                  "value": "output"
                }
              ]
            }
          ],
          "inputsMetadata": [],
          "style": null,
          "config": null
        },
        {
          "id": "node_Pow_669",
          "label": "Pow",
          "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.norm: transformers.models.llama.modeling_llama.LlamaRMSNorm/pow_5: aten.pow.Tensor_Scalar",
          "subgraphIds": [],
          "attrs": [
            {
              "key": "[metadata] namespace",
              "value": ": transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.norm: transformers.models.llama.modeling_llama.LlamaRMSNorm/pow_5: aten.pow.Tensor_Scalar"
            },
            {
              "key": "[metadata] pkg.torch.onnx.class_hierarchy",
              "value": "['transformers.models.llama.modeling_llama.LlamaForCausalLM', 'transformers.models.llama.modeling_llama.LlamaModel', 'transformers.models.llama.modeling_llama.LlamaRMSNorm', 'aten.pow.Tensor_Scalar']"
            },
            {
              "key": "[metadata] pkg.torch.onnx.fx_node",
              "value": "%pow_5 : [num_users=1] = call_function[target=torch.ops.aten.pow.Tensor_Scalar](args = (%add_747, 2), kwargs = {})"
            },
            {
              "key": "[metadata] pkg.torch.onnx.name_scopes",
              "value": "['', 'model', 'model.norm', 'pow_5']"
            },
            {
              "key": "[metadata] pkg.torch.onnx.stack_trace",
              "value": "File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 688, in forward\n    outputs: BaseModelOutputWithPast = self.model(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 470, in forward\n    hidden_states = self.norm(hidden_states)\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 71, in forward\n    variance = hidden_states.pow(2).mean(-1, keepdim=True)"
            }
          ],
          "incomingEdges": [
            {
              "sourceNodeId": "node_Add_667",
              "sourceNodeOutputId": "0",
              "targetNodeInputId": "0"
            },
            {
              "sourceNodeId": "node_Constant_668",
              "sourceNodeOutputId": "0",
              "targetNodeInputId": "1"
            }
          ],
          "outputsMetadata": [
            {
              "id": "0",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "pow_5"
                },
                {
                  "key": "tensor_shape",
                  "value": "FLOAT[s34,s16,2048]"
                },
                {
                  "key": "param_name",
                  "value": "Z"
                }
              ]
            }
          ],
          "inputsMetadata": [
            {
              "id": "0",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "add_747"
                },
                {
                  "key": "tensor_shape",
                  "value": "FLOAT[s34,s16,2048]"
                },
                {
                  "key": "param_name",
                  "value": "X"
                }
              ]
            },
            {
              "id": "1",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "val_466"
                },
                {
                  "key": "tensor_shape",
                  "value": "FLOAT[]"
                },
                {
                  "key": "param_name",
                  "value": "Y"
                }
              ]
            }
          ],
          "style": null,
          "config": null
        },
        {
          "id": "node_Constant_981",
          "label": "Constant",
          "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.norm: transformers.models.llama.modeling_llama.LlamaRMSNorm/mean_4: aten.mean.dim",
          "subgraphIds": [],
          "attrs": [
            {
              "key": "__value",
              "value": "[-1]"
            },
            {
              "key": "value",
              "value": "TensorProtoTensor<INT64,[1]>(array([-1]), name='val_468')"
            }
          ],
          "incomingEdges": [],
          "outputsMetadata": [
            {
              "id": "0",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "val_468"
                },
                {
                  "key": "tensor_shape",
                  "value": "INT64[1]"
                },
                {
                  "key": "param_name",
                  "value": "output"
                }
              ]
            }
          ],
          "inputsMetadata": [],
          "style": null,
          "config": null
        },
        {
          "id": "node_ReduceMean_672",
          "label": "ReduceMean",
          "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.norm: transformers.models.llama.modeling_llama.LlamaRMSNorm/mean_4: aten.mean.dim",
          "subgraphIds": [],
          "attrs": [
            {
              "key": "noop_with_empty_axes",
              "value": "0"
            },
            {
              "key": "keepdims",
              "value": "1"
            },
            {
              "key": "[metadata] namespace",
              "value": ": transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.norm: transformers.models.llama.modeling_llama.LlamaRMSNorm/mean_4: aten.mean.dim"
            },
            {
              "key": "[metadata] pkg.torch.onnx.class_hierarchy",
              "value": "['transformers.models.llama.modeling_llama.LlamaForCausalLM', 'transformers.models.llama.modeling_llama.LlamaModel', 'transformers.models.llama.modeling_llama.LlamaRMSNorm', 'aten.mean.dim']"
            },
            {
              "key": "[metadata] pkg.torch.onnx.fx_node",
              "value": "%mean_4 : [num_users=1] = call_function[target=torch.ops.aten.mean.dim](args = (%pow_5, [-1], True), kwargs = {})"
            },
            {
              "key": "[metadata] pkg.torch.onnx.name_scopes",
              "value": "['', 'model', 'model.norm', 'mean_4']"
            },
            {
              "key": "[metadata] pkg.torch.onnx.stack_trace",
              "value": "File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 688, in forward\n    outputs: BaseModelOutputWithPast = self.model(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 470, in forward\n    hidden_states = self.norm(hidden_states)\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 71, in forward\n    variance = hidden_states.pow(2).mean(-1, keepdim=True)"
            }
          ],
          "incomingEdges": [
            {
              "sourceNodeId": "node_Pow_669",
              "sourceNodeOutputId": "0",
              "targetNodeInputId": "0"
            },
            {
              "sourceNodeId": "node_Constant_981",
              "sourceNodeOutputId": "0",
              "targetNodeInputId": "1"
            }
          ],
          "outputsMetadata": [
            {
              "id": "0",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "mean_4"
                },
                {
                  "key": "tensor_shape",
                  "value": "FLOAT[s34,s16,1]"
                },
                {
                  "key": "param_name",
                  "value": "reduced"
                }
              ]
            }
          ],
          "inputsMetadata": [
            {
              "id": "0",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "pow_5"
                },
                {
                  "key": "tensor_shape",
                  "value": "FLOAT[s34,s16,2048]"
                },
                {
                  "key": "param_name",
                  "value": "data"
                }
              ]
            },
            {
              "id": "1",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "val_468"
                },
                {
                  "key": "tensor_shape",
                  "value": "INT64[1]"
                },
                {
                  "key": "param_name",
                  "value": "axes"
                }
              ]
            }
          ],
          "style": null,
          "config": null
        },
        {
          "id": "node_Add_673",
          "label": "Add",
          "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.norm: transformers.models.llama.modeling_llama.LlamaRMSNorm/add_760: aten.add.Tensor",
          "subgraphIds": [],
          "attrs": [
            {
              "key": "[metadata] namespace",
              "value": ": transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.norm: transformers.models.llama.modeling_llama.LlamaRMSNorm/add_760: aten.add.Tensor"
            },
            {
              "key": "[metadata] pkg.torch.onnx.class_hierarchy",
              "value": "['transformers.models.llama.modeling_llama.LlamaForCausalLM', 'transformers.models.llama.modeling_llama.LlamaModel', 'transformers.models.llama.modeling_llama.LlamaRMSNorm', 'aten.add.Tensor']"
            },
            {
              "key": "[metadata] pkg.torch.onnx.fx_node",
              "value": "%add_760 : [num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%mean_4, 1e-05), kwargs = {})"
            },
            {
              "key": "[metadata] pkg.torch.onnx.name_scopes",
              "value": "['', 'model', 'model.norm', 'add_760']"
            },
            {
              "key": "[metadata] pkg.torch.onnx.stack_trace",
              "value": "File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 688, in forward\n    outputs: BaseModelOutputWithPast = self.model(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 470, in forward\n    hidden_states = self.norm(hidden_states)\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 72, in forward\n    hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)"
            }
          ],
          "incomingEdges": [
            {
              "sourceNodeId": "node_ReduceMean_672",
              "sourceNodeOutputId": "0",
              "targetNodeInputId": "0"
            },
            {
              "sourceNodeId": "node_Constant_333",
              "sourceNodeOutputId": "0",
              "targetNodeInputId": "1"
            }
          ],
          "outputsMetadata": [
            {
              "id": "0",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "add_760"
                },
                {
                  "key": "tensor_shape",
                  "value": "FLOAT[s34,s16,1]"
                },
                {
                  "key": "param_name",
                  "value": "C"
                }
              ]
            }
          ],
          "inputsMetadata": [
            {
              "id": "0",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "mean_4"
                },
                {
                  "key": "tensor_shape",
                  "value": "FLOAT[s34,s16,1]"
                },
                {
                  "key": "param_name",
                  "value": "A"
                }
              ]
            },
            {
              "id": "1",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "val_251"
                },
                {
                  "key": "tensor_shape",
                  "value": "FLOAT[]"
                },
                {
                  "key": "param_name",
                  "value": "B"
                }
              ]
            }
          ],
          "style": null,
          "config": null
        },
        {
          "id": "node_Sqrt_674",
          "label": "Sqrt",
          "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.norm: transformers.models.llama.modeling_llama.LlamaRMSNorm/rsqrt_4: aten.rsqrt.default",
          "subgraphIds": [],
          "attrs": [
            {
              "key": "[metadata] namespace",
              "value": ": transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.norm: transformers.models.llama.modeling_llama.LlamaRMSNorm/rsqrt_4: aten.rsqrt.default"
            },
            {
              "key": "[metadata] pkg.torch.onnx.class_hierarchy",
              "value": "['transformers.models.llama.modeling_llama.LlamaForCausalLM', 'transformers.models.llama.modeling_llama.LlamaModel', 'transformers.models.llama.modeling_llama.LlamaRMSNorm', 'aten.rsqrt.default']"
            },
            {
              "key": "[metadata] pkg.torch.onnx.fx_node",
              "value": "%rsqrt_4 : [num_users=1] = call_function[target=torch.ops.aten.rsqrt.default](args = (%add_760,), kwargs = {})"
            },
            {
              "key": "[metadata] pkg.torch.onnx.name_scopes",
              "value": "['', 'model', 'model.norm', 'rsqrt_4']"
            },
            {
              "key": "[metadata] pkg.torch.onnx.stack_trace",
              "value": "File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 688, in forward\n    outputs: BaseModelOutputWithPast = self.model(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 470, in forward\n    hidden_states = self.norm(hidden_states)\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 72, in forward\n    hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)"
            }
          ],
          "incomingEdges": [
            {
              "sourceNodeId": "node_Add_673",
              "sourceNodeOutputId": "0",
              "targetNodeInputId": "0"
            }
          ],
          "outputsMetadata": [
            {
              "id": "0",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "val_469"
                },
                {
                  "key": "tensor_shape",
                  "value": "FLOAT[s34,s16,1]"
                },
                {
                  "key": "param_name",
                  "value": "Y"
                }
              ]
            }
          ],
          "inputsMetadata": [
            {
              "id": "0",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "add_760"
                },
                {
                  "key": "tensor_shape",
                  "value": "FLOAT[s34,s16,1]"
                },
                {
                  "key": "param_name",
                  "value": "X"
                }
              ]
            }
          ],
          "style": null,
          "config": null
        },
        {
          "id": "node_Reciprocal_675",
          "label": "Reciprocal",
          "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.norm: transformers.models.llama.modeling_llama.LlamaRMSNorm/rsqrt_4: aten.rsqrt.default",
          "subgraphIds": [],
          "attrs": [
            {
              "key": "[metadata] namespace",
              "value": ": transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.norm: transformers.models.llama.modeling_llama.LlamaRMSNorm/rsqrt_4: aten.rsqrt.default"
            },
            {
              "key": "[metadata] pkg.torch.onnx.class_hierarchy",
              "value": "['transformers.models.llama.modeling_llama.LlamaForCausalLM', 'transformers.models.llama.modeling_llama.LlamaModel', 'transformers.models.llama.modeling_llama.LlamaRMSNorm', 'aten.rsqrt.default']"
            },
            {
              "key": "[metadata] pkg.torch.onnx.fx_node",
              "value": "%rsqrt_4 : [num_users=1] = call_function[target=torch.ops.aten.rsqrt.default](args = (%add_760,), kwargs = {})"
            },
            {
              "key": "[metadata] pkg.torch.onnx.name_scopes",
              "value": "['', 'model', 'model.norm', 'rsqrt_4']"
            },
            {
              "key": "[metadata] pkg.torch.onnx.stack_trace",
              "value": "File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 688, in forward\n    outputs: BaseModelOutputWithPast = self.model(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 470, in forward\n    hidden_states = self.norm(hidden_states)\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 72, in forward\n    hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)"
            }
          ],
          "incomingEdges": [
            {
              "sourceNodeId": "node_Sqrt_674",
              "sourceNodeOutputId": "0",
              "targetNodeInputId": "0"
            }
          ],
          "outputsMetadata": [
            {
              "id": "0",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "rsqrt_4"
                },
                {
                  "key": "tensor_shape",
                  "value": "FLOAT[s34,s16,1]"
                },
                {
                  "key": "param_name",
                  "value": "Y"
                }
              ]
            }
          ],
          "inputsMetadata": [
            {
              "id": "0",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "val_469"
                },
                {
                  "key": "tensor_shape",
                  "value": "FLOAT[s34,s16,1]"
                },
                {
                  "key": "param_name",
                  "value": "X"
                }
              ]
            }
          ],
          "style": null,
          "config": null
        },
        {
          "id": "node_Mul_676",
          "label": "Mul",
          "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.norm: transformers.models.llama.modeling_llama.LlamaRMSNorm/mul_599: aten.mul.Tensor",
          "subgraphIds": [],
          "attrs": [
            {
              "key": "[metadata] namespace",
              "value": ": transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.norm: transformers.models.llama.modeling_llama.LlamaRMSNorm/mul_599: aten.mul.Tensor"
            },
            {
              "key": "[metadata] pkg.torch.onnx.class_hierarchy",
              "value": "['transformers.models.llama.modeling_llama.LlamaForCausalLM', 'transformers.models.llama.modeling_llama.LlamaModel', 'transformers.models.llama.modeling_llama.LlamaRMSNorm', 'aten.mul.Tensor']"
            },
            {
              "key": "[metadata] pkg.torch.onnx.fx_node",
              "value": "%mul_599 : [num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%add_747, %rsqrt_4), kwargs = {})"
            },
            {
              "key": "[metadata] pkg.torch.onnx.name_scopes",
              "value": "['', 'model', 'model.norm', 'mul_599']"
            },
            {
              "key": "[metadata] pkg.torch.onnx.stack_trace",
              "value": "File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 688, in forward\n    outputs: BaseModelOutputWithPast = self.model(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 470, in forward\n    hidden_states = self.norm(hidden_states)\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 72, in forward\n    hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)"
            }
          ],
          "incomingEdges": [
            {
              "sourceNodeId": "node_Add_667",
              "sourceNodeOutputId": "0",
              "targetNodeInputId": "0"
            },
            {
              "sourceNodeId": "node_Reciprocal_675",
              "sourceNodeOutputId": "0",
              "targetNodeInputId": "1"
            }
          ],
          "outputsMetadata": [
            {
              "id": "0",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "mul_599"
                },
                {
                  "key": "tensor_shape",
                  "value": "FLOAT[s34,s16,2048]"
                },
                {
                  "key": "param_name",
                  "value": "C"
                }
              ]
            }
          ],
          "inputsMetadata": [
            {
              "id": "0",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "add_747"
                },
                {
                  "key": "tensor_shape",
                  "value": "FLOAT[s34,s16,2048]"
                },
                {
                  "key": "param_name",
                  "value": "A"
                }
              ]
            },
            {
              "id": "1",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "rsqrt_4"
                },
                {
                  "key": "tensor_shape",
                  "value": "FLOAT[s34,s16,1]"
                },
                {
                  "key": "param_name",
                  "value": "B"
                }
              ]
            }
          ],
          "style": null,
          "config": null
        },
        {
          "id": "node_Mul_677",
          "label": "Mul",
          "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.norm: transformers.models.llama.modeling_llama.LlamaRMSNorm/mul_603: aten.mul.Tensor",
          "subgraphIds": [],
          "attrs": [
            {
              "key": "[metadata] namespace",
              "value": ": transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.norm: transformers.models.llama.modeling_llama.LlamaRMSNorm/mul_603: aten.mul.Tensor"
            },
            {
              "key": "[metadata] pkg.torch.onnx.class_hierarchy",
              "value": "['transformers.models.llama.modeling_llama.LlamaForCausalLM', 'transformers.models.llama.modeling_llama.LlamaModel', 'transformers.models.llama.modeling_llama.LlamaRMSNorm', 'aten.mul.Tensor']"
            },
            {
              "key": "[metadata] pkg.torch.onnx.fx_node",
              "value": "%mul_603 : [num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%p_model_norm_weight, %mul_599), kwargs = {})"
            },
            {
              "key": "[metadata] pkg.torch.onnx.name_scopes",
              "value": "['', 'model', 'model.norm', 'mul_603']"
            },
            {
              "key": "[metadata] pkg.torch.onnx.stack_trace",
              "value": "File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 688, in forward\n    outputs: BaseModelOutputWithPast = self.model(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 470, in forward\n    hidden_states = self.norm(hidden_states)\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 73, in forward\n    return self.weight * hidden_states.to(input_dtype)"
            }
          ],
          "incomingEdges": [
            {
              "sourceNodeId": "[value] model.norm.weight",
              "sourceNodeOutputId": "0",
              "targetNodeInputId": "0"
            },
            {
              "sourceNodeId": "node_Mul_676",
              "sourceNodeOutputId": "0",
              "targetNodeInputId": "1"
            }
          ],
          "outputsMetadata": [
            {
              "id": "0",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "mul_603"
                },
                {
                  "key": "tensor_shape",
                  "value": "FLOAT[s34,s16,2048]"
                },
                {
                  "key": "param_name",
                  "value": "C"
                }
              ]
            }
          ],
          "inputsMetadata": [
            {
              "id": "0",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "model.norm.weight"
                },
                {
                  "key": "tensor_shape",
                  "value": "FLOAT[2048]"
                },
                {
                  "key": "param_name",
                  "value": "A"
                }
              ]
            },
            {
              "id": "1",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "mul_599"
                },
                {
                  "key": "tensor_shape",
                  "value": "FLOAT[s34,s16,2048]"
                },
                {
                  "key": "param_name",
                  "value": "B"
                }
              ]
            }
          ],
          "style": null,
          "config": null
        },
        {
          "id": "node_Transpose_701",
          "label": "Transpose",
          "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/lm_head: torch.nn.modules.linear.Linear/linear_14: aten.linear.default",
          "subgraphIds": [],
          "attrs": [
            {
              "key": "perm",
              "value": "[1, 0]"
            },
            {
              "key": "[metadata] namespace",
              "value": ": transformers.models.llama.modeling_llama.LlamaForCausalLM/lm_head: torch.nn.modules.linear.Linear/linear_14: aten.linear.default"
            },
            {
              "key": "[metadata] pkg.torch.onnx.class_hierarchy",
              "value": "['transformers.models.llama.modeling_llama.LlamaForCausalLM', 'torch.nn.modules.linear.Linear', 'aten.linear.default']"
            },
            {
              "key": "[metadata] pkg.torch.onnx.fx_node",
              "value": "%linear_14 : [num_users=1] = call_function[target=torch.ops.aten.linear.default](args = (%slice_52, %p_lm_head_weight), kwargs = {})"
            },
            {
              "key": "[metadata] pkg.torch.onnx.name_scopes",
              "value": "['', 'lm_head', 'linear_14']"
            },
            {
              "key": "[metadata] pkg.torch.onnx.stack_trace",
              "value": "File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 704, in forward\n    logits = self.lm_head(hidden_states[:, slice_indices, :])\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/torch/nn/modules/linear.py\", line 125, in forward\n    return F.linear(input, self.weight, self.bias)"
            }
          ],
          "incomingEdges": [
            {
              "sourceNodeId": "[value] lm_head.weight",
              "sourceNodeOutputId": "0",
              "targetNodeInputId": "0"
            }
          ],
          "outputsMetadata": [
            {
              "id": "0",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "val_490"
                },
                {
                  "key": "tensor_shape",
                  "value": "FLOAT[2048,49152]"
                },
                {
                  "key": "param_name",
                  "value": "transposed"
                }
              ]
            }
          ],
          "inputsMetadata": [
            {
              "id": "0",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "lm_head.weight"
                },
                {
                  "key": "tensor_shape",
                  "value": "FLOAT[49152,2048]"
                },
                {
                  "key": "param_name",
                  "value": "data"
                }
              ]
            }
          ],
          "style": null,
          "config": null
        },
        {
          "id": "node_MatMul_702",
          "label": "MatMul",
          "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/lm_head: torch.nn.modules.linear.Linear/linear_14: aten.linear.default",
          "subgraphIds": [],
          "attrs": [
            {
              "key": "[metadata] namespace",
              "value": ": transformers.models.llama.modeling_llama.LlamaForCausalLM/lm_head: torch.nn.modules.linear.Linear/linear_14: aten.linear.default"
            },
            {
              "key": "[metadata] pkg.torch.onnx.class_hierarchy",
              "value": "['transformers.models.llama.modeling_llama.LlamaForCausalLM', 'torch.nn.modules.linear.Linear', 'aten.linear.default']"
            },
            {
              "key": "[metadata] pkg.torch.onnx.fx_node",
              "value": "%linear_14 : [num_users=1] = call_function[target=torch.ops.aten.linear.default](args = (%slice_52, %p_lm_head_weight), kwargs = {})"
            },
            {
              "key": "[metadata] pkg.torch.onnx.name_scopes",
              "value": "['', 'lm_head', 'linear_14']"
            },
            {
              "key": "[metadata] pkg.torch.onnx.stack_trace",
              "value": "File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 704, in forward\n    logits = self.lm_head(hidden_states[:, slice_indices, :])\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/torch/nn/modules/linear.py\", line 125, in forward\n    return F.linear(input, self.weight, self.bias)"
            }
          ],
          "incomingEdges": [
            {
              "sourceNodeId": "node_Mul_677",
              "sourceNodeOutputId": "0",
              "targetNodeInputId": "0"
            },
            {
              "sourceNodeId": "node_Transpose_701",
              "sourceNodeOutputId": "0",
              "targetNodeInputId": "1"
            }
          ],
          "outputsMetadata": [
            {
              "id": "0",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "linear_14"
                },
                {
                  "key": "tensor_shape",
                  "value": "FLOAT[s34,s16,49152]"
                },
                {
                  "key": "[metadata] pkg.torch.export.graph_signature.OutputSpec.kind",
                  "value": "USER_OUTPUT"
                },
                {
                  "key": "param_name",
                  "value": "Y"
                }
              ]
            }
          ],
          "inputsMetadata": [
            {
              "id": "0",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "mul_603"
                },
                {
                  "key": "tensor_shape",
                  "value": "FLOAT[s34,s16,2048]"
                },
                {
                  "key": "param_name",
                  "value": "A"
                }
              ]
            },
            {
              "id": "1",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "val_490"
                },
                {
                  "key": "tensor_shape",
                  "value": "FLOAT[2048,49152]"
                },
                {
                  "key": "param_name",
                  "value": "B"
                }
              ]
            }
          ],
          "style": null,
          "config": null
        },
        {
          "id": "[value] model.layers.0.self_attn.q_proj.weight",
          "label": "Initializer",
          "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.0: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.0.self_attn: transformers.models.llama.modeling_llama.LlamaAttention/model.layers.0.self_attn.q_proj: torch.nn.modules.linear.Linear/linear: aten.linear.default",
          "subgraphIds": [],
          "attrs": [],
          "incomingEdges": [],
          "outputsMetadata": [
            {
              "id": "0",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "model.layers.0.self_attn.q_proj.weight"
                },
                {
                  "key": "tensor_shape",
                  "value": "FLOAT[2048,2048]"
                },
                {
                  "key": "value",
                  "value": "ExternalTensor<FLOAT,[2048,2048]>(location='model_SmolLM17b_2Layer-onnx_dynamo-ir-d1rt1.onnx.data', name='model.layers.0.self_attn.q_proj.weight', offset=65536, length=16777216, base_dir='')"
                }
              ]
            }
          ],
          "inputsMetadata": [],
          "style": null,
          "config": null
        },
        {
          "id": "[value] model.layers.0.self_attn.k_proj.weight",
          "label": "Initializer",
          "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.0: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.0.self_attn: transformers.models.llama.modeling_llama.LlamaAttention/model.layers.0.self_attn.k_proj: torch.nn.modules.linear.Linear/linear_1: aten.linear.default",
          "subgraphIds": [],
          "attrs": [],
          "incomingEdges": [],
          "outputsMetadata": [
            {
              "id": "0",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "model.layers.0.self_attn.k_proj.weight"
                },
                {
                  "key": "tensor_shape",
                  "value": "FLOAT[2048,2048]"
                },
                {
                  "key": "value",
                  "value": "ExternalTensor<FLOAT,[2048,2048]>(location='model_SmolLM17b_2Layer-onnx_dynamo-ir-d1rt1.onnx.data', name='model.layers.0.self_attn.k_proj.weight', offset=16842752, length=16777216, base_dir='')"
                }
              ]
            }
          ],
          "inputsMetadata": [],
          "style": null,
          "config": null
        },
        {
          "id": "[value] model.layers.0.self_attn.v_proj.weight",
          "label": "Initializer",
          "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.0: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.0.self_attn: transformers.models.llama.modeling_llama.LlamaAttention/model.layers.0.self_attn.v_proj: torch.nn.modules.linear.Linear/linear_2: aten.linear.default",
          "subgraphIds": [],
          "attrs": [],
          "incomingEdges": [],
          "outputsMetadata": [
            {
              "id": "0",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "model.layers.0.self_attn.v_proj.weight"
                },
                {
                  "key": "tensor_shape",
                  "value": "FLOAT[2048,2048]"
                },
                {
                  "key": "value",
                  "value": "ExternalTensor<FLOAT,[2048,2048]>(location='model_SmolLM17b_2Layer-onnx_dynamo-ir-d1rt1.onnx.data', name='model.layers.0.self_attn.v_proj.weight', offset=33619968, length=16777216, base_dir='')"
                }
              ]
            }
          ],
          "inputsMetadata": [],
          "style": null,
          "config": null
        },
        {
          "id": "[value] model.layers.0.self_attn.o_proj.weight",
          "label": "Initializer",
          "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.0: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.0.self_attn: transformers.models.llama.modeling_llama.LlamaAttention/model.layers.0.self_attn.o_proj: torch.nn.modules.linear.Linear/linear_3: aten.linear.default",
          "subgraphIds": [],
          "attrs": [],
          "incomingEdges": [],
          "outputsMetadata": [
            {
              "id": "0",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "model.layers.0.self_attn.o_proj.weight"
                },
                {
                  "key": "tensor_shape",
                  "value": "FLOAT[2048,2048]"
                },
                {
                  "key": "value",
                  "value": "ExternalTensor<FLOAT,[2048,2048]>(location='model_SmolLM17b_2Layer-onnx_dynamo-ir-d1rt1.onnx.data', name='model.layers.0.self_attn.o_proj.weight', offset=50397184, length=16777216, base_dir='')"
                }
              ]
            }
          ],
          "inputsMetadata": [],
          "style": null,
          "config": null
        },
        {
          "id": "[value] model.layers.0.mlp.gate_proj.weight",
          "label": "Initializer",
          "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.0: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.0.mlp: transformers.models.llama.modeling_llama.LlamaMLP/model.layers.0.mlp.gate_proj: torch.nn.modules.linear.Linear/linear_4: aten.linear.default",
          "subgraphIds": [],
          "attrs": [],
          "incomingEdges": [],
          "outputsMetadata": [
            {
              "id": "0",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "model.layers.0.mlp.gate_proj.weight"
                },
                {
                  "key": "tensor_shape",
                  "value": "FLOAT[8192,2048]"
                },
                {
                  "key": "value",
                  "value": "ExternalTensor<FLOAT,[8192,2048]>(location='model_SmolLM17b_2Layer-onnx_dynamo-ir-d1rt1.onnx.data', name='model.layers.0.mlp.gate_proj.weight', offset=134283264, length=67108864, base_dir='')"
                }
              ]
            }
          ],
          "inputsMetadata": [],
          "style": null,
          "config": null
        },
        {
          "id": "[value] model.layers.0.mlp.up_proj.weight",
          "label": "Initializer",
          "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.0: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.0.mlp: transformers.models.llama.modeling_llama.LlamaMLP/model.layers.0.mlp.up_proj: torch.nn.modules.linear.Linear/linear_5: aten.linear.default",
          "subgraphIds": [],
          "attrs": [],
          "incomingEdges": [],
          "outputsMetadata": [
            {
              "id": "0",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "model.layers.0.mlp.up_proj.weight"
                },
                {
                  "key": "tensor_shape",
                  "value": "FLOAT[8192,2048]"
                },
                {
                  "key": "value",
                  "value": "ExternalTensor<FLOAT,[8192,2048]>(location='model_SmolLM17b_2Layer-onnx_dynamo-ir-d1rt1.onnx.data', name='model.layers.0.mlp.up_proj.weight', offset=201392128, length=67108864, base_dir='')"
                }
              ]
            }
          ],
          "inputsMetadata": [],
          "style": null,
          "config": null
        },
        {
          "id": "[value] model.layers.0.mlp.down_proj.weight",
          "label": "Initializer",
          "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.0: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.0.mlp: transformers.models.llama.modeling_llama.LlamaMLP/model.layers.0.mlp.down_proj: torch.nn.modules.linear.Linear/linear_6: aten.linear.default",
          "subgraphIds": [],
          "attrs": [],
          "incomingEdges": [],
          "outputsMetadata": [
            {
              "id": "0",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "model.layers.0.mlp.down_proj.weight"
                },
                {
                  "key": "tensor_shape",
                  "value": "FLOAT[2048,8192]"
                },
                {
                  "key": "value",
                  "value": "ExternalTensor<FLOAT,[2048,8192]>(location='model_SmolLM17b_2Layer-onnx_dynamo-ir-d1rt1.onnx.data', name='model.layers.0.mlp.down_proj.weight', offset=268500992, length=67108864, base_dir='')"
                }
              ]
            }
          ],
          "inputsMetadata": [],
          "style": null,
          "config": null
        },
        {
          "id": "[value] model.layers.0.input_layernorm.weight",
          "label": "Initializer",
          "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.0: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.0.input_layernorm: transformers.models.llama.modeling_llama.LlamaRMSNorm/mul_173: aten.mul.Tensor",
          "subgraphIds": [],
          "attrs": [],
          "incomingEdges": [],
          "outputsMetadata": [
            {
              "id": "0",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "model.layers.0.input_layernorm.weight"
                },
                {
                  "key": "tensor_shape",
                  "value": "FLOAT[2048]"
                },
                {
                  "key": "value",
                  "value": "ExternalTensor<FLOAT,[2048]>(location='model_SmolLM17b_2Layer-onnx_dynamo-ir-d1rt1.onnx.data', name='model.layers.0.input_layernorm.weight', offset=0, length=8192, base_dir='')"
                }
              ]
            }
          ],
          "inputsMetadata": [],
          "style": null,
          "config": null
        },
        {
          "id": "[value] model.layers.0.post_attention_layernorm.weight",
          "label": "Initializer",
          "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.0: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.0.post_attention_layernorm: transformers.models.llama.modeling_llama.LlamaRMSNorm/mul_352: aten.mul.Tensor",
          "subgraphIds": [],
          "attrs": [],
          "incomingEdges": [],
          "outputsMetadata": [
            {
              "id": "0",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "model.layers.0.post_attention_layernorm.weight"
                },
                {
                  "key": "tensor_shape",
                  "value": "FLOAT[2048]"
                },
                {
                  "key": "value",
                  "value": "ExternalTensor<FLOAT,[2048]>(location='model_SmolLM17b_2Layer-onnx_dynamo-ir-d1rt1.onnx.data', name='model.layers.0.post_attention_layernorm.weight', offset=8192, length=8192, base_dir='')"
                }
              ]
            }
          ],
          "inputsMetadata": [],
          "style": null,
          "config": null
        },
        {
          "id": "[value] model.layers.1.self_attn.q_proj.weight",
          "label": "Initializer",
          "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.1: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.1.self_attn: transformers.models.llama.modeling_llama.LlamaAttention/model.layers.1.self_attn.q_proj: torch.nn.modules.linear.Linear/linear_7: aten.linear.default",
          "subgraphIds": [],
          "attrs": [],
          "incomingEdges": [],
          "outputsMetadata": [
            {
              "id": "0",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "model.layers.1.self_attn.q_proj.weight"
                },
                {
                  "key": "tensor_shape",
                  "value": "FLOAT[2048,2048]"
                },
                {
                  "key": "value",
                  "value": "ExternalTensor<FLOAT,[2048,2048]>(location='model_SmolLM17b_2Layer-onnx_dynamo-ir-d1rt1.onnx.data', name='model.layers.1.self_attn.q_proj.weight', offset=67174400, length=16777216, base_dir='')"
                }
              ]
            }
          ],
          "inputsMetadata": [],
          "style": null,
          "config": null
        },
        {
          "id": "[value] model.layers.1.self_attn.k_proj.weight",
          "label": "Initializer",
          "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.1: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.1.self_attn: transformers.models.llama.modeling_llama.LlamaAttention/model.layers.1.self_attn.k_proj: torch.nn.modules.linear.Linear/linear_8: aten.linear.default",
          "subgraphIds": [],
          "attrs": [],
          "incomingEdges": [],
          "outputsMetadata": [
            {
              "id": "0",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "model.layers.1.self_attn.k_proj.weight"
                },
                {
                  "key": "tensor_shape",
                  "value": "FLOAT[2048,2048]"
                },
                {
                  "key": "value",
                  "value": "ExternalTensor<FLOAT,[2048,2048]>(location='model_SmolLM17b_2Layer-onnx_dynamo-ir-d1rt1.onnx.data', name='model.layers.1.self_attn.k_proj.weight', offset=83951616, length=16777216, base_dir='')"
                }
              ]
            }
          ],
          "inputsMetadata": [],
          "style": null,
          "config": null
        },
        {
          "id": "[value] model.layers.1.self_attn.v_proj.weight",
          "label": "Initializer",
          "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.1: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.1.self_attn: transformers.models.llama.modeling_llama.LlamaAttention/model.layers.1.self_attn.v_proj: torch.nn.modules.linear.Linear/linear_9: aten.linear.default",
          "subgraphIds": [],
          "attrs": [],
          "incomingEdges": [],
          "outputsMetadata": [
            {
              "id": "0",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "model.layers.1.self_attn.v_proj.weight"
                },
                {
                  "key": "tensor_shape",
                  "value": "FLOAT[2048,2048]"
                },
                {
                  "key": "value",
                  "value": "ExternalTensor<FLOAT,[2048,2048]>(location='model_SmolLM17b_2Layer-onnx_dynamo-ir-d1rt1.onnx.data', name='model.layers.1.self_attn.v_proj.weight', offset=100728832, length=16777216, base_dir='')"
                }
              ]
            }
          ],
          "inputsMetadata": [],
          "style": null,
          "config": null
        },
        {
          "id": "[value] model.layers.1.self_attn.o_proj.weight",
          "label": "Initializer",
          "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.1: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.1.self_attn: transformers.models.llama.modeling_llama.LlamaAttention/model.layers.1.self_attn.o_proj: torch.nn.modules.linear.Linear/linear_10: aten.linear.default",
          "subgraphIds": [],
          "attrs": [],
          "incomingEdges": [],
          "outputsMetadata": [
            {
              "id": "0",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "model.layers.1.self_attn.o_proj.weight"
                },
                {
                  "key": "tensor_shape",
                  "value": "FLOAT[2048,2048]"
                },
                {
                  "key": "value",
                  "value": "ExternalTensor<FLOAT,[2048,2048]>(location='model_SmolLM17b_2Layer-onnx_dynamo-ir-d1rt1.onnx.data', name='model.layers.1.self_attn.o_proj.weight', offset=117506048, length=16777216, base_dir='')"
                }
              ]
            }
          ],
          "inputsMetadata": [],
          "style": null,
          "config": null
        },
        {
          "id": "[value] model.layers.1.mlp.gate_proj.weight",
          "label": "Initializer",
          "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.1: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.1.mlp: transformers.models.llama.modeling_llama.LlamaMLP/model.layers.1.mlp.gate_proj: torch.nn.modules.linear.Linear/linear_11: aten.linear.default",
          "subgraphIds": [],
          "attrs": [],
          "incomingEdges": [],
          "outputsMetadata": [
            {
              "id": "0",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "model.layers.1.mlp.gate_proj.weight"
                },
                {
                  "key": "tensor_shape",
                  "value": "FLOAT[8192,2048]"
                },
                {
                  "key": "value",
                  "value": "ExternalTensor<FLOAT,[8192,2048]>(location='model_SmolLM17b_2Layer-onnx_dynamo-ir-d1rt1.onnx.data', name='model.layers.1.mlp.gate_proj.weight', offset=335609856, length=67108864, base_dir='')"
                }
              ]
            }
          ],
          "inputsMetadata": [],
          "style": null,
          "config": null
        },
        {
          "id": "[value] model.layers.1.mlp.up_proj.weight",
          "label": "Initializer",
          "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.1: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.1.mlp: transformers.models.llama.modeling_llama.LlamaMLP/model.layers.1.mlp.up_proj: torch.nn.modules.linear.Linear/linear_12: aten.linear.default",
          "subgraphIds": [],
          "attrs": [],
          "incomingEdges": [],
          "outputsMetadata": [
            {
              "id": "0",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "model.layers.1.mlp.up_proj.weight"
                },
                {
                  "key": "tensor_shape",
                  "value": "FLOAT[8192,2048]"
                },
                {
                  "key": "value",
                  "value": "ExternalTensor<FLOAT,[8192,2048]>(location='model_SmolLM17b_2Layer-onnx_dynamo-ir-d1rt1.onnx.data', name='model.layers.1.mlp.up_proj.weight', offset=402718720, length=67108864, base_dir='')"
                }
              ]
            }
          ],
          "inputsMetadata": [],
          "style": null,
          "config": null
        },
        {
          "id": "[value] model.layers.1.mlp.down_proj.weight",
          "label": "Initializer",
          "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.1: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.1.mlp: transformers.models.llama.modeling_llama.LlamaMLP/model.layers.1.mlp.down_proj: torch.nn.modules.linear.Linear/linear_13: aten.linear.default",
          "subgraphIds": [],
          "attrs": [],
          "incomingEdges": [],
          "outputsMetadata": [
            {
              "id": "0",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "model.layers.1.mlp.down_proj.weight"
                },
                {
                  "key": "tensor_shape",
                  "value": "FLOAT[2048,8192]"
                },
                {
                  "key": "value",
                  "value": "ExternalTensor<FLOAT,[2048,8192]>(location='model_SmolLM17b_2Layer-onnx_dynamo-ir-d1rt1.onnx.data', name='model.layers.1.mlp.down_proj.weight', offset=469827584, length=67108864, base_dir='')"
                }
              ]
            }
          ],
          "inputsMetadata": [],
          "style": null,
          "config": null
        },
        {
          "id": "[value] model.layers.1.input_layernorm.weight",
          "label": "Initializer",
          "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.1: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.1.input_layernorm: transformers.models.llama.modeling_llama.LlamaRMSNorm/mul_388: aten.mul.Tensor",
          "subgraphIds": [],
          "attrs": [],
          "incomingEdges": [],
          "outputsMetadata": [
            {
              "id": "0",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "model.layers.1.input_layernorm.weight"
                },
                {
                  "key": "tensor_shape",
                  "value": "FLOAT[2048]"
                },
                {
                  "key": "value",
                  "value": "ExternalTensor<FLOAT,[2048]>(location='model_SmolLM17b_2Layer-onnx_dynamo-ir-d1rt1.onnx.data', name='model.layers.1.input_layernorm.weight', offset=16384, length=8192, base_dir='')"
                }
              ]
            }
          ],
          "inputsMetadata": [],
          "style": null,
          "config": null
        },
        {
          "id": "[value] model.layers.1.post_attention_layernorm.weight",
          "label": "Initializer",
          "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.1: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.1.post_attention_layernorm: transformers.models.llama.modeling_llama.LlamaRMSNorm/mul_567: aten.mul.Tensor",
          "subgraphIds": [],
          "attrs": [],
          "incomingEdges": [],
          "outputsMetadata": [
            {
              "id": "0",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "model.layers.1.post_attention_layernorm.weight"
                },
                {
                  "key": "tensor_shape",
                  "value": "FLOAT[2048]"
                },
                {
                  "key": "value",
                  "value": "ExternalTensor<FLOAT,[2048]>(location='model_SmolLM17b_2Layer-onnx_dynamo-ir-d1rt1.onnx.data', name='model.layers.1.post_attention_layernorm.weight', offset=24576, length=8192, base_dir='')"
                }
              ]
            }
          ],
          "inputsMetadata": [],
          "style": null,
          "config": null
        },
        {
          "id": "[value] model.norm.weight",
          "label": "Initializer",
          "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.norm: transformers.models.llama.modeling_llama.LlamaRMSNorm/mul_603: aten.mul.Tensor",
          "subgraphIds": [],
          "attrs": [],
          "incomingEdges": [],
          "outputsMetadata": [
            {
              "id": "0",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "model.norm.weight"
                },
                {
                  "key": "tensor_shape",
                  "value": "FLOAT[2048]"
                },
                {
                  "key": "value",
                  "value": "ExternalTensor<FLOAT,[2048]>(location='model_SmolLM17b_2Layer-onnx_dynamo-ir-d1rt1.onnx.data', name='model.norm.weight', offset=32768, length=8192, base_dir='')"
                }
              ]
            }
          ],
          "inputsMetadata": [],
          "style": null,
          "config": null
        },
        {
          "id": "[value] lm_head.weight",
          "label": "Initializer",
          "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM",
          "subgraphIds": [],
          "attrs": [],
          "incomingEdges": [],
          "outputsMetadata": [
            {
              "id": "0",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "lm_head.weight"
                },
                {
                  "key": "tensor_shape",
                  "value": "FLOAT[49152,2048]"
                },
                {
                  "key": "value",
                  "value": "ExternalTensor<FLOAT,[49152,2048]>(location='model_SmolLM17b_2Layer-onnx_dynamo-ir-d1rt1.onnx.data', name='lm_head.weight', offset=536936448, length=402653184, base_dir='')"
                }
              ]
            }
          ],
          "inputsMetadata": [],
          "style": null,
          "config": null
        },
        {
          "id": "[value] expand_2",
          "label": "Initializer",
          "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.rotary_emb: transformers.models.llama.modeling_llama.LlamaRotaryEmbedding/matmul: aten.matmul.default",
          "subgraphIds": [],
          "attrs": [
            {
              "key": "__value",
              "value": "[1.0,0.7498942017555237,0.5623413324356079,0.4216965138912201,0.3162277638912201,0.23713736236095428,0.17782793939113617,0.1333521455526352,0.10000000149011612,0.07498941570520401,0.05623412877321243,0.04216964915394783,0.03162277862429619,0.0237137358635664,0.017782794311642647,0.01333521492779255]"
            }
          ],
          "incomingEdges": [],
          "outputsMetadata": [
            {
              "id": "0",
              "attrs": [
                {
                  "key": "__tensor_tag",
                  "value": "expand_2"
                },
                {
                  "key": "tensor_shape",
                  "value": "FLOAT[1,32,1]"
                },
                {
                  "key": "value",
                  "value": "TensorProtoTensor<FLOAT,[1,32,1]>(name='expand_2')"
                }
              ]
            }
          ],
          "inputsMetadata": [],
          "style": null,
          "config": null
        },
        {
          "id": "[value] linear_14",
          "label": "Output",
          "namespace": "",
          "subgraphIds": [],
          "attrs": [
            {
              "key": "name",
              "value": "linear_14"
            },
            {
              "key": "index",
              "value": "0"
            }
          ],
          "incomingEdges": [
            {
              "sourceNodeId": "node_MatMul_702",
              "sourceNodeOutputId": "0",
              "targetNodeInputId": "0"
            }
          ],
          "outputsMetadata": [],
          "inputsMetadata": [],
          "style": null,
          "config": null
        },
        {
          "id": "[value] cat_3",
          "label": "Output",
          "namespace": "",
          "subgraphIds": [],
          "attrs": [
            {
              "key": "name",
              "value": "cat_3"
            },
            {
              "key": "index",
              "value": "1"
            }
          ],
          "incomingEdges": [
            {
              "sourceNodeId": "node_Concat_427",
              "sourceNodeOutputId": "0",
              "targetNodeInputId": "0"
            }
          ],
          "outputsMetadata": [],
          "inputsMetadata": [],
          "style": null,
          "config": null
        },
        {
          "id": "[value] cat_7",
          "label": "Output",
          "namespace": "",
          "subgraphIds": [],
          "attrs": [
            {
              "key": "name",
              "value": "cat_7"
            },
            {
              "key": "index",
              "value": "2"
            }
          ],
          "incomingEdges": [
            {
              "sourceNodeId": "node_Concat_596",
              "sourceNodeOutputId": "0",
              "targetNodeInputId": "0"
            }
          ],
          "outputsMetadata": [],
          "inputsMetadata": [],
          "style": null,
          "config": null
        },
        {
          "id": "[value] cat_4",
          "label": "Output",
          "namespace": "",
          "subgraphIds": [],
          "attrs": [
            {
              "key": "name",
              "value": "cat_4"
            },
            {
              "key": "index",
              "value": "3"
            }
          ],
          "incomingEdges": [
            {
              "sourceNodeId": "node_Concat_428",
              "sourceNodeOutputId": "0",
              "targetNodeInputId": "0"
            }
          ],
          "outputsMetadata": [],
          "inputsMetadata": [],
          "style": null,
          "config": null
        },
        {
          "id": "[value] cat_8",
          "label": "Output",
          "namespace": "",
          "subgraphIds": [],
          "attrs": [
            {
              "key": "name",
              "value": "cat_8"
            },
            {
              "key": "index",
              "value": "4"
            }
          ],
          "incomingEdges": [
            {
              "sourceNodeId": "node_Concat_597",
              "sourceNodeOutputId": "0",
              "targetNodeInputId": "0"
            }
          ],
          "outputsMetadata": [],
          "inputsMetadata": [],
          "style": null,
          "config": null
        }
      ],
      "groupNodeAttributes": {
        "": {
          "opset_imports": "{'': 18}",
          "producer_name": "pytorch",
          "producer_version": "2.8.0.dev20250530+cu126",
          "domain": "None",
          "model_version": "None",
          "doc_string": "None"
        },
        "main_graph": {
          "[metadata] pkg.torch.export.ExportedProgram.graph_signature": "\n# inputs\np_model_embed_tokens_weight: PARAMETER target='model.embed_tokens.weight'\np_model_layers_0_self_attn_q_proj_weight: PARAMETER target='model.layers.0.self_attn.q_proj.weight'\np_model_layers_0_self_attn_k_proj_weight: PARAMETER target='model.layers.0.self_attn.k_proj.weight'\np_model_layers_0_self_attn_v_proj_weight: PARAMETER target='model.layers.0.self_attn.v_proj.weight'\np_model_layers_0_self_attn_o_proj_weight: PARAMETER target='model.layers.0.self_attn.o_proj.weight'\np_model_layers_0_mlp_gate_proj_weight: PARAMETER target='model.layers.0.mlp.gate_proj.weight'\np_model_layers_0_mlp_up_proj_weight: PARAMETER target='model.layers.0.mlp.up_proj.weight'\np_model_layers_0_mlp_down_proj_weight: PARAMETER target='model.layers.0.mlp.down_proj.weight'\np_model_layers_0_input_layernorm_weight: PARAMETER target='model.layers.0.input_layernorm.weight'\np_model_layers_0_post_attention_layernorm_weight: PARAMETER target='model.layers.0.post_attention_layernorm.weight'\np_model_layers_1_self_attn_q_proj_weight: PARAMETER target='model.layers.1.self_attn.q_proj.weight'\np_model_layers_1_self_attn_k_proj_weight: PARAMETER target='model.layers.1.self_attn.k_proj.weight'\np_model_layers_1_self_attn_v_proj_weight: PARAMETER target='model.layers.1.self_attn.v_proj.weight'\np_model_layers_1_self_attn_o_proj_weight: PARAMETER target='model.layers.1.self_attn.o_proj.weight'\np_model_layers_1_mlp_gate_proj_weight: PARAMETER target='model.layers.1.mlp.gate_proj.weight'\np_model_layers_1_mlp_up_proj_weight: PARAMETER target='model.layers.1.mlp.up_proj.weight'\np_model_layers_1_mlp_down_proj_weight: PARAMETER target='model.layers.1.mlp.down_proj.weight'\np_model_layers_1_input_layernorm_weight: PARAMETER target='model.layers.1.input_layernorm.weight'\np_model_layers_1_post_attention_layernorm_weight: PARAMETER target='model.layers.1.post_attention_layernorm.weight'\np_model_norm_weight: PARAMETER target='model.norm.weight'\np_lm_head_weight: PARAMETER target='lm_head.weight'\nb_model_rotary_emb_inv_freq: BUFFER target='model.rotary_emb.inv_freq' persistent=False\ninput_ids: USER_INPUT\nattention_mask: USER_INPUT\nposition_ids: USER_INPUT\npast_key_values_key_cache_0: USER_INPUT\npast_key_values_key_cache_1: USER_INPUT\npast_key_values_value_cache_0: USER_INPUT\npast_key_values_value_cache_1: USER_INPUT\ninputs_embeds: USER_INPUT\nlabels: USER_INPUT\nuse_cache: USER_INPUT\noutput_attentions: USER_INPUT\noutput_hidden_states: USER_INPUT\ncache_position: USER_INPUT\nlogits_to_keep: USER_INPUT\n\n# outputs\nlinear_14: USER_OUTPUT\ncat_3: USER_OUTPUT\ncat_7: USER_OUTPUT\ncat_4: USER_OUTPUT\ncat_8: USER_OUTPUT\n",
          "[metadata] pkg.torch.export.ExportedProgram.range_constraints": "{s34: VR[1, 1024], s16: VR[8, 32768], s16 + s17: VR[10, 36864], s17: VR[1, 4096], s61: VR[2, int_oo], s75: VR[2, int_oo], s54: VR[2, int_oo]}"
        }
      },
      "collectionLabel": "model_SmolLM17b_2Layer-onnx_dynamo-ir-d1rt1.onnx"
    }
  ],
  "graphsWithLevel": [
    {
      "graph": {
        "id": "main_graph",
        "nodes": [
          {
            "id": "[value] input_ids",
            "label": "Input",
            "namespace": "",
            "subgraphIds": [],
            "attrs": [
              {
                "key": "name",
                "value": "input_ids"
              },
              {
                "key": "index",
                "value": "0"
              }
            ],
            "incomingEdges": [],
            "outputsMetadata": [
              {
                "id": "0",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "input_ids"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "INT64[s34,s16]"
                  },
                  {
                    "key": "[metadata] pkg.torch.export.graph_signature.InputSpec.kind",
                    "value": "USER_INPUT"
                  },
                  {
                    "key": "[metadata] pkg.torch.export.graph_signature.InputSpec.persistent",
                    "value": "None"
                  }
                ]
              }
            ],
            "inputsMetadata": [],
            "style": null,
            "config": null
          },
          {
            "id": "[value] attention_mask",
            "label": "Input",
            "namespace": "",
            "subgraphIds": [],
            "attrs": [
              {
                "key": "name",
                "value": "attention_mask"
              },
              {
                "key": "index",
                "value": "1"
              }
            ],
            "incomingEdges": [],
            "outputsMetadata": [
              {
                "id": "0",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "attention_mask"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "INT64[s34,s16 + s17]"
                  },
                  {
                    "key": "[metadata] pkg.torch.export.graph_signature.InputSpec.kind",
                    "value": "USER_INPUT"
                  },
                  {
                    "key": "[metadata] pkg.torch.export.graph_signature.InputSpec.persistent",
                    "value": "None"
                  }
                ]
              }
            ],
            "inputsMetadata": [],
            "style": null,
            "config": null
          },
          {
            "id": "[value] past_key_values_key_cache_0",
            "label": "Input",
            "namespace": "",
            "subgraphIds": [],
            "attrs": [
              {
                "key": "name",
                "value": "past_key_values_key_cache_0"
              },
              {
                "key": "index",
                "value": "2"
              }
            ],
            "incomingEdges": [],
            "outputsMetadata": [
              {
                "id": "0",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "past_key_values_key_cache_0"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "FLOAT[s34,32,s17,64]"
                  },
                  {
                    "key": "[metadata] pkg.torch.export.graph_signature.InputSpec.kind",
                    "value": "USER_INPUT"
                  },
                  {
                    "key": "[metadata] pkg.torch.export.graph_signature.InputSpec.persistent",
                    "value": "None"
                  }
                ]
              }
            ],
            "inputsMetadata": [],
            "style": null,
            "config": null
          },
          {
            "id": "[value] past_key_values_key_cache_1",
            "label": "Input",
            "namespace": "",
            "subgraphIds": [],
            "attrs": [
              {
                "key": "name",
                "value": "past_key_values_key_cache_1"
              },
              {
                "key": "index",
                "value": "3"
              }
            ],
            "incomingEdges": [],
            "outputsMetadata": [
              {
                "id": "0",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "past_key_values_key_cache_1"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "FLOAT[s34,32,s17,64]"
                  },
                  {
                    "key": "[metadata] pkg.torch.export.graph_signature.InputSpec.kind",
                    "value": "USER_INPUT"
                  },
                  {
                    "key": "[metadata] pkg.torch.export.graph_signature.InputSpec.persistent",
                    "value": "None"
                  }
                ]
              }
            ],
            "inputsMetadata": [],
            "style": null,
            "config": null
          },
          {
            "id": "[value] past_key_values_value_cache_0",
            "label": "Input",
            "namespace": "",
            "subgraphIds": [],
            "attrs": [
              {
                "key": "name",
                "value": "past_key_values_value_cache_0"
              },
              {
                "key": "index",
                "value": "4"
              }
            ],
            "incomingEdges": [],
            "outputsMetadata": [
              {
                "id": "0",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "past_key_values_value_cache_0"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "FLOAT[s34,32,s17,64]"
                  },
                  {
                    "key": "[metadata] pkg.torch.export.graph_signature.InputSpec.kind",
                    "value": "USER_INPUT"
                  },
                  {
                    "key": "[metadata] pkg.torch.export.graph_signature.InputSpec.persistent",
                    "value": "None"
                  }
                ]
              }
            ],
            "inputsMetadata": [],
            "style": null,
            "config": null
          },
          {
            "id": "[value] past_key_values_value_cache_1",
            "label": "Input",
            "namespace": "",
            "subgraphIds": [],
            "attrs": [
              {
                "key": "name",
                "value": "past_key_values_value_cache_1"
              },
              {
                "key": "index",
                "value": "5"
              }
            ],
            "incomingEdges": [],
            "outputsMetadata": [
              {
                "id": "0",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "past_key_values_value_cache_1"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "FLOAT[s34,32,s17,64]"
                  },
                  {
                    "key": "[metadata] pkg.torch.export.graph_signature.InputSpec.kind",
                    "value": "USER_INPUT"
                  },
                  {
                    "key": "[metadata] pkg.torch.export.graph_signature.InputSpec.persistent",
                    "value": "None"
                  }
                ]
              }
            ],
            "inputsMetadata": [],
            "style": null,
            "config": null
          },
          {
            "id": "node_Shape_2",
            "label": "Shape",
            "namespace": "main_graph/_empty_nn_module_stack_from_metadata_hook: _empty_nn_module_stack_from_metadata_hook/sym_size_int_61: aten.sym_size.int",
            "subgraphIds": [],
            "attrs": [
              {
                "key": "end",
                "value": "2"
              },
              {
                "key": "start",
                "value": "1"
              },
              {
                "key": "[metadata] namespace",
                "value": "_empty_nn_module_stack_from_metadata_hook: _empty_nn_module_stack_from_metadata_hook/sym_size_int_61: aten.sym_size.int"
              },
              {
                "key": "[metadata] pkg.torch.onnx.class_hierarchy",
                "value": "['_empty_nn_module_stack_from_metadata_hook', 'aten.sym_size.int']"
              },
              {
                "key": "[metadata] pkg.torch.onnx.fx_node",
                "value": "%sym_size_int_61 : [num_users=10] = call_function[target=torch.ops.aten.sym_size.int](args = (%input_ids, 1), kwargs = {})"
              },
              {
                "key": "[metadata] pkg.torch.onnx.name_scopes",
                "value": "['_empty_nn_module_stack_from_metadata_hook', 'sym_size_int_61']"
              },
              {
                "key": "[metadata] pkg.torch.onnx.stack_trace",
                "value": "File \"torch/fx/passes/runtime_assert.py\", line 24, in insert_deferred_runtime_asserts"
              }
            ],
            "incomingEdges": [
              {
                "sourceNodeId": "[value] input_ids",
                "sourceNodeOutputId": "0",
                "targetNodeInputId": "0"
              }
            ],
            "outputsMetadata": [
              {
                "id": "0",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "val_1"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "INT64[1]"
                  },
                  {
                    "key": "param_name",
                    "value": "shape"
                  }
                ]
              }
            ],
            "inputsMetadata": [
              {
                "id": "0",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "input_ids"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "INT64[s34,s16]"
                  },
                  {
                    "key": "[metadata] pkg.torch.export.graph_signature.InputSpec.kind",
                    "value": "USER_INPUT"
                  },
                  {
                    "key": "[metadata] pkg.torch.export.graph_signature.InputSpec.persistent",
                    "value": "None"
                  },
                  {
                    "key": "param_name",
                    "value": "data"
                  }
                ]
              }
            ],
            "style": null,
            "config": null
          },
          {
            "id": "node_Squeeze_3",
            "label": "Squeeze",
            "namespace": "main_graph/_empty_nn_module_stack_from_metadata_hook: _empty_nn_module_stack_from_metadata_hook/sym_size_int_61: aten.sym_size.int",
            "subgraphIds": [],
            "attrs": [
              {
                "key": "[metadata] namespace",
                "value": "_empty_nn_module_stack_from_metadata_hook: _empty_nn_module_stack_from_metadata_hook/sym_size_int_61: aten.sym_size.int"
              },
              {
                "key": "[metadata] pkg.torch.onnx.class_hierarchy",
                "value": "['_empty_nn_module_stack_from_metadata_hook', 'aten.sym_size.int']"
              },
              {
                "key": "[metadata] pkg.torch.onnx.fx_node",
                "value": "%sym_size_int_61 : [num_users=10] = call_function[target=torch.ops.aten.sym_size.int](args = (%input_ids, 1), kwargs = {})"
              },
              {
                "key": "[metadata] pkg.torch.onnx.name_scopes",
                "value": "['_empty_nn_module_stack_from_metadata_hook', 'sym_size_int_61']"
              },
              {
                "key": "[metadata] pkg.torch.onnx.stack_trace",
                "value": "File \"torch/fx/passes/runtime_assert.py\", line 24, in insert_deferred_runtime_asserts"
              }
            ],
            "incomingEdges": [
              {
                "sourceNodeId": "node_Shape_2",
                "sourceNodeOutputId": "0",
                "targetNodeInputId": "0"
              }
            ],
            "outputsMetadata": [
              {
                "id": "0",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "sym_size_int_61"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "INT64[]"
                  },
                  {
                    "key": "param_name",
                    "value": "squeezed"
                  }
                ]
              }
            ],
            "inputsMetadata": [
              {
                "id": "0",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "val_1"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "INT64[1]"
                  },
                  {
                    "key": "param_name",
                    "value": "data"
                  }
                ]
              }
            ],
            "style": null,
            "config": null
          },
          {
            "id": "node_Shape_10",
            "label": "Shape",
            "namespace": "main_graph/_empty_nn_module_stack_from_metadata_hook: _empty_nn_module_stack_from_metadata_hook/sym_size_int_67: aten.sym_size.int",
            "subgraphIds": [],
            "attrs": [
              {
                "key": "end",
                "value": "3"
              },
              {
                "key": "start",
                "value": "2"
              },
              {
                "key": "[metadata] namespace",
                "value": "_empty_nn_module_stack_from_metadata_hook: _empty_nn_module_stack_from_metadata_hook/sym_size_int_67: aten.sym_size.int"
              },
              {
                "key": "[metadata] pkg.torch.onnx.class_hierarchy",
                "value": "['_empty_nn_module_stack_from_metadata_hook', 'aten.sym_size.int']"
              },
              {
                "key": "[metadata] pkg.torch.onnx.fx_node",
                "value": "%sym_size_int_67 : [num_users=5] = call_function[target=torch.ops.aten.sym_size.int](args = (%past_key_values_key_cache_1, 2), kwargs = {})"
              },
              {
                "key": "[metadata] pkg.torch.onnx.name_scopes",
                "value": "['_empty_nn_module_stack_from_metadata_hook', 'sym_size_int_67']"
              },
              {
                "key": "[metadata] pkg.torch.onnx.stack_trace",
                "value": "File \"torch/fx/passes/runtime_assert.py\", line 24, in insert_deferred_runtime_asserts"
              }
            ],
            "incomingEdges": [
              {
                "sourceNodeId": "[value] past_key_values_key_cache_1",
                "sourceNodeOutputId": "0",
                "targetNodeInputId": "0"
              }
            ],
            "outputsMetadata": [
              {
                "id": "0",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "val_5"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "INT64[1]"
                  },
                  {
                    "key": "param_name",
                    "value": "shape"
                  }
                ]
              }
            ],
            "inputsMetadata": [
              {
                "id": "0",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "past_key_values_key_cache_1"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "FLOAT[s34,32,s17,64]"
                  },
                  {
                    "key": "[metadata] pkg.torch.export.graph_signature.InputSpec.kind",
                    "value": "USER_INPUT"
                  },
                  {
                    "key": "[metadata] pkg.torch.export.graph_signature.InputSpec.persistent",
                    "value": "None"
                  },
                  {
                    "key": "param_name",
                    "value": "data"
                  }
                ]
              }
            ],
            "style": null,
            "config": null
          },
          {
            "id": "node_Squeeze_11",
            "label": "Squeeze",
            "namespace": "main_graph/_empty_nn_module_stack_from_metadata_hook: _empty_nn_module_stack_from_metadata_hook/sym_size_int_67: aten.sym_size.int",
            "subgraphIds": [],
            "attrs": [
              {
                "key": "[metadata] namespace",
                "value": "_empty_nn_module_stack_from_metadata_hook: _empty_nn_module_stack_from_metadata_hook/sym_size_int_67: aten.sym_size.int"
              },
              {
                "key": "[metadata] pkg.torch.onnx.class_hierarchy",
                "value": "['_empty_nn_module_stack_from_metadata_hook', 'aten.sym_size.int']"
              },
              {
                "key": "[metadata] pkg.torch.onnx.fx_node",
                "value": "%sym_size_int_67 : [num_users=5] = call_function[target=torch.ops.aten.sym_size.int](args = (%past_key_values_key_cache_1, 2), kwargs = {})"
              },
              {
                "key": "[metadata] pkg.torch.onnx.name_scopes",
                "value": "['_empty_nn_module_stack_from_metadata_hook', 'sym_size_int_67']"
              },
              {
                "key": "[metadata] pkg.torch.onnx.stack_trace",
                "value": "File \"torch/fx/passes/runtime_assert.py\", line 24, in insert_deferred_runtime_asserts"
              }
            ],
            "incomingEdges": [
              {
                "sourceNodeId": "node_Shape_10",
                "sourceNodeOutputId": "0",
                "targetNodeInputId": "0"
              }
            ],
            "outputsMetadata": [
              {
                "id": "0",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "sym_size_int_67"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "INT64[]"
                  },
                  {
                    "key": "param_name",
                    "value": "squeezed"
                  }
                ]
              }
            ],
            "inputsMetadata": [
              {
                "id": "0",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "val_5"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "INT64[1]"
                  },
                  {
                    "key": "param_name",
                    "value": "data"
                  }
                ]
              }
            ],
            "style": null,
            "config": null
          },
          {
            "id": "node_Shape_12",
            "label": "Shape",
            "namespace": "main_graph/_empty_nn_module_stack_from_metadata_hook: _empty_nn_module_stack_from_metadata_hook/sym_size_int_68: aten.sym_size.int",
            "subgraphIds": [],
            "attrs": [
              {
                "key": "end",
                "value": "1"
              },
              {
                "key": "start",
                "value": "0"
              },
              {
                "key": "[metadata] namespace",
                "value": "_empty_nn_module_stack_from_metadata_hook: _empty_nn_module_stack_from_metadata_hook/sym_size_int_68: aten.sym_size.int"
              },
              {
                "key": "[metadata] pkg.torch.onnx.class_hierarchy",
                "value": "['_empty_nn_module_stack_from_metadata_hook', 'aten.sym_size.int']"
              },
              {
                "key": "[metadata] pkg.torch.onnx.fx_node",
                "value": "%sym_size_int_68 : [num_users=16] = call_function[target=torch.ops.aten.sym_size.int](args = (%past_key_values_value_cache_0, 0), kwargs = {})"
              },
              {
                "key": "[metadata] pkg.torch.onnx.name_scopes",
                "value": "['_empty_nn_module_stack_from_metadata_hook', 'sym_size_int_68']"
              },
              {
                "key": "[metadata] pkg.torch.onnx.stack_trace",
                "value": "File \"torch/fx/passes/runtime_assert.py\", line 24, in insert_deferred_runtime_asserts"
              }
            ],
            "incomingEdges": [
              {
                "sourceNodeId": "[value] past_key_values_value_cache_0",
                "sourceNodeOutputId": "0",
                "targetNodeInputId": "0"
              }
            ],
            "outputsMetadata": [
              {
                "id": "0",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "val_6"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "INT64[1]"
                  },
                  {
                    "key": "param_name",
                    "value": "shape"
                  }
                ]
              }
            ],
            "inputsMetadata": [
              {
                "id": "0",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "past_key_values_value_cache_0"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "FLOAT[s34,32,s17,64]"
                  },
                  {
                    "key": "[metadata] pkg.torch.export.graph_signature.InputSpec.kind",
                    "value": "USER_INPUT"
                  },
                  {
                    "key": "[metadata] pkg.torch.export.graph_signature.InputSpec.persistent",
                    "value": "None"
                  },
                  {
                    "key": "param_name",
                    "value": "data"
                  }
                ]
              }
            ],
            "style": null,
            "config": null
          },
          {
            "id": "node_Gather_20",
            "label": "Gather",
            "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.embed_tokens: torch.nn.modules.sparse.Embedding/embedding: aten.embedding.default",
            "subgraphIds": [],
            "attrs": [
              {
                "key": "axis",
                "value": "0"
              },
              {
                "key": "[metadata] namespace",
                "value": ": transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.embed_tokens: torch.nn.modules.sparse.Embedding/embedding: aten.embedding.default"
              },
              {
                "key": "[metadata] pkg.torch.onnx.class_hierarchy",
                "value": "['transformers.models.llama.modeling_llama.LlamaForCausalLM', 'transformers.models.llama.modeling_llama.LlamaModel', 'torch.nn.modules.sparse.Embedding', 'aten.embedding.default']"
              },
              {
                "key": "[metadata] pkg.torch.onnx.fx_node",
                "value": "%embedding : [num_users=3] = call_function[target=torch.ops.aten.embedding.default](args = (%p_lm_head_weight, %input_ids), kwargs = {})"
              },
              {
                "key": "[metadata] pkg.torch.onnx.name_scopes",
                "value": "['', 'model', 'model.embed_tokens', 'embedding']"
              },
              {
                "key": "[metadata] pkg.torch.onnx.stack_trace",
                "value": "File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 688, in forward\n    outputs: BaseModelOutputWithPast = self.model(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 422, in forward\n    inputs_embeds = self.embed_tokens(input_ids)\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/torch/nn/modules/sparse.py\", line 190, in forward\n    return F.embedding("
              }
            ],
            "incomingEdges": [
              {
                "sourceNodeId": "[value] lm_head.weight",
                "sourceNodeOutputId": "0",
                "targetNodeInputId": "0"
              },
              {
                "sourceNodeId": "[value] input_ids",
                "sourceNodeOutputId": "0",
                "targetNodeInputId": "1"
              }
            ],
            "outputsMetadata": [
              {
                "id": "0",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "embedding"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "FLOAT[s34,s16,2048]"
                  },
                  {
                    "key": "param_name",
                    "value": "output"
                  }
                ]
              }
            ],
            "inputsMetadata": [
              {
                "id": "0",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "lm_head.weight"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "FLOAT[49152,2048]"
                  },
                  {
                    "key": "param_name",
                    "value": "data"
                  }
                ]
              },
              {
                "id": "1",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "input_ids"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "INT64[s34,s16]"
                  },
                  {
                    "key": "[metadata] pkg.torch.export.graph_signature.InputSpec.kind",
                    "value": "USER_INPUT"
                  },
                  {
                    "key": "[metadata] pkg.torch.export.graph_signature.InputSpec.persistent",
                    "value": "None"
                  },
                  {
                    "key": "param_name",
                    "value": "indices"
                  }
                ]
              }
            ],
            "style": null,
            "config": null
          },
          {
            "id": "node_Add_33",
            "label": "Add",
            "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/add_4: <built-in function add>",
            "subgraphIds": [],
            "attrs": [
              {
                "key": "[metadata] namespace",
                "value": ": transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/add_4: <built-in function add>"
              },
              {
                "key": "[metadata] pkg.torch.onnx.class_hierarchy",
                "value": "['transformers.models.llama.modeling_llama.LlamaForCausalLM', 'transformers.models.llama.modeling_llama.LlamaModel', '<built-in function add>']"
              },
              {
                "key": "[metadata] pkg.torch.onnx.fx_node",
                "value": "%add_4 : [num_users=7] = call_function[target=operator.add](args = (%sym_size_int_67, %sym_size_int_61), kwargs = {})"
              },
              {
                "key": "[metadata] pkg.torch.onnx.name_scopes",
                "value": "['', 'model', 'add_4']"
              },
              {
                "key": "[metadata] pkg.torch.onnx.stack_trace",
                "value": "File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 688, in forward\n    outputs: BaseModelOutputWithPast = self.model(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 430, in forward\n    past_seen_tokens, past_seen_tokens + inputs_embeds.shape[1], device=inputs_embeds.device"
              }
            ],
            "incomingEdges": [
              {
                "sourceNodeId": "node_Squeeze_11",
                "sourceNodeOutputId": "0",
                "targetNodeInputId": "0"
              },
              {
                "sourceNodeId": "node_Squeeze_3",
                "sourceNodeOutputId": "0",
                "targetNodeInputId": "1"
              }
            ],
            "outputsMetadata": [
              {
                "id": "0",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "add_4"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "INT64[]"
                  },
                  {
                    "key": "param_name",
                    "value": "C"
                  }
                ]
              }
            ],
            "inputsMetadata": [
              {
                "id": "0",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "sym_size_int_67"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "INT64[]"
                  },
                  {
                    "key": "param_name",
                    "value": "A"
                  }
                ]
              },
              {
                "id": "1",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "sym_size_int_61"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "INT64[]"
                  },
                  {
                    "key": "param_name",
                    "value": "B"
                  }
                ]
              }
            ],
            "style": null,
            "config": null
          },
          {
            "id": "node_Constant_704",
            "label": "Constant",
            "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/arange: aten.arange.start",
            "subgraphIds": [],
            "attrs": [
              {
                "key": "__value",
                "value": "1"
              },
              {
                "key": "value",
                "value": "TensorProtoTensor<INT64,[]>(array(1), name='val_11')"
              }
            ],
            "incomingEdges": [],
            "outputsMetadata": [
              {
                "id": "0",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "val_11"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "INT64[]"
                  },
                  {
                    "key": "param_name",
                    "value": "output"
                  }
                ]
              }
            ],
            "inputsMetadata": [],
            "style": null,
            "config": null
          },
          {
            "id": "node_Range_36",
            "label": "Range",
            "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/arange: aten.arange.start",
            "subgraphIds": [],
            "attrs": [
              {
                "key": "[metadata] namespace",
                "value": ": transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/arange: aten.arange.start"
              },
              {
                "key": "[metadata] pkg.torch.onnx.class_hierarchy",
                "value": "['transformers.models.llama.modeling_llama.LlamaForCausalLM', 'transformers.models.llama.modeling_llama.LlamaModel', 'aten.arange.start']"
              },
              {
                "key": "[metadata] pkg.torch.onnx.fx_node",
                "value": "%arange : [num_users=2] = call_function[target=torch.ops.aten.arange.start](args = (%sym_size_int_67, %add_4), kwargs = {device: cuda:0, pin_memory: False})"
              },
              {
                "key": "[metadata] pkg.torch.onnx.name_scopes",
                "value": "['', 'model', 'arange']"
              },
              {
                "key": "[metadata] pkg.torch.onnx.stack_trace",
                "value": "File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 688, in forward\n    outputs: BaseModelOutputWithPast = self.model(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 429, in forward\n    cache_position = torch.arange("
              }
            ],
            "incomingEdges": [
              {
                "sourceNodeId": "node_Squeeze_11",
                "sourceNodeOutputId": "0",
                "targetNodeInputId": "0"
              },
              {
                "sourceNodeId": "node_Add_33",
                "sourceNodeOutputId": "0",
                "targetNodeInputId": "1"
              },
              {
                "sourceNodeId": "node_Constant_704",
                "sourceNodeOutputId": "0",
                "targetNodeInputId": "2"
              }
            ],
            "outputsMetadata": [
              {
                "id": "0",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "arange"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "INT64[s16]"
                  },
                  {
                    "key": "param_name",
                    "value": "output"
                  }
                ]
              }
            ],
            "inputsMetadata": [
              {
                "id": "0",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "sym_size_int_67"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "INT64[]"
                  },
                  {
                    "key": "param_name",
                    "value": "start"
                  }
                ]
              },
              {
                "id": "1",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "add_4"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "INT64[]"
                  },
                  {
                    "key": "param_name",
                    "value": "limit"
                  }
                ]
              },
              {
                "id": "2",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "val_11"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "INT64[]"
                  },
                  {
                    "key": "param_name",
                    "value": "delta"
                  }
                ]
              }
            ],
            "style": null,
            "config": null
          },
          {
            "id": "node_Constant_37",
            "label": "Constant",
            "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/unsqueeze: aten.unsqueeze.default",
            "subgraphIds": [],
            "attrs": [
              {
                "key": "__value",
                "value": "[0]"
              },
              {
                "key": "value",
                "value": "TensorProtoTensor<INT64,[1]>(array([0]), name='')"
              },
              {
                "key": "[metadata] namespace",
                "value": ": transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/unsqueeze: aten.unsqueeze.default"
              },
              {
                "key": "[metadata] pkg.torch.onnx.class_hierarchy",
                "value": "['transformers.models.llama.modeling_llama.LlamaForCausalLM', 'transformers.models.llama.modeling_llama.LlamaModel', 'aten.unsqueeze.default']"
              },
              {
                "key": "[metadata] pkg.torch.onnx.fx_node",
                "value": "%unsqueeze : [num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%arange, 0), kwargs = {})"
              },
              {
                "key": "[metadata] pkg.torch.onnx.name_scopes",
                "value": "['', 'model', 'unsqueeze']"
              },
              {
                "key": "[metadata] pkg.torch.onnx.stack_trace",
                "value": "File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 688, in forward\n    outputs: BaseModelOutputWithPast = self.model(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 434, in forward\n    position_ids = cache_position.unsqueeze(0)"
              }
            ],
            "incomingEdges": [],
            "outputsMetadata": [
              {
                "id": "0",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "val_12"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "INT64[1]"
                  },
                  {
                    "key": "param_name",
                    "value": "output"
                  }
                ]
              }
            ],
            "inputsMetadata": [],
            "style": null,
            "config": null
          },
          {
            "id": "node_Unsqueeze_38",
            "label": "Unsqueeze",
            "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/unsqueeze: aten.unsqueeze.default",
            "subgraphIds": [],
            "attrs": [
              {
                "key": "[metadata] namespace",
                "value": ": transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/unsqueeze: aten.unsqueeze.default"
              },
              {
                "key": "[metadata] pkg.torch.onnx.class_hierarchy",
                "value": "['transformers.models.llama.modeling_llama.LlamaForCausalLM', 'transformers.models.llama.modeling_llama.LlamaModel', 'aten.unsqueeze.default']"
              },
              {
                "key": "[metadata] pkg.torch.onnx.fx_node",
                "value": "%unsqueeze : [num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%arange, 0), kwargs = {})"
              },
              {
                "key": "[metadata] pkg.torch.onnx.name_scopes",
                "value": "['', 'model', 'unsqueeze']"
              },
              {
                "key": "[metadata] pkg.torch.onnx.stack_trace",
                "value": "File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 688, in forward\n    outputs: BaseModelOutputWithPast = self.model(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 434, in forward\n    position_ids = cache_position.unsqueeze(0)"
              }
            ],
            "incomingEdges": [
              {
                "sourceNodeId": "node_Range_36",
                "sourceNodeOutputId": "0",
                "targetNodeInputId": "0"
              },
              {
                "sourceNodeId": "node_Constant_37",
                "sourceNodeOutputId": "0",
                "targetNodeInputId": "1"
              }
            ],
            "outputsMetadata": [
              {
                "id": "0",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "unsqueeze"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "INT64[1,s16]"
                  },
                  {
                    "key": "param_name",
                    "value": "expanded"
                  }
                ]
              }
            ],
            "inputsMetadata": [
              {
                "id": "0",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "arange"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "INT64[s16]"
                  },
                  {
                    "key": "param_name",
                    "value": "data"
                  }
                ]
              },
              {
                "id": "1",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "val_12"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "INT64[1]"
                  },
                  {
                    "key": "param_name",
                    "value": "axes"
                  }
                ]
              }
            ],
            "style": null,
            "config": null
          },
          {
            "id": "node_Constant_706",
            "label": "Constant",
            "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/full: aten.full.default",
            "subgraphIds": [],
            "attrs": [
              {
                "key": "__value",
                "value": "-3.4028234663852886e+38"
              },
              {
                "key": "value",
                "value": "TensorProtoTensor<FLOAT,[]>(array(-3.4028235e+38, dtype=float32), name='val_14')"
              }
            ],
            "incomingEdges": [],
            "outputsMetadata": [
              {
                "id": "0",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "val_14"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "FLOAT[]"
                  },
                  {
                    "key": "param_name",
                    "value": "output"
                  }
                ]
              }
            ],
            "inputsMetadata": [],
            "style": null,
            "config": null
          },
          {
            "id": "node_Constant_43",
            "label": "Constant",
            "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/full: aten.full.default",
            "subgraphIds": [],
            "attrs": [
              {
                "key": "__value",
                "value": "[-1]"
              },
              {
                "key": "value",
                "value": "TensorProtoTensor<INT64,[1]>(array([-1]), name='')"
              },
              {
                "key": "[metadata] namespace",
                "value": ": transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/full: aten.full.default"
              },
              {
                "key": "[metadata] pkg.torch.onnx.class_hierarchy",
                "value": "['transformers.models.llama.modeling_llama.LlamaForCausalLM', 'transformers.models.llama.modeling_llama.LlamaModel', 'aten.full.default']"
              },
              {
                "key": "[metadata] pkg.torch.onnx.fx_node",
                "value": "%full : [num_users=1] = call_function[target=torch.ops.aten.full.default](args = ([%sym_size_int_61, %add_4], -3.4028234663852886e+38), kwargs = {dtype: torch.float32, device: cuda:0, pin_memory: False})"
              },
              {
                "key": "[metadata] pkg.torch.onnx.name_scopes",
                "value": "['', 'model', 'full']"
              },
              {
                "key": "[metadata] pkg.torch.onnx.stack_trace",
                "value": "File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 688, in forward\n    outputs: BaseModelOutputWithPast = self.model(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 436, in forward\n    causal_mask = self._update_causal_mask("
              }
            ],
            "incomingEdges": [],
            "outputsMetadata": [
              {
                "id": "0",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "val_17"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "INT64[1]"
                  },
                  {
                    "key": "param_name",
                    "value": "output"
                  }
                ]
              }
            ],
            "inputsMetadata": [],
            "style": null,
            "config": null
          },
          {
            "id": "node_Reshape_44",
            "label": "Reshape",
            "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/full: aten.full.default",
            "subgraphIds": [],
            "attrs": [
              {
                "key": "allowzero",
                "value": "0"
              },
              {
                "key": "[metadata] namespace",
                "value": ": transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/full: aten.full.default"
              },
              {
                "key": "[metadata] pkg.torch.onnx.class_hierarchy",
                "value": "['transformers.models.llama.modeling_llama.LlamaForCausalLM', 'transformers.models.llama.modeling_llama.LlamaModel', 'aten.full.default']"
              },
              {
                "key": "[metadata] pkg.torch.onnx.fx_node",
                "value": "%full : [num_users=1] = call_function[target=torch.ops.aten.full.default](args = ([%sym_size_int_61, %add_4], -3.4028234663852886e+38), kwargs = {dtype: torch.float32, device: cuda:0, pin_memory: False})"
              },
              {
                "key": "[metadata] pkg.torch.onnx.name_scopes",
                "value": "['', 'model', 'full']"
              },
              {
                "key": "[metadata] pkg.torch.onnx.stack_trace",
                "value": "File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 688, in forward\n    outputs: BaseModelOutputWithPast = self.model(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 436, in forward\n    causal_mask = self._update_causal_mask("
              }
            ],
            "incomingEdges": [
              {
                "sourceNodeId": "node_Add_33",
                "sourceNodeOutputId": "0",
                "targetNodeInputId": "0"
              },
              {
                "sourceNodeId": "node_Constant_43",
                "sourceNodeOutputId": "0",
                "targetNodeInputId": "1"
              }
            ],
            "outputsMetadata": [
              {
                "id": "0",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "val_18"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "INT64[1]"
                  },
                  {
                    "key": "param_name",
                    "value": "reshaped"
                  }
                ]
              }
            ],
            "inputsMetadata": [
              {
                "id": "0",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "add_4"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "INT64[]"
                  },
                  {
                    "key": "param_name",
                    "value": "data"
                  }
                ]
              },
              {
                "id": "1",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "val_17"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "INT64[1]"
                  },
                  {
                    "key": "param_name",
                    "value": "shape"
                  }
                ]
              }
            ],
            "style": null,
            "config": null
          },
          {
            "id": "node_Concat_45",
            "label": "Concat",
            "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/full: aten.full.default",
            "subgraphIds": [],
            "attrs": [
              {
                "key": "axis",
                "value": "0"
              },
              {
                "key": "[metadata] namespace",
                "value": ": transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/full: aten.full.default"
              },
              {
                "key": "[metadata] pkg.torch.onnx.class_hierarchy",
                "value": "['transformers.models.llama.modeling_llama.LlamaForCausalLM', 'transformers.models.llama.modeling_llama.LlamaModel', 'aten.full.default']"
              },
              {
                "key": "[metadata] pkg.torch.onnx.fx_node",
                "value": "%full : [num_users=1] = call_function[target=torch.ops.aten.full.default](args = ([%sym_size_int_61, %add_4], -3.4028234663852886e+38), kwargs = {dtype: torch.float32, device: cuda:0, pin_memory: False})"
              },
              {
                "key": "[metadata] pkg.torch.onnx.name_scopes",
                "value": "['', 'model', 'full']"
              },
              {
                "key": "[metadata] pkg.torch.onnx.stack_trace",
                "value": "File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 688, in forward\n    outputs: BaseModelOutputWithPast = self.model(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 436, in forward\n    causal_mask = self._update_causal_mask("
              }
            ],
            "incomingEdges": [
              {
                "sourceNodeId": "node_Shape_2",
                "sourceNodeOutputId": "0",
                "targetNodeInputId": "0"
              },
              {
                "sourceNodeId": "node_Reshape_44",
                "sourceNodeOutputId": "0",
                "targetNodeInputId": "1"
              }
            ],
            "outputsMetadata": [
              {
                "id": "0",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "val_19"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "INT64[2]"
                  },
                  {
                    "key": "param_name",
                    "value": "concat_result"
                  }
                ]
              }
            ],
            "inputsMetadata": [
              {
                "id": "0",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "val_1"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "INT64[1]"
                  },
                  {
                    "key": "param_name",
                    "value": "inputs"
                  }
                ]
              },
              {
                "id": "1",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "val_18"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "INT64[1]"
                  },
                  {
                    "key": "param_name",
                    "value": "inputs"
                  }
                ]
              }
            ],
            "style": null,
            "config": null
          },
          {
            "id": "node_Expand_47",
            "label": "Expand",
            "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/full: aten.full.default",
            "subgraphIds": [],
            "attrs": [
              {
                "key": "[metadata] namespace",
                "value": ": transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/full: aten.full.default"
              },
              {
                "key": "[metadata] pkg.torch.onnx.class_hierarchy",
                "value": "['transformers.models.llama.modeling_llama.LlamaForCausalLM', 'transformers.models.llama.modeling_llama.LlamaModel', 'aten.full.default']"
              },
              {
                "key": "[metadata] pkg.torch.onnx.fx_node",
                "value": "%full : [num_users=1] = call_function[target=torch.ops.aten.full.default](args = ([%sym_size_int_61, %add_4], -3.4028234663852886e+38), kwargs = {dtype: torch.float32, device: cuda:0, pin_memory: False})"
              },
              {
                "key": "[metadata] pkg.torch.onnx.name_scopes",
                "value": "['', 'model', 'full']"
              },
              {
                "key": "[metadata] pkg.torch.onnx.stack_trace",
                "value": "File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 688, in forward\n    outputs: BaseModelOutputWithPast = self.model(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 436, in forward\n    causal_mask = self._update_causal_mask("
              }
            ],
            "incomingEdges": [
              {
                "sourceNodeId": "node_Constant_706",
                "sourceNodeOutputId": "0",
                "targetNodeInputId": "0"
              },
              {
                "sourceNodeId": "node_Concat_45",
                "sourceNodeOutputId": "0",
                "targetNodeInputId": "1"
              }
            ],
            "outputsMetadata": [
              {
                "id": "0",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "full"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "FLOAT[s16,s16 + s17]"
                  },
                  {
                    "key": "param_name",
                    "value": "output"
                  }
                ]
              }
            ],
            "inputsMetadata": [
              {
                "id": "0",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "val_14"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "FLOAT[]"
                  },
                  {
                    "key": "param_name",
                    "value": "input"
                  }
                ]
              },
              {
                "id": "1",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "val_19"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "INT64[2]"
                  },
                  {
                    "key": "param_name",
                    "value": "shape"
                  }
                ]
              }
            ],
            "style": null,
            "config": null
          },
          {
            "id": "n0",
            "label": "Constant",
            "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/triu: aten.triu.default",
            "subgraphIds": [],
            "attrs": [
              {
                "key": "__value",
                "value": "1"
              },
              {
                "key": "value_int",
                "value": "1"
              },
              {
                "key": "[metadata] namespace",
                "value": ": transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/triu: aten.triu.default"
              },
              {
                "key": "[metadata] pkg.torch.onnx.class_hierarchy",
                "value": "['transformers.models.llama.modeling_llama.LlamaForCausalLM', 'transformers.models.llama.modeling_llama.LlamaModel', 'aten.triu.default']"
              },
              {
                "key": "[metadata] pkg.torch.onnx.fx_node",
                "value": "%triu : [num_users=1] = call_function[target=torch.ops.aten.triu.default](args = (%full, 1), kwargs = {})"
              },
              {
                "key": "[metadata] pkg.torch.onnx.name_scopes",
                "value": "['', 'model', 'triu']"
              },
              {
                "key": "[metadata] pkg.torch.onnx.stack_trace",
                "value": "File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 688, in forward\n    outputs: BaseModelOutputWithPast = self.model(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 436, in forward\n    causal_mask = self._update_causal_mask("
              }
            ],
            "incomingEdges": [],
            "outputsMetadata": [
              {
                "id": "0",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "diagonal"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "INT64[]"
                  },
                  {
                    "key": "param_name",
                    "value": "output"
                  }
                ]
              }
            ],
            "inputsMetadata": [],
            "style": null,
            "config": null
          },
          {
            "id": "n1",
            "label": "Trilu",
            "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/triu: aten.triu.default",
            "subgraphIds": [],
            "attrs": [
              {
                "key": "upper",
                "value": "1"
              },
              {
                "key": "[metadata] namespace",
                "value": ": transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/triu: aten.triu.default"
              },
              {
                "key": "[metadata] pkg.torch.onnx.class_hierarchy",
                "value": "['transformers.models.llama.modeling_llama.LlamaForCausalLM', 'transformers.models.llama.modeling_llama.LlamaModel', 'aten.triu.default']"
              },
              {
                "key": "[metadata] pkg.torch.onnx.fx_node",
                "value": "%triu : [num_users=1] = call_function[target=torch.ops.aten.triu.default](args = (%full, 1), kwargs = {})"
              },
              {
                "key": "[metadata] pkg.torch.onnx.name_scopes",
                "value": "['', 'model', 'triu']"
              },
              {
                "key": "[metadata] pkg.torch.onnx.stack_trace",
                "value": "File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 688, in forward\n    outputs: BaseModelOutputWithPast = self.model(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 436, in forward\n    causal_mask = self._update_causal_mask("
              }
            ],
            "incomingEdges": [
              {
                "sourceNodeId": "node_Expand_47",
                "sourceNodeOutputId": "0",
                "targetNodeInputId": "0"
              },
              {
                "sourceNodeId": "n0",
                "sourceNodeOutputId": "0",
                "targetNodeInputId": "1"
              }
            ],
            "outputsMetadata": [
              {
                "id": "0",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "triu"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "FLOAT[s16,s16 + s17]"
                  },
                  {
                    "key": "param_name",
                    "value": "output"
                  }
                ]
              }
            ],
            "inputsMetadata": [
              {
                "id": "0",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "full"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "FLOAT[s16,s16 + s17]"
                  },
                  {
                    "key": "param_name",
                    "value": "input"
                  }
                ]
              },
              {
                "id": "1",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "diagonal"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "INT64[]"
                  },
                  {
                    "key": "param_name",
                    "value": "k"
                  }
                ]
              }
            ],
            "style": null,
            "config": null
          },
          {
            "id": "node_Constant_709",
            "label": "Constant",
            "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/arange_1: aten.arange.default",
            "subgraphIds": [],
            "attrs": [
              {
                "key": "__value",
                "value": "0"
              },
              {
                "key": "value",
                "value": "TensorProtoTensor<INT64,[]>(array(0), name='val_22')"
              }
            ],
            "incomingEdges": [],
            "outputsMetadata": [
              {
                "id": "0",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "val_22"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "INT64[]"
                  },
                  {
                    "key": "param_name",
                    "value": "output"
                  }
                ]
              }
            ],
            "inputsMetadata": [],
            "style": null,
            "config": null
          },
          {
            "id": "node_Constant_711",
            "label": "Constant",
            "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/arange_1: aten.arange.default",
            "subgraphIds": [],
            "attrs": [
              {
                "key": "__value",
                "value": "1"
              },
              {
                "key": "value",
                "value": "TensorProtoTensor<INT64,[]>(array(1), name='val_23')"
              }
            ],
            "incomingEdges": [],
            "outputsMetadata": [
              {
                "id": "0",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "val_23"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "INT64[]"
                  },
                  {
                    "key": "param_name",
                    "value": "output"
                  }
                ]
              }
            ],
            "inputsMetadata": [],
            "style": null,
            "config": null
          },
          {
            "id": "node_Range_52",
            "label": "Range",
            "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/arange_1: aten.arange.default",
            "subgraphIds": [],
            "attrs": [
              {
                "key": "[metadata] namespace",
                "value": ": transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/arange_1: aten.arange.default"
              },
              {
                "key": "[metadata] pkg.torch.onnx.class_hierarchy",
                "value": "['transformers.models.llama.modeling_llama.LlamaForCausalLM', 'transformers.models.llama.modeling_llama.LlamaModel', 'aten.arange.default']"
              },
              {
                "key": "[metadata] pkg.torch.onnx.fx_node",
                "value": "%arange_1 : [num_users=1] = call_function[target=torch.ops.aten.arange.default](args = (%add_4,), kwargs = {device: cuda:0, pin_memory: False})"
              },
              {
                "key": "[metadata] pkg.torch.onnx.name_scopes",
                "value": "['', 'model', 'arange_1']"
              },
              {
                "key": "[metadata] pkg.torch.onnx.stack_trace",
                "value": "File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 688, in forward\n    outputs: BaseModelOutputWithPast = self.model(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 436, in forward\n    causal_mask = self._update_causal_mask("
              }
            ],
            "incomingEdges": [
              {
                "sourceNodeId": "node_Constant_709",
                "sourceNodeOutputId": "0",
                "targetNodeInputId": "0"
              },
              {
                "sourceNodeId": "node_Add_33",
                "sourceNodeOutputId": "0",
                "targetNodeInputId": "1"
              },
              {
                "sourceNodeId": "node_Constant_711",
                "sourceNodeOutputId": "0",
                "targetNodeInputId": "2"
              }
            ],
            "outputsMetadata": [
              {
                "id": "0",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "arange_1"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "INT64[s16 + s17]"
                  },
                  {
                    "key": "param_name",
                    "value": "output"
                  }
                ]
              }
            ],
            "inputsMetadata": [
              {
                "id": "0",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "val_22"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "INT64[]"
                  },
                  {
                    "key": "param_name",
                    "value": "start"
                  }
                ]
              },
              {
                "id": "1",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "add_4"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "INT64[]"
                  },
                  {
                    "key": "param_name",
                    "value": "limit"
                  }
                ]
              },
              {
                "id": "2",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "val_23"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "INT64[]"
                  },
                  {
                    "key": "param_name",
                    "value": "delta"
                  }
                ]
              }
            ],
            "style": null,
            "config": null
          },
          {
            "id": "node_Constant_713",
            "label": "Constant",
            "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/view: aten.view.default",
            "subgraphIds": [],
            "attrs": [
              {
                "key": "__value",
                "value": "[-1,1]"
              },
              {
                "key": "value",
                "value": "TensorProtoTensor<INT64,[2]>(array([-1,  1]), name='val_25')"
              }
            ],
            "incomingEdges": [],
            "outputsMetadata": [
              {
                "id": "0",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "val_25"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "INT64[2]"
                  },
                  {
                    "key": "param_name",
                    "value": "output"
                  }
                ]
              }
            ],
            "inputsMetadata": [],
            "style": null,
            "config": null
          },
          {
            "id": "node_Reshape_55",
            "label": "Reshape",
            "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/view: aten.view.default",
            "subgraphIds": [],
            "attrs": [
              {
                "key": "allowzero",
                "value": "1"
              },
              {
                "key": "[metadata] namespace",
                "value": ": transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/view: aten.view.default"
              },
              {
                "key": "[metadata] pkg.torch.onnx.class_hierarchy",
                "value": "['transformers.models.llama.modeling_llama.LlamaForCausalLM', 'transformers.models.llama.modeling_llama.LlamaModel', 'aten.view.default']"
              },
              {
                "key": "[metadata] pkg.torch.onnx.fx_node",
                "value": "%view : [num_users=1] = call_function[target=torch.ops.aten.view.default](args = (%arange, [-1, 1]), kwargs = {})"
              },
              {
                "key": "[metadata] pkg.torch.onnx.name_scopes",
                "value": "['', 'model', 'view']"
              },
              {
                "key": "[metadata] pkg.torch.onnx.stack_trace",
                "value": "File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 688, in forward\n    outputs: BaseModelOutputWithPast = self.model(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 436, in forward\n    causal_mask = self._update_causal_mask("
              }
            ],
            "incomingEdges": [
              {
                "sourceNodeId": "node_Range_36",
                "sourceNodeOutputId": "0",
                "targetNodeInputId": "0"
              },
              {
                "sourceNodeId": "node_Constant_713",
                "sourceNodeOutputId": "0",
                "targetNodeInputId": "1"
              }
            ],
            "outputsMetadata": [
              {
                "id": "0",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "view"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "INT64[s16,1]"
                  },
                  {
                    "key": "param_name",
                    "value": "reshaped"
                  }
                ]
              }
            ],
            "inputsMetadata": [
              {
                "id": "0",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "arange"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "INT64[s16]"
                  },
                  {
                    "key": "param_name",
                    "value": "data"
                  }
                ]
              },
              {
                "id": "1",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "val_25"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "INT64[2]"
                  },
                  {
                    "key": "param_name",
                    "value": "shape"
                  }
                ]
              }
            ],
            "style": null,
            "config": null
          },
          {
            "id": "node_Greater_56",
            "label": "Greater",
            "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/gt: aten.gt.Tensor",
            "subgraphIds": [],
            "attrs": [
              {
                "key": "[metadata] namespace",
                "value": ": transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/gt: aten.gt.Tensor"
              },
              {
                "key": "[metadata] pkg.torch.onnx.class_hierarchy",
                "value": "['transformers.models.llama.modeling_llama.LlamaForCausalLM', 'transformers.models.llama.modeling_llama.LlamaModel', 'aten.gt.Tensor']"
              },
              {
                "key": "[metadata] pkg.torch.onnx.fx_node",
                "value": "%gt : [num_users=1] = call_function[target=torch.ops.aten.gt.Tensor](args = (%arange_1, %view), kwargs = {})"
              },
              {
                "key": "[metadata] pkg.torch.onnx.name_scopes",
                "value": "['', 'model', 'gt']"
              },
              {
                "key": "[metadata] pkg.torch.onnx.stack_trace",
                "value": "File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 688, in forward\n    outputs: BaseModelOutputWithPast = self.model(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 436, in forward\n    causal_mask = self._update_causal_mask("
              }
            ],
            "incomingEdges": [
              {
                "sourceNodeId": "node_Range_52",
                "sourceNodeOutputId": "0",
                "targetNodeInputId": "0"
              },
              {
                "sourceNodeId": "node_Reshape_55",
                "sourceNodeOutputId": "0",
                "targetNodeInputId": "1"
              }
            ],
            "outputsMetadata": [
              {
                "id": "0",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "gt"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "BOOL[s16,s16 + s17]"
                  },
                  {
                    "key": "param_name",
                    "value": "C"
                  }
                ]
              }
            ],
            "inputsMetadata": [
              {
                "id": "0",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "arange_1"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "INT64[s16 + s17]"
                  },
                  {
                    "key": "param_name",
                    "value": "A"
                  }
                ]
              },
              {
                "id": "1",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "view"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "INT64[s16,1]"
                  },
                  {
                    "key": "param_name",
                    "value": "B"
                  }
                ]
              }
            ],
            "style": null,
            "config": null
          },
          {
            "id": "node_Cast_57",
            "label": "Cast",
            "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/convert_element_type_default: prims.convert_element_type.default",
            "subgraphIds": [],
            "attrs": [
              {
                "key": "to",
                "value": "FLOAT"
              },
              {
                "key": "[metadata] namespace",
                "value": ": transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/convert_element_type_default: prims.convert_element_type.default"
              },
              {
                "key": "[metadata] pkg.torch.onnx.class_hierarchy",
                "value": "['transformers.models.llama.modeling_llama.LlamaForCausalLM', 'transformers.models.llama.modeling_llama.LlamaModel', 'prims.convert_element_type.default']"
              },
              {
                "key": "[metadata] pkg.torch.onnx.fx_node",
                "value": "%convert_element_type_default : [num_users=1] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%gt,), kwargs = {dtype: torch.float32})"
              },
              {
                "key": "[metadata] pkg.torch.onnx.name_scopes",
                "value": "['', 'model', 'convert_element_type_default']"
              },
              {
                "key": "[metadata] pkg.torch.onnx.stack_trace",
                "value": "File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 688, in forward\n    outputs: BaseModelOutputWithPast = self.model(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 436, in forward\n    causal_mask = self._update_causal_mask("
              }
            ],
            "incomingEdges": [
              {
                "sourceNodeId": "node_Greater_56",
                "sourceNodeOutputId": "0",
                "targetNodeInputId": "0"
              }
            ],
            "outputsMetadata": [
              {
                "id": "0",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "convert_element_type_default"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "FLOAT[s16,s16 + s17]"
                  },
                  {
                    "key": "param_name",
                    "value": "output"
                  }
                ]
              }
            ],
            "inputsMetadata": [
              {
                "id": "0",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "gt"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "BOOL[s16,s16 + s17]"
                  },
                  {
                    "key": "param_name",
                    "value": "input"
                  }
                ]
              }
            ],
            "style": null,
            "config": null
          },
          {
            "id": "node_Mul_58",
            "label": "Mul",
            "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/mul_16: aten.mul.Tensor",
            "subgraphIds": [],
            "attrs": [
              {
                "key": "[metadata] namespace",
                "value": ": transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/mul_16: aten.mul.Tensor"
              },
              {
                "key": "[metadata] pkg.torch.onnx.class_hierarchy",
                "value": "['transformers.models.llama.modeling_llama.LlamaForCausalLM', 'transformers.models.llama.modeling_llama.LlamaModel', 'aten.mul.Tensor']"
              },
              {
                "key": "[metadata] pkg.torch.onnx.fx_node",
                "value": "%mul_16 : [num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%triu, %convert_element_type_default), kwargs = {})"
              },
              {
                "key": "[metadata] pkg.torch.onnx.name_scopes",
                "value": "['', 'model', 'mul_16']"
              },
              {
                "key": "[metadata] pkg.torch.onnx.stack_trace",
                "value": "File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 688, in forward\n    outputs: BaseModelOutputWithPast = self.model(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 436, in forward\n    causal_mask = self._update_causal_mask("
              }
            ],
            "incomingEdges": [
              {
                "sourceNodeId": "n1",
                "sourceNodeOutputId": "0",
                "targetNodeInputId": "0"
              },
              {
                "sourceNodeId": "node_Cast_57",
                "sourceNodeOutputId": "0",
                "targetNodeInputId": "1"
              }
            ],
            "outputsMetadata": [
              {
                "id": "0",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "mul_16"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "FLOAT[s16,s16 + s17]"
                  },
                  {
                    "key": "param_name",
                    "value": "C"
                  }
                ]
              }
            ],
            "inputsMetadata": [
              {
                "id": "0",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "triu"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "FLOAT[s16,s16 + s17]"
                  },
                  {
                    "key": "param_name",
                    "value": "A"
                  }
                ]
              },
              {
                "id": "1",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "convert_element_type_default"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "FLOAT[s16,s16 + s17]"
                  },
                  {
                    "key": "param_name",
                    "value": "B"
                  }
                ]
              }
            ],
            "style": null,
            "config": null
          },
          {
            "id": "node_Constant_60",
            "label": "Constant",
            "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel",
            "subgraphIds": [],
            "attrs": [
              {
                "key": "__value",
                "value": "[1]"
              },
              {
                "key": "value",
                "value": "TensorProtoTensor<INT64,[1]>(array([1]), name='')"
              },
              {
                "key": "[metadata] namespace",
                "value": ": transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/unsqueeze_4: aten.unsqueeze.default"
              },
              {
                "key": "[metadata] pkg.torch.onnx.class_hierarchy",
                "value": "['transformers.models.llama.modeling_llama.LlamaForCausalLM', 'transformers.models.llama.modeling_llama.LlamaModel', 'aten.unsqueeze.default']"
              },
              {
                "key": "[metadata] pkg.torch.onnx.fx_node",
                "value": "%unsqueeze_4 : [num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%unsqueeze_3, 1), kwargs = {})"
              },
              {
                "key": "[metadata] pkg.torch.onnx.name_scopes",
                "value": "['', 'model', 'unsqueeze_4']"
              },
              {
                "key": "[metadata] pkg.torch.onnx.stack_trace",
                "value": "File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 688, in forward\n    outputs: BaseModelOutputWithPast = self.model(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 436, in forward\n    causal_mask = self._update_causal_mask("
              }
            ],
            "incomingEdges": [],
            "outputsMetadata": [
              {
                "id": "0",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "val_26"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "INT64[1]"
                  },
                  {
                    "key": "param_name",
                    "value": "output"
                  }
                ]
              }
            ],
            "inputsMetadata": [],
            "style": null,
            "config": null
          },
          {
            "id": "node_Constant_1026",
            "label": "Constant",
            "namespace": "main_graph",
            "subgraphIds": [],
            "attrs": [
              {
                "key": "__value",
                "value": "[0,1]"
              },
              {
                "key": "value",
                "value": "TensorProtoTensor<INT64,[2]>(array([0, 1]), name='val_491')"
              }
            ],
            "incomingEdges": [],
            "outputsMetadata": [
              {
                "id": "0",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "val_491"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "INT64[2]"
                  },
                  {
                    "key": "param_name",
                    "value": "output"
                  }
                ]
              }
            ],
            "inputsMetadata": [],
            "style": null,
            "config": null
          },
          {
            "id": "node_Unsqueeze_1027",
            "label": "Unsqueeze",
            "namespace": "main_graph",
            "subgraphIds": [],
            "attrs": [],
            "incomingEdges": [
              {
                "sourceNodeId": "node_Mul_58",
                "sourceNodeOutputId": "0",
                "targetNodeInputId": "0"
              },
              {
                "sourceNodeId": "node_Constant_1026",
                "sourceNodeOutputId": "0",
                "targetNodeInputId": "1"
              }
            ],
            "outputsMetadata": [
              {
                "id": "0",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "unsqueeze_4"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "FLOAT[1,1,s16,s16 + s17]"
                  },
                  {
                    "key": "param_name",
                    "value": "expanded"
                  }
                ]
              }
            ],
            "inputsMetadata": [
              {
                "id": "0",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "mul_16"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "FLOAT[s16,s16 + s17]"
                  },
                  {
                    "key": "param_name",
                    "value": "data"
                  }
                ]
              },
              {
                "id": "1",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "val_491"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "INT64[2]"
                  },
                  {
                    "key": "param_name",
                    "value": "axes"
                  }
                ]
              }
            ],
            "style": null,
            "config": null
          },
          {
            "id": "node_Constant_62",
            "label": "Constant",
            "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel",
            "subgraphIds": [],
            "attrs": [
              {
                "key": "__value",
                "value": "0"
              },
              {
                "key": "value",
                "value": "TensorProtoTensor<INT64,[]>(array(0), name='')"
              },
              {
                "key": "[metadata] namespace",
                "value": ": transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/slice_3: aten.slice.Tensor"
              },
              {
                "key": "[metadata] pkg.torch.onnx.class_hierarchy",
                "value": "['transformers.models.llama.modeling_llama.LlamaForCausalLM', 'transformers.models.llama.modeling_llama.LlamaModel', 'aten.slice.Tensor']"
              },
              {
                "key": "[metadata] pkg.torch.onnx.fx_node",
                "value": "%slice_3 : [num_users=1] = call_function[target=torch.ops.aten.slice.Tensor](args = (%unsqueeze_4, 2, 0, 9223372036854775807), kwargs = {})"
              },
              {
                "key": "[metadata] pkg.torch.onnx.name_scopes",
                "value": "['', 'model', 'slice_3']"
              },
              {
                "key": "[metadata] pkg.torch.onnx.stack_trace",
                "value": "File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 688, in forward\n    outputs: BaseModelOutputWithPast = self.model(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 436, in forward\n    causal_mask = self._update_causal_mask("
              }
            ],
            "incomingEdges": [],
            "outputsMetadata": [
              {
                "id": "0",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "val_27"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "INT64[]"
                  },
                  {
                    "key": "param_name",
                    "value": "output"
                  }
                ]
              }
            ],
            "inputsMetadata": [],
            "style": null,
            "config": null
          },
          {
            "id": "node_Constant_70",
            "label": "Constant",
            "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/slice_scatter: aten.slice_scatter.default",
            "subgraphIds": [],
            "attrs": [
              {
                "key": "__value",
                "value": "2"
              },
              {
                "key": "value",
                "value": "TensorProtoTensor<INT64,[]>(array(2), name='')"
              },
              {
                "key": "[metadata] namespace",
                "value": ": transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/slice_3: aten.slice.Tensor"
              },
              {
                "key": "[metadata] pkg.torch.onnx.class_hierarchy",
                "value": "['transformers.models.llama.modeling_llama.LlamaForCausalLM', 'transformers.models.llama.modeling_llama.LlamaModel', 'aten.slice.Tensor']"
              },
              {
                "key": "[metadata] pkg.torch.onnx.fx_node",
                "value": "%slice_3 : [num_users=1] = call_function[target=torch.ops.aten.slice.Tensor](args = (%unsqueeze_4, 2, 0, 9223372036854775807), kwargs = {})"
              },
              {
                "key": "[metadata] pkg.torch.onnx.name_scopes",
                "value": "['', 'model', 'slice_3']"
              },
              {
                "key": "[metadata] pkg.torch.onnx.stack_trace",
                "value": "File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 688, in forward\n    outputs: BaseModelOutputWithPast = self.model(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 436, in forward\n    causal_mask = self._update_causal_mask("
              }
            ],
            "incomingEdges": [],
            "outputsMetadata": [
              {
                "id": "0",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "val_35"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "INT64[]"
                  },
                  {
                    "key": "param_name",
                    "value": "output"
                  }
                ]
              }
            ],
            "inputsMetadata": [],
            "style": null,
            "config": null
          },
          {
            "id": "node_Constant_90",
            "label": "Constant",
            "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel",
            "subgraphIds": [],
            "attrs": [
              {
                "key": "__value",
                "value": "[-1]"
              },
              {
                "key": "value",
                "value": "TensorProtoTensor<INT64,[1]>(array([-1]), name='')"
              },
              {
                "key": "[metadata] namespace",
                "value": ": transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/expand_1: aten.expand.default"
              },
              {
                "key": "[metadata] pkg.torch.onnx.class_hierarchy",
                "value": "['transformers.models.llama.modeling_llama.LlamaForCausalLM', 'transformers.models.llama.modeling_llama.LlamaModel', 'aten.expand.default']"
              },
              {
                "key": "[metadata] pkg.torch.onnx.fx_node",
                "value": "%expand_1 : [num_users=1] = call_function[target=torch.ops.aten.expand.default](args = (%slice_4, [%sym_size_int_68, 1, -1, -1]), kwargs = {})"
              },
              {
                "key": "[metadata] pkg.torch.onnx.name_scopes",
                "value": "['', 'model', 'expand_1']"
              },
              {
                "key": "[metadata] pkg.torch.onnx.stack_trace",
                "value": "File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 688, in forward\n    outputs: BaseModelOutputWithPast = self.model(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 436, in forward\n    causal_mask = self._update_causal_mask("
              }
            ],
            "incomingEdges": [],
            "outputsMetadata": [
              {
                "id": "0",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "val_53"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "INT64[1]"
                  },
                  {
                    "key": "param_name",
                    "value": "output"
                  }
                ]
              }
            ],
            "inputsMetadata": [],
            "style": null,
            "config": null
          },
          {
            "id": "node_Concat_91",
            "label": "Concat",
            "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/expand_1: aten.expand.default",
            "subgraphIds": [],
            "attrs": [
              {
                "key": "axis",
                "value": "0"
              },
              {
                "key": "[metadata] namespace",
                "value": ": transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/expand_1: aten.expand.default"
              },
              {
                "key": "[metadata] pkg.torch.onnx.class_hierarchy",
                "value": "['transformers.models.llama.modeling_llama.LlamaForCausalLM', 'transformers.models.llama.modeling_llama.LlamaModel', 'aten.expand.default']"
              },
              {
                "key": "[metadata] pkg.torch.onnx.fx_node",
                "value": "%expand_1 : [num_users=1] = call_function[target=torch.ops.aten.expand.default](args = (%slice_4, [%sym_size_int_68, 1, -1, -1]), kwargs = {})"
              },
              {
                "key": "[metadata] pkg.torch.onnx.name_scopes",
                "value": "['', 'model', 'expand_1']"
              },
              {
                "key": "[metadata] pkg.torch.onnx.stack_trace",
                "value": "File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 688, in forward\n    outputs: BaseModelOutputWithPast = self.model(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 436, in forward\n    causal_mask = self._update_causal_mask("
              }
            ],
            "incomingEdges": [
              {
                "sourceNodeId": "node_Shape_12",
                "sourceNodeOutputId": "0",
                "targetNodeInputId": "0"
              },
              {
                "sourceNodeId": "node_Constant_60",
                "sourceNodeOutputId": "0",
                "targetNodeInputId": "1"
              },
              {
                "sourceNodeId": "node_Constant_90",
                "sourceNodeOutputId": "0",
                "targetNodeInputId": "2"
              },
              {
                "sourceNodeId": "node_Constant_90",
                "sourceNodeOutputId": "0",
                "targetNodeInputId": "3"
              }
            ],
            "outputsMetadata": [
              {
                "id": "0",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "val_54"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "INT64[4]"
                  },
                  {
                    "key": "param_name",
                    "value": "concat_result"
                  }
                ]
              }
            ],
            "inputsMetadata": [
              {
                "id": "0",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "val_6"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "INT64[1]"
                  },
                  {
                    "key": "param_name",
                    "value": "inputs"
                  }
                ]
              },
              {
                "id": "1",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "val_26"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "INT64[1]"
                  },
                  {
                    "key": "param_name",
                    "value": "inputs"
                  }
                ]
              },
              {
                "id": "2",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "val_53"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "INT64[1]"
                  },
                  {
                    "key": "param_name",
                    "value": "inputs"
                  }
                ]
              },
              {
                "id": "3",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "val_53"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "INT64[1]"
                  },
                  {
                    "key": "param_name",
                    "value": "inputs"
                  }
                ]
              }
            ],
            "style": null,
            "config": null
          },
          {
            "id": "node_Abs_93",
            "label": "Abs",
            "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/expand_1: aten.expand.default",
            "subgraphIds": [],
            "attrs": [
              {
                "key": "[metadata] namespace",
                "value": ": transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/expand_1: aten.expand.default"
              },
              {
                "key": "[metadata] pkg.torch.onnx.class_hierarchy",
                "value": "['transformers.models.llama.modeling_llama.LlamaForCausalLM', 'transformers.models.llama.modeling_llama.LlamaModel', 'aten.expand.default']"
              },
              {
                "key": "[metadata] pkg.torch.onnx.fx_node",
                "value": "%expand_1 : [num_users=1] = call_function[target=torch.ops.aten.expand.default](args = (%slice_4, [%sym_size_int_68, 1, -1, -1]), kwargs = {})"
              },
              {
                "key": "[metadata] pkg.torch.onnx.name_scopes",
                "value": "['', 'model', 'expand_1']"
              },
              {
                "key": "[metadata] pkg.torch.onnx.stack_trace",
                "value": "File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 688, in forward\n    outputs: BaseModelOutputWithPast = self.model(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 436, in forward\n    causal_mask = self._update_causal_mask("
              }
            ],
            "incomingEdges": [
              {
                "sourceNodeId": "node_Concat_91",
                "sourceNodeOutputId": "0",
                "targetNodeInputId": "0"
              }
            ],
            "outputsMetadata": [
              {
                "id": "0",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "val_56"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "INT64[4]"
                  },
                  {
                    "key": "param_name",
                    "value": "Y"
                  }
                ]
              }
            ],
            "inputsMetadata": [
              {
                "id": "0",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "val_54"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "INT64[4]"
                  },
                  {
                    "key": "param_name",
                    "value": "X"
                  }
                ]
              }
            ],
            "style": null,
            "config": null
          },
          {
            "id": "node_Expand_94",
            "label": "Expand",
            "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/expand_1: aten.expand.default",
            "subgraphIds": [],
            "attrs": [
              {
                "key": "[metadata] namespace",
                "value": ": transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/expand_1: aten.expand.default"
              },
              {
                "key": "[metadata] pkg.torch.onnx.class_hierarchy",
                "value": "['transformers.models.llama.modeling_llama.LlamaForCausalLM', 'transformers.models.llama.modeling_llama.LlamaModel', 'aten.expand.default']"
              },
              {
                "key": "[metadata] pkg.torch.onnx.fx_node",
                "value": "%expand_1 : [num_users=1] = call_function[target=torch.ops.aten.expand.default](args = (%slice_4, [%sym_size_int_68, 1, -1, -1]), kwargs = {})"
              },
              {
                "key": "[metadata] pkg.torch.onnx.name_scopes",
                "value": "['', 'model', 'expand_1']"
              },
              {
                "key": "[metadata] pkg.torch.onnx.stack_trace",
                "value": "File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 688, in forward\n    outputs: BaseModelOutputWithPast = self.model(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 436, in forward\n    causal_mask = self._update_causal_mask("
              }
            ],
            "incomingEdges": [
              {
                "sourceNodeId": "node_Unsqueeze_1027",
                "sourceNodeOutputId": "0",
                "targetNodeInputId": "0"
              },
              {
                "sourceNodeId": "node_Abs_93",
                "sourceNodeOutputId": "0",
                "targetNodeInputId": "1"
              }
            ],
            "outputsMetadata": [
              {
                "id": "0",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "expand_1"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "FLOAT[s34,1,s16,s16 + s17]"
                  },
                  {
                    "key": "param_name",
                    "value": "output"
                  }
                ]
              }
            ],
            "inputsMetadata": [
              {
                "id": "0",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "unsqueeze_4"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "FLOAT[1,1,s16,s16 + s17]"
                  },
                  {
                    "key": "param_name",
                    "value": "input"
                  }
                ]
              },
              {
                "id": "1",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "val_56"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "INT64[4]"
                  },
                  {
                    "key": "param_name",
                    "value": "shape"
                  }
                ]
              }
            ],
            "style": null,
            "config": null
          },
          {
            "id": "node_Constant_105",
            "label": "Constant",
            "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel",
            "subgraphIds": [],
            "attrs": [
              {
                "key": "__value",
                "value": "1"
              },
              {
                "key": "value",
                "value": "TensorProtoTensor<INT64,[]>(array(1), name='')"
              },
              {
                "key": "[metadata] namespace",
                "value": ": transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/slice_6: aten.slice.Tensor"
              },
              {
                "key": "[metadata] pkg.torch.onnx.class_hierarchy",
                "value": "['transformers.models.llama.modeling_llama.LlamaForCausalLM', 'transformers.models.llama.modeling_llama.LlamaModel', 'aten.slice.Tensor']"
              },
              {
                "key": "[metadata] pkg.torch.onnx.fx_node",
                "value": "%slice_6 : [num_users=1] = call_function[target=torch.ops.aten.slice.Tensor](args = (%slice_5, 1), kwargs = {})"
              },
              {
                "key": "[metadata] pkg.torch.onnx.name_scopes",
                "value": "['', 'model', 'slice_6']"
              },
              {
                "key": "[metadata] pkg.torch.onnx.stack_trace",
                "value": "File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 688, in forward\n    outputs: BaseModelOutputWithPast = self.model(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 436, in forward\n    causal_mask = self._update_causal_mask("
              }
            ],
            "incomingEdges": [],
            "outputsMetadata": [
              {
                "id": "0",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "val_65"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "INT64[]"
                  },
                  {
                    "key": "param_name",
                    "value": "output"
                  }
                ]
              }
            ],
            "inputsMetadata": [],
            "style": null,
            "config": null
          },
          {
            "id": "node_Constant_118",
            "label": "Constant",
            "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/slice_8: aten.slice.Tensor",
            "subgraphIds": [],
            "attrs": [
              {
                "key": "__value",
                "value": "[0]"
              },
              {
                "key": "value_ints",
                "value": "[0]"
              },
              {
                "key": "[metadata] namespace",
                "value": ": transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/slice_8: aten.slice.Tensor"
              },
              {
                "key": "[metadata] pkg.torch.onnx.class_hierarchy",
                "value": "['transformers.models.llama.modeling_llama.LlamaForCausalLM', 'transformers.models.llama.modeling_llama.LlamaModel', 'aten.slice.Tensor']"
              },
              {
                "key": "[metadata] pkg.torch.onnx.fx_node",
                "value": "%slice_8 : [num_users=1] = call_function[target=torch.ops.aten.slice.Tensor](args = (%slice_7, 3, None, %add_4), kwargs = {})"
              },
              {
                "key": "[metadata] pkg.torch.onnx.name_scopes",
                "value": "['', 'model', 'slice_8']"
              },
              {
                "key": "[metadata] pkg.torch.onnx.stack_trace",
                "value": "File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 688, in forward\n    outputs: BaseModelOutputWithPast = self.model(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 436, in forward\n    causal_mask = self._update_causal_mask("
              }
            ],
            "incomingEdges": [],
            "outputsMetadata": [
              {
                "id": "0",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "val_76"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "INT64[1]"
                  },
                  {
                    "key": "param_name",
                    "value": "output"
                  }
                ]
              }
            ],
            "inputsMetadata": [],
            "style": null,
            "config": null
          },
          {
            "id": "node_Constant_120",
            "label": "Constant",
            "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/slice_8: aten.slice.Tensor",
            "subgraphIds": [],
            "attrs": [
              {
                "key": "__value",
                "value": "[-1]"
              },
              {
                "key": "value_ints",
                "value": "[-1]"
              },
              {
                "key": "[metadata] namespace",
                "value": ": transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/slice_8: aten.slice.Tensor"
              },
              {
                "key": "[metadata] pkg.torch.onnx.class_hierarchy",
                "value": "['transformers.models.llama.modeling_llama.LlamaForCausalLM', 'transformers.models.llama.modeling_llama.LlamaModel', 'aten.slice.Tensor']"
              },
              {
                "key": "[metadata] pkg.torch.onnx.fx_node",
                "value": "%slice_8 : [num_users=1] = call_function[target=torch.ops.aten.slice.Tensor](args = (%slice_7, 3, None, %add_4), kwargs = {})"
              },
              {
                "key": "[metadata] pkg.torch.onnx.name_scopes",
                "value": "['', 'model', 'slice_8']"
              },
              {
                "key": "[metadata] pkg.torch.onnx.stack_trace",
                "value": "File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 688, in forward\n    outputs: BaseModelOutputWithPast = self.model(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 436, in forward\n    causal_mask = self._update_causal_mask("
              }
            ],
            "incomingEdges": [],
            "outputsMetadata": [
              {
                "id": "0",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "val_78"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "INT64[1]"
                  },
                  {
                    "key": "param_name",
                    "value": "output"
                  }
                ]
              }
            ],
            "inputsMetadata": [],
            "style": null,
            "config": null
          },
          {
            "id": "node_Reshape_121",
            "label": "Reshape",
            "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/slice_8: aten.slice.Tensor",
            "subgraphIds": [],
            "attrs": [
              {
                "key": "allowzero",
                "value": "0"
              },
              {
                "key": "[metadata] namespace",
                "value": ": transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/slice_8: aten.slice.Tensor"
              },
              {
                "key": "[metadata] pkg.torch.onnx.class_hierarchy",
                "value": "['transformers.models.llama.modeling_llama.LlamaForCausalLM', 'transformers.models.llama.modeling_llama.LlamaModel', 'aten.slice.Tensor']"
              },
              {
                "key": "[metadata] pkg.torch.onnx.fx_node",
                "value": "%slice_8 : [num_users=1] = call_function[target=torch.ops.aten.slice.Tensor](args = (%slice_7, 3, None, %add_4), kwargs = {})"
              },
              {
                "key": "[metadata] pkg.torch.onnx.name_scopes",
                "value": "['', 'model', 'slice_8']"
              },
              {
                "key": "[metadata] pkg.torch.onnx.stack_trace",
                "value": "File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 688, in forward\n    outputs: BaseModelOutputWithPast = self.model(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 436, in forward\n    causal_mask = self._update_causal_mask("
              }
            ],
            "incomingEdges": [
              {
                "sourceNodeId": "node_Add_33",
                "sourceNodeOutputId": "0",
                "targetNodeInputId": "0"
              },
              {
                "sourceNodeId": "node_Constant_120",
                "sourceNodeOutputId": "0",
                "targetNodeInputId": "1"
              }
            ],
            "outputsMetadata": [
              {
                "id": "0",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "val_79"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "INT64[1]"
                  },
                  {
                    "key": "param_name",
                    "value": "reshaped"
                  }
                ]
              }
            ],
            "inputsMetadata": [
              {
                "id": "0",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "add_4"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "INT64[]"
                  },
                  {
                    "key": "param_name",
                    "value": "data"
                  }
                ]
              },
              {
                "id": "1",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "val_78"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "INT64[1]"
                  },
                  {
                    "key": "param_name",
                    "value": "shape"
                  }
                ]
              }
            ],
            "style": null,
            "config": null
          },
          {
            "id": "node_Constant_745",
            "label": "Constant",
            "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/slice_8: aten.slice.Tensor",
            "subgraphIds": [],
            "attrs": [
              {
                "key": "__value",
                "value": "[3]"
              },
              {
                "key": "value",
                "value": "TensorProtoTensor<INT64,[1]>(array([3]), name='val_82')"
              }
            ],
            "incomingEdges": [],
            "outputsMetadata": [
              {
                "id": "0",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "val_82"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "INT64[1]"
                  },
                  {
                    "key": "param_name",
                    "value": "output"
                  }
                ]
              }
            ],
            "inputsMetadata": [],
            "style": null,
            "config": null
          },
          {
            "id": "node_Constant_125",
            "label": "Constant",
            "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/slice_8: aten.slice.Tensor",
            "subgraphIds": [],
            "attrs": [
              {
                "key": "__value",
                "value": "[1]"
              },
              {
                "key": "value_ints",
                "value": "[1]"
              },
              {
                "key": "[metadata] namespace",
                "value": ": transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/slice_8: aten.slice.Tensor"
              },
              {
                "key": "[metadata] pkg.torch.onnx.class_hierarchy",
                "value": "['transformers.models.llama.modeling_llama.LlamaForCausalLM', 'transformers.models.llama.modeling_llama.LlamaModel', 'aten.slice.Tensor']"
              },
              {
                "key": "[metadata] pkg.torch.onnx.fx_node",
                "value": "%slice_8 : [num_users=1] = call_function[target=torch.ops.aten.slice.Tensor](args = (%slice_7, 3, None, %add_4), kwargs = {})"
              },
              {
                "key": "[metadata] pkg.torch.onnx.name_scopes",
                "value": "['', 'model', 'slice_8']"
              },
              {
                "key": "[metadata] pkg.torch.onnx.stack_trace",
                "value": "File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 688, in forward\n    outputs: BaseModelOutputWithPast = self.model(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 436, in forward\n    causal_mask = self._update_causal_mask("
              }
            ],
            "incomingEdges": [],
            "outputsMetadata": [
              {
                "id": "0",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "val_83"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "INT64[1]"
                  },
                  {
                    "key": "param_name",
                    "value": "output"
                  }
                ]
              }
            ],
            "inputsMetadata": [],
            "style": null,
            "config": null
          },
          {
            "id": "node_Slice_126",
            "label": "Slice",
            "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/slice_8: aten.slice.Tensor",
            "subgraphIds": [],
            "attrs": [
              {
                "key": "[metadata] namespace",
                "value": ": transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/slice_8: aten.slice.Tensor"
              },
              {
                "key": "[metadata] pkg.torch.onnx.class_hierarchy",
                "value": "['transformers.models.llama.modeling_llama.LlamaForCausalLM', 'transformers.models.llama.modeling_llama.LlamaModel', 'aten.slice.Tensor']"
              },
              {
                "key": "[metadata] pkg.torch.onnx.fx_node",
                "value": "%slice_8 : [num_users=1] = call_function[target=torch.ops.aten.slice.Tensor](args = (%slice_7, 3, None, %add_4), kwargs = {})"
              },
              {
                "key": "[metadata] pkg.torch.onnx.name_scopes",
                "value": "['', 'model', 'slice_8']"
              },
              {
                "key": "[metadata] pkg.torch.onnx.stack_trace",
                "value": "File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 688, in forward\n    outputs: BaseModelOutputWithPast = self.model(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 436, in forward\n    causal_mask = self._update_causal_mask("
              }
            ],
            "incomingEdges": [
              {
                "sourceNodeId": "node_Expand_94",
                "sourceNodeOutputId": "0",
                "targetNodeInputId": "0"
              },
              {
                "sourceNodeId": "node_Constant_118",
                "sourceNodeOutputId": "0",
                "targetNodeInputId": "1"
              },
              {
                "sourceNodeId": "node_Reshape_121",
                "sourceNodeOutputId": "0",
                "targetNodeInputId": "2"
              },
              {
                "sourceNodeId": "node_Constant_745",
                "sourceNodeOutputId": "0",
                "targetNodeInputId": "3"
              },
              {
                "sourceNodeId": "node_Constant_125",
                "sourceNodeOutputId": "0",
                "targetNodeInputId": "4"
              }
            ],
            "outputsMetadata": [
              {
                "id": "0",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "slice_8"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "FLOAT[s34,1,s16,s16 + s17]"
                  },
                  {
                    "key": "param_name",
                    "value": "output"
                  }
                ]
              }
            ],
            "inputsMetadata": [
              {
                "id": "0",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "expand_1"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "FLOAT[s34,1,s16,s16 + s17]"
                  },
                  {
                    "key": "param_name",
                    "value": "data"
                  }
                ]
              },
              {
                "id": "1",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "val_76"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "INT64[1]"
                  },
                  {
                    "key": "param_name",
                    "value": "starts"
                  }
                ]
              },
              {
                "id": "2",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "val_79"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "INT64[1]"
                  },
                  {
                    "key": "param_name",
                    "value": "ends"
                  }
                ]
              },
              {
                "id": "3",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "val_82"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "INT64[1]"
                  },
                  {
                    "key": "param_name",
                    "value": "axes"
                  }
                ]
              },
              {
                "id": "4",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "val_83"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "INT64[1]"
                  },
                  {
                    "key": "param_name",
                    "value": "steps"
                  }
                ]
              }
            ],
            "style": null,
            "config": null
          },
          {
            "id": "node_Constant_1028",
            "label": "Constant",
            "namespace": "main_graph",
            "subgraphIds": [],
            "attrs": [
              {
                "key": "__value",
                "value": "[1,2]"
              },
              {
                "key": "value",
                "value": "TensorProtoTensor<INT64,[2]>(array([1, 2]), name='val_492')"
              }
            ],
            "incomingEdges": [],
            "outputsMetadata": [
              {
                "id": "0",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "val_492"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "INT64[2]"
                  },
                  {
                    "key": "param_name",
                    "value": "output"
                  }
                ]
              }
            ],
            "inputsMetadata": [],
            "style": null,
            "config": null
          },
          {
            "id": "node_Unsqueeze_1029",
            "label": "Unsqueeze",
            "namespace": "main_graph",
            "subgraphIds": [],
            "attrs": [],
            "incomingEdges": [
              {
                "sourceNodeId": "[value] attention_mask",
                "sourceNodeOutputId": "0",
                "targetNodeInputId": "0"
              },
              {
                "sourceNodeId": "node_Constant_1028",
                "sourceNodeOutputId": "0",
                "targetNodeInputId": "1"
              }
            ],
            "outputsMetadata": [
              {
                "id": "0",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "unsqueeze_6"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "INT64[s34,1,1,s16 + s17]"
                  },
                  {
                    "key": "param_name",
                    "value": "expanded"
                  }
                ]
              }
            ],
            "inputsMetadata": [
              {
                "id": "0",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "attention_mask"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "INT64[s34,s16 + s17]"
                  },
                  {
                    "key": "[metadata] pkg.torch.export.graph_signature.InputSpec.kind",
                    "value": "USER_INPUT"
                  },
                  {
                    "key": "[metadata] pkg.torch.export.graph_signature.InputSpec.persistent",
                    "value": "None"
                  },
                  {
                    "key": "param_name",
                    "value": "data"
                  }
                ]
              },
              {
                "id": "1",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "val_492"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "INT64[2]"
                  },
                  {
                    "key": "param_name",
                    "value": "axes"
                  }
                ]
              }
            ],
            "style": null,
            "config": null
          },
          {
            "id": "node_Cast_152",
            "label": "Cast",
            "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/convert_element_type_default_1: prims.convert_element_type.default",
            "subgraphIds": [],
            "attrs": [
              {
                "key": "to",
                "value": "FLOAT"
              },
              {
                "key": "[metadata] namespace",
                "value": ": transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/convert_element_type_default_1: prims.convert_element_type.default"
              },
              {
                "key": "[metadata] pkg.torch.onnx.class_hierarchy",
                "value": "['transformers.models.llama.modeling_llama.LlamaForCausalLM', 'transformers.models.llama.modeling_llama.LlamaModel', 'prims.convert_element_type.default']"
              },
              {
                "key": "[metadata] pkg.torch.onnx.fx_node",
                "value": "%convert_element_type_default_1 : [num_users=1] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%slice_10,), kwargs = {dtype: torch.float32})"
              },
              {
                "key": "[metadata] pkg.torch.onnx.name_scopes",
                "value": "['', 'model', 'convert_element_type_default_1']"
              },
              {
                "key": "[metadata] pkg.torch.onnx.stack_trace",
                "value": "File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 688, in forward\n    outputs: BaseModelOutputWithPast = self.model(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 436, in forward\n    causal_mask = self._update_causal_mask("
              }
            ],
            "incomingEdges": [
              {
                "sourceNodeId": "node_Unsqueeze_1029",
                "sourceNodeOutputId": "0",
                "targetNodeInputId": "0"
              }
            ],
            "outputsMetadata": [
              {
                "id": "0",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "convert_element_type_default_1"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "FLOAT[s34,1,1,s16 + s17]"
                  },
                  {
                    "key": "param_name",
                    "value": "output"
                  }
                ]
              }
            ],
            "inputsMetadata": [
              {
                "id": "0",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "unsqueeze_6"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "INT64[s34,1,1,s16 + s17]"
                  },
                  {
                    "key": "param_name",
                    "value": "input"
                  }
                ]
              }
            ],
            "style": null,
            "config": null
          },
          {
            "id": "node_Add_153",
            "label": "Add",
            "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/add_89: aten.add.Tensor",
            "subgraphIds": [],
            "attrs": [
              {
                "key": "[metadata] namespace",
                "value": ": transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/add_89: aten.add.Tensor"
              },
              {
                "key": "[metadata] pkg.torch.onnx.class_hierarchy",
                "value": "['transformers.models.llama.modeling_llama.LlamaForCausalLM', 'transformers.models.llama.modeling_llama.LlamaModel', 'aten.add.Tensor']"
              },
              {
                "key": "[metadata] pkg.torch.onnx.fx_node",
                "value": "%add_89 : [num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%slice_8, %convert_element_type_default_1), kwargs = {})"
              },
              {
                "key": "[metadata] pkg.torch.onnx.name_scopes",
                "value": "['', 'model', 'add_89']"
              },
              {
                "key": "[metadata] pkg.torch.onnx.stack_trace",
                "value": "File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 688, in forward\n    outputs: BaseModelOutputWithPast = self.model(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 436, in forward\n    causal_mask = self._update_causal_mask("
              }
            ],
            "incomingEdges": [
              {
                "sourceNodeId": "node_Slice_126",
                "sourceNodeOutputId": "0",
                "targetNodeInputId": "0"
              },
              {
                "sourceNodeId": "node_Cast_152",
                "sourceNodeOutputId": "0",
                "targetNodeInputId": "1"
              }
            ],
            "outputsMetadata": [
              {
                "id": "0",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "add_89"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "FLOAT[s34,1,s16,s16 + s17]"
                  },
                  {
                    "key": "param_name",
                    "value": "C"
                  }
                ]
              }
            ],
            "inputsMetadata": [
              {
                "id": "0",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "slice_8"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "FLOAT[s34,1,s16,s16 + s17]"
                  },
                  {
                    "key": "param_name",
                    "value": "A"
                  }
                ]
              },
              {
                "id": "1",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "convert_element_type_default_1"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "FLOAT[s34,1,1,s16 + s17]"
                  },
                  {
                    "key": "param_name",
                    "value": "B"
                  }
                ]
              }
            ],
            "style": null,
            "config": null
          },
          {
            "id": "node_Constant_764",
            "label": "Constant",
            "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/eq_67: aten.eq.Tensor",
            "subgraphIds": [],
            "attrs": [
              {
                "key": "__value",
                "value": "0.0"
              },
              {
                "key": "value",
                "value": "TensorProtoTensor<FLOAT,[]>(array(0., dtype=float32), name='scalar_tensor_default')"
              }
            ],
            "incomingEdges": [],
            "outputsMetadata": [
              {
                "id": "0",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "scalar_tensor_default"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "FLOAT[]"
                  },
                  {
                    "key": "param_name",
                    "value": "output"
                  }
                ]
              }
            ],
            "inputsMetadata": [],
            "style": null,
            "config": null
          },
          {
            "id": "node_Equal_155",
            "label": "Equal",
            "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/eq_67: aten.eq.Tensor",
            "subgraphIds": [],
            "attrs": [
              {
                "key": "[metadata] namespace",
                "value": ": transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/eq_67: aten.eq.Tensor"
              },
              {
                "key": "[metadata] pkg.torch.onnx.class_hierarchy",
                "value": "['transformers.models.llama.modeling_llama.LlamaForCausalLM', 'transformers.models.llama.modeling_llama.LlamaModel', 'aten.eq.Tensor']"
              },
              {
                "key": "[metadata] pkg.torch.onnx.fx_node",
                "value": "%eq_67 : [num_users=1] = call_function[target=torch.ops.aten.eq.Tensor](args = (%add_89, %scalar_tensor_default), kwargs = {})"
              },
              {
                "key": "[metadata] pkg.torch.onnx.name_scopes",
                "value": "['', 'model', 'eq_67']"
              },
              {
                "key": "[metadata] pkg.torch.onnx.stack_trace",
                "value": "File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 688, in forward\n    outputs: BaseModelOutputWithPast = self.model(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 436, in forward\n    causal_mask = self._update_causal_mask("
              }
            ],
            "incomingEdges": [
              {
                "sourceNodeId": "node_Add_153",
                "sourceNodeOutputId": "0",
                "targetNodeInputId": "0"
              },
              {
                "sourceNodeId": "node_Constant_764",
                "sourceNodeOutputId": "0",
                "targetNodeInputId": "1"
              }
            ],
            "outputsMetadata": [
              {
                "id": "0",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "eq_67"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "BOOL[s34,1,s16,s16 + s17]"
                  },
                  {
                    "key": "param_name",
                    "value": "C"
                  }
                ]
              }
            ],
            "inputsMetadata": [
              {
                "id": "0",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "add_89"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "FLOAT[s34,1,s16,s16 + s17]"
                  },
                  {
                    "key": "param_name",
                    "value": "A"
                  }
                ]
              },
              {
                "id": "1",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "scalar_tensor_default"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "FLOAT[]"
                  },
                  {
                    "key": "param_name",
                    "value": "B"
                  }
                ]
              }
            ],
            "style": null,
            "config": null
          },
          {
            "id": "node_Constant_177",
            "label": "Constant",
            "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/slice_14: aten.slice.Tensor",
            "subgraphIds": [],
            "attrs": [
              {
                "key": "__value",
                "value": "[0]"
              },
              {
                "key": "value_ints",
                "value": "[0]"
              },
              {
                "key": "[metadata] namespace",
                "value": ": transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/slice_14: aten.slice.Tensor"
              },
              {
                "key": "[metadata] pkg.torch.onnx.class_hierarchy",
                "value": "['transformers.models.llama.modeling_llama.LlamaForCausalLM', 'transformers.models.llama.modeling_llama.LlamaModel', 'aten.slice.Tensor']"
              },
              {
                "key": "[metadata] pkg.torch.onnx.fx_node",
                "value": "%slice_14 : [num_users=1] = call_function[target=torch.ops.aten.slice.Tensor](args = (%slice_13, 3, None, %add_4), kwargs = {})"
              },
              {
                "key": "[metadata] pkg.torch.onnx.name_scopes",
                "value": "['', 'model', 'slice_14']"
              },
              {
                "key": "[metadata] pkg.torch.onnx.stack_trace",
                "value": "File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 688, in forward\n    outputs: BaseModelOutputWithPast = self.model(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 436, in forward\n    causal_mask = self._update_causal_mask("
              }
            ],
            "incomingEdges": [],
            "outputsMetadata": [
              {
                "id": "0",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "val_123"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "INT64[1]"
                  },
                  {
                    "key": "param_name",
                    "value": "output"
                  }
                ]
              }
            ],
            "inputsMetadata": [],
            "style": null,
            "config": null
          },
          {
            "id": "node_Constant_179",
            "label": "Constant",
            "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/slice_14: aten.slice.Tensor",
            "subgraphIds": [],
            "attrs": [
              {
                "key": "__value",
                "value": "[-1]"
              },
              {
                "key": "value_ints",
                "value": "[-1]"
              },
              {
                "key": "[metadata] namespace",
                "value": ": transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/slice_14: aten.slice.Tensor"
              },
              {
                "key": "[metadata] pkg.torch.onnx.class_hierarchy",
                "value": "['transformers.models.llama.modeling_llama.LlamaForCausalLM', 'transformers.models.llama.modeling_llama.LlamaModel', 'aten.slice.Tensor']"
              },
              {
                "key": "[metadata] pkg.torch.onnx.fx_node",
                "value": "%slice_14 : [num_users=1] = call_function[target=torch.ops.aten.slice.Tensor](args = (%slice_13, 3, None, %add_4), kwargs = {})"
              },
              {
                "key": "[metadata] pkg.torch.onnx.name_scopes",
                "value": "['', 'model', 'slice_14']"
              },
              {
                "key": "[metadata] pkg.torch.onnx.stack_trace",
                "value": "File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 688, in forward\n    outputs: BaseModelOutputWithPast = self.model(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 436, in forward\n    causal_mask = self._update_causal_mask("
              }
            ],
            "incomingEdges": [],
            "outputsMetadata": [
              {
                "id": "0",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "val_125"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "INT64[1]"
                  },
                  {
                    "key": "param_name",
                    "value": "output"
                  }
                ]
              }
            ],
            "inputsMetadata": [],
            "style": null,
            "config": null
          },
          {
            "id": "node_Reshape_180",
            "label": "Reshape",
            "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/slice_14: aten.slice.Tensor",
            "subgraphIds": [],
            "attrs": [
              {
                "key": "allowzero",
                "value": "0"
              },
              {
                "key": "[metadata] namespace",
                "value": ": transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/slice_14: aten.slice.Tensor"
              },
              {
                "key": "[metadata] pkg.torch.onnx.class_hierarchy",
                "value": "['transformers.models.llama.modeling_llama.LlamaForCausalLM', 'transformers.models.llama.modeling_llama.LlamaModel', 'aten.slice.Tensor']"
              },
              {
                "key": "[metadata] pkg.torch.onnx.fx_node",
                "value": "%slice_14 : [num_users=1] = call_function[target=torch.ops.aten.slice.Tensor](args = (%slice_13, 3, None, %add_4), kwargs = {})"
              },
              {
                "key": "[metadata] pkg.torch.onnx.name_scopes",
                "value": "['', 'model', 'slice_14']"
              },
              {
                "key": "[metadata] pkg.torch.onnx.stack_trace",
                "value": "File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 688, in forward\n    outputs: BaseModelOutputWithPast = self.model(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 436, in forward\n    causal_mask = self._update_causal_mask("
              }
            ],
            "incomingEdges": [
              {
                "sourceNodeId": "node_Add_33",
                "sourceNodeOutputId": "0",
                "targetNodeInputId": "0"
              },
              {
                "sourceNodeId": "node_Constant_179",
                "sourceNodeOutputId": "0",
                "targetNodeInputId": "1"
              }
            ],
            "outputsMetadata": [
              {
                "id": "0",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "val_126"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "INT64[1]"
                  },
                  {
                    "key": "param_name",
                    "value": "reshaped"
                  }
                ]
              }
            ],
            "inputsMetadata": [
              {
                "id": "0",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "add_4"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "INT64[]"
                  },
                  {
                    "key": "param_name",
                    "value": "data"
                  }
                ]
              },
              {
                "id": "1",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "val_125"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "INT64[1]"
                  },
                  {
                    "key": "param_name",
                    "value": "shape"
                  }
                ]
              }
            ],
            "style": null,
            "config": null
          },
          {
            "id": "node_Constant_777",
            "label": "Constant",
            "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/slice_14: aten.slice.Tensor",
            "subgraphIds": [],
            "attrs": [
              {
                "key": "__value",
                "value": "[3]"
              },
              {
                "key": "value",
                "value": "TensorProtoTensor<INT64,[1]>(array([3]), name='val_129')"
              }
            ],
            "incomingEdges": [],
            "outputsMetadata": [
              {
                "id": "0",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "val_129"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "INT64[1]"
                  },
                  {
                    "key": "param_name",
                    "value": "output"
                  }
                ]
              }
            ],
            "inputsMetadata": [],
            "style": null,
            "config": null
          },
          {
            "id": "node_Constant_184",
            "label": "Constant",
            "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/slice_14: aten.slice.Tensor",
            "subgraphIds": [],
            "attrs": [
              {
                "key": "__value",
                "value": "[1]"
              },
              {
                "key": "value_ints",
                "value": "[1]"
              },
              {
                "key": "[metadata] namespace",
                "value": ": transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/slice_14: aten.slice.Tensor"
              },
              {
                "key": "[metadata] pkg.torch.onnx.class_hierarchy",
                "value": "['transformers.models.llama.modeling_llama.LlamaForCausalLM', 'transformers.models.llama.modeling_llama.LlamaModel', 'aten.slice.Tensor']"
              },
              {
                "key": "[metadata] pkg.torch.onnx.fx_node",
                "value": "%slice_14 : [num_users=1] = call_function[target=torch.ops.aten.slice.Tensor](args = (%slice_13, 3, None, %add_4), kwargs = {})"
              },
              {
                "key": "[metadata] pkg.torch.onnx.name_scopes",
                "value": "['', 'model', 'slice_14']"
              },
              {
                "key": "[metadata] pkg.torch.onnx.stack_trace",
                "value": "File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 688, in forward\n    outputs: BaseModelOutputWithPast = self.model(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 436, in forward\n    causal_mask = self._update_causal_mask("
              }
            ],
            "incomingEdges": [],
            "outputsMetadata": [
              {
                "id": "0",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "val_130"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "INT64[1]"
                  },
                  {
                    "key": "param_name",
                    "value": "output"
                  }
                ]
              }
            ],
            "inputsMetadata": [],
            "style": null,
            "config": null
          },
          {
            "id": "node_Slice_185",
            "label": "Slice",
            "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/slice_14: aten.slice.Tensor",
            "subgraphIds": [],
            "attrs": [
              {
                "key": "[metadata] namespace",
                "value": ": transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/slice_14: aten.slice.Tensor"
              },
              {
                "key": "[metadata] pkg.torch.onnx.class_hierarchy",
                "value": "['transformers.models.llama.modeling_llama.LlamaForCausalLM', 'transformers.models.llama.modeling_llama.LlamaModel', 'aten.slice.Tensor']"
              },
              {
                "key": "[metadata] pkg.torch.onnx.fx_node",
                "value": "%slice_14 : [num_users=1] = call_function[target=torch.ops.aten.slice.Tensor](args = (%slice_13, 3, None, %add_4), kwargs = {})"
              },
              {
                "key": "[metadata] pkg.torch.onnx.name_scopes",
                "value": "['', 'model', 'slice_14']"
              },
              {
                "key": "[metadata] pkg.torch.onnx.stack_trace",
                "value": "File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 688, in forward\n    outputs: BaseModelOutputWithPast = self.model(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 436, in forward\n    causal_mask = self._update_causal_mask("
              }
            ],
            "incomingEdges": [
              {
                "sourceNodeId": "node_Expand_94",
                "sourceNodeOutputId": "0",
                "targetNodeInputId": "0"
              },
              {
                "sourceNodeId": "node_Constant_177",
                "sourceNodeOutputId": "0",
                "targetNodeInputId": "1"
              },
              {
                "sourceNodeId": "node_Reshape_180",
                "sourceNodeOutputId": "0",
                "targetNodeInputId": "2"
              },
              {
                "sourceNodeId": "node_Constant_777",
                "sourceNodeOutputId": "0",
                "targetNodeInputId": "3"
              },
              {
                "sourceNodeId": "node_Constant_184",
                "sourceNodeOutputId": "0",
                "targetNodeInputId": "4"
              }
            ],
            "outputsMetadata": [
              {
                "id": "0",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "slice_14"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "FLOAT[s34,1,s16,s16 + s17]"
                  },
                  {
                    "key": "param_name",
                    "value": "output"
                  }
                ]
              }
            ],
            "inputsMetadata": [
              {
                "id": "0",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "expand_1"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "FLOAT[s34,1,s16,s16 + s17]"
                  },
                  {
                    "key": "param_name",
                    "value": "data"
                  }
                ]
              },
              {
                "id": "1",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "val_123"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "INT64[1]"
                  },
                  {
                    "key": "param_name",
                    "value": "starts"
                  }
                ]
              },
              {
                "id": "2",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "val_126"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "INT64[1]"
                  },
                  {
                    "key": "param_name",
                    "value": "ends"
                  }
                ]
              },
              {
                "id": "3",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "val_129"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "INT64[1]"
                  },
                  {
                    "key": "param_name",
                    "value": "axes"
                  }
                ]
              },
              {
                "id": "4",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "val_130"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "INT64[1]"
                  },
                  {
                    "key": "param_name",
                    "value": "steps"
                  }
                ]
              }
            ],
            "style": null,
            "config": null
          },
          {
            "id": "node_Constant_779",
            "label": "Constant",
            "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/masked_fill: aten.masked_fill.Scalar",
            "subgraphIds": [],
            "attrs": [
              {
                "key": "__value",
                "value": "-3.4028234663852886e+38"
              },
              {
                "key": "value",
                "value": "TensorProtoTensor<FLOAT,[]>(array(-3.4028235e+38, dtype=float32), name='val_131')"
              }
            ],
            "incomingEdges": [],
            "outputsMetadata": [
              {
                "id": "0",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "val_131"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "FLOAT[]"
                  },
                  {
                    "key": "param_name",
                    "value": "output"
                  }
                ]
              }
            ],
            "inputsMetadata": [],
            "style": null,
            "config": null
          },
          {
            "id": "node_Where_187",
            "label": "Where",
            "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/masked_fill: aten.masked_fill.Scalar",
            "subgraphIds": [],
            "attrs": [
              {
                "key": "[metadata] namespace",
                "value": ": transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/masked_fill: aten.masked_fill.Scalar"
              },
              {
                "key": "[metadata] pkg.torch.onnx.class_hierarchy",
                "value": "['transformers.models.llama.modeling_llama.LlamaForCausalLM', 'transformers.models.llama.modeling_llama.LlamaModel', 'aten.masked_fill.Scalar']"
              },
              {
                "key": "[metadata] pkg.torch.onnx.fx_node",
                "value": "%masked_fill : [num_users=1] = call_function[target=torch.ops.aten.masked_fill.Scalar](args = (%slice_14, %eq_67, -3.4028234663852886e+38), kwargs = {})"
              },
              {
                "key": "[metadata] pkg.torch.onnx.name_scopes",
                "value": "['', 'model', 'masked_fill']"
              },
              {
                "key": "[metadata] pkg.torch.onnx.stack_trace",
                "value": "File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 688, in forward\n    outputs: BaseModelOutputWithPast = self.model(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 436, in forward\n    causal_mask = self._update_causal_mask("
              }
            ],
            "incomingEdges": [
              {
                "sourceNodeId": "node_Equal_155",
                "sourceNodeOutputId": "0",
                "targetNodeInputId": "0"
              },
              {
                "sourceNodeId": "node_Constant_779",
                "sourceNodeOutputId": "0",
                "targetNodeInputId": "1"
              },
              {
                "sourceNodeId": "node_Slice_185",
                "sourceNodeOutputId": "0",
                "targetNodeInputId": "2"
              }
            ],
            "outputsMetadata": [
              {
                "id": "0",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "masked_fill"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "FLOAT[s34,1,s16,s16 + s17]"
                  },
                  {
                    "key": "param_name",
                    "value": "output"
                  }
                ]
              }
            ],
            "inputsMetadata": [
              {
                "id": "0",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "eq_67"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "BOOL[s34,1,s16,s16 + s17]"
                  },
                  {
                    "key": "param_name",
                    "value": "condition"
                  }
                ]
              },
              {
                "id": "1",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "val_131"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "FLOAT[]"
                  },
                  {
                    "key": "param_name",
                    "value": "X"
                  }
                ]
              },
              {
                "id": "2",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "slice_14"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "FLOAT[s34,1,s16,s16 + s17]"
                  },
                  {
                    "key": "param_name",
                    "value": "Y"
                  }
                ]
              }
            ],
            "style": null,
            "config": null
          },
          {
            "id": "node_Shape_245",
            "label": "Shape",
            "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/slice_scatter: aten.slice_scatter.default",
            "subgraphIds": [],
            "attrs": [
              {
                "key": "start",
                "value": "0"
              },
              {
                "key": "[metadata] namespace",
                "value": ": transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/slice_scatter: aten.slice_scatter.default"
              },
              {
                "key": "[metadata] pkg.torch.onnx.class_hierarchy",
                "value": "['transformers.models.llama.modeling_llama.LlamaForCausalLM', 'transformers.models.llama.modeling_llama.LlamaModel', 'aten.slice_scatter.default']"
              },
              {
                "key": "[metadata] pkg.torch.onnx.fx_node",
                "value": "%slice_scatter : [num_users=1] = call_function[target=torch.ops.aten.slice_scatter.default](args = (%slice_19, %copy, 2, 0, 9223372036854775807), kwargs = {})"
              },
              {
                "key": "[metadata] pkg.torch.onnx.name_scopes",
                "value": "['', 'model', 'slice_scatter']"
              },
              {
                "key": "[metadata] pkg.torch.onnx.stack_trace",
                "value": "File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 688, in forward\n    outputs: BaseModelOutputWithPast = self.model(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 436, in forward\n    causal_mask = self._update_causal_mask("
              }
            ],
            "incomingEdges": [
              {
                "sourceNodeId": "node_Expand_94",
                "sourceNodeOutputId": "0",
                "targetNodeInputId": "0"
              }
            ],
            "outputsMetadata": [
              {
                "id": "0",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "val_183"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "INT64[4]"
                  },
                  {
                    "key": "param_name",
                    "value": "shape"
                  }
                ]
              }
            ],
            "inputsMetadata": [
              {
                "id": "0",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "expand_1"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "FLOAT[s34,1,s16,s16 + s17]"
                  },
                  {
                    "key": "param_name",
                    "value": "data"
                  }
                ]
              }
            ],
            "style": null,
            "config": null
          },
          {
            "id": "node_Gather_246",
            "label": "Gather",
            "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/slice_scatter: aten.slice_scatter.default",
            "subgraphIds": [],
            "attrs": [
              {
                "key": "axis",
                "value": "0"
              },
              {
                "key": "[metadata] namespace",
                "value": ": transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/slice_scatter: aten.slice_scatter.default"
              },
              {
                "key": "[metadata] pkg.torch.onnx.class_hierarchy",
                "value": "['transformers.models.llama.modeling_llama.LlamaForCausalLM', 'transformers.models.llama.modeling_llama.LlamaModel', 'aten.slice_scatter.default']"
              },
              {
                "key": "[metadata] pkg.torch.onnx.fx_node",
                "value": "%slice_scatter : [num_users=1] = call_function[target=torch.ops.aten.slice_scatter.default](args = (%slice_19, %copy, 2, 0, 9223372036854775807), kwargs = {})"
              },
              {
                "key": "[metadata] pkg.torch.onnx.name_scopes",
                "value": "['', 'model', 'slice_scatter']"
              },
              {
                "key": "[metadata] pkg.torch.onnx.stack_trace",
                "value": "File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 688, in forward\n    outputs: BaseModelOutputWithPast = self.model(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 436, in forward\n    causal_mask = self._update_causal_mask("
              }
            ],
            "incomingEdges": [
              {
                "sourceNodeId": "node_Shape_245",
                "sourceNodeOutputId": "0",
                "targetNodeInputId": "0"
              },
              {
                "sourceNodeId": "node_Constant_70",
                "sourceNodeOutputId": "0",
                "targetNodeInputId": "1"
              }
            ],
            "outputsMetadata": [
              {
                "id": "0",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "val_184"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "INT64[]"
                  },
                  {
                    "key": "param_name",
                    "value": "output"
                  }
                ]
              }
            ],
            "inputsMetadata": [
              {
                "id": "0",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "val_183"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "INT64[4]"
                  },
                  {
                    "key": "param_name",
                    "value": "data"
                  }
                ]
              },
              {
                "id": "1",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "val_35"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "INT64[]"
                  },
                  {
                    "key": "param_name",
                    "value": "indices"
                  }
                ]
              }
            ],
            "style": null,
            "config": null
          },
          {
            "id": "node_Range_247",
            "label": "Range",
            "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/slice_scatter: aten.slice_scatter.default",
            "subgraphIds": [],
            "attrs": [
              {
                "key": "[metadata] namespace",
                "value": ": transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/slice_scatter: aten.slice_scatter.default"
              },
              {
                "key": "[metadata] pkg.torch.onnx.class_hierarchy",
                "value": "['transformers.models.llama.modeling_llama.LlamaForCausalLM', 'transformers.models.llama.modeling_llama.LlamaModel', 'aten.slice_scatter.default']"
              },
              {
                "key": "[metadata] pkg.torch.onnx.fx_node",
                "value": "%slice_scatter : [num_users=1] = call_function[target=torch.ops.aten.slice_scatter.default](args = (%slice_19, %copy, 2, 0, 9223372036854775807), kwargs = {})"
              },
              {
                "key": "[metadata] pkg.torch.onnx.name_scopes",
                "value": "['', 'model', 'slice_scatter']"
              },
              {
                "key": "[metadata] pkg.torch.onnx.stack_trace",
                "value": "File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 688, in forward\n    outputs: BaseModelOutputWithPast = self.model(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 436, in forward\n    causal_mask = self._update_causal_mask("
              }
            ],
            "incomingEdges": [
              {
                "sourceNodeId": "node_Constant_62",
                "sourceNodeOutputId": "0",
                "targetNodeInputId": "0"
              },
              {
                "sourceNodeId": "node_Gather_246",
                "sourceNodeOutputId": "0",
                "targetNodeInputId": "1"
              },
              {
                "sourceNodeId": "node_Constant_105",
                "sourceNodeOutputId": "0",
                "targetNodeInputId": "2"
              }
            ],
            "outputsMetadata": [
              {
                "id": "0",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "val_185"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "INT64[unk__16]"
                  },
                  {
                    "key": "param_name",
                    "value": "output"
                  }
                ]
              }
            ],
            "inputsMetadata": [
              {
                "id": "0",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "val_27"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "INT64[]"
                  },
                  {
                    "key": "param_name",
                    "value": "start"
                  }
                ]
              },
              {
                "id": "1",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "val_184"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "INT64[]"
                  },
                  {
                    "key": "param_name",
                    "value": "limit"
                  }
                ]
              },
              {
                "id": "2",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "val_65"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "INT64[]"
                  },
                  {
                    "key": "param_name",
                    "value": "delta"
                  }
                ]
              }
            ],
            "style": null,
            "config": null
          },
          {
            "id": "node_Unsqueeze_252",
            "label": "Unsqueeze",
            "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/slice_scatter: aten.slice_scatter.default",
            "subgraphIds": [],
            "attrs": [
              {
                "key": "[metadata] namespace",
                "value": ": transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/slice_scatter: aten.slice_scatter.default"
              },
              {
                "key": "[metadata] pkg.torch.onnx.class_hierarchy",
                "value": "['transformers.models.llama.modeling_llama.LlamaForCausalLM', 'transformers.models.llama.modeling_llama.LlamaModel', 'aten.slice_scatter.default']"
              },
              {
                "key": "[metadata] pkg.torch.onnx.fx_node",
                "value": "%slice_scatter : [num_users=1] = call_function[target=torch.ops.aten.slice_scatter.default](args = (%slice_19, %copy, 2, 0, 9223372036854775807), kwargs = {})"
              },
              {
                "key": "[metadata] pkg.torch.onnx.name_scopes",
                "value": "['', 'model', 'slice_scatter']"
              },
              {
                "key": "[metadata] pkg.torch.onnx.stack_trace",
                "value": "File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 688, in forward\n    outputs: BaseModelOutputWithPast = self.model(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 436, in forward\n    causal_mask = self._update_causal_mask("
              }
            ],
            "incomingEdges": [
              {
                "sourceNodeId": "node_Range_247",
                "sourceNodeOutputId": "0",
                "targetNodeInputId": "0"
              },
              {
                "sourceNodeId": "node_Constant_90",
                "sourceNodeOutputId": "0",
                "targetNodeInputId": "1"
              }
            ],
            "outputsMetadata": [
              {
                "id": "0",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "val_190"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "INT64[unk__16,1]"
                  },
                  {
                    "key": "param_name",
                    "value": "expanded"
                  }
                ]
              }
            ],
            "inputsMetadata": [
              {
                "id": "0",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "val_185"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "INT64[unk__16]"
                  },
                  {
                    "key": "param_name",
                    "value": "data"
                  }
                ]
              },
              {
                "id": "1",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "val_53"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "INT64[1]"
                  },
                  {
                    "key": "param_name",
                    "value": "axes"
                  }
                ]
              }
            ],
            "style": null,
            "config": null
          },
          {
            "id": "node_Transpose_253",
            "label": "Transpose",
            "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/slice_scatter: aten.slice_scatter.default",
            "subgraphIds": [],
            "attrs": [
              {
                "key": "perm",
                "value": "[2, 1, 0, 3]"
              },
              {
                "key": "[metadata] namespace",
                "value": ": transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/slice_scatter: aten.slice_scatter.default"
              },
              {
                "key": "[metadata] pkg.torch.onnx.class_hierarchy",
                "value": "['transformers.models.llama.modeling_llama.LlamaForCausalLM', 'transformers.models.llama.modeling_llama.LlamaModel', 'aten.slice_scatter.default']"
              },
              {
                "key": "[metadata] pkg.torch.onnx.fx_node",
                "value": "%slice_scatter : [num_users=1] = call_function[target=torch.ops.aten.slice_scatter.default](args = (%slice_19, %copy, 2, 0, 9223372036854775807), kwargs = {})"
              },
              {
                "key": "[metadata] pkg.torch.onnx.name_scopes",
                "value": "['', 'model', 'slice_scatter']"
              },
              {
                "key": "[metadata] pkg.torch.onnx.stack_trace",
                "value": "File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 688, in forward\n    outputs: BaseModelOutputWithPast = self.model(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 436, in forward\n    causal_mask = self._update_causal_mask("
              }
            ],
            "incomingEdges": [
              {
                "sourceNodeId": "node_Where_187",
                "sourceNodeOutputId": "0",
                "targetNodeInputId": "0"
              }
            ],
            "outputsMetadata": [
              {
                "id": "0",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "val_191"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "FLOAT[s16,1,s34,s16 + s17]"
                  },
                  {
                    "key": "param_name",
                    "value": "transposed"
                  }
                ]
              }
            ],
            "inputsMetadata": [
              {
                "id": "0",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "masked_fill"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "FLOAT[s34,1,s16,s16 + s17]"
                  },
                  {
                    "key": "param_name",
                    "value": "data"
                  }
                ]
              }
            ],
            "style": null,
            "config": null
          },
          {
            "id": "node_Transpose_254",
            "label": "Transpose",
            "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/slice_scatter: aten.slice_scatter.default",
            "subgraphIds": [],
            "attrs": [
              {
                "key": "perm",
                "value": "[2, 1, 0, 3]"
              },
              {
                "key": "[metadata] namespace",
                "value": ": transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/slice_scatter: aten.slice_scatter.default"
              },
              {
                "key": "[metadata] pkg.torch.onnx.class_hierarchy",
                "value": "['transformers.models.llama.modeling_llama.LlamaForCausalLM', 'transformers.models.llama.modeling_llama.LlamaModel', 'aten.slice_scatter.default']"
              },
              {
                "key": "[metadata] pkg.torch.onnx.fx_node",
                "value": "%slice_scatter : [num_users=1] = call_function[target=torch.ops.aten.slice_scatter.default](args = (%slice_19, %copy, 2, 0, 9223372036854775807), kwargs = {})"
              },
              {
                "key": "[metadata] pkg.torch.onnx.name_scopes",
                "value": "['', 'model', 'slice_scatter']"
              },
              {
                "key": "[metadata] pkg.torch.onnx.stack_trace",
                "value": "File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 688, in forward\n    outputs: BaseModelOutputWithPast = self.model(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 436, in forward\n    causal_mask = self._update_causal_mask("
              }
            ],
            "incomingEdges": [
              {
                "sourceNodeId": "node_Expand_94",
                "sourceNodeOutputId": "0",
                "targetNodeInputId": "0"
              }
            ],
            "outputsMetadata": [
              {
                "id": "0",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "val_192"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "FLOAT[s16,1,s34,s16 + s17]"
                  },
                  {
                    "key": "param_name",
                    "value": "transposed"
                  }
                ]
              }
            ],
            "inputsMetadata": [
              {
                "id": "0",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "expand_1"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "FLOAT[s34,1,s16,s16 + s17]"
                  },
                  {
                    "key": "param_name",
                    "value": "data"
                  }
                ]
              }
            ],
            "style": null,
            "config": null
          },
          {
            "id": "node_ScatterND_255",
            "label": "ScatterND",
            "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/slice_scatter: aten.slice_scatter.default",
            "subgraphIds": [],
            "attrs": [
              {
                "key": "reduction",
                "value": "none"
              },
              {
                "key": "[metadata] namespace",
                "value": ": transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/slice_scatter: aten.slice_scatter.default"
              },
              {
                "key": "[metadata] pkg.torch.onnx.class_hierarchy",
                "value": "['transformers.models.llama.modeling_llama.LlamaForCausalLM', 'transformers.models.llama.modeling_llama.LlamaModel', 'aten.slice_scatter.default']"
              },
              {
                "key": "[metadata] pkg.torch.onnx.fx_node",
                "value": "%slice_scatter : [num_users=1] = call_function[target=torch.ops.aten.slice_scatter.default](args = (%slice_19, %copy, 2, 0, 9223372036854775807), kwargs = {})"
              },
              {
                "key": "[metadata] pkg.torch.onnx.name_scopes",
                "value": "['', 'model', 'slice_scatter']"
              },
              {
                "key": "[metadata] pkg.torch.onnx.stack_trace",
                "value": "File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 688, in forward\n    outputs: BaseModelOutputWithPast = self.model(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 436, in forward\n    causal_mask = self._update_causal_mask("
              }
            ],
            "incomingEdges": [
              {
                "sourceNodeId": "node_Transpose_254",
                "sourceNodeOutputId": "0",
                "targetNodeInputId": "0"
              },
              {
                "sourceNodeId": "node_Unsqueeze_252",
                "sourceNodeOutputId": "0",
                "targetNodeInputId": "1"
              },
              {
                "sourceNodeId": "node_Transpose_253",
                "sourceNodeOutputId": "0",
                "targetNodeInputId": "2"
              }
            ],
            "outputsMetadata": [
              {
                "id": "0",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "val_193"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "FLOAT[s16,1,s34,s16 + s17]"
                  },
                  {
                    "key": "param_name",
                    "value": "output"
                  }
                ]
              }
            ],
            "inputsMetadata": [
              {
                "id": "0",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "val_192"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "FLOAT[s16,1,s34,s16 + s17]"
                  },
                  {
                    "key": "param_name",
                    "value": "data"
                  }
                ]
              },
              {
                "id": "1",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "val_190"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "INT64[unk__16,1]"
                  },
                  {
                    "key": "param_name",
                    "value": "indices"
                  }
                ]
              },
              {
                "id": "2",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "val_191"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "FLOAT[s16,1,s34,s16 + s17]"
                  },
                  {
                    "key": "param_name",
                    "value": "updates"
                  }
                ]
              }
            ],
            "style": null,
            "config": null
          },
          {
            "id": "node_Shape_258",
            "label": "Shape",
            "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/slice_scatter_1: aten.slice_scatter.default",
            "subgraphIds": [],
            "attrs": [
              {
                "key": "start",
                "value": "0"
              },
              {
                "key": "[metadata] namespace",
                "value": ": transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/slice_scatter_1: aten.slice_scatter.default"
              },
              {
                "key": "[metadata] pkg.torch.onnx.class_hierarchy",
                "value": "['transformers.models.llama.modeling_llama.LlamaForCausalLM', 'transformers.models.llama.modeling_llama.LlamaModel', 'aten.slice_scatter.default']"
              },
              {
                "key": "[metadata] pkg.torch.onnx.fx_node",
                "value": "%slice_scatter_1 : [num_users=1] = call_function[target=torch.ops.aten.slice_scatter.default](args = (%slice_18, %slice_scatter, 1, 0, 9223372036854775807), kwargs = {})"
              },
              {
                "key": "[metadata] pkg.torch.onnx.name_scopes",
                "value": "['', 'model', 'slice_scatter_1']"
              },
              {
                "key": "[metadata] pkg.torch.onnx.stack_trace",
                "value": "File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 688, in forward\n    outputs: BaseModelOutputWithPast = self.model(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 436, in forward\n    causal_mask = self._update_causal_mask("
              }
            ],
            "incomingEdges": [
              {
                "sourceNodeId": "node_Expand_94",
                "sourceNodeOutputId": "0",
                "targetNodeInputId": "0"
              }
            ],
            "outputsMetadata": [
              {
                "id": "0",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "val_195"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "INT64[4]"
                  },
                  {
                    "key": "param_name",
                    "value": "shape"
                  }
                ]
              }
            ],
            "inputsMetadata": [
              {
                "id": "0",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "expand_1"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "FLOAT[s34,1,s16,s16 + s17]"
                  },
                  {
                    "key": "param_name",
                    "value": "data"
                  }
                ]
              }
            ],
            "style": null,
            "config": null
          },
          {
            "id": "node_Gather_259",
            "label": "Gather",
            "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/slice_scatter_1: aten.slice_scatter.default",
            "subgraphIds": [],
            "attrs": [
              {
                "key": "axis",
                "value": "0"
              },
              {
                "key": "[metadata] namespace",
                "value": ": transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/slice_scatter_1: aten.slice_scatter.default"
              },
              {
                "key": "[metadata] pkg.torch.onnx.class_hierarchy",
                "value": "['transformers.models.llama.modeling_llama.LlamaForCausalLM', 'transformers.models.llama.modeling_llama.LlamaModel', 'aten.slice_scatter.default']"
              },
              {
                "key": "[metadata] pkg.torch.onnx.fx_node",
                "value": "%slice_scatter_1 : [num_users=1] = call_function[target=torch.ops.aten.slice_scatter.default](args = (%slice_18, %slice_scatter, 1, 0, 9223372036854775807), kwargs = {})"
              },
              {
                "key": "[metadata] pkg.torch.onnx.name_scopes",
                "value": "['', 'model', 'slice_scatter_1']"
              },
              {
                "key": "[metadata] pkg.torch.onnx.stack_trace",
                "value": "File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 688, in forward\n    outputs: BaseModelOutputWithPast = self.model(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 436, in forward\n    causal_mask = self._update_causal_mask("
              }
            ],
            "incomingEdges": [
              {
                "sourceNodeId": "node_Shape_258",
                "sourceNodeOutputId": "0",
                "targetNodeInputId": "0"
              },
              {
                "sourceNodeId": "node_Constant_105",
                "sourceNodeOutputId": "0",
                "targetNodeInputId": "1"
              }
            ],
            "outputsMetadata": [
              {
                "id": "0",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "val_196"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "INT64[]"
                  },
                  {
                    "key": "param_name",
                    "value": "output"
                  }
                ]
              }
            ],
            "inputsMetadata": [
              {
                "id": "0",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "val_195"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "INT64[4]"
                  },
                  {
                    "key": "param_name",
                    "value": "data"
                  }
                ]
              },
              {
                "id": "1",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "val_65"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "INT64[]"
                  },
                  {
                    "key": "param_name",
                    "value": "indices"
                  }
                ]
              }
            ],
            "style": null,
            "config": null
          },
          {
            "id": "node_Range_260",
            "label": "Range",
            "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/slice_scatter_1: aten.slice_scatter.default",
            "subgraphIds": [],
            "attrs": [
              {
                "key": "[metadata] namespace",
                "value": ": transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/slice_scatter_1: aten.slice_scatter.default"
              },
              {
                "key": "[metadata] pkg.torch.onnx.class_hierarchy",
                "value": "['transformers.models.llama.modeling_llama.LlamaForCausalLM', 'transformers.models.llama.modeling_llama.LlamaModel', 'aten.slice_scatter.default']"
              },
              {
                "key": "[metadata] pkg.torch.onnx.fx_node",
                "value": "%slice_scatter_1 : [num_users=1] = call_function[target=torch.ops.aten.slice_scatter.default](args = (%slice_18, %slice_scatter, 1, 0, 9223372036854775807), kwargs = {})"
              },
              {
                "key": "[metadata] pkg.torch.onnx.name_scopes",
                "value": "['', 'model', 'slice_scatter_1']"
              },
              {
                "key": "[metadata] pkg.torch.onnx.stack_trace",
                "value": "File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 688, in forward\n    outputs: BaseModelOutputWithPast = self.model(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 436, in forward\n    causal_mask = self._update_causal_mask("
              }
            ],
            "incomingEdges": [
              {
                "sourceNodeId": "node_Constant_62",
                "sourceNodeOutputId": "0",
                "targetNodeInputId": "0"
              },
              {
                "sourceNodeId": "node_Gather_259",
                "sourceNodeOutputId": "0",
                "targetNodeInputId": "1"
              },
              {
                "sourceNodeId": "node_Constant_105",
                "sourceNodeOutputId": "0",
                "targetNodeInputId": "2"
              }
            ],
            "outputsMetadata": [
              {
                "id": "0",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "val_197"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "INT64[unk__17]"
                  },
                  {
                    "key": "param_name",
                    "value": "output"
                  }
                ]
              }
            ],
            "inputsMetadata": [
              {
                "id": "0",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "val_27"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "INT64[]"
                  },
                  {
                    "key": "param_name",
                    "value": "start"
                  }
                ]
              },
              {
                "id": "1",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "val_196"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "INT64[]"
                  },
                  {
                    "key": "param_name",
                    "value": "limit"
                  }
                ]
              },
              {
                "id": "2",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "val_65"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "INT64[]"
                  },
                  {
                    "key": "param_name",
                    "value": "delta"
                  }
                ]
              }
            ],
            "style": null,
            "config": null
          },
          {
            "id": "node_Unsqueeze_265",
            "label": "Unsqueeze",
            "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/slice_scatter_1: aten.slice_scatter.default",
            "subgraphIds": [],
            "attrs": [
              {
                "key": "[metadata] namespace",
                "value": ": transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/slice_scatter_1: aten.slice_scatter.default"
              },
              {
                "key": "[metadata] pkg.torch.onnx.class_hierarchy",
                "value": "['transformers.models.llama.modeling_llama.LlamaForCausalLM', 'transformers.models.llama.modeling_llama.LlamaModel', 'aten.slice_scatter.default']"
              },
              {
                "key": "[metadata] pkg.torch.onnx.fx_node",
                "value": "%slice_scatter_1 : [num_users=1] = call_function[target=torch.ops.aten.slice_scatter.default](args = (%slice_18, %slice_scatter, 1, 0, 9223372036854775807), kwargs = {})"
              },
              {
                "key": "[metadata] pkg.torch.onnx.name_scopes",
                "value": "['', 'model', 'slice_scatter_1']"
              },
              {
                "key": "[metadata] pkg.torch.onnx.stack_trace",
                "value": "File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 688, in forward\n    outputs: BaseModelOutputWithPast = self.model(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 436, in forward\n    causal_mask = self._update_causal_mask("
              }
            ],
            "incomingEdges": [
              {
                "sourceNodeId": "node_Range_260",
                "sourceNodeOutputId": "0",
                "targetNodeInputId": "0"
              },
              {
                "sourceNodeId": "node_Constant_90",
                "sourceNodeOutputId": "0",
                "targetNodeInputId": "1"
              }
            ],
            "outputsMetadata": [
              {
                "id": "0",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "val_202"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "INT64[unk__17,1]"
                  },
                  {
                    "key": "param_name",
                    "value": "expanded"
                  }
                ]
              }
            ],
            "inputsMetadata": [
              {
                "id": "0",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "val_197"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "INT64[unk__17]"
                  },
                  {
                    "key": "param_name",
                    "value": "data"
                  }
                ]
              },
              {
                "id": "1",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "val_53"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "INT64[1]"
                  },
                  {
                    "key": "param_name",
                    "value": "axes"
                  }
                ]
              }
            ],
            "style": null,
            "config": null
          },
          {
            "id": "node_Transpose_1025",
            "label": "Transpose",
            "namespace": "main_graph",
            "subgraphIds": [],
            "attrs": [
              {
                "key": "perm",
                "value": "[1, 2, 0, 3]"
              }
            ],
            "incomingEdges": [
              {
                "sourceNodeId": "node_ScatterND_255",
                "sourceNodeOutputId": "0",
                "targetNodeInputId": "0"
              }
            ],
            "outputsMetadata": [
              {
                "id": "0",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "val_203"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "FLOAT[1,s34,s16,s16 + s17]"
                  },
                  {
                    "key": "param_name",
                    "value": "transposed"
                  }
                ]
              }
            ],
            "inputsMetadata": [
              {
                "id": "0",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "val_193"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "FLOAT[s16,1,s34,s16 + s17]"
                  },
                  {
                    "key": "param_name",
                    "value": "data"
                  }
                ]
              }
            ],
            "style": null,
            "config": null
          },
          {
            "id": "node_Transpose_267",
            "label": "Transpose",
            "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/slice_scatter_1: aten.slice_scatter.default",
            "subgraphIds": [],
            "attrs": [
              {
                "key": "perm",
                "value": "[1, 0, 2, 3]"
              },
              {
                "key": "[metadata] namespace",
                "value": ": transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/slice_scatter_1: aten.slice_scatter.default"
              },
              {
                "key": "[metadata] pkg.torch.onnx.class_hierarchy",
                "value": "['transformers.models.llama.modeling_llama.LlamaForCausalLM', 'transformers.models.llama.modeling_llama.LlamaModel', 'aten.slice_scatter.default']"
              },
              {
                "key": "[metadata] pkg.torch.onnx.fx_node",
                "value": "%slice_scatter_1 : [num_users=1] = call_function[target=torch.ops.aten.slice_scatter.default](args = (%slice_18, %slice_scatter, 1, 0, 9223372036854775807), kwargs = {})"
              },
              {
                "key": "[metadata] pkg.torch.onnx.name_scopes",
                "value": "['', 'model', 'slice_scatter_1']"
              },
              {
                "key": "[metadata] pkg.torch.onnx.stack_trace",
                "value": "File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 688, in forward\n    outputs: BaseModelOutputWithPast = self.model(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 436, in forward\n    causal_mask = self._update_causal_mask("
              }
            ],
            "incomingEdges": [
              {
                "sourceNodeId": "node_Expand_94",
                "sourceNodeOutputId": "0",
                "targetNodeInputId": "0"
              }
            ],
            "outputsMetadata": [
              {
                "id": "0",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "val_204"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "FLOAT[1,s34,s16,s16 + s17]"
                  },
                  {
                    "key": "param_name",
                    "value": "transposed"
                  }
                ]
              }
            ],
            "inputsMetadata": [
              {
                "id": "0",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "expand_1"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "FLOAT[s34,1,s16,s16 + s17]"
                  },
                  {
                    "key": "param_name",
                    "value": "data"
                  }
                ]
              }
            ],
            "style": null,
            "config": null
          },
          {
            "id": "node_ScatterND_268",
            "label": "ScatterND",
            "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/slice_scatter_1: aten.slice_scatter.default",
            "subgraphIds": [],
            "attrs": [
              {
                "key": "reduction",
                "value": "none"
              },
              {
                "key": "[metadata] namespace",
                "value": ": transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/slice_scatter_1: aten.slice_scatter.default"
              },
              {
                "key": "[metadata] pkg.torch.onnx.class_hierarchy",
                "value": "['transformers.models.llama.modeling_llama.LlamaForCausalLM', 'transformers.models.llama.modeling_llama.LlamaModel', 'aten.slice_scatter.default']"
              },
              {
                "key": "[metadata] pkg.torch.onnx.fx_node",
                "value": "%slice_scatter_1 : [num_users=1] = call_function[target=torch.ops.aten.slice_scatter.default](args = (%slice_18, %slice_scatter, 1, 0, 9223372036854775807), kwargs = {})"
              },
              {
                "key": "[metadata] pkg.torch.onnx.name_scopes",
                "value": "['', 'model', 'slice_scatter_1']"
              },
              {
                "key": "[metadata] pkg.torch.onnx.stack_trace",
                "value": "File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 688, in forward\n    outputs: BaseModelOutputWithPast = self.model(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 436, in forward\n    causal_mask = self._update_causal_mask("
              }
            ],
            "incomingEdges": [
              {
                "sourceNodeId": "node_Transpose_267",
                "sourceNodeOutputId": "0",
                "targetNodeInputId": "0"
              },
              {
                "sourceNodeId": "node_Unsqueeze_265",
                "sourceNodeOutputId": "0",
                "targetNodeInputId": "1"
              },
              {
                "sourceNodeId": "node_Transpose_1025",
                "sourceNodeOutputId": "0",
                "targetNodeInputId": "2"
              }
            ],
            "outputsMetadata": [
              {
                "id": "0",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "val_205"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "FLOAT[1,s34,s16,s16 + s17]"
                  },
                  {
                    "key": "param_name",
                    "value": "output"
                  }
                ]
              }
            ],
            "inputsMetadata": [
              {
                "id": "0",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "val_204"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "FLOAT[1,s34,s16,s16 + s17]"
                  },
                  {
                    "key": "param_name",
                    "value": "data"
                  }
                ]
              },
              {
                "id": "1",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "val_202"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "INT64[unk__17,1]"
                  },
                  {
                    "key": "param_name",
                    "value": "indices"
                  }
                ]
              },
              {
                "id": "2",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "val_203"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "FLOAT[1,s34,s16,s16 + s17]"
                  },
                  {
                    "key": "param_name",
                    "value": "updates"
                  }
                ]
              }
            ],
            "style": null,
            "config": null
          },
          {
            "id": "node_Transpose_269",
            "label": "Transpose",
            "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/slice_scatter_1: aten.slice_scatter.default",
            "subgraphIds": [],
            "attrs": [
              {
                "key": "perm",
                "value": "[1, 0, 2, 3]"
              },
              {
                "key": "[metadata] namespace",
                "value": ": transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/slice_scatter_1: aten.slice_scatter.default"
              },
              {
                "key": "[metadata] pkg.torch.onnx.class_hierarchy",
                "value": "['transformers.models.llama.modeling_llama.LlamaForCausalLM', 'transformers.models.llama.modeling_llama.LlamaModel', 'aten.slice_scatter.default']"
              },
              {
                "key": "[metadata] pkg.torch.onnx.fx_node",
                "value": "%slice_scatter_1 : [num_users=1] = call_function[target=torch.ops.aten.slice_scatter.default](args = (%slice_18, %slice_scatter, 1, 0, 9223372036854775807), kwargs = {})"
              },
              {
                "key": "[metadata] pkg.torch.onnx.name_scopes",
                "value": "['', 'model', 'slice_scatter_1']"
              },
              {
                "key": "[metadata] pkg.torch.onnx.stack_trace",
                "value": "File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 688, in forward\n    outputs: BaseModelOutputWithPast = self.model(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 436, in forward\n    causal_mask = self._update_causal_mask("
              }
            ],
            "incomingEdges": [
              {
                "sourceNodeId": "node_ScatterND_268",
                "sourceNodeOutputId": "0",
                "targetNodeInputId": "0"
              }
            ],
            "outputsMetadata": [
              {
                "id": "0",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "slice_scatter_1"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "FLOAT[s34,1,s16,s16 + s17]"
                  },
                  {
                    "key": "param_name",
                    "value": "transposed"
                  }
                ]
              }
            ],
            "inputsMetadata": [
              {
                "id": "0",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "val_205"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "FLOAT[1,s34,s16,s16 + s17]"
                  },
                  {
                    "key": "param_name",
                    "value": "data"
                  }
                ]
              }
            ],
            "style": null,
            "config": null
          },
          {
            "id": "node_Shape_271",
            "label": "Shape",
            "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/slice_scatter_2: aten.slice_scatter.default",
            "subgraphIds": [],
            "attrs": [
              {
                "key": "start",
                "value": "0"
              },
              {
                "key": "[metadata] namespace",
                "value": ": transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/slice_scatter_2: aten.slice_scatter.default"
              },
              {
                "key": "[metadata] pkg.torch.onnx.class_hierarchy",
                "value": "['transformers.models.llama.modeling_llama.LlamaForCausalLM', 'transformers.models.llama.modeling_llama.LlamaModel', 'aten.slice_scatter.default']"
              },
              {
                "key": "[metadata] pkg.torch.onnx.fx_node",
                "value": "%slice_scatter_2 : [num_users=2] = call_function[target=torch.ops.aten.slice_scatter.default](args = (%clone, %slice_scatter_1, 0, 0, 9223372036854775807), kwargs = {})"
              },
              {
                "key": "[metadata] pkg.torch.onnx.name_scopes",
                "value": "['', 'model', 'slice_scatter_2']"
              },
              {
                "key": "[metadata] pkg.torch.onnx.stack_trace",
                "value": "File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 688, in forward\n    outputs: BaseModelOutputWithPast = self.model(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 436, in forward\n    causal_mask = self._update_causal_mask("
              }
            ],
            "incomingEdges": [
              {
                "sourceNodeId": "node_Expand_94",
                "sourceNodeOutputId": "0",
                "targetNodeInputId": "0"
              }
            ],
            "outputsMetadata": [
              {
                "id": "0",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "val_207"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "INT64[4]"
                  },
                  {
                    "key": "param_name",
                    "value": "shape"
                  }
                ]
              }
            ],
            "inputsMetadata": [
              {
                "id": "0",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "expand_1"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "FLOAT[s34,1,s16,s16 + s17]"
                  },
                  {
                    "key": "param_name",
                    "value": "data"
                  }
                ]
              }
            ],
            "style": null,
            "config": null
          },
          {
            "id": "node_Gather_272",
            "label": "Gather",
            "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/slice_scatter_2: aten.slice_scatter.default",
            "subgraphIds": [],
            "attrs": [
              {
                "key": "axis",
                "value": "0"
              },
              {
                "key": "[metadata] namespace",
                "value": ": transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/slice_scatter_2: aten.slice_scatter.default"
              },
              {
                "key": "[metadata] pkg.torch.onnx.class_hierarchy",
                "value": "['transformers.models.llama.modeling_llama.LlamaForCausalLM', 'transformers.models.llama.modeling_llama.LlamaModel', 'aten.slice_scatter.default']"
              },
              {
                "key": "[metadata] pkg.torch.onnx.fx_node",
                "value": "%slice_scatter_2 : [num_users=2] = call_function[target=torch.ops.aten.slice_scatter.default](args = (%clone, %slice_scatter_1, 0, 0, 9223372036854775807), kwargs = {})"
              },
              {
                "key": "[metadata] pkg.torch.onnx.name_scopes",
                "value": "['', 'model', 'slice_scatter_2']"
              },
              {
                "key": "[metadata] pkg.torch.onnx.stack_trace",
                "value": "File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 688, in forward\n    outputs: BaseModelOutputWithPast = self.model(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 436, in forward\n    causal_mask = self._update_causal_mask("
              }
            ],
            "incomingEdges": [
              {
                "sourceNodeId": "node_Shape_271",
                "sourceNodeOutputId": "0",
                "targetNodeInputId": "0"
              },
              {
                "sourceNodeId": "node_Constant_62",
                "sourceNodeOutputId": "0",
                "targetNodeInputId": "1"
              }
            ],
            "outputsMetadata": [
              {
                "id": "0",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "val_208"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "INT64[]"
                  },
                  {
                    "key": "param_name",
                    "value": "output"
                  }
                ]
              }
            ],
            "inputsMetadata": [
              {
                "id": "0",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "val_207"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "INT64[4]"
                  },
                  {
                    "key": "param_name",
                    "value": "data"
                  }
                ]
              },
              {
                "id": "1",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "val_27"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "INT64[]"
                  },
                  {
                    "key": "param_name",
                    "value": "indices"
                  }
                ]
              }
            ],
            "style": null,
            "config": null
          },
          {
            "id": "node_Range_273",
            "label": "Range",
            "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/slice_scatter_2: aten.slice_scatter.default",
            "subgraphIds": [],
            "attrs": [
              {
                "key": "[metadata] namespace",
                "value": ": transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/slice_scatter_2: aten.slice_scatter.default"
              },
              {
                "key": "[metadata] pkg.torch.onnx.class_hierarchy",
                "value": "['transformers.models.llama.modeling_llama.LlamaForCausalLM', 'transformers.models.llama.modeling_llama.LlamaModel', 'aten.slice_scatter.default']"
              },
              {
                "key": "[metadata] pkg.torch.onnx.fx_node",
                "value": "%slice_scatter_2 : [num_users=2] = call_function[target=torch.ops.aten.slice_scatter.default](args = (%clone, %slice_scatter_1, 0, 0, 9223372036854775807), kwargs = {})"
              },
              {
                "key": "[metadata] pkg.torch.onnx.name_scopes",
                "value": "['', 'model', 'slice_scatter_2']"
              },
              {
                "key": "[metadata] pkg.torch.onnx.stack_trace",
                "value": "File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 688, in forward\n    outputs: BaseModelOutputWithPast = self.model(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 436, in forward\n    causal_mask = self._update_causal_mask("
              }
            ],
            "incomingEdges": [
              {
                "sourceNodeId": "node_Constant_62",
                "sourceNodeOutputId": "0",
                "targetNodeInputId": "0"
              },
              {
                "sourceNodeId": "node_Gather_272",
                "sourceNodeOutputId": "0",
                "targetNodeInputId": "1"
              },
              {
                "sourceNodeId": "node_Constant_105",
                "sourceNodeOutputId": "0",
                "targetNodeInputId": "2"
              }
            ],
            "outputsMetadata": [
              {
                "id": "0",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "val_209"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "INT64[unk__18]"
                  },
                  {
                    "key": "param_name",
                    "value": "output"
                  }
                ]
              }
            ],
            "inputsMetadata": [
              {
                "id": "0",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "val_27"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "INT64[]"
                  },
                  {
                    "key": "param_name",
                    "value": "start"
                  }
                ]
              },
              {
                "id": "1",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "val_208"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "INT64[]"
                  },
                  {
                    "key": "param_name",
                    "value": "limit"
                  }
                ]
              },
              {
                "id": "2",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "val_65"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "INT64[]"
                  },
                  {
                    "key": "param_name",
                    "value": "delta"
                  }
                ]
              }
            ],
            "style": null,
            "config": null
          },
          {
            "id": "node_Unsqueeze_278",
            "label": "Unsqueeze",
            "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/slice_scatter_2: aten.slice_scatter.default",
            "subgraphIds": [],
            "attrs": [
              {
                "key": "[metadata] namespace",
                "value": ": transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/slice_scatter_2: aten.slice_scatter.default"
              },
              {
                "key": "[metadata] pkg.torch.onnx.class_hierarchy",
                "value": "['transformers.models.llama.modeling_llama.LlamaForCausalLM', 'transformers.models.llama.modeling_llama.LlamaModel', 'aten.slice_scatter.default']"
              },
              {
                "key": "[metadata] pkg.torch.onnx.fx_node",
                "value": "%slice_scatter_2 : [num_users=2] = call_function[target=torch.ops.aten.slice_scatter.default](args = (%clone, %slice_scatter_1, 0, 0, 9223372036854775807), kwargs = {})"
              },
              {
                "key": "[metadata] pkg.torch.onnx.name_scopes",
                "value": "['', 'model', 'slice_scatter_2']"
              },
              {
                "key": "[metadata] pkg.torch.onnx.stack_trace",
                "value": "File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 688, in forward\n    outputs: BaseModelOutputWithPast = self.model(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 436, in forward\n    causal_mask = self._update_causal_mask("
              }
            ],
            "incomingEdges": [
              {
                "sourceNodeId": "node_Range_273",
                "sourceNodeOutputId": "0",
                "targetNodeInputId": "0"
              },
              {
                "sourceNodeId": "node_Constant_90",
                "sourceNodeOutputId": "0",
                "targetNodeInputId": "1"
              }
            ],
            "outputsMetadata": [
              {
                "id": "0",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "val_214"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "INT64[unk__18,1]"
                  },
                  {
                    "key": "param_name",
                    "value": "expanded"
                  }
                ]
              }
            ],
            "inputsMetadata": [
              {
                "id": "0",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "val_209"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "INT64[unk__18]"
                  },
                  {
                    "key": "param_name",
                    "value": "data"
                  }
                ]
              },
              {
                "id": "1",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "val_53"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "INT64[1]"
                  },
                  {
                    "key": "param_name",
                    "value": "axes"
                  }
                ]
              }
            ],
            "style": null,
            "config": null
          },
          {
            "id": "node_ScatterND_279",
            "label": "ScatterND",
            "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/slice_scatter_2: aten.slice_scatter.default",
            "subgraphIds": [],
            "attrs": [
              {
                "key": "reduction",
                "value": "none"
              },
              {
                "key": "[metadata] namespace",
                "value": ": transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/slice_scatter_2: aten.slice_scatter.default"
              },
              {
                "key": "[metadata] pkg.torch.onnx.class_hierarchy",
                "value": "['transformers.models.llama.modeling_llama.LlamaForCausalLM', 'transformers.models.llama.modeling_llama.LlamaModel', 'aten.slice_scatter.default']"
              },
              {
                "key": "[metadata] pkg.torch.onnx.fx_node",
                "value": "%slice_scatter_2 : [num_users=2] = call_function[target=torch.ops.aten.slice_scatter.default](args = (%clone, %slice_scatter_1, 0, 0, 9223372036854775807), kwargs = {})"
              },
              {
                "key": "[metadata] pkg.torch.onnx.name_scopes",
                "value": "['', 'model', 'slice_scatter_2']"
              },
              {
                "key": "[metadata] pkg.torch.onnx.stack_trace",
                "value": "File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 688, in forward\n    outputs: BaseModelOutputWithPast = self.model(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 436, in forward\n    causal_mask = self._update_causal_mask("
              }
            ],
            "incomingEdges": [
              {
                "sourceNodeId": "node_Expand_94",
                "sourceNodeOutputId": "0",
                "targetNodeInputId": "0"
              },
              {
                "sourceNodeId": "node_Unsqueeze_278",
                "sourceNodeOutputId": "0",
                "targetNodeInputId": "1"
              },
              {
                "sourceNodeId": "node_Transpose_269",
                "sourceNodeOutputId": "0",
                "targetNodeInputId": "2"
              }
            ],
            "outputsMetadata": [
              {
                "id": "0",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "slice_scatter_2"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "FLOAT[s34,1,s16,s16 + s17]"
                  },
                  {
                    "key": "param_name",
                    "value": "output"
                  }
                ]
              }
            ],
            "inputsMetadata": [
              {
                "id": "0",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "expand_1"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "FLOAT[s34,1,s16,s16 + s17]"
                  },
                  {
                    "key": "param_name",
                    "value": "data"
                  }
                ]
              },
              {
                "id": "1",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "val_214"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "INT64[unk__18,1]"
                  },
                  {
                    "key": "param_name",
                    "value": "indices"
                  }
                ]
              },
              {
                "id": "2",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "slice_scatter_1"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "FLOAT[s34,1,s16,s16 + s17]"
                  },
                  {
                    "key": "param_name",
                    "value": "updates"
                  }
                ]
              }
            ],
            "style": null,
            "config": null
          },
          {
            "id": "node_Unsqueeze_308",
            "label": "Unsqueeze",
            "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.rotary_emb: transformers.models.llama.modeling_llama.LlamaRotaryEmbedding/unsqueeze_9: aten.unsqueeze.default",
            "subgraphIds": [],
            "attrs": [
              {
                "key": "[metadata] namespace",
                "value": ": transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.rotary_emb: transformers.models.llama.modeling_llama.LlamaRotaryEmbedding/unsqueeze_9: aten.unsqueeze.default"
              },
              {
                "key": "[metadata] pkg.torch.onnx.class_hierarchy",
                "value": "['transformers.models.llama.modeling_llama.LlamaForCausalLM', 'transformers.models.llama.modeling_llama.LlamaModel', 'transformers.models.llama.modeling_llama.LlamaRotaryEmbedding', 'aten.unsqueeze.default']"
              },
              {
                "key": "[metadata] pkg.torch.onnx.fx_node",
                "value": "%unsqueeze_9 : [num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%slice_24, 1), kwargs = {})"
              },
              {
                "key": "[metadata] pkg.torch.onnx.name_scopes",
                "value": "['', 'model', 'model.rotary_emb', 'unsqueeze_9']"
              },
              {
                "key": "[metadata] pkg.torch.onnx.stack_trace",
                "value": "File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 688, in forward\n    outputs: BaseModelOutputWithPast = self.model(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 443, in forward\n    position_embeddings = self.rotary_emb(hidden_states, position_ids)\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 104, in forward\n    position_ids_expanded = position_ids[:, None, :].float()"
              }
            ],
            "incomingEdges": [
              {
                "sourceNodeId": "node_Unsqueeze_38",
                "sourceNodeOutputId": "0",
                "targetNodeInputId": "0"
              },
              {
                "sourceNodeId": "node_Constant_60",
                "sourceNodeOutputId": "0",
                "targetNodeInputId": "1"
              }
            ],
            "outputsMetadata": [
              {
                "id": "0",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "unsqueeze_9"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "INT64[1,1,s16]"
                  },
                  {
                    "key": "param_name",
                    "value": "expanded"
                  }
                ]
              }
            ],
            "inputsMetadata": [
              {
                "id": "0",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "unsqueeze"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "INT64[1,s16]"
                  },
                  {
                    "key": "param_name",
                    "value": "data"
                  }
                ]
              },
              {
                "id": "1",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "val_26"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "INT64[1]"
                  },
                  {
                    "key": "param_name",
                    "value": "axes"
                  }
                ]
              }
            ],
            "style": null,
            "config": null
          },
          {
            "id": "node_Cast_320",
            "label": "Cast",
            "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.rotary_emb: transformers.models.llama.modeling_llama.LlamaRotaryEmbedding/_to_copy: aten._to_copy.default",
            "subgraphIds": [],
            "attrs": [
              {
                "key": "to",
                "value": "FLOAT"
              },
              {
                "key": "[metadata] namespace",
                "value": ": transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.rotary_emb: transformers.models.llama.modeling_llama.LlamaRotaryEmbedding/_to_copy: aten._to_copy.default"
              },
              {
                "key": "[metadata] pkg.torch.onnx.class_hierarchy",
                "value": "['transformers.models.llama.modeling_llama.LlamaForCausalLM', 'transformers.models.llama.modeling_llama.LlamaModel', 'transformers.models.llama.modeling_llama.LlamaRotaryEmbedding', 'aten._to_copy.default']"
              },
              {
                "key": "[metadata] pkg.torch.onnx.fx_node",
                "value": "%_to_copy : [num_users=1] = call_function[target=torch.ops.aten._to_copy.default](args = (%slice_25,), kwargs = {dtype: torch.float32})"
              },
              {
                "key": "[metadata] pkg.torch.onnx.name_scopes",
                "value": "['', 'model', 'model.rotary_emb', '_to_copy']"
              },
              {
                "key": "[metadata] pkg.torch.onnx.stack_trace",
                "value": "File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 688, in forward\n    outputs: BaseModelOutputWithPast = self.model(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 443, in forward\n    position_embeddings = self.rotary_emb(hidden_states, position_ids)\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 104, in forward\n    position_ids_expanded = position_ids[:, None, :].float()"
              }
            ],
            "incomingEdges": [
              {
                "sourceNodeId": "node_Unsqueeze_308",
                "sourceNodeOutputId": "0",
                "targetNodeInputId": "0"
              }
            ],
            "outputsMetadata": [
              {
                "id": "0",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "_to_copy"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "FLOAT[1,1,s16]"
                  },
                  {
                    "key": "param_name",
                    "value": "output"
                  }
                ]
              }
            ],
            "inputsMetadata": [
              {
                "id": "0",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "unsqueeze_9"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "INT64[1,1,s16]"
                  },
                  {
                    "key": "param_name",
                    "value": "input"
                  }
                ]
              }
            ],
            "style": null,
            "config": null
          },
          {
            "id": "node_MatMul_321",
            "label": "MatMul",
            "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.rotary_emb: transformers.models.llama.modeling_llama.LlamaRotaryEmbedding/matmul: aten.matmul.default",
            "subgraphIds": [],
            "attrs": [
              {
                "key": "[metadata] namespace",
                "value": ": transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.rotary_emb: transformers.models.llama.modeling_llama.LlamaRotaryEmbedding/matmul: aten.matmul.default"
              },
              {
                "key": "[metadata] pkg.torch.onnx.class_hierarchy",
                "value": "['transformers.models.llama.modeling_llama.LlamaForCausalLM', 'transformers.models.llama.modeling_llama.LlamaModel', 'transformers.models.llama.modeling_llama.LlamaRotaryEmbedding', 'aten.matmul.default']"
              },
              {
                "key": "[metadata] pkg.torch.onnx.fx_node",
                "value": "%matmul : [num_users=1] = call_function[target=torch.ops.aten.matmul.default](args = (%expand_2, %_to_copy), kwargs = {})"
              },
              {
                "key": "[metadata] pkg.torch.onnx.name_scopes",
                "value": "['', 'model', 'model.rotary_emb', 'matmul']"
              },
              {
                "key": "[metadata] pkg.torch.onnx.stack_trace",
                "value": "File \"<eval_with_key>.20\", line 9, in forward\n    matmul = torch.ops.aten.matmul.default(to_4, to_5);  to_4 = to_5 = None"
              }
            ],
            "incomingEdges": [
              {
                "sourceNodeId": "[value] expand_2",
                "sourceNodeOutputId": "0",
                "targetNodeInputId": "0"
              },
              {
                "sourceNodeId": "node_Cast_320",
                "sourceNodeOutputId": "0",
                "targetNodeInputId": "1"
              }
            ],
            "outputsMetadata": [
              {
                "id": "0",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "matmul"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "FLOAT[1,32,s16]"
                  },
                  {
                    "key": "param_name",
                    "value": "Y"
                  }
                ]
              }
            ],
            "inputsMetadata": [
              {
                "id": "0",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "expand_2"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "FLOAT[1,32,1]"
                  },
                  {
                    "key": "param_name",
                    "value": "A"
                  }
                ]
              },
              {
                "id": "1",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "_to_copy"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "FLOAT[1,1,s16]"
                  },
                  {
                    "key": "param_name",
                    "value": "B"
                  }
                ]
              }
            ],
            "style": null,
            "config": null
          },
          {
            "id": "node_Transpose_322",
            "label": "Transpose",
            "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.rotary_emb: transformers.models.llama.modeling_llama.LlamaRotaryEmbedding/transpose: aten.transpose.int",
            "subgraphIds": [],
            "attrs": [
              {
                "key": "perm",
                "value": "[0, 2, 1]"
              },
              {
                "key": "[metadata] namespace",
                "value": ": transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.rotary_emb: transformers.models.llama.modeling_llama.LlamaRotaryEmbedding/transpose: aten.transpose.int"
              },
              {
                "key": "[metadata] pkg.torch.onnx.class_hierarchy",
                "value": "['transformers.models.llama.modeling_llama.LlamaForCausalLM', 'transformers.models.llama.modeling_llama.LlamaModel', 'transformers.models.llama.modeling_llama.LlamaRotaryEmbedding', 'aten.transpose.int']"
              },
              {
                "key": "[metadata] pkg.torch.onnx.fx_node",
                "value": "%transpose : [num_users=1] = call_function[target=torch.ops.aten.transpose.int](args = (%matmul, 1, 2), kwargs = {})"
              },
              {
                "key": "[metadata] pkg.torch.onnx.name_scopes",
                "value": "['', 'model', 'model.rotary_emb', 'transpose']"
              },
              {
                "key": "[metadata] pkg.torch.onnx.stack_trace",
                "value": "File \"<eval_with_key>.20\", line 10, in forward\n    transpose = torch.ops.aten.transpose.int(matmul, 1, 2);  matmul = None"
              }
            ],
            "incomingEdges": [
              {
                "sourceNodeId": "node_MatMul_321",
                "sourceNodeOutputId": "0",
                "targetNodeInputId": "0"
              }
            ],
            "outputsMetadata": [
              {
                "id": "0",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "transpose"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "FLOAT[1,s16,32]"
                  },
                  {
                    "key": "param_name",
                    "value": "transposed"
                  }
                ]
              }
            ],
            "inputsMetadata": [
              {
                "id": "0",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "matmul"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "FLOAT[1,32,s16]"
                  },
                  {
                    "key": "param_name",
                    "value": "data"
                  }
                ]
              }
            ],
            "style": null,
            "config": null
          },
          {
            "id": "node_Concat_323",
            "label": "Concat",
            "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.rotary_emb: transformers.models.llama.modeling_llama.LlamaRotaryEmbedding/cat: aten.cat.default",
            "subgraphIds": [],
            "attrs": [
              {
                "key": "axis",
                "value": "-1"
              },
              {
                "key": "[metadata] namespace",
                "value": ": transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.rotary_emb: transformers.models.llama.modeling_llama.LlamaRotaryEmbedding/cat: aten.cat.default"
              },
              {
                "key": "[metadata] pkg.torch.onnx.class_hierarchy",
                "value": "['transformers.models.llama.modeling_llama.LlamaForCausalLM', 'transformers.models.llama.modeling_llama.LlamaModel', 'transformers.models.llama.modeling_llama.LlamaRotaryEmbedding', 'aten.cat.default']"
              },
              {
                "key": "[metadata] pkg.torch.onnx.fx_node",
                "value": "%cat : [num_users=2] = call_function[target=torch.ops.aten.cat.default](args = ([%transpose, %transpose], -1), kwargs = {})"
              },
              {
                "key": "[metadata] pkg.torch.onnx.name_scopes",
                "value": "['', 'model', 'model.rotary_emb', 'cat']"
              },
              {
                "key": "[metadata] pkg.torch.onnx.stack_trace",
                "value": "File \"<eval_with_key>.20\", line 11, in forward\n    cat = torch.ops.aten.cat.default([transpose, transpose], -1);  transpose = None"
              }
            ],
            "incomingEdges": [
              {
                "sourceNodeId": "node_Transpose_322",
                "sourceNodeOutputId": "0",
                "targetNodeInputId": "0"
              },
              {
                "sourceNodeId": "node_Transpose_322",
                "sourceNodeOutputId": "0",
                "targetNodeInputId": "1"
              }
            ],
            "outputsMetadata": [
              {
                "id": "0",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "cat"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "FLOAT[1,s16,64]"
                  },
                  {
                    "key": "param_name",
                    "value": "concat_result"
                  }
                ]
              }
            ],
            "inputsMetadata": [
              {
                "id": "0",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "transpose"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "FLOAT[1,s16,32]"
                  },
                  {
                    "key": "param_name",
                    "value": "inputs"
                  }
                ]
              },
              {
                "id": "1",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "transpose"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "FLOAT[1,s16,32]"
                  },
                  {
                    "key": "param_name",
                    "value": "inputs"
                  }
                ]
              }
            ],
            "style": null,
            "config": null
          },
          {
            "id": "node_Cos_324",
            "label": "Cos",
            "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.rotary_emb: transformers.models.llama.modeling_llama.LlamaRotaryEmbedding/cos: aten.cos.default",
            "subgraphIds": [],
            "attrs": [
              {
                "key": "[metadata] namespace",
                "value": ": transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.rotary_emb: transformers.models.llama.modeling_llama.LlamaRotaryEmbedding/cos: aten.cos.default"
              },
              {
                "key": "[metadata] pkg.torch.onnx.class_hierarchy",
                "value": "['transformers.models.llama.modeling_llama.LlamaForCausalLM', 'transformers.models.llama.modeling_llama.LlamaModel', 'transformers.models.llama.modeling_llama.LlamaRotaryEmbedding', 'aten.cos.default']"
              },
              {
                "key": "[metadata] pkg.torch.onnx.fx_node",
                "value": "%cos : [num_users=1] = call_function[target=torch.ops.aten.cos.default](args = (%cat,), kwargs = {})"
              },
              {
                "key": "[metadata] pkg.torch.onnx.name_scopes",
                "value": "['', 'model', 'model.rotary_emb', 'cos']"
              },
              {
                "key": "[metadata] pkg.torch.onnx.stack_trace",
                "value": "File \"<eval_with_key>.20\", line 12, in forward\n    cos = torch.ops.aten.cos.default(cat)"
              }
            ],
            "incomingEdges": [
              {
                "sourceNodeId": "node_Concat_323",
                "sourceNodeOutputId": "0",
                "targetNodeInputId": "0"
              }
            ],
            "outputsMetadata": [
              {
                "id": "0",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "cos"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "FLOAT[1,s16,64]"
                  },
                  {
                    "key": "param_name",
                    "value": "output"
                  }
                ]
              }
            ],
            "inputsMetadata": [
              {
                "id": "0",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "cat"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "FLOAT[1,s16,64]"
                  },
                  {
                    "key": "param_name",
                    "value": "input"
                  }
                ]
              }
            ],
            "style": null,
            "config": null
          },
          {
            "id": "node_Sin_326",
            "label": "Sin",
            "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.rotary_emb: transformers.models.llama.modeling_llama.LlamaRotaryEmbedding/sin: aten.sin.default",
            "subgraphIds": [],
            "attrs": [
              {
                "key": "[metadata] namespace",
                "value": ": transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.rotary_emb: transformers.models.llama.modeling_llama.LlamaRotaryEmbedding/sin: aten.sin.default"
              },
              {
                "key": "[metadata] pkg.torch.onnx.class_hierarchy",
                "value": "['transformers.models.llama.modeling_llama.LlamaForCausalLM', 'transformers.models.llama.modeling_llama.LlamaModel', 'transformers.models.llama.modeling_llama.LlamaRotaryEmbedding', 'aten.sin.default']"
              },
              {
                "key": "[metadata] pkg.torch.onnx.fx_node",
                "value": "%sin : [num_users=1] = call_function[target=torch.ops.aten.sin.default](args = (%cat,), kwargs = {})"
              },
              {
                "key": "[metadata] pkg.torch.onnx.name_scopes",
                "value": "['', 'model', 'model.rotary_emb', 'sin']"
              },
              {
                "key": "[metadata] pkg.torch.onnx.stack_trace",
                "value": "File \"<eval_with_key>.20\", line 14, in forward\n    sin = torch.ops.aten.sin.default(cat);  cat = None"
              }
            ],
            "incomingEdges": [
              {
                "sourceNodeId": "node_Concat_323",
                "sourceNodeOutputId": "0",
                "targetNodeInputId": "0"
              }
            ],
            "outputsMetadata": [
              {
                "id": "0",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "sin"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "FLOAT[1,s16,64]"
                  },
                  {
                    "key": "param_name",
                    "value": "output"
                  }
                ]
              }
            ],
            "inputsMetadata": [
              {
                "id": "0",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "cat"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "FLOAT[1,s16,64]"
                  },
                  {
                    "key": "param_name",
                    "value": "input"
                  }
                ]
              }
            ],
            "style": null,
            "config": null
          },
          {
            "id": "node_Constant_328",
            "label": "Constant",
            "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.0: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.0.input_layernorm: transformers.models.llama.modeling_llama.LlamaRMSNorm/pow_1: aten.pow.Tensor_Scalar",
            "subgraphIds": [],
            "attrs": [
              {
                "key": "__value",
                "value": "2.0"
              },
              {
                "key": "value",
                "value": "TensorProtoTensor<FLOAT,[]>(array(2., dtype=float32), name='')"
              },
              {
                "key": "[metadata] namespace",
                "value": ": transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.0: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.0.input_layernorm: transformers.models.llama.modeling_llama.LlamaRMSNorm/pow_1: aten.pow.Tensor_Scalar"
              },
              {
                "key": "[metadata] pkg.torch.onnx.class_hierarchy",
                "value": "['transformers.models.llama.modeling_llama.LlamaForCausalLM', 'transformers.models.llama.modeling_llama.LlamaModel', 'transformers.models.llama.modeling_llama.LlamaDecoderLayer', 'transformers.models.llama.modeling_llama.LlamaRMSNorm', 'aten.pow.Tensor_Scalar']"
              },
              {
                "key": "[metadata] pkg.torch.onnx.fx_node",
                "value": "%pow_1 : [num_users=1] = call_function[target=torch.ops.aten.pow.Tensor_Scalar](args = (%embedding, 2), kwargs = {})"
              },
              {
                "key": "[metadata] pkg.torch.onnx.name_scopes",
                "value": "['', 'model', 'model.layers.0', 'model.layers.0.input_layernorm', 'pow_1']"
              },
              {
                "key": "[metadata] pkg.torch.onnx.stack_trace",
                "value": "File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 688, in forward\n    outputs: BaseModelOutputWithPast = self.model(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 453, in forward\n    layer_outputs = decoder_layer(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 305, in forward\n    hidden_states = self.input_layernorm(hidden_states)\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 71, in forward\n    variance = hidden_states.pow(2).mean(-1, keepdim=True)"
              }
            ],
            "incomingEdges": [],
            "outputsMetadata": [
              {
                "id": "0",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "val_248"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "FLOAT[]"
                  },
                  {
                    "key": "param_name",
                    "value": "output"
                  }
                ]
              }
            ],
            "inputsMetadata": [],
            "style": null,
            "config": null
          },
          {
            "id": "node_Pow_329",
            "label": "Pow",
            "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.0: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.0.input_layernorm: transformers.models.llama.modeling_llama.LlamaRMSNorm/pow_1: aten.pow.Tensor_Scalar",
            "subgraphIds": [],
            "attrs": [
              {
                "key": "[metadata] namespace",
                "value": ": transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.0: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.0.input_layernorm: transformers.models.llama.modeling_llama.LlamaRMSNorm/pow_1: aten.pow.Tensor_Scalar"
              },
              {
                "key": "[metadata] pkg.torch.onnx.class_hierarchy",
                "value": "['transformers.models.llama.modeling_llama.LlamaForCausalLM', 'transformers.models.llama.modeling_llama.LlamaModel', 'transformers.models.llama.modeling_llama.LlamaDecoderLayer', 'transformers.models.llama.modeling_llama.LlamaRMSNorm', 'aten.pow.Tensor_Scalar']"
              },
              {
                "key": "[metadata] pkg.torch.onnx.fx_node",
                "value": "%pow_1 : [num_users=1] = call_function[target=torch.ops.aten.pow.Tensor_Scalar](args = (%embedding, 2), kwargs = {})"
              },
              {
                "key": "[metadata] pkg.torch.onnx.name_scopes",
                "value": "['', 'model', 'model.layers.0', 'model.layers.0.input_layernorm', 'pow_1']"
              },
              {
                "key": "[metadata] pkg.torch.onnx.stack_trace",
                "value": "File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 688, in forward\n    outputs: BaseModelOutputWithPast = self.model(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 453, in forward\n    layer_outputs = decoder_layer(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 305, in forward\n    hidden_states = self.input_layernorm(hidden_states)\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 71, in forward\n    variance = hidden_states.pow(2).mean(-1, keepdim=True)"
              }
            ],
            "incomingEdges": [
              {
                "sourceNodeId": "node_Gather_20",
                "sourceNodeOutputId": "0",
                "targetNodeInputId": "0"
              },
              {
                "sourceNodeId": "node_Constant_328",
                "sourceNodeOutputId": "0",
                "targetNodeInputId": "1"
              }
            ],
            "outputsMetadata": [
              {
                "id": "0",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "pow_1"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "FLOAT[s34,s16,2048]"
                  },
                  {
                    "key": "param_name",
                    "value": "Z"
                  }
                ]
              }
            ],
            "inputsMetadata": [
              {
                "id": "0",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "embedding"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "FLOAT[s34,s16,2048]"
                  },
                  {
                    "key": "param_name",
                    "value": "X"
                  }
                ]
              },
              {
                "id": "1",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "val_248"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "FLOAT[]"
                  },
                  {
                    "key": "param_name",
                    "value": "Y"
                  }
                ]
              }
            ],
            "style": null,
            "config": null
          },
          {
            "id": "node_Constant_869",
            "label": "Constant",
            "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.0: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.0.input_layernorm: transformers.models.llama.modeling_llama.LlamaRMSNorm/mean: aten.mean.dim",
            "subgraphIds": [],
            "attrs": [
              {
                "key": "__value",
                "value": "[-1]"
              },
              {
                "key": "value",
                "value": "TensorProtoTensor<INT64,[1]>(array([-1]), name='val_250')"
              }
            ],
            "incomingEdges": [],
            "outputsMetadata": [
              {
                "id": "0",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "val_250"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "INT64[1]"
                  },
                  {
                    "key": "param_name",
                    "value": "output"
                  }
                ]
              }
            ],
            "inputsMetadata": [],
            "style": null,
            "config": null
          },
          {
            "id": "node_ReduceMean_332",
            "label": "ReduceMean",
            "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.0: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.0.input_layernorm: transformers.models.llama.modeling_llama.LlamaRMSNorm/mean: aten.mean.dim",
            "subgraphIds": [],
            "attrs": [
              {
                "key": "noop_with_empty_axes",
                "value": "0"
              },
              {
                "key": "keepdims",
                "value": "1"
              },
              {
                "key": "[metadata] namespace",
                "value": ": transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.0: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.0.input_layernorm: transformers.models.llama.modeling_llama.LlamaRMSNorm/mean: aten.mean.dim"
              },
              {
                "key": "[metadata] pkg.torch.onnx.class_hierarchy",
                "value": "['transformers.models.llama.modeling_llama.LlamaForCausalLM', 'transformers.models.llama.modeling_llama.LlamaModel', 'transformers.models.llama.modeling_llama.LlamaDecoderLayer', 'transformers.models.llama.modeling_llama.LlamaRMSNorm', 'aten.mean.dim']"
              },
              {
                "key": "[metadata] pkg.torch.onnx.fx_node",
                "value": "%mean : [num_users=1] = call_function[target=torch.ops.aten.mean.dim](args = (%pow_1, [-1], True), kwargs = {})"
              },
              {
                "key": "[metadata] pkg.torch.onnx.name_scopes",
                "value": "['', 'model', 'model.layers.0', 'model.layers.0.input_layernorm', 'mean']"
              },
              {
                "key": "[metadata] pkg.torch.onnx.stack_trace",
                "value": "File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 688, in forward\n    outputs: BaseModelOutputWithPast = self.model(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 453, in forward\n    layer_outputs = decoder_layer(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 305, in forward\n    hidden_states = self.input_layernorm(hidden_states)\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 71, in forward\n    variance = hidden_states.pow(2).mean(-1, keepdim=True)"
              }
            ],
            "incomingEdges": [
              {
                "sourceNodeId": "node_Pow_329",
                "sourceNodeOutputId": "0",
                "targetNodeInputId": "0"
              },
              {
                "sourceNodeId": "node_Constant_869",
                "sourceNodeOutputId": "0",
                "targetNodeInputId": "1"
              }
            ],
            "outputsMetadata": [
              {
                "id": "0",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "mean"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "FLOAT[s34,s16,1]"
                  },
                  {
                    "key": "param_name",
                    "value": "reduced"
                  }
                ]
              }
            ],
            "inputsMetadata": [
              {
                "id": "0",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "pow_1"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "FLOAT[s34,s16,2048]"
                  },
                  {
                    "key": "param_name",
                    "value": "data"
                  }
                ]
              },
              {
                "id": "1",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "val_250"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "INT64[1]"
                  },
                  {
                    "key": "param_name",
                    "value": "axes"
                  }
                ]
              }
            ],
            "style": null,
            "config": null
          },
          {
            "id": "node_Constant_333",
            "label": "Constant",
            "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel",
            "subgraphIds": [],
            "attrs": [
              {
                "key": "__value",
                "value": "9.999999747378752e-06"
              },
              {
                "key": "value",
                "value": "TensorProtoTensor<FLOAT,[]>(array(1.e-05, dtype=float32), name='')"
              },
              {
                "key": "[metadata] namespace",
                "value": ": transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.0: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.0.input_layernorm: transformers.models.llama.modeling_llama.LlamaRMSNorm/add_192: aten.add.Tensor"
              },
              {
                "key": "[metadata] pkg.torch.onnx.class_hierarchy",
                "value": "['transformers.models.llama.modeling_llama.LlamaForCausalLM', 'transformers.models.llama.modeling_llama.LlamaModel', 'transformers.models.llama.modeling_llama.LlamaDecoderLayer', 'transformers.models.llama.modeling_llama.LlamaRMSNorm', 'aten.add.Tensor']"
              },
              {
                "key": "[metadata] pkg.torch.onnx.fx_node",
                "value": "%add_192 : [num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%mean, 1e-05), kwargs = {})"
              },
              {
                "key": "[metadata] pkg.torch.onnx.name_scopes",
                "value": "['', 'model', 'model.layers.0', 'model.layers.0.input_layernorm', 'add_192']"
              },
              {
                "key": "[metadata] pkg.torch.onnx.stack_trace",
                "value": "File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 688, in forward\n    outputs: BaseModelOutputWithPast = self.model(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 453, in forward\n    layer_outputs = decoder_layer(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 305, in forward\n    hidden_states = self.input_layernorm(hidden_states)\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 72, in forward\n    hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)"
              }
            ],
            "incomingEdges": [],
            "outputsMetadata": [
              {
                "id": "0",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "val_251"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "FLOAT[]"
                  },
                  {
                    "key": "param_name",
                    "value": "output"
                  }
                ]
              }
            ],
            "inputsMetadata": [],
            "style": null,
            "config": null
          },
          {
            "id": "node_Add_334",
            "label": "Add",
            "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.0: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.0.input_layernorm: transformers.models.llama.modeling_llama.LlamaRMSNorm/add_192: aten.add.Tensor",
            "subgraphIds": [],
            "attrs": [
              {
                "key": "[metadata] namespace",
                "value": ": transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.0: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.0.input_layernorm: transformers.models.llama.modeling_llama.LlamaRMSNorm/add_192: aten.add.Tensor"
              },
              {
                "key": "[metadata] pkg.torch.onnx.class_hierarchy",
                "value": "['transformers.models.llama.modeling_llama.LlamaForCausalLM', 'transformers.models.llama.modeling_llama.LlamaModel', 'transformers.models.llama.modeling_llama.LlamaDecoderLayer', 'transformers.models.llama.modeling_llama.LlamaRMSNorm', 'aten.add.Tensor']"
              },
              {
                "key": "[metadata] pkg.torch.onnx.fx_node",
                "value": "%add_192 : [num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%mean, 1e-05), kwargs = {})"
              },
              {
                "key": "[metadata] pkg.torch.onnx.name_scopes",
                "value": "['', 'model', 'model.layers.0', 'model.layers.0.input_layernorm', 'add_192']"
              },
              {
                "key": "[metadata] pkg.torch.onnx.stack_trace",
                "value": "File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 688, in forward\n    outputs: BaseModelOutputWithPast = self.model(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 453, in forward\n    layer_outputs = decoder_layer(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 305, in forward\n    hidden_states = self.input_layernorm(hidden_states)\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 72, in forward\n    hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)"
              }
            ],
            "incomingEdges": [
              {
                "sourceNodeId": "node_ReduceMean_332",
                "sourceNodeOutputId": "0",
                "targetNodeInputId": "0"
              },
              {
                "sourceNodeId": "node_Constant_333",
                "sourceNodeOutputId": "0",
                "targetNodeInputId": "1"
              }
            ],
            "outputsMetadata": [
              {
                "id": "0",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "add_192"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "FLOAT[s34,s16,1]"
                  },
                  {
                    "key": "param_name",
                    "value": "C"
                  }
                ]
              }
            ],
            "inputsMetadata": [
              {
                "id": "0",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "mean"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "FLOAT[s34,s16,1]"
                  },
                  {
                    "key": "param_name",
                    "value": "A"
                  }
                ]
              },
              {
                "id": "1",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "val_251"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "FLOAT[]"
                  },
                  {
                    "key": "param_name",
                    "value": "B"
                  }
                ]
              }
            ],
            "style": null,
            "config": null
          },
          {
            "id": "node_Sqrt_335",
            "label": "Sqrt",
            "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.0: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.0.input_layernorm: transformers.models.llama.modeling_llama.LlamaRMSNorm/rsqrt: aten.rsqrt.default",
            "subgraphIds": [],
            "attrs": [
              {
                "key": "[metadata] namespace",
                "value": ": transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.0: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.0.input_layernorm: transformers.models.llama.modeling_llama.LlamaRMSNorm/rsqrt: aten.rsqrt.default"
              },
              {
                "key": "[metadata] pkg.torch.onnx.class_hierarchy",
                "value": "['transformers.models.llama.modeling_llama.LlamaForCausalLM', 'transformers.models.llama.modeling_llama.LlamaModel', 'transformers.models.llama.modeling_llama.LlamaDecoderLayer', 'transformers.models.llama.modeling_llama.LlamaRMSNorm', 'aten.rsqrt.default']"
              },
              {
                "key": "[metadata] pkg.torch.onnx.fx_node",
                "value": "%rsqrt : [num_users=1] = call_function[target=torch.ops.aten.rsqrt.default](args = (%add_192,), kwargs = {})"
              },
              {
                "key": "[metadata] pkg.torch.onnx.name_scopes",
                "value": "['', 'model', 'model.layers.0', 'model.layers.0.input_layernorm', 'rsqrt']"
              },
              {
                "key": "[metadata] pkg.torch.onnx.stack_trace",
                "value": "File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 688, in forward\n    outputs: BaseModelOutputWithPast = self.model(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 453, in forward\n    layer_outputs = decoder_layer(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 305, in forward\n    hidden_states = self.input_layernorm(hidden_states)\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 72, in forward\n    hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)"
              }
            ],
            "incomingEdges": [
              {
                "sourceNodeId": "node_Add_334",
                "sourceNodeOutputId": "0",
                "targetNodeInputId": "0"
              }
            ],
            "outputsMetadata": [
              {
                "id": "0",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "val_252"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "FLOAT[s34,s16,1]"
                  },
                  {
                    "key": "param_name",
                    "value": "Y"
                  }
                ]
              }
            ],
            "inputsMetadata": [
              {
                "id": "0",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "add_192"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "FLOAT[s34,s16,1]"
                  },
                  {
                    "key": "param_name",
                    "value": "X"
                  }
                ]
              }
            ],
            "style": null,
            "config": null
          },
          {
            "id": "node_Reciprocal_336",
            "label": "Reciprocal",
            "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.0: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.0.input_layernorm: transformers.models.llama.modeling_llama.LlamaRMSNorm/rsqrt: aten.rsqrt.default",
            "subgraphIds": [],
            "attrs": [
              {
                "key": "[metadata] namespace",
                "value": ": transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.0: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.0.input_layernorm: transformers.models.llama.modeling_llama.LlamaRMSNorm/rsqrt: aten.rsqrt.default"
              },
              {
                "key": "[metadata] pkg.torch.onnx.class_hierarchy",
                "value": "['transformers.models.llama.modeling_llama.LlamaForCausalLM', 'transformers.models.llama.modeling_llama.LlamaModel', 'transformers.models.llama.modeling_llama.LlamaDecoderLayer', 'transformers.models.llama.modeling_llama.LlamaRMSNorm', 'aten.rsqrt.default']"
              },
              {
                "key": "[metadata] pkg.torch.onnx.fx_node",
                "value": "%rsqrt : [num_users=1] = call_function[target=torch.ops.aten.rsqrt.default](args = (%add_192,), kwargs = {})"
              },
              {
                "key": "[metadata] pkg.torch.onnx.name_scopes",
                "value": "['', 'model', 'model.layers.0', 'model.layers.0.input_layernorm', 'rsqrt']"
              },
              {
                "key": "[metadata] pkg.torch.onnx.stack_trace",
                "value": "File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 688, in forward\n    outputs: BaseModelOutputWithPast = self.model(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 453, in forward\n    layer_outputs = decoder_layer(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 305, in forward\n    hidden_states = self.input_layernorm(hidden_states)\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 72, in forward\n    hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)"
              }
            ],
            "incomingEdges": [
              {
                "sourceNodeId": "node_Sqrt_335",
                "sourceNodeOutputId": "0",
                "targetNodeInputId": "0"
              }
            ],
            "outputsMetadata": [
              {
                "id": "0",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "rsqrt"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "FLOAT[s34,s16,1]"
                  },
                  {
                    "key": "param_name",
                    "value": "Y"
                  }
                ]
              }
            ],
            "inputsMetadata": [
              {
                "id": "0",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "val_252"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "FLOAT[s34,s16,1]"
                  },
                  {
                    "key": "param_name",
                    "value": "X"
                  }
                ]
              }
            ],
            "style": null,
            "config": null
          },
          {
            "id": "node_Mul_337",
            "label": "Mul",
            "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.0: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.0.input_layernorm: transformers.models.llama.modeling_llama.LlamaRMSNorm/mul_169: aten.mul.Tensor",
            "subgraphIds": [],
            "attrs": [
              {
                "key": "[metadata] namespace",
                "value": ": transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.0: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.0.input_layernorm: transformers.models.llama.modeling_llama.LlamaRMSNorm/mul_169: aten.mul.Tensor"
              },
              {
                "key": "[metadata] pkg.torch.onnx.class_hierarchy",
                "value": "['transformers.models.llama.modeling_llama.LlamaForCausalLM', 'transformers.models.llama.modeling_llama.LlamaModel', 'transformers.models.llama.modeling_llama.LlamaDecoderLayer', 'transformers.models.llama.modeling_llama.LlamaRMSNorm', 'aten.mul.Tensor']"
              },
              {
                "key": "[metadata] pkg.torch.onnx.fx_node",
                "value": "%mul_169 : [num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%embedding, %rsqrt), kwargs = {})"
              },
              {
                "key": "[metadata] pkg.torch.onnx.name_scopes",
                "value": "['', 'model', 'model.layers.0', 'model.layers.0.input_layernorm', 'mul_169']"
              },
              {
                "key": "[metadata] pkg.torch.onnx.stack_trace",
                "value": "File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 688, in forward\n    outputs: BaseModelOutputWithPast = self.model(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 453, in forward\n    layer_outputs = decoder_layer(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 305, in forward\n    hidden_states = self.input_layernorm(hidden_states)\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 72, in forward\n    hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)"
              }
            ],
            "incomingEdges": [
              {
                "sourceNodeId": "node_Gather_20",
                "sourceNodeOutputId": "0",
                "targetNodeInputId": "0"
              },
              {
                "sourceNodeId": "node_Reciprocal_336",
                "sourceNodeOutputId": "0",
                "targetNodeInputId": "1"
              }
            ],
            "outputsMetadata": [
              {
                "id": "0",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "mul_169"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "FLOAT[s34,s16,2048]"
                  },
                  {
                    "key": "param_name",
                    "value": "C"
                  }
                ]
              }
            ],
            "inputsMetadata": [
              {
                "id": "0",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "embedding"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "FLOAT[s34,s16,2048]"
                  },
                  {
                    "key": "param_name",
                    "value": "A"
                  }
                ]
              },
              {
                "id": "1",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "rsqrt"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "FLOAT[s34,s16,1]"
                  },
                  {
                    "key": "param_name",
                    "value": "B"
                  }
                ]
              }
            ],
            "style": null,
            "config": null
          },
          {
            "id": "node_Mul_338",
            "label": "Mul",
            "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.0: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.0.input_layernorm: transformers.models.llama.modeling_llama.LlamaRMSNorm/mul_173: aten.mul.Tensor",
            "subgraphIds": [],
            "attrs": [
              {
                "key": "[metadata] namespace",
                "value": ": transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.0: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.0.input_layernorm: transformers.models.llama.modeling_llama.LlamaRMSNorm/mul_173: aten.mul.Tensor"
              },
              {
                "key": "[metadata] pkg.torch.onnx.class_hierarchy",
                "value": "['transformers.models.llama.modeling_llama.LlamaForCausalLM', 'transformers.models.llama.modeling_llama.LlamaModel', 'transformers.models.llama.modeling_llama.LlamaDecoderLayer', 'transformers.models.llama.modeling_llama.LlamaRMSNorm', 'aten.mul.Tensor']"
              },
              {
                "key": "[metadata] pkg.torch.onnx.fx_node",
                "value": "%mul_173 : [num_users=3] = call_function[target=torch.ops.aten.mul.Tensor](args = (%p_model_layers_0_input_layernorm_weight, %mul_169), kwargs = {})"
              },
              {
                "key": "[metadata] pkg.torch.onnx.name_scopes",
                "value": "['', 'model', 'model.layers.0', 'model.layers.0.input_layernorm', 'mul_173']"
              },
              {
                "key": "[metadata] pkg.torch.onnx.stack_trace",
                "value": "File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 688, in forward\n    outputs: BaseModelOutputWithPast = self.model(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 453, in forward\n    layer_outputs = decoder_layer(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 305, in forward\n    hidden_states = self.input_layernorm(hidden_states)\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 73, in forward\n    return self.weight * hidden_states.to(input_dtype)"
              }
            ],
            "incomingEdges": [
              {
                "sourceNodeId": "[value] model.layers.0.input_layernorm.weight",
                "sourceNodeOutputId": "0",
                "targetNodeInputId": "0"
              },
              {
                "sourceNodeId": "node_Mul_337",
                "sourceNodeOutputId": "0",
                "targetNodeInputId": "1"
              }
            ],
            "outputsMetadata": [
              {
                "id": "0",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "mul_173"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "FLOAT[s34,s16,2048]"
                  },
                  {
                    "key": "param_name",
                    "value": "C"
                  }
                ]
              }
            ],
            "inputsMetadata": [
              {
                "id": "0",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "model.layers.0.input_layernorm.weight"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "FLOAT[2048]"
                  },
                  {
                    "key": "param_name",
                    "value": "A"
                  }
                ]
              },
              {
                "id": "1",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "mul_169"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "FLOAT[s34,s16,2048]"
                  },
                  {
                    "key": "param_name",
                    "value": "B"
                  }
                ]
              }
            ],
            "style": null,
            "config": null
          },
          {
            "id": "node_Transpose_339",
            "label": "Transpose",
            "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.0: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.0.self_attn: transformers.models.llama.modeling_llama.LlamaAttention/model.layers.0.self_attn.q_proj: torch.nn.modules.linear.Linear/linear: aten.linear.default",
            "subgraphIds": [],
            "attrs": [
              {
                "key": "perm",
                "value": "[1, 0]"
              },
              {
                "key": "[metadata] namespace",
                "value": ": transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.0: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.0.self_attn: transformers.models.llama.modeling_llama.LlamaAttention/model.layers.0.self_attn.q_proj: torch.nn.modules.linear.Linear/linear: aten.linear.default"
              },
              {
                "key": "[metadata] pkg.torch.onnx.class_hierarchy",
                "value": "['transformers.models.llama.modeling_llama.LlamaForCausalLM', 'transformers.models.llama.modeling_llama.LlamaModel', 'transformers.models.llama.modeling_llama.LlamaDecoderLayer', 'transformers.models.llama.modeling_llama.LlamaAttention', 'torch.nn.modules.linear.Linear', 'aten.linear.default']"
              },
              {
                "key": "[metadata] pkg.torch.onnx.fx_node",
                "value": "%linear : [num_users=1] = call_function[target=torch.ops.aten.linear.default](args = (%mul_173, %p_model_layers_0_self_attn_q_proj_weight), kwargs = {})"
              },
              {
                "key": "[metadata] pkg.torch.onnx.name_scopes",
                "value": "['', 'model', 'model.layers.0', 'model.layers.0.self_attn', 'model.layers.0.self_attn.q_proj', 'linear']"
              },
              {
                "key": "[metadata] pkg.torch.onnx.stack_trace",
                "value": "File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 688, in forward\n    outputs: BaseModelOutputWithPast = self.model(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 453, in forward\n    layer_outputs = decoder_layer(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 308, in forward\n    hidden_states, self_attn_weights = self.self_attn(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 242, in forward\n    query_states = self.q_proj(hidden_states).view(hidden_shape).transpose(1, 2)\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/torch/nn/modules/linear.py\", line 125, in forward\n    return F.linear(input, self.weight, self.bias)"
              }
            ],
            "incomingEdges": [
              {
                "sourceNodeId": "[value] model.layers.0.self_attn.q_proj.weight",
                "sourceNodeOutputId": "0",
                "targetNodeInputId": "0"
              }
            ],
            "outputsMetadata": [
              {
                "id": "0",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "val_253"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "FLOAT[2048,2048]"
                  },
                  {
                    "key": "param_name",
                    "value": "transposed"
                  }
                ]
              }
            ],
            "inputsMetadata": [
              {
                "id": "0",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "model.layers.0.self_attn.q_proj.weight"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "FLOAT[2048,2048]"
                  },
                  {
                    "key": "param_name",
                    "value": "data"
                  }
                ]
              }
            ],
            "style": null,
            "config": null
          },
          {
            "id": "node_MatMul_340",
            "label": "MatMul",
            "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.0: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.0.self_attn: transformers.models.llama.modeling_llama.LlamaAttention/model.layers.0.self_attn.q_proj: torch.nn.modules.linear.Linear/linear: aten.linear.default",
            "subgraphIds": [],
            "attrs": [
              {
                "key": "[metadata] namespace",
                "value": ": transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.0: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.0.self_attn: transformers.models.llama.modeling_llama.LlamaAttention/model.layers.0.self_attn.q_proj: torch.nn.modules.linear.Linear/linear: aten.linear.default"
              },
              {
                "key": "[metadata] pkg.torch.onnx.class_hierarchy",
                "value": "['transformers.models.llama.modeling_llama.LlamaForCausalLM', 'transformers.models.llama.modeling_llama.LlamaModel', 'transformers.models.llama.modeling_llama.LlamaDecoderLayer', 'transformers.models.llama.modeling_llama.LlamaAttention', 'torch.nn.modules.linear.Linear', 'aten.linear.default']"
              },
              {
                "key": "[metadata] pkg.torch.onnx.fx_node",
                "value": "%linear : [num_users=1] = call_function[target=torch.ops.aten.linear.default](args = (%mul_173, %p_model_layers_0_self_attn_q_proj_weight), kwargs = {})"
              },
              {
                "key": "[metadata] pkg.torch.onnx.name_scopes",
                "value": "['', 'model', 'model.layers.0', 'model.layers.0.self_attn', 'model.layers.0.self_attn.q_proj', 'linear']"
              },
              {
                "key": "[metadata] pkg.torch.onnx.stack_trace",
                "value": "File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 688, in forward\n    outputs: BaseModelOutputWithPast = self.model(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 453, in forward\n    layer_outputs = decoder_layer(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 308, in forward\n    hidden_states, self_attn_weights = self.self_attn(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 242, in forward\n    query_states = self.q_proj(hidden_states).view(hidden_shape).transpose(1, 2)\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/torch/nn/modules/linear.py\", line 125, in forward\n    return F.linear(input, self.weight, self.bias)"
              }
            ],
            "incomingEdges": [
              {
                "sourceNodeId": "node_Mul_338",
                "sourceNodeOutputId": "0",
                "targetNodeInputId": "0"
              },
              {
                "sourceNodeId": "node_Transpose_339",
                "sourceNodeOutputId": "0",
                "targetNodeInputId": "1"
              }
            ],
            "outputsMetadata": [
              {
                "id": "0",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "linear"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "FLOAT[s34,s16,2048]"
                  },
                  {
                    "key": "param_name",
                    "value": "Y"
                  }
                ]
              }
            ],
            "inputsMetadata": [
              {
                "id": "0",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "mul_173"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "FLOAT[s34,s16,2048]"
                  },
                  {
                    "key": "param_name",
                    "value": "A"
                  }
                ]
              },
              {
                "id": "1",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "val_253"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "FLOAT[2048,2048]"
                  },
                  {
                    "key": "param_name",
                    "value": "B"
                  }
                ]
              }
            ],
            "style": null,
            "config": null
          },
          {
            "id": "node_Constant_345",
            "label": "Constant",
            "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel",
            "subgraphIds": [],
            "attrs": [
              {
                "key": "__value",
                "value": "[64]"
              },
              {
                "key": "value",
                "value": "TensorProtoTensor<INT64,[1]>(array([64]), name='')"
              },
              {
                "key": "[metadata] namespace",
                "value": ": transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.0: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.0.self_attn: transformers.models.llama.modeling_llama.LlamaAttention/view_1: aten.view.default"
              },
              {
                "key": "[metadata] pkg.torch.onnx.class_hierarchy",
                "value": "['transformers.models.llama.modeling_llama.LlamaForCausalLM', 'transformers.models.llama.modeling_llama.LlamaModel', 'transformers.models.llama.modeling_llama.LlamaDecoderLayer', 'transformers.models.llama.modeling_llama.LlamaAttention', 'aten.view.default']"
              },
              {
                "key": "[metadata] pkg.torch.onnx.fx_node",
                "value": "%view_1 : [num_users=1] = call_function[target=torch.ops.aten.view.default](args = (%linear, [%sym_size_int_68, %sym_size_int_61, -1, 64]), kwargs = {})"
              },
              {
                "key": "[metadata] pkg.torch.onnx.name_scopes",
                "value": "['', 'model', 'model.layers.0', 'model.layers.0.self_attn', 'view_1']"
              },
              {
                "key": "[metadata] pkg.torch.onnx.stack_trace",
                "value": "File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 688, in forward\n    outputs: BaseModelOutputWithPast = self.model(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 453, in forward\n    layer_outputs = decoder_layer(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 308, in forward\n    hidden_states, self_attn_weights = self.self_attn(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 242, in forward\n    query_states = self.q_proj(hidden_states).view(hidden_shape).transpose(1, 2)"
              }
            ],
            "incomingEdges": [],
            "outputsMetadata": [
              {
                "id": "0",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "val_258"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "INT64[1]"
                  },
                  {
                    "key": "param_name",
                    "value": "output"
                  }
                ]
              }
            ],
            "inputsMetadata": [],
            "style": null,
            "config": null
          },
          {
            "id": "node_Concat_346",
            "label": "Concat",
            "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.0: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.0.self_attn: transformers.models.llama.modeling_llama.LlamaAttention/view_1: aten.view.default",
            "subgraphIds": [],
            "attrs": [
              {
                "key": "axis",
                "value": "0"
              },
              {
                "key": "[metadata] namespace",
                "value": ": transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.0: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.0.self_attn: transformers.models.llama.modeling_llama.LlamaAttention/view_1: aten.view.default"
              },
              {
                "key": "[metadata] pkg.torch.onnx.class_hierarchy",
                "value": "['transformers.models.llama.modeling_llama.LlamaForCausalLM', 'transformers.models.llama.modeling_llama.LlamaModel', 'transformers.models.llama.modeling_llama.LlamaDecoderLayer', 'transformers.models.llama.modeling_llama.LlamaAttention', 'aten.view.default']"
              },
              {
                "key": "[metadata] pkg.torch.onnx.fx_node",
                "value": "%view_1 : [num_users=1] = call_function[target=torch.ops.aten.view.default](args = (%linear, [%sym_size_int_68, %sym_size_int_61, -1, 64]), kwargs = {})"
              },
              {
                "key": "[metadata] pkg.torch.onnx.name_scopes",
                "value": "['', 'model', 'model.layers.0', 'model.layers.0.self_attn', 'view_1']"
              },
              {
                "key": "[metadata] pkg.torch.onnx.stack_trace",
                "value": "File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 688, in forward\n    outputs: BaseModelOutputWithPast = self.model(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 453, in forward\n    layer_outputs = decoder_layer(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 308, in forward\n    hidden_states, self_attn_weights = self.self_attn(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 242, in forward\n    query_states = self.q_proj(hidden_states).view(hidden_shape).transpose(1, 2)"
              }
            ],
            "incomingEdges": [
              {
                "sourceNodeId": "node_Shape_12",
                "sourceNodeOutputId": "0",
                "targetNodeInputId": "0"
              },
              {
                "sourceNodeId": "node_Shape_2",
                "sourceNodeOutputId": "0",
                "targetNodeInputId": "1"
              },
              {
                "sourceNodeId": "node_Constant_90",
                "sourceNodeOutputId": "0",
                "targetNodeInputId": "2"
              },
              {
                "sourceNodeId": "node_Constant_345",
                "sourceNodeOutputId": "0",
                "targetNodeInputId": "3"
              }
            ],
            "outputsMetadata": [
              {
                "id": "0",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "val_259"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "INT64[4]"
                  },
                  {
                    "key": "param_name",
                    "value": "concat_result"
                  }
                ]
              }
            ],
            "inputsMetadata": [
              {
                "id": "0",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "val_6"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "INT64[1]"
                  },
                  {
                    "key": "param_name",
                    "value": "inputs"
                  }
                ]
              },
              {
                "id": "1",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "val_1"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "INT64[1]"
                  },
                  {
                    "key": "param_name",
                    "value": "inputs"
                  }
                ]
              },
              {
                "id": "2",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "val_53"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "INT64[1]"
                  },
                  {
                    "key": "param_name",
                    "value": "inputs"
                  }
                ]
              },
              {
                "id": "3",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "val_258"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "INT64[1]"
                  },
                  {
                    "key": "param_name",
                    "value": "inputs"
                  }
                ]
              }
            ],
            "style": null,
            "config": null
          },
          {
            "id": "node_Reshape_348",
            "label": "Reshape",
            "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.0: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.0.self_attn: transformers.models.llama.modeling_llama.LlamaAttention/view_1: aten.view.default",
            "subgraphIds": [],
            "attrs": [
              {
                "key": "allowzero",
                "value": "1"
              },
              {
                "key": "[metadata] namespace",
                "value": ": transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.0: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.0.self_attn: transformers.models.llama.modeling_llama.LlamaAttention/view_1: aten.view.default"
              },
              {
                "key": "[metadata] pkg.torch.onnx.class_hierarchy",
                "value": "['transformers.models.llama.modeling_llama.LlamaForCausalLM', 'transformers.models.llama.modeling_llama.LlamaModel', 'transformers.models.llama.modeling_llama.LlamaDecoderLayer', 'transformers.models.llama.modeling_llama.LlamaAttention', 'aten.view.default']"
              },
              {
                "key": "[metadata] pkg.torch.onnx.fx_node",
                "value": "%view_1 : [num_users=1] = call_function[target=torch.ops.aten.view.default](args = (%linear, [%sym_size_int_68, %sym_size_int_61, -1, 64]), kwargs = {})"
              },
              {
                "key": "[metadata] pkg.torch.onnx.name_scopes",
                "value": "['', 'model', 'model.layers.0', 'model.layers.0.self_attn', 'view_1']"
              },
              {
                "key": "[metadata] pkg.torch.onnx.stack_trace",
                "value": "File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 688, in forward\n    outputs: BaseModelOutputWithPast = self.model(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 453, in forward\n    layer_outputs = decoder_layer(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 308, in forward\n    hidden_states, self_attn_weights = self.self_attn(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 242, in forward\n    query_states = self.q_proj(hidden_states).view(hidden_shape).transpose(1, 2)"
              }
            ],
            "incomingEdges": [
              {
                "sourceNodeId": "node_MatMul_340",
                "sourceNodeOutputId": "0",
                "targetNodeInputId": "0"
              },
              {
                "sourceNodeId": "node_Concat_346",
                "sourceNodeOutputId": "0",
                "targetNodeInputId": "1"
              }
            ],
            "outputsMetadata": [
              {
                "id": "0",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "view_1"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "FLOAT[s34,s16,32,64]"
                  },
                  {
                    "key": "param_name",
                    "value": "reshaped"
                  }
                ]
              }
            ],
            "inputsMetadata": [
              {
                "id": "0",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "linear"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "FLOAT[s34,s16,2048]"
                  },
                  {
                    "key": "param_name",
                    "value": "data"
                  }
                ]
              },
              {
                "id": "1",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "val_259"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "INT64[4]"
                  },
                  {
                    "key": "param_name",
                    "value": "shape"
                  }
                ]
              }
            ],
            "style": null,
            "config": null
          },
          {
            "id": "node_Transpose_349",
            "label": "Transpose",
            "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.0: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.0.self_attn: transformers.models.llama.modeling_llama.LlamaAttention/transpose_1: aten.transpose.int",
            "subgraphIds": [],
            "attrs": [
              {
                "key": "perm",
                "value": "[0, 2, 1, 3]"
              },
              {
                "key": "[metadata] namespace",
                "value": ": transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.0: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.0.self_attn: transformers.models.llama.modeling_llama.LlamaAttention/transpose_1: aten.transpose.int"
              },
              {
                "key": "[metadata] pkg.torch.onnx.class_hierarchy",
                "value": "['transformers.models.llama.modeling_llama.LlamaForCausalLM', 'transformers.models.llama.modeling_llama.LlamaModel', 'transformers.models.llama.modeling_llama.LlamaDecoderLayer', 'transformers.models.llama.modeling_llama.LlamaAttention', 'aten.transpose.int']"
              },
              {
                "key": "[metadata] pkg.torch.onnx.fx_node",
                "value": "%transpose_1 : [num_users=3] = call_function[target=torch.ops.aten.transpose.int](args = (%view_1, 1, 2), kwargs = {})"
              },
              {
                "key": "[metadata] pkg.torch.onnx.name_scopes",
                "value": "['', 'model', 'model.layers.0', 'model.layers.0.self_attn', 'transpose_1']"
              },
              {
                "key": "[metadata] pkg.torch.onnx.stack_trace",
                "value": "File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 688, in forward\n    outputs: BaseModelOutputWithPast = self.model(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 453, in forward\n    layer_outputs = decoder_layer(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 308, in forward\n    hidden_states, self_attn_weights = self.self_attn(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 242, in forward\n    query_states = self.q_proj(hidden_states).view(hidden_shape).transpose(1, 2)"
              }
            ],
            "incomingEdges": [
              {
                "sourceNodeId": "node_Reshape_348",
                "sourceNodeOutputId": "0",
                "targetNodeInputId": "0"
              }
            ],
            "outputsMetadata": [
              {
                "id": "0",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "transpose_1"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "FLOAT[s34,32,s16,64]"
                  },
                  {
                    "key": "param_name",
                    "value": "transposed"
                  }
                ]
              }
            ],
            "inputsMetadata": [
              {
                "id": "0",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "view_1"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "FLOAT[s34,s16,32,64]"
                  },
                  {
                    "key": "param_name",
                    "value": "data"
                  }
                ]
              }
            ],
            "style": null,
            "config": null
          },
          {
            "id": "node_Transpose_350",
            "label": "Transpose",
            "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.0: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.0.self_attn: transformers.models.llama.modeling_llama.LlamaAttention/model.layers.0.self_attn.k_proj: torch.nn.modules.linear.Linear/linear_1: aten.linear.default",
            "subgraphIds": [],
            "attrs": [
              {
                "key": "perm",
                "value": "[1, 0]"
              },
              {
                "key": "[metadata] namespace",
                "value": ": transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.0: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.0.self_attn: transformers.models.llama.modeling_llama.LlamaAttention/model.layers.0.self_attn.k_proj: torch.nn.modules.linear.Linear/linear_1: aten.linear.default"
              },
              {
                "key": "[metadata] pkg.torch.onnx.class_hierarchy",
                "value": "['transformers.models.llama.modeling_llama.LlamaForCausalLM', 'transformers.models.llama.modeling_llama.LlamaModel', 'transformers.models.llama.modeling_llama.LlamaDecoderLayer', 'transformers.models.llama.modeling_llama.LlamaAttention', 'torch.nn.modules.linear.Linear', 'aten.linear.default']"
              },
              {
                "key": "[metadata] pkg.torch.onnx.fx_node",
                "value": "%linear_1 : [num_users=1] = call_function[target=torch.ops.aten.linear.default](args = (%mul_173, %p_model_layers_0_self_attn_k_proj_weight), kwargs = {})"
              },
              {
                "key": "[metadata] pkg.torch.onnx.name_scopes",
                "value": "['', 'model', 'model.layers.0', 'model.layers.0.self_attn', 'model.layers.0.self_attn.k_proj', 'linear_1']"
              },
              {
                "key": "[metadata] pkg.torch.onnx.stack_trace",
                "value": "File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 688, in forward\n    outputs: BaseModelOutputWithPast = self.model(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 453, in forward\n    layer_outputs = decoder_layer(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 308, in forward\n    hidden_states, self_attn_weights = self.self_attn(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 243, in forward\n    key_states = self.k_proj(hidden_states).view(hidden_shape).transpose(1, 2)\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/torch/nn/modules/linear.py\", line 125, in forward\n    return F.linear(input, self.weight, self.bias)"
              }
            ],
            "incomingEdges": [
              {
                "sourceNodeId": "[value] model.layers.0.self_attn.k_proj.weight",
                "sourceNodeOutputId": "0",
                "targetNodeInputId": "0"
              }
            ],
            "outputsMetadata": [
              {
                "id": "0",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "val_261"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "FLOAT[2048,2048]"
                  },
                  {
                    "key": "param_name",
                    "value": "transposed"
                  }
                ]
              }
            ],
            "inputsMetadata": [
              {
                "id": "0",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "model.layers.0.self_attn.k_proj.weight"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "FLOAT[2048,2048]"
                  },
                  {
                    "key": "param_name",
                    "value": "data"
                  }
                ]
              }
            ],
            "style": null,
            "config": null
          },
          {
            "id": "node_MatMul_351",
            "label": "MatMul",
            "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.0: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.0.self_attn: transformers.models.llama.modeling_llama.LlamaAttention/model.layers.0.self_attn.k_proj: torch.nn.modules.linear.Linear/linear_1: aten.linear.default",
            "subgraphIds": [],
            "attrs": [
              {
                "key": "[metadata] namespace",
                "value": ": transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.0: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.0.self_attn: transformers.models.llama.modeling_llama.LlamaAttention/model.layers.0.self_attn.k_proj: torch.nn.modules.linear.Linear/linear_1: aten.linear.default"
              },
              {
                "key": "[metadata] pkg.torch.onnx.class_hierarchy",
                "value": "['transformers.models.llama.modeling_llama.LlamaForCausalLM', 'transformers.models.llama.modeling_llama.LlamaModel', 'transformers.models.llama.modeling_llama.LlamaDecoderLayer', 'transformers.models.llama.modeling_llama.LlamaAttention', 'torch.nn.modules.linear.Linear', 'aten.linear.default']"
              },
              {
                "key": "[metadata] pkg.torch.onnx.fx_node",
                "value": "%linear_1 : [num_users=1] = call_function[target=torch.ops.aten.linear.default](args = (%mul_173, %p_model_layers_0_self_attn_k_proj_weight), kwargs = {})"
              },
              {
                "key": "[metadata] pkg.torch.onnx.name_scopes",
                "value": "['', 'model', 'model.layers.0', 'model.layers.0.self_attn', 'model.layers.0.self_attn.k_proj', 'linear_1']"
              },
              {
                "key": "[metadata] pkg.torch.onnx.stack_trace",
                "value": "File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 688, in forward\n    outputs: BaseModelOutputWithPast = self.model(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 453, in forward\n    layer_outputs = decoder_layer(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 308, in forward\n    hidden_states, self_attn_weights = self.self_attn(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 243, in forward\n    key_states = self.k_proj(hidden_states).view(hidden_shape).transpose(1, 2)\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/torch/nn/modules/linear.py\", line 125, in forward\n    return F.linear(input, self.weight, self.bias)"
              }
            ],
            "incomingEdges": [
              {
                "sourceNodeId": "node_Mul_338",
                "sourceNodeOutputId": "0",
                "targetNodeInputId": "0"
              },
              {
                "sourceNodeId": "node_Transpose_350",
                "sourceNodeOutputId": "0",
                "targetNodeInputId": "1"
              }
            ],
            "outputsMetadata": [
              {
                "id": "0",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "linear_1"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "FLOAT[s34,s16,2048]"
                  },
                  {
                    "key": "param_name",
                    "value": "Y"
                  }
                ]
              }
            ],
            "inputsMetadata": [
              {
                "id": "0",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "mul_173"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "FLOAT[s34,s16,2048]"
                  },
                  {
                    "key": "param_name",
                    "value": "A"
                  }
                ]
              },
              {
                "id": "1",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "val_261"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "FLOAT[2048,2048]"
                  },
                  {
                    "key": "param_name",
                    "value": "B"
                  }
                ]
              }
            ],
            "style": null,
            "config": null
          },
          {
            "id": "node_Concat_356",
            "label": "Concat",
            "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.0: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.0.self_attn: transformers.models.llama.modeling_llama.LlamaAttention/view_2: aten.view.default",
            "subgraphIds": [],
            "attrs": [
              {
                "key": "axis",
                "value": "0"
              },
              {
                "key": "[metadata] namespace",
                "value": ": transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.0: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.0.self_attn: transformers.models.llama.modeling_llama.LlamaAttention/view_2: aten.view.default"
              },
              {
                "key": "[metadata] pkg.torch.onnx.class_hierarchy",
                "value": "['transformers.models.llama.modeling_llama.LlamaForCausalLM', 'transformers.models.llama.modeling_llama.LlamaModel', 'transformers.models.llama.modeling_llama.LlamaDecoderLayer', 'transformers.models.llama.modeling_llama.LlamaAttention', 'aten.view.default']"
              },
              {
                "key": "[metadata] pkg.torch.onnx.fx_node",
                "value": "%view_2 : [num_users=1] = call_function[target=torch.ops.aten.view.default](args = (%linear_1, [%sym_size_int_68, %sym_size_int_61, -1, 64]), kwargs = {})"
              },
              {
                "key": "[metadata] pkg.torch.onnx.name_scopes",
                "value": "['', 'model', 'model.layers.0', 'model.layers.0.self_attn', 'view_2']"
              },
              {
                "key": "[metadata] pkg.torch.onnx.stack_trace",
                "value": "File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 688, in forward\n    outputs: BaseModelOutputWithPast = self.model(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 453, in forward\n    layer_outputs = decoder_layer(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 308, in forward\n    hidden_states, self_attn_weights = self.self_attn(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 243, in forward\n    key_states = self.k_proj(hidden_states).view(hidden_shape).transpose(1, 2)"
              }
            ],
            "incomingEdges": [
              {
                "sourceNodeId": "node_Shape_12",
                "sourceNodeOutputId": "0",
                "targetNodeInputId": "0"
              },
              {
                "sourceNodeId": "node_Shape_2",
                "sourceNodeOutputId": "0",
                "targetNodeInputId": "1"
              },
              {
                "sourceNodeId": "node_Constant_90",
                "sourceNodeOutputId": "0",
                "targetNodeInputId": "2"
              },
              {
                "sourceNodeId": "node_Constant_345",
                "sourceNodeOutputId": "0",
                "targetNodeInputId": "3"
              }
            ],
            "outputsMetadata": [
              {
                "id": "0",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "val_266"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "INT64[4]"
                  },
                  {
                    "key": "param_name",
                    "value": "concat_result"
                  }
                ]
              }
            ],
            "inputsMetadata": [
              {
                "id": "0",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "val_6"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "INT64[1]"
                  },
                  {
                    "key": "param_name",
                    "value": "inputs"
                  }
                ]
              },
              {
                "id": "1",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "val_1"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "INT64[1]"
                  },
                  {
                    "key": "param_name",
                    "value": "inputs"
                  }
                ]
              },
              {
                "id": "2",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "val_53"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "INT64[1]"
                  },
                  {
                    "key": "param_name",
                    "value": "inputs"
                  }
                ]
              },
              {
                "id": "3",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "val_258"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "INT64[1]"
                  },
                  {
                    "key": "param_name",
                    "value": "inputs"
                  }
                ]
              }
            ],
            "style": null,
            "config": null
          },
          {
            "id": "node_Reshape_358",
            "label": "Reshape",
            "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.0: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.0.self_attn: transformers.models.llama.modeling_llama.LlamaAttention/view_2: aten.view.default",
            "subgraphIds": [],
            "attrs": [
              {
                "key": "allowzero",
                "value": "1"
              },
              {
                "key": "[metadata] namespace",
                "value": ": transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.0: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.0.self_attn: transformers.models.llama.modeling_llama.LlamaAttention/view_2: aten.view.default"
              },
              {
                "key": "[metadata] pkg.torch.onnx.class_hierarchy",
                "value": "['transformers.models.llama.modeling_llama.LlamaForCausalLM', 'transformers.models.llama.modeling_llama.LlamaModel', 'transformers.models.llama.modeling_llama.LlamaDecoderLayer', 'transformers.models.llama.modeling_llama.LlamaAttention', 'aten.view.default']"
              },
              {
                "key": "[metadata] pkg.torch.onnx.fx_node",
                "value": "%view_2 : [num_users=1] = call_function[target=torch.ops.aten.view.default](args = (%linear_1, [%sym_size_int_68, %sym_size_int_61, -1, 64]), kwargs = {})"
              },
              {
                "key": "[metadata] pkg.torch.onnx.name_scopes",
                "value": "['', 'model', 'model.layers.0', 'model.layers.0.self_attn', 'view_2']"
              },
              {
                "key": "[metadata] pkg.torch.onnx.stack_trace",
                "value": "File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 688, in forward\n    outputs: BaseModelOutputWithPast = self.model(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 453, in forward\n    layer_outputs = decoder_layer(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 308, in forward\n    hidden_states, self_attn_weights = self.self_attn(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 243, in forward\n    key_states = self.k_proj(hidden_states).view(hidden_shape).transpose(1, 2)"
              }
            ],
            "incomingEdges": [
              {
                "sourceNodeId": "node_MatMul_351",
                "sourceNodeOutputId": "0",
                "targetNodeInputId": "0"
              },
              {
                "sourceNodeId": "node_Concat_356",
                "sourceNodeOutputId": "0",
                "targetNodeInputId": "1"
              }
            ],
            "outputsMetadata": [
              {
                "id": "0",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "view_2"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "FLOAT[s34,s16,32,64]"
                  },
                  {
                    "key": "param_name",
                    "value": "reshaped"
                  }
                ]
              }
            ],
            "inputsMetadata": [
              {
                "id": "0",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "linear_1"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "FLOAT[s34,s16,2048]"
                  },
                  {
                    "key": "param_name",
                    "value": "data"
                  }
                ]
              },
              {
                "id": "1",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "val_266"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "INT64[4]"
                  },
                  {
                    "key": "param_name",
                    "value": "shape"
                  }
                ]
              }
            ],
            "style": null,
            "config": null
          },
          {
            "id": "node_Transpose_359",
            "label": "Transpose",
            "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.0: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.0.self_attn: transformers.models.llama.modeling_llama.LlamaAttention/transpose_2: aten.transpose.int",
            "subgraphIds": [],
            "attrs": [
              {
                "key": "perm",
                "value": "[0, 2, 1, 3]"
              },
              {
                "key": "[metadata] namespace",
                "value": ": transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.0: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.0.self_attn: transformers.models.llama.modeling_llama.LlamaAttention/transpose_2: aten.transpose.int"
              },
              {
                "key": "[metadata] pkg.torch.onnx.class_hierarchy",
                "value": "['transformers.models.llama.modeling_llama.LlamaForCausalLM', 'transformers.models.llama.modeling_llama.LlamaModel', 'transformers.models.llama.modeling_llama.LlamaDecoderLayer', 'transformers.models.llama.modeling_llama.LlamaAttention', 'aten.transpose.int']"
              },
              {
                "key": "[metadata] pkg.torch.onnx.fx_node",
                "value": "%transpose_2 : [num_users=3] = call_function[target=torch.ops.aten.transpose.int](args = (%view_2, 1, 2), kwargs = {})"
              },
              {
                "key": "[metadata] pkg.torch.onnx.name_scopes",
                "value": "['', 'model', 'model.layers.0', 'model.layers.0.self_attn', 'transpose_2']"
              },
              {
                "key": "[metadata] pkg.torch.onnx.stack_trace",
                "value": "File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 688, in forward\n    outputs: BaseModelOutputWithPast = self.model(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 453, in forward\n    layer_outputs = decoder_layer(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 308, in forward\n    hidden_states, self_attn_weights = self.self_attn(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 243, in forward\n    key_states = self.k_proj(hidden_states).view(hidden_shape).transpose(1, 2)"
              }
            ],
            "incomingEdges": [
              {
                "sourceNodeId": "node_Reshape_358",
                "sourceNodeOutputId": "0",
                "targetNodeInputId": "0"
              }
            ],
            "outputsMetadata": [
              {
                "id": "0",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "transpose_2"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "FLOAT[s34,32,s16,64]"
                  },
                  {
                    "key": "param_name",
                    "value": "transposed"
                  }
                ]
              }
            ],
            "inputsMetadata": [
              {
                "id": "0",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "view_2"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "FLOAT[s34,s16,32,64]"
                  },
                  {
                    "key": "param_name",
                    "value": "data"
                  }
                ]
              }
            ],
            "style": null,
            "config": null
          },
          {
            "id": "node_Transpose_360",
            "label": "Transpose",
            "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.0: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.0.self_attn: transformers.models.llama.modeling_llama.LlamaAttention/model.layers.0.self_attn.v_proj: torch.nn.modules.linear.Linear/linear_2: aten.linear.default",
            "subgraphIds": [],
            "attrs": [
              {
                "key": "perm",
                "value": "[1, 0]"
              },
              {
                "key": "[metadata] namespace",
                "value": ": transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.0: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.0.self_attn: transformers.models.llama.modeling_llama.LlamaAttention/model.layers.0.self_attn.v_proj: torch.nn.modules.linear.Linear/linear_2: aten.linear.default"
              },
              {
                "key": "[metadata] pkg.torch.onnx.class_hierarchy",
                "value": "['transformers.models.llama.modeling_llama.LlamaForCausalLM', 'transformers.models.llama.modeling_llama.LlamaModel', 'transformers.models.llama.modeling_llama.LlamaDecoderLayer', 'transformers.models.llama.modeling_llama.LlamaAttention', 'torch.nn.modules.linear.Linear', 'aten.linear.default']"
              },
              {
                "key": "[metadata] pkg.torch.onnx.fx_node",
                "value": "%linear_2 : [num_users=1] = call_function[target=torch.ops.aten.linear.default](args = (%mul_173, %p_model_layers_0_self_attn_v_proj_weight), kwargs = {})"
              },
              {
                "key": "[metadata] pkg.torch.onnx.name_scopes",
                "value": "['', 'model', 'model.layers.0', 'model.layers.0.self_attn', 'model.layers.0.self_attn.v_proj', 'linear_2']"
              },
              {
                "key": "[metadata] pkg.torch.onnx.stack_trace",
                "value": "File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 688, in forward\n    outputs: BaseModelOutputWithPast = self.model(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 453, in forward\n    layer_outputs = decoder_layer(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 308, in forward\n    hidden_states, self_attn_weights = self.self_attn(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 244, in forward\n    value_states = self.v_proj(hidden_states).view(hidden_shape).transpose(1, 2)\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/torch/nn/modules/linear.py\", line 125, in forward\n    return F.linear(input, self.weight, self.bias)"
              }
            ],
            "incomingEdges": [
              {
                "sourceNodeId": "[value] model.layers.0.self_attn.v_proj.weight",
                "sourceNodeOutputId": "0",
                "targetNodeInputId": "0"
              }
            ],
            "outputsMetadata": [
              {
                "id": "0",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "val_268"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "FLOAT[2048,2048]"
                  },
                  {
                    "key": "param_name",
                    "value": "transposed"
                  }
                ]
              }
            ],
            "inputsMetadata": [
              {
                "id": "0",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "model.layers.0.self_attn.v_proj.weight"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "FLOAT[2048,2048]"
                  },
                  {
                    "key": "param_name",
                    "value": "data"
                  }
                ]
              }
            ],
            "style": null,
            "config": null
          },
          {
            "id": "node_MatMul_361",
            "label": "MatMul",
            "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.0: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.0.self_attn: transformers.models.llama.modeling_llama.LlamaAttention/model.layers.0.self_attn.v_proj: torch.nn.modules.linear.Linear/linear_2: aten.linear.default",
            "subgraphIds": [],
            "attrs": [
              {
                "key": "[metadata] namespace",
                "value": ": transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.0: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.0.self_attn: transformers.models.llama.modeling_llama.LlamaAttention/model.layers.0.self_attn.v_proj: torch.nn.modules.linear.Linear/linear_2: aten.linear.default"
              },
              {
                "key": "[metadata] pkg.torch.onnx.class_hierarchy",
                "value": "['transformers.models.llama.modeling_llama.LlamaForCausalLM', 'transformers.models.llama.modeling_llama.LlamaModel', 'transformers.models.llama.modeling_llama.LlamaDecoderLayer', 'transformers.models.llama.modeling_llama.LlamaAttention', 'torch.nn.modules.linear.Linear', 'aten.linear.default']"
              },
              {
                "key": "[metadata] pkg.torch.onnx.fx_node",
                "value": "%linear_2 : [num_users=1] = call_function[target=torch.ops.aten.linear.default](args = (%mul_173, %p_model_layers_0_self_attn_v_proj_weight), kwargs = {})"
              },
              {
                "key": "[metadata] pkg.torch.onnx.name_scopes",
                "value": "['', 'model', 'model.layers.0', 'model.layers.0.self_attn', 'model.layers.0.self_attn.v_proj', 'linear_2']"
              },
              {
                "key": "[metadata] pkg.torch.onnx.stack_trace",
                "value": "File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 688, in forward\n    outputs: BaseModelOutputWithPast = self.model(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 453, in forward\n    layer_outputs = decoder_layer(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 308, in forward\n    hidden_states, self_attn_weights = self.self_attn(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 244, in forward\n    value_states = self.v_proj(hidden_states).view(hidden_shape).transpose(1, 2)\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/torch/nn/modules/linear.py\", line 125, in forward\n    return F.linear(input, self.weight, self.bias)"
              }
            ],
            "incomingEdges": [
              {
                "sourceNodeId": "node_Mul_338",
                "sourceNodeOutputId": "0",
                "targetNodeInputId": "0"
              },
              {
                "sourceNodeId": "node_Transpose_360",
                "sourceNodeOutputId": "0",
                "targetNodeInputId": "1"
              }
            ],
            "outputsMetadata": [
              {
                "id": "0",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "linear_2"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "FLOAT[s34,s16,2048]"
                  },
                  {
                    "key": "param_name",
                    "value": "Y"
                  }
                ]
              }
            ],
            "inputsMetadata": [
              {
                "id": "0",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "mul_173"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "FLOAT[s34,s16,2048]"
                  },
                  {
                    "key": "param_name",
                    "value": "A"
                  }
                ]
              },
              {
                "id": "1",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "val_268"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "FLOAT[2048,2048]"
                  },
                  {
                    "key": "param_name",
                    "value": "B"
                  }
                ]
              }
            ],
            "style": null,
            "config": null
          },
          {
            "id": "node_Concat_366",
            "label": "Concat",
            "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.0: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.0.self_attn: transformers.models.llama.modeling_llama.LlamaAttention/view_3: aten.view.default",
            "subgraphIds": [],
            "attrs": [
              {
                "key": "axis",
                "value": "0"
              },
              {
                "key": "[metadata] namespace",
                "value": ": transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.0: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.0.self_attn: transformers.models.llama.modeling_llama.LlamaAttention/view_3: aten.view.default"
              },
              {
                "key": "[metadata] pkg.torch.onnx.class_hierarchy",
                "value": "['transformers.models.llama.modeling_llama.LlamaForCausalLM', 'transformers.models.llama.modeling_llama.LlamaModel', 'transformers.models.llama.modeling_llama.LlamaDecoderLayer', 'transformers.models.llama.modeling_llama.LlamaAttention', 'aten.view.default']"
              },
              {
                "key": "[metadata] pkg.torch.onnx.fx_node",
                "value": "%view_3 : [num_users=1] = call_function[target=torch.ops.aten.view.default](args = (%linear_2, [%sym_size_int_68, %sym_size_int_61, -1, 64]), kwargs = {})"
              },
              {
                "key": "[metadata] pkg.torch.onnx.name_scopes",
                "value": "['', 'model', 'model.layers.0', 'model.layers.0.self_attn', 'view_3']"
              },
              {
                "key": "[metadata] pkg.torch.onnx.stack_trace",
                "value": "File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 688, in forward\n    outputs: BaseModelOutputWithPast = self.model(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 453, in forward\n    layer_outputs = decoder_layer(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 308, in forward\n    hidden_states, self_attn_weights = self.self_attn(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 244, in forward\n    value_states = self.v_proj(hidden_states).view(hidden_shape).transpose(1, 2)"
              }
            ],
            "incomingEdges": [
              {
                "sourceNodeId": "node_Shape_12",
                "sourceNodeOutputId": "0",
                "targetNodeInputId": "0"
              },
              {
                "sourceNodeId": "node_Shape_2",
                "sourceNodeOutputId": "0",
                "targetNodeInputId": "1"
              },
              {
                "sourceNodeId": "node_Constant_90",
                "sourceNodeOutputId": "0",
                "targetNodeInputId": "2"
              },
              {
                "sourceNodeId": "node_Constant_345",
                "sourceNodeOutputId": "0",
                "targetNodeInputId": "3"
              }
            ],
            "outputsMetadata": [
              {
                "id": "0",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "val_273"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "INT64[4]"
                  },
                  {
                    "key": "param_name",
                    "value": "concat_result"
                  }
                ]
              }
            ],
            "inputsMetadata": [
              {
                "id": "0",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "val_6"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "INT64[1]"
                  },
                  {
                    "key": "param_name",
                    "value": "inputs"
                  }
                ]
              },
              {
                "id": "1",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "val_1"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "INT64[1]"
                  },
                  {
                    "key": "param_name",
                    "value": "inputs"
                  }
                ]
              },
              {
                "id": "2",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "val_53"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "INT64[1]"
                  },
                  {
                    "key": "param_name",
                    "value": "inputs"
                  }
                ]
              },
              {
                "id": "3",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "val_258"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "INT64[1]"
                  },
                  {
                    "key": "param_name",
                    "value": "inputs"
                  }
                ]
              }
            ],
            "style": null,
            "config": null
          },
          {
            "id": "node_Reshape_368",
            "label": "Reshape",
            "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.0: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.0.self_attn: transformers.models.llama.modeling_llama.LlamaAttention/view_3: aten.view.default",
            "subgraphIds": [],
            "attrs": [
              {
                "key": "allowzero",
                "value": "1"
              },
              {
                "key": "[metadata] namespace",
                "value": ": transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.0: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.0.self_attn: transformers.models.llama.modeling_llama.LlamaAttention/view_3: aten.view.default"
              },
              {
                "key": "[metadata] pkg.torch.onnx.class_hierarchy",
                "value": "['transformers.models.llama.modeling_llama.LlamaForCausalLM', 'transformers.models.llama.modeling_llama.LlamaModel', 'transformers.models.llama.modeling_llama.LlamaDecoderLayer', 'transformers.models.llama.modeling_llama.LlamaAttention', 'aten.view.default']"
              },
              {
                "key": "[metadata] pkg.torch.onnx.fx_node",
                "value": "%view_3 : [num_users=1] = call_function[target=torch.ops.aten.view.default](args = (%linear_2, [%sym_size_int_68, %sym_size_int_61, -1, 64]), kwargs = {})"
              },
              {
                "key": "[metadata] pkg.torch.onnx.name_scopes",
                "value": "['', 'model', 'model.layers.0', 'model.layers.0.self_attn', 'view_3']"
              },
              {
                "key": "[metadata] pkg.torch.onnx.stack_trace",
                "value": "File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 688, in forward\n    outputs: BaseModelOutputWithPast = self.model(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 453, in forward\n    layer_outputs = decoder_layer(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 308, in forward\n    hidden_states, self_attn_weights = self.self_attn(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 244, in forward\n    value_states = self.v_proj(hidden_states).view(hidden_shape).transpose(1, 2)"
              }
            ],
            "incomingEdges": [
              {
                "sourceNodeId": "node_MatMul_361",
                "sourceNodeOutputId": "0",
                "targetNodeInputId": "0"
              },
              {
                "sourceNodeId": "node_Concat_366",
                "sourceNodeOutputId": "0",
                "targetNodeInputId": "1"
              }
            ],
            "outputsMetadata": [
              {
                "id": "0",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "view_3"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "FLOAT[s34,s16,32,64]"
                  },
                  {
                    "key": "param_name",
                    "value": "reshaped"
                  }
                ]
              }
            ],
            "inputsMetadata": [
              {
                "id": "0",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "linear_2"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "FLOAT[s34,s16,2048]"
                  },
                  {
                    "key": "param_name",
                    "value": "data"
                  }
                ]
              },
              {
                "id": "1",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "val_273"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "INT64[4]"
                  },
                  {
                    "key": "param_name",
                    "value": "shape"
                  }
                ]
              }
            ],
            "style": null,
            "config": null
          },
          {
            "id": "node_Transpose_369",
            "label": "Transpose",
            "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.0: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.0.self_attn: transformers.models.llama.modeling_llama.LlamaAttention/transpose_3: aten.transpose.int",
            "subgraphIds": [],
            "attrs": [
              {
                "key": "perm",
                "value": "[0, 2, 1, 3]"
              },
              {
                "key": "[metadata] namespace",
                "value": ": transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.0: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.0.self_attn: transformers.models.llama.modeling_llama.LlamaAttention/transpose_3: aten.transpose.int"
              },
              {
                "key": "[metadata] pkg.torch.onnx.class_hierarchy",
                "value": "['transformers.models.llama.modeling_llama.LlamaForCausalLM', 'transformers.models.llama.modeling_llama.LlamaModel', 'transformers.models.llama.modeling_llama.LlamaDecoderLayer', 'transformers.models.llama.modeling_llama.LlamaAttention', 'aten.transpose.int']"
              },
              {
                "key": "[metadata] pkg.torch.onnx.fx_node",
                "value": "%transpose_3 : [num_users=1] = call_function[target=torch.ops.aten.transpose.int](args = (%view_3, 1, 2), kwargs = {})"
              },
              {
                "key": "[metadata] pkg.torch.onnx.name_scopes",
                "value": "['', 'model', 'model.layers.0', 'model.layers.0.self_attn', 'transpose_3']"
              },
              {
                "key": "[metadata] pkg.torch.onnx.stack_trace",
                "value": "File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 688, in forward\n    outputs: BaseModelOutputWithPast = self.model(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 453, in forward\n    layer_outputs = decoder_layer(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 308, in forward\n    hidden_states, self_attn_weights = self.self_attn(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 244, in forward\n    value_states = self.v_proj(hidden_states).view(hidden_shape).transpose(1, 2)"
              }
            ],
            "incomingEdges": [
              {
                "sourceNodeId": "node_Reshape_368",
                "sourceNodeOutputId": "0",
                "targetNodeInputId": "0"
              }
            ],
            "outputsMetadata": [
              {
                "id": "0",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "transpose_3"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "FLOAT[s34,32,s16,64]"
                  },
                  {
                    "key": "param_name",
                    "value": "transposed"
                  }
                ]
              }
            ],
            "inputsMetadata": [
              {
                "id": "0",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "view_3"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "FLOAT[s34,s16,32,64]"
                  },
                  {
                    "key": "param_name",
                    "value": "data"
                  }
                ]
              }
            ],
            "style": null,
            "config": null
          },
          {
            "id": "node_Unsqueeze_370",
            "label": "Unsqueeze",
            "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.0: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.0.self_attn: transformers.models.llama.modeling_llama.LlamaAttention/unsqueeze_10: aten.unsqueeze.default",
            "subgraphIds": [],
            "attrs": [
              {
                "key": "[metadata] namespace",
                "value": ": transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.0: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.0.self_attn: transformers.models.llama.modeling_llama.LlamaAttention/unsqueeze_10: aten.unsqueeze.default"
              },
              {
                "key": "[metadata] pkg.torch.onnx.class_hierarchy",
                "value": "['transformers.models.llama.modeling_llama.LlamaForCausalLM', 'transformers.models.llama.modeling_llama.LlamaModel', 'transformers.models.llama.modeling_llama.LlamaDecoderLayer', 'transformers.models.llama.modeling_llama.LlamaAttention', 'aten.unsqueeze.default']"
              },
              {
                "key": "[metadata] pkg.torch.onnx.fx_node",
                "value": "%unsqueeze_10 : [num_users=2] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%mul_149, 1), kwargs = {})"
              },
              {
                "key": "[metadata] pkg.torch.onnx.name_scopes",
                "value": "['', 'model', 'model.layers.0', 'model.layers.0.self_attn', 'unsqueeze_10']"
              },
              {
                "key": "[metadata] pkg.torch.onnx.stack_trace",
                "value": "File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 688, in forward\n    outputs: BaseModelOutputWithPast = self.model(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 453, in forward\n    layer_outputs = decoder_layer(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 308, in forward\n    hidden_states, self_attn_weights = self.self_attn(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 247, in forward\n    query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)"
              }
            ],
            "incomingEdges": [
              {
                "sourceNodeId": "node_Cos_324",
                "sourceNodeOutputId": "0",
                "targetNodeInputId": "0"
              },
              {
                "sourceNodeId": "node_Constant_60",
                "sourceNodeOutputId": "0",
                "targetNodeInputId": "1"
              }
            ],
            "outputsMetadata": [
              {
                "id": "0",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "unsqueeze_10"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "FLOAT[1,1,s16,64]"
                  },
                  {
                    "key": "param_name",
                    "value": "expanded"
                  }
                ]
              }
            ],
            "inputsMetadata": [
              {
                "id": "0",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "cos"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "FLOAT[1,s16,64]"
                  },
                  {
                    "key": "param_name",
                    "value": "data"
                  }
                ]
              },
              {
                "id": "1",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "val_26"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "INT64[1]"
                  },
                  {
                    "key": "param_name",
                    "value": "axes"
                  }
                ]
              }
            ],
            "style": null,
            "config": null
          },
          {
            "id": "node_Unsqueeze_371",
            "label": "Unsqueeze",
            "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.0: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.0.self_attn: transformers.models.llama.modeling_llama.LlamaAttention/unsqueeze_11: aten.unsqueeze.default",
            "subgraphIds": [],
            "attrs": [
              {
                "key": "[metadata] namespace",
                "value": ": transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.0: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.0.self_attn: transformers.models.llama.modeling_llama.LlamaAttention/unsqueeze_11: aten.unsqueeze.default"
              },
              {
                "key": "[metadata] pkg.torch.onnx.class_hierarchy",
                "value": "['transformers.models.llama.modeling_llama.LlamaForCausalLM', 'transformers.models.llama.modeling_llama.LlamaModel', 'transformers.models.llama.modeling_llama.LlamaDecoderLayer', 'transformers.models.llama.modeling_llama.LlamaAttention', 'aten.unsqueeze.default']"
              },
              {
                "key": "[metadata] pkg.torch.onnx.fx_node",
                "value": "%unsqueeze_11 : [num_users=2] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%mul_156, 1), kwargs = {})"
              },
              {
                "key": "[metadata] pkg.torch.onnx.name_scopes",
                "value": "['', 'model', 'model.layers.0', 'model.layers.0.self_attn', 'unsqueeze_11']"
              },
              {
                "key": "[metadata] pkg.torch.onnx.stack_trace",
                "value": "File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 688, in forward\n    outputs: BaseModelOutputWithPast = self.model(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 453, in forward\n    layer_outputs = decoder_layer(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 308, in forward\n    hidden_states, self_attn_weights = self.self_attn(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 247, in forward\n    query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)"
              }
            ],
            "incomingEdges": [
              {
                "sourceNodeId": "node_Sin_326",
                "sourceNodeOutputId": "0",
                "targetNodeInputId": "0"
              },
              {
                "sourceNodeId": "node_Constant_60",
                "sourceNodeOutputId": "0",
                "targetNodeInputId": "1"
              }
            ],
            "outputsMetadata": [
              {
                "id": "0",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "unsqueeze_11"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "FLOAT[1,1,s16,64]"
                  },
                  {
                    "key": "param_name",
                    "value": "expanded"
                  }
                ]
              }
            ],
            "inputsMetadata": [
              {
                "id": "0",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "sin"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "FLOAT[1,s16,64]"
                  },
                  {
                    "key": "param_name",
                    "value": "data"
                  }
                ]
              },
              {
                "id": "1",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "val_26"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "INT64[1]"
                  },
                  {
                    "key": "param_name",
                    "value": "axes"
                  }
                ]
              }
            ],
            "style": null,
            "config": null
          },
          {
            "id": "node_Mul_372",
            "label": "Mul",
            "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.0: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.0.self_attn: transformers.models.llama.modeling_llama.LlamaAttention/mul_212: aten.mul.Tensor",
            "subgraphIds": [],
            "attrs": [
              {
                "key": "[metadata] namespace",
                "value": ": transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.0: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.0.self_attn: transformers.models.llama.modeling_llama.LlamaAttention/mul_212: aten.mul.Tensor"
              },
              {
                "key": "[metadata] pkg.torch.onnx.class_hierarchy",
                "value": "['transformers.models.llama.modeling_llama.LlamaForCausalLM', 'transformers.models.llama.modeling_llama.LlamaModel', 'transformers.models.llama.modeling_llama.LlamaDecoderLayer', 'transformers.models.llama.modeling_llama.LlamaAttention', 'aten.mul.Tensor']"
              },
              {
                "key": "[metadata] pkg.torch.onnx.fx_node",
                "value": "%mul_212 : [num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%transpose_1, %unsqueeze_10), kwargs = {})"
              },
              {
                "key": "[metadata] pkg.torch.onnx.name_scopes",
                "value": "['', 'model', 'model.layers.0', 'model.layers.0.self_attn', 'mul_212']"
              },
              {
                "key": "[metadata] pkg.torch.onnx.stack_trace",
                "value": "File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 688, in forward\n    outputs: BaseModelOutputWithPast = self.model(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 453, in forward\n    layer_outputs = decoder_layer(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 308, in forward\n    hidden_states, self_attn_weights = self.self_attn(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 247, in forward\n    query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)"
              }
            ],
            "incomingEdges": [
              {
                "sourceNodeId": "node_Transpose_349",
                "sourceNodeOutputId": "0",
                "targetNodeInputId": "0"
              },
              {
                "sourceNodeId": "node_Unsqueeze_370",
                "sourceNodeOutputId": "0",
                "targetNodeInputId": "1"
              }
            ],
            "outputsMetadata": [
              {
                "id": "0",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "mul_212"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "FLOAT[s34,32,s16,64]"
                  },
                  {
                    "key": "param_name",
                    "value": "C"
                  }
                ]
              }
            ],
            "inputsMetadata": [
              {
                "id": "0",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "transpose_1"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "FLOAT[s34,32,s16,64]"
                  },
                  {
                    "key": "param_name",
                    "value": "A"
                  }
                ]
              },
              {
                "id": "1",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "unsqueeze_10"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "FLOAT[1,1,s16,64]"
                  },
                  {
                    "key": "param_name",
                    "value": "B"
                  }
                ]
              }
            ],
            "style": null,
            "config": null
          },
          {
            "id": "node_Constant_875",
            "label": "Constant",
            "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.0: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.0.self_attn: transformers.models.llama.modeling_llama.LlamaAttention/slice_26: aten.slice.Tensor",
            "subgraphIds": [],
            "attrs": [
              {
                "key": "__value",
                "value": "[0]"
              },
              {
                "key": "value",
                "value": "TensorProtoTensor<INT64,[1]>(array([0]), name='val_277')"
              }
            ],
            "incomingEdges": [],
            "outputsMetadata": [
              {
                "id": "0",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "val_277"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "INT64[1]"
                  },
                  {
                    "key": "param_name",
                    "value": "output"
                  }
                ]
              }
            ],
            "inputsMetadata": [],
            "style": null,
            "config": null
          },
          {
            "id": "node_Constant_878",
            "label": "Constant",
            "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.0: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.0.self_attn: transformers.models.llama.modeling_llama.LlamaAttention/slice_26: aten.slice.Tensor",
            "subgraphIds": [],
            "attrs": [
              {
                "key": "__value",
                "value": "[32]"
              },
              {
                "key": "value",
                "value": "TensorProtoTensor<INT64,[1]>(array([32]), name='val_281')"
              }
            ],
            "incomingEdges": [],
            "outputsMetadata": [
              {
                "id": "0",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "val_281"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "INT64[1]"
                  },
                  {
                    "key": "param_name",
                    "value": "output"
                  }
                ]
              }
            ],
            "inputsMetadata": [],
            "style": null,
            "config": null
          },
          {
            "id": "node_Constant_881",
            "label": "Constant",
            "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.0: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.0.self_attn: transformers.models.llama.modeling_llama.LlamaAttention/slice_26: aten.slice.Tensor",
            "subgraphIds": [],
            "attrs": [
              {
                "key": "__value",
                "value": "[3]"
              },
              {
                "key": "value",
                "value": "TensorProtoTensor<INT64,[1]>(array([3]), name='val_284')"
              }
            ],
            "incomingEdges": [],
            "outputsMetadata": [
              {
                "id": "0",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "val_284"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "INT64[1]"
                  },
                  {
                    "key": "param_name",
                    "value": "output"
                  }
                ]
              }
            ],
            "inputsMetadata": [],
            "style": null,
            "config": null
          },
          {
            "id": "node_Constant_383",
            "label": "Constant",
            "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.0: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.0.self_attn: transformers.models.llama.modeling_llama.LlamaAttention/slice_26: aten.slice.Tensor",
            "subgraphIds": [],
            "attrs": [
              {
                "key": "__value",
                "value": "[1]"
              },
              {
                "key": "value_ints",
                "value": "[1]"
              },
              {
                "key": "[metadata] namespace",
                "value": ": transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.0: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.0.self_attn: transformers.models.llama.modeling_llama.LlamaAttention/slice_26: aten.slice.Tensor"
              },
              {
                "key": "[metadata] pkg.torch.onnx.class_hierarchy",
                "value": "['transformers.models.llama.modeling_llama.LlamaForCausalLM', 'transformers.models.llama.modeling_llama.LlamaModel', 'transformers.models.llama.modeling_llama.LlamaDecoderLayer', 'transformers.models.llama.modeling_llama.LlamaAttention', 'aten.slice.Tensor']"
              },
              {
                "key": "[metadata] pkg.torch.onnx.fx_node",
                "value": "%slice_26 : [num_users=1] = call_function[target=torch.ops.aten.slice.Tensor](args = (%transpose_1, 3, 0, 32), kwargs = {})"
              },
              {
                "key": "[metadata] pkg.torch.onnx.name_scopes",
                "value": "['', 'model', 'model.layers.0', 'model.layers.0.self_attn', 'slice_26']"
              },
              {
                "key": "[metadata] pkg.torch.onnx.stack_trace",
                "value": "File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 688, in forward\n    outputs: BaseModelOutputWithPast = self.model(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 453, in forward\n    layer_outputs = decoder_layer(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 308, in forward\n    hidden_states, self_attn_weights = self.self_attn(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 247, in forward\n    query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)"
              }
            ],
            "incomingEdges": [],
            "outputsMetadata": [
              {
                "id": "0",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "val_285"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "INT64[1]"
                  },
                  {
                    "key": "param_name",
                    "value": "output"
                  }
                ]
              }
            ],
            "inputsMetadata": [],
            "style": null,
            "config": null
          },
          {
            "id": "node_Slice_384",
            "label": "Slice",
            "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.0: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.0.self_attn: transformers.models.llama.modeling_llama.LlamaAttention/slice_26: aten.slice.Tensor",
            "subgraphIds": [],
            "attrs": [
              {
                "key": "[metadata] namespace",
                "value": ": transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.0: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.0.self_attn: transformers.models.llama.modeling_llama.LlamaAttention/slice_26: aten.slice.Tensor"
              },
              {
                "key": "[metadata] pkg.torch.onnx.class_hierarchy",
                "value": "['transformers.models.llama.modeling_llama.LlamaForCausalLM', 'transformers.models.llama.modeling_llama.LlamaModel', 'transformers.models.llama.modeling_llama.LlamaDecoderLayer', 'transformers.models.llama.modeling_llama.LlamaAttention', 'aten.slice.Tensor']"
              },
              {
                "key": "[metadata] pkg.torch.onnx.fx_node",
                "value": "%slice_26 : [num_users=1] = call_function[target=torch.ops.aten.slice.Tensor](args = (%transpose_1, 3, 0, 32), kwargs = {})"
              },
              {
                "key": "[metadata] pkg.torch.onnx.name_scopes",
                "value": "['', 'model', 'model.layers.0', 'model.layers.0.self_attn', 'slice_26']"
              },
              {
                "key": "[metadata] pkg.torch.onnx.stack_trace",
                "value": "File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 688, in forward\n    outputs: BaseModelOutputWithPast = self.model(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 453, in forward\n    layer_outputs = decoder_layer(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 308, in forward\n    hidden_states, self_attn_weights = self.self_attn(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 247, in forward\n    query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)"
              }
            ],
            "incomingEdges": [
              {
                "sourceNodeId": "node_Transpose_349",
                "sourceNodeOutputId": "0",
                "targetNodeInputId": "0"
              },
              {
                "sourceNodeId": "node_Constant_875",
                "sourceNodeOutputId": "0",
                "targetNodeInputId": "1"
              },
              {
                "sourceNodeId": "node_Constant_878",
                "sourceNodeOutputId": "0",
                "targetNodeInputId": "2"
              },
              {
                "sourceNodeId": "node_Constant_881",
                "sourceNodeOutputId": "0",
                "targetNodeInputId": "3"
              },
              {
                "sourceNodeId": "node_Constant_383",
                "sourceNodeOutputId": "0",
                "targetNodeInputId": "4"
              }
            ],
            "outputsMetadata": [
              {
                "id": "0",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "slice_26"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "FLOAT[s34,32,s16,32]"
                  },
                  {
                    "key": "param_name",
                    "value": "output"
                  }
                ]
              }
            ],
            "inputsMetadata": [
              {
                "id": "0",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "transpose_1"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "FLOAT[s34,32,s16,64]"
                  },
                  {
                    "key": "param_name",
                    "value": "data"
                  }
                ]
              },
              {
                "id": "1",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "val_277"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "INT64[1]"
                  },
                  {
                    "key": "param_name",
                    "value": "starts"
                  }
                ]
              },
              {
                "id": "2",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "val_281"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "INT64[1]"
                  },
                  {
                    "key": "param_name",
                    "value": "ends"
                  }
                ]
              },
              {
                "id": "3",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "val_284"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "INT64[1]"
                  },
                  {
                    "key": "param_name",
                    "value": "axes"
                  }
                ]
              },
              {
                "id": "4",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "val_285"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "INT64[1]"
                  },
                  {
                    "key": "param_name",
                    "value": "steps"
                  }
                ]
              }
            ],
            "style": null,
            "config": null
          },
          {
            "id": "node_Constant_884",
            "label": "Constant",
            "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.0: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.0.self_attn: transformers.models.llama.modeling_llama.LlamaAttention/slice_27: aten.slice.Tensor",
            "subgraphIds": [],
            "attrs": [
              {
                "key": "__value",
                "value": "[32]"
              },
              {
                "key": "value",
                "value": "TensorProtoTensor<INT64,[1]>(array([32]), name='val_288')"
              }
            ],
            "incomingEdges": [],
            "outputsMetadata": [
              {
                "id": "0",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "val_288"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "INT64[1]"
                  },
                  {
                    "key": "param_name",
                    "value": "output"
                  }
                ]
              }
            ],
            "inputsMetadata": [],
            "style": null,
            "config": null
          },
          {
            "id": "node_Constant_887",
            "label": "Constant",
            "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.0: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.0.self_attn: transformers.models.llama.modeling_llama.LlamaAttention/slice_27: aten.slice.Tensor",
            "subgraphIds": [],
            "attrs": [
              {
                "key": "__value",
                "value": "[9223372036854775807]"
              },
              {
                "key": "value",
                "value": "TensorProtoTensor<INT64,[1]>(array([9223372036854775807]), name='val_291')"
              }
            ],
            "incomingEdges": [],
            "outputsMetadata": [
              {
                "id": "0",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "val_291"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "INT64[1]"
                  },
                  {
                    "key": "param_name",
                    "value": "output"
                  }
                ]
              }
            ],
            "inputsMetadata": [],
            "style": null,
            "config": null
          },
          {
            "id": "node_Constant_890",
            "label": "Constant",
            "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.0: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.0.self_attn: transformers.models.llama.modeling_llama.LlamaAttention/slice_27: aten.slice.Tensor",
            "subgraphIds": [],
            "attrs": [
              {
                "key": "__value",
                "value": "[3]"
              },
              {
                "key": "value",
                "value": "TensorProtoTensor<INT64,[1]>(array([3]), name='val_294')"
              }
            ],
            "incomingEdges": [],
            "outputsMetadata": [
              {
                "id": "0",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "val_294"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "INT64[1]"
                  },
                  {
                    "key": "param_name",
                    "value": "output"
                  }
                ]
              }
            ],
            "inputsMetadata": [],
            "style": null,
            "config": null
          },
          {
            "id": "node_Constant_394",
            "label": "Constant",
            "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.0: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.0.self_attn: transformers.models.llama.modeling_llama.LlamaAttention/slice_27: aten.slice.Tensor",
            "subgraphIds": [],
            "attrs": [
              {
                "key": "__value",
                "value": "[1]"
              },
              {
                "key": "value_ints",
                "value": "[1]"
              },
              {
                "key": "[metadata] namespace",
                "value": ": transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.0: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.0.self_attn: transformers.models.llama.modeling_llama.LlamaAttention/slice_27: aten.slice.Tensor"
              },
              {
                "key": "[metadata] pkg.torch.onnx.class_hierarchy",
                "value": "['transformers.models.llama.modeling_llama.LlamaForCausalLM', 'transformers.models.llama.modeling_llama.LlamaModel', 'transformers.models.llama.modeling_llama.LlamaDecoderLayer', 'transformers.models.llama.modeling_llama.LlamaAttention', 'aten.slice.Tensor']"
              },
              {
                "key": "[metadata] pkg.torch.onnx.fx_node",
                "value": "%slice_27 : [num_users=1] = call_function[target=torch.ops.aten.slice.Tensor](args = (%transpose_1, 3, 32, 9223372036854775807), kwargs = {})"
              },
              {
                "key": "[metadata] pkg.torch.onnx.name_scopes",
                "value": "['', 'model', 'model.layers.0', 'model.layers.0.self_attn', 'slice_27']"
              },
              {
                "key": "[metadata] pkg.torch.onnx.stack_trace",
                "value": "File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 688, in forward\n    outputs: BaseModelOutputWithPast = self.model(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 453, in forward\n    layer_outputs = decoder_layer(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 308, in forward\n    hidden_states, self_attn_weights = self.self_attn(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 247, in forward\n    query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)"
              }
            ],
            "incomingEdges": [],
            "outputsMetadata": [
              {
                "id": "0",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "val_295"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "INT64[1]"
                  },
                  {
                    "key": "param_name",
                    "value": "output"
                  }
                ]
              }
            ],
            "inputsMetadata": [],
            "style": null,
            "config": null
          },
          {
            "id": "node_Slice_395",
            "label": "Slice",
            "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.0: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.0.self_attn: transformers.models.llama.modeling_llama.LlamaAttention/slice_27: aten.slice.Tensor",
            "subgraphIds": [],
            "attrs": [
              {
                "key": "[metadata] namespace",
                "value": ": transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.0: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.0.self_attn: transformers.models.llama.modeling_llama.LlamaAttention/slice_27: aten.slice.Tensor"
              },
              {
                "key": "[metadata] pkg.torch.onnx.class_hierarchy",
                "value": "['transformers.models.llama.modeling_llama.LlamaForCausalLM', 'transformers.models.llama.modeling_llama.LlamaModel', 'transformers.models.llama.modeling_llama.LlamaDecoderLayer', 'transformers.models.llama.modeling_llama.LlamaAttention', 'aten.slice.Tensor']"
              },
              {
                "key": "[metadata] pkg.torch.onnx.fx_node",
                "value": "%slice_27 : [num_users=1] = call_function[target=torch.ops.aten.slice.Tensor](args = (%transpose_1, 3, 32, 9223372036854775807), kwargs = {})"
              },
              {
                "key": "[metadata] pkg.torch.onnx.name_scopes",
                "value": "['', 'model', 'model.layers.0', 'model.layers.0.self_attn', 'slice_27']"
              },
              {
                "key": "[metadata] pkg.torch.onnx.stack_trace",
                "value": "File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 688, in forward\n    outputs: BaseModelOutputWithPast = self.model(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 453, in forward\n    layer_outputs = decoder_layer(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 308, in forward\n    hidden_states, self_attn_weights = self.self_attn(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 247, in forward\n    query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)"
              }
            ],
            "incomingEdges": [
              {
                "sourceNodeId": "node_Transpose_349",
                "sourceNodeOutputId": "0",
                "targetNodeInputId": "0"
              },
              {
                "sourceNodeId": "node_Constant_884",
                "sourceNodeOutputId": "0",
                "targetNodeInputId": "1"
              },
              {
                "sourceNodeId": "node_Constant_887",
                "sourceNodeOutputId": "0",
                "targetNodeInputId": "2"
              },
              {
                "sourceNodeId": "node_Constant_890",
                "sourceNodeOutputId": "0",
                "targetNodeInputId": "3"
              },
              {
                "sourceNodeId": "node_Constant_394",
                "sourceNodeOutputId": "0",
                "targetNodeInputId": "4"
              }
            ],
            "outputsMetadata": [
              {
                "id": "0",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "slice_27"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "FLOAT[s34,32,s16,32]"
                  },
                  {
                    "key": "param_name",
                    "value": "output"
                  }
                ]
              }
            ],
            "inputsMetadata": [
              {
                "id": "0",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "transpose_1"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "FLOAT[s34,32,s16,64]"
                  },
                  {
                    "key": "param_name",
                    "value": "data"
                  }
                ]
              },
              {
                "id": "1",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "val_288"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "INT64[1]"
                  },
                  {
                    "key": "param_name",
                    "value": "starts"
                  }
                ]
              },
              {
                "id": "2",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "val_291"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "INT64[1]"
                  },
                  {
                    "key": "param_name",
                    "value": "ends"
                  }
                ]
              },
              {
                "id": "3",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "val_294"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "INT64[1]"
                  },
                  {
                    "key": "param_name",
                    "value": "axes"
                  }
                ]
              },
              {
                "id": "4",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "val_295"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "INT64[1]"
                  },
                  {
                    "key": "param_name",
                    "value": "steps"
                  }
                ]
              }
            ],
            "style": null,
            "config": null
          },
          {
            "id": "node_Neg_396",
            "label": "Neg",
            "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.0: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.0.self_attn: transformers.models.llama.modeling_llama.LlamaAttention/neg: aten.neg.default",
            "subgraphIds": [],
            "attrs": [
              {
                "key": "[metadata] namespace",
                "value": ": transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.0: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.0.self_attn: transformers.models.llama.modeling_llama.LlamaAttention/neg: aten.neg.default"
              },
              {
                "key": "[metadata] pkg.torch.onnx.class_hierarchy",
                "value": "['transformers.models.llama.modeling_llama.LlamaForCausalLM', 'transformers.models.llama.modeling_llama.LlamaModel', 'transformers.models.llama.modeling_llama.LlamaDecoderLayer', 'transformers.models.llama.modeling_llama.LlamaAttention', 'aten.neg.default']"
              },
              {
                "key": "[metadata] pkg.torch.onnx.fx_node",
                "value": "%neg : [num_users=1] = call_function[target=torch.ops.aten.neg.default](args = (%slice_27,), kwargs = {})"
              },
              {
                "key": "[metadata] pkg.torch.onnx.name_scopes",
                "value": "['', 'model', 'model.layers.0', 'model.layers.0.self_attn', 'neg']"
              },
              {
                "key": "[metadata] pkg.torch.onnx.stack_trace",
                "value": "File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 688, in forward\n    outputs: BaseModelOutputWithPast = self.model(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 453, in forward\n    layer_outputs = decoder_layer(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 308, in forward\n    hidden_states, self_attn_weights = self.self_attn(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 247, in forward\n    query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)"
              }
            ],
            "incomingEdges": [
              {
                "sourceNodeId": "node_Slice_395",
                "sourceNodeOutputId": "0",
                "targetNodeInputId": "0"
              }
            ],
            "outputsMetadata": [
              {
                "id": "0",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "neg"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "FLOAT[s34,32,s16,32]"
                  },
                  {
                    "key": "param_name",
                    "value": "Y"
                  }
                ]
              }
            ],
            "inputsMetadata": [
              {
                "id": "0",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "slice_27"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "FLOAT[s34,32,s16,32]"
                  },
                  {
                    "key": "param_name",
                    "value": "X"
                  }
                ]
              }
            ],
            "style": null,
            "config": null
          },
          {
            "id": "node_Concat_397",
            "label": "Concat",
            "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.0: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.0.self_attn: transformers.models.llama.modeling_llama.LlamaAttention/cat_1: aten.cat.default",
            "subgraphIds": [],
            "attrs": [
              {
                "key": "axis",
                "value": "-1"
              },
              {
                "key": "[metadata] namespace",
                "value": ": transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.0: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.0.self_attn: transformers.models.llama.modeling_llama.LlamaAttention/cat_1: aten.cat.default"
              },
              {
                "key": "[metadata] pkg.torch.onnx.class_hierarchy",
                "value": "['transformers.models.llama.modeling_llama.LlamaForCausalLM', 'transformers.models.llama.modeling_llama.LlamaModel', 'transformers.models.llama.modeling_llama.LlamaDecoderLayer', 'transformers.models.llama.modeling_llama.LlamaAttention', 'aten.cat.default']"
              },
              {
                "key": "[metadata] pkg.torch.onnx.fx_node",
                "value": "%cat_1 : [num_users=1] = call_function[target=torch.ops.aten.cat.default](args = ([%neg, %slice_26], -1), kwargs = {})"
              },
              {
                "key": "[metadata] pkg.torch.onnx.name_scopes",
                "value": "['', 'model', 'model.layers.0', 'model.layers.0.self_attn', 'cat_1']"
              },
              {
                "key": "[metadata] pkg.torch.onnx.stack_trace",
                "value": "File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 688, in forward\n    outputs: BaseModelOutputWithPast = self.model(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 453, in forward\n    layer_outputs = decoder_layer(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 308, in forward\n    hidden_states, self_attn_weights = self.self_attn(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 247, in forward\n    query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)"
              }
            ],
            "incomingEdges": [
              {
                "sourceNodeId": "node_Neg_396",
                "sourceNodeOutputId": "0",
                "targetNodeInputId": "0"
              },
              {
                "sourceNodeId": "node_Slice_384",
                "sourceNodeOutputId": "0",
                "targetNodeInputId": "1"
              }
            ],
            "outputsMetadata": [
              {
                "id": "0",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "cat_1"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "FLOAT[s34,32,s16,64]"
                  },
                  {
                    "key": "param_name",
                    "value": "concat_result"
                  }
                ]
              }
            ],
            "inputsMetadata": [
              {
                "id": "0",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "neg"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "FLOAT[s34,32,s16,32]"
                  },
                  {
                    "key": "param_name",
                    "value": "inputs"
                  }
                ]
              },
              {
                "id": "1",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "slice_26"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "FLOAT[s34,32,s16,32]"
                  },
                  {
                    "key": "param_name",
                    "value": "inputs"
                  }
                ]
              }
            ],
            "style": null,
            "config": null
          },
          {
            "id": "node_Mul_398",
            "label": "Mul",
            "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.0: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.0.self_attn: transformers.models.llama.modeling_llama.LlamaAttention/mul_229: aten.mul.Tensor",
            "subgraphIds": [],
            "attrs": [
              {
                "key": "[metadata] namespace",
                "value": ": transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.0: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.0.self_attn: transformers.models.llama.modeling_llama.LlamaAttention/mul_229: aten.mul.Tensor"
              },
              {
                "key": "[metadata] pkg.torch.onnx.class_hierarchy",
                "value": "['transformers.models.llama.modeling_llama.LlamaForCausalLM', 'transformers.models.llama.modeling_llama.LlamaModel', 'transformers.models.llama.modeling_llama.LlamaDecoderLayer', 'transformers.models.llama.modeling_llama.LlamaAttention', 'aten.mul.Tensor']"
              },
              {
                "key": "[metadata] pkg.torch.onnx.fx_node",
                "value": "%mul_229 : [num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%cat_1, %unsqueeze_11), kwargs = {})"
              },
              {
                "key": "[metadata] pkg.torch.onnx.name_scopes",
                "value": "['', 'model', 'model.layers.0', 'model.layers.0.self_attn', 'mul_229']"
              },
              {
                "key": "[metadata] pkg.torch.onnx.stack_trace",
                "value": "File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 688, in forward\n    outputs: BaseModelOutputWithPast = self.model(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 453, in forward\n    layer_outputs = decoder_layer(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 308, in forward\n    hidden_states, self_attn_weights = self.self_attn(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 247, in forward\n    query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)"
              }
            ],
            "incomingEdges": [
              {
                "sourceNodeId": "node_Concat_397",
                "sourceNodeOutputId": "0",
                "targetNodeInputId": "0"
              },
              {
                "sourceNodeId": "node_Unsqueeze_371",
                "sourceNodeOutputId": "0",
                "targetNodeInputId": "1"
              }
            ],
            "outputsMetadata": [
              {
                "id": "0",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "mul_229"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "FLOAT[s34,32,s16,64]"
                  },
                  {
                    "key": "param_name",
                    "value": "C"
                  }
                ]
              }
            ],
            "inputsMetadata": [
              {
                "id": "0",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "cat_1"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "FLOAT[s34,32,s16,64]"
                  },
                  {
                    "key": "param_name",
                    "value": "A"
                  }
                ]
              },
              {
                "id": "1",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "unsqueeze_11"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "FLOAT[1,1,s16,64]"
                  },
                  {
                    "key": "param_name",
                    "value": "B"
                  }
                ]
              }
            ],
            "style": null,
            "config": null
          },
          {
            "id": "node_Add_399",
            "label": "Add",
            "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.0: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.0.self_attn: transformers.models.llama.modeling_llama.LlamaAttention/add_287: aten.add.Tensor",
            "subgraphIds": [],
            "attrs": [
              {
                "key": "[metadata] namespace",
                "value": ": transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.0: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.0.self_attn: transformers.models.llama.modeling_llama.LlamaAttention/add_287: aten.add.Tensor"
              },
              {
                "key": "[metadata] pkg.torch.onnx.class_hierarchy",
                "value": "['transformers.models.llama.modeling_llama.LlamaForCausalLM', 'transformers.models.llama.modeling_llama.LlamaModel', 'transformers.models.llama.modeling_llama.LlamaDecoderLayer', 'transformers.models.llama.modeling_llama.LlamaAttention', 'aten.add.Tensor']"
              },
              {
                "key": "[metadata] pkg.torch.onnx.fx_node",
                "value": "%add_287 : [num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%mul_212, %mul_229), kwargs = {})"
              },
              {
                "key": "[metadata] pkg.torch.onnx.name_scopes",
                "value": "['', 'model', 'model.layers.0', 'model.layers.0.self_attn', 'add_287']"
              },
              {
                "key": "[metadata] pkg.torch.onnx.stack_trace",
                "value": "File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 688, in forward\n    outputs: BaseModelOutputWithPast = self.model(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 453, in forward\n    layer_outputs = decoder_layer(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 308, in forward\n    hidden_states, self_attn_weights = self.self_attn(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 247, in forward\n    query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)"
              }
            ],
            "incomingEdges": [
              {
                "sourceNodeId": "node_Mul_372",
                "sourceNodeOutputId": "0",
                "targetNodeInputId": "0"
              },
              {
                "sourceNodeId": "node_Mul_398",
                "sourceNodeOutputId": "0",
                "targetNodeInputId": "1"
              }
            ],
            "outputsMetadata": [
              {
                "id": "0",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "add_287"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "FLOAT[s34,32,s16,64]"
                  },
                  {
                    "key": "param_name",
                    "value": "C"
                  }
                ]
              }
            ],
            "inputsMetadata": [
              {
                "id": "0",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "mul_212"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "FLOAT[s34,32,s16,64]"
                  },
                  {
                    "key": "param_name",
                    "value": "A"
                  }
                ]
              },
              {
                "id": "1",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "mul_229"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "FLOAT[s34,32,s16,64]"
                  },
                  {
                    "key": "param_name",
                    "value": "B"
                  }
                ]
              }
            ],
            "style": null,
            "config": null
          },
          {
            "id": "node_Mul_400",
            "label": "Mul",
            "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.0: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.0.self_attn: transformers.models.llama.modeling_llama.LlamaAttention/mul_237: aten.mul.Tensor",
            "subgraphIds": [],
            "attrs": [
              {
                "key": "[metadata] namespace",
                "value": ": transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.0: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.0.self_attn: transformers.models.llama.modeling_llama.LlamaAttention/mul_237: aten.mul.Tensor"
              },
              {
                "key": "[metadata] pkg.torch.onnx.class_hierarchy",
                "value": "['transformers.models.llama.modeling_llama.LlamaForCausalLM', 'transformers.models.llama.modeling_llama.LlamaModel', 'transformers.models.llama.modeling_llama.LlamaDecoderLayer', 'transformers.models.llama.modeling_llama.LlamaAttention', 'aten.mul.Tensor']"
              },
              {
                "key": "[metadata] pkg.torch.onnx.fx_node",
                "value": "%mul_237 : [num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%transpose_2, %unsqueeze_10), kwargs = {})"
              },
              {
                "key": "[metadata] pkg.torch.onnx.name_scopes",
                "value": "['', 'model', 'model.layers.0', 'model.layers.0.self_attn', 'mul_237']"
              },
              {
                "key": "[metadata] pkg.torch.onnx.stack_trace",
                "value": "File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 688, in forward\n    outputs: BaseModelOutputWithPast = self.model(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 453, in forward\n    layer_outputs = decoder_layer(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 308, in forward\n    hidden_states, self_attn_weights = self.self_attn(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 247, in forward\n    query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)"
              }
            ],
            "incomingEdges": [
              {
                "sourceNodeId": "node_Transpose_359",
                "sourceNodeOutputId": "0",
                "targetNodeInputId": "0"
              },
              {
                "sourceNodeId": "node_Unsqueeze_370",
                "sourceNodeOutputId": "0",
                "targetNodeInputId": "1"
              }
            ],
            "outputsMetadata": [
              {
                "id": "0",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "mul_237"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "FLOAT[s34,32,s16,64]"
                  },
                  {
                    "key": "param_name",
                    "value": "C"
                  }
                ]
              }
            ],
            "inputsMetadata": [
              {
                "id": "0",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "transpose_2"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "FLOAT[s34,32,s16,64]"
                  },
                  {
                    "key": "param_name",
                    "value": "A"
                  }
                ]
              },
              {
                "id": "1",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "unsqueeze_10"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "FLOAT[1,1,s16,64]"
                  },
                  {
                    "key": "param_name",
                    "value": "B"
                  }
                ]
              }
            ],
            "style": null,
            "config": null
          },
          {
            "id": "node_Constant_893",
            "label": "Constant",
            "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.0: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.0.self_attn: transformers.models.llama.modeling_llama.LlamaAttention/slice_28: aten.slice.Tensor",
            "subgraphIds": [],
            "attrs": [
              {
                "key": "__value",
                "value": "[0]"
              },
              {
                "key": "value",
                "value": "TensorProtoTensor<INT64,[1]>(array([0]), name='val_298')"
              }
            ],
            "incomingEdges": [],
            "outputsMetadata": [
              {
                "id": "0",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "val_298"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "INT64[1]"
                  },
                  {
                    "key": "param_name",
                    "value": "output"
                  }
                ]
              }
            ],
            "inputsMetadata": [],
            "style": null,
            "config": null
          },
          {
            "id": "node_Constant_896",
            "label": "Constant",
            "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.0: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.0.self_attn: transformers.models.llama.modeling_llama.LlamaAttention/slice_28: aten.slice.Tensor",
            "subgraphIds": [],
            "attrs": [
              {
                "key": "__value",
                "value": "[32]"
              },
              {
                "key": "value",
                "value": "TensorProtoTensor<INT64,[1]>(array([32]), name='val_301')"
              }
            ],
            "incomingEdges": [],
            "outputsMetadata": [
              {
                "id": "0",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "val_301"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "INT64[1]"
                  },
                  {
                    "key": "param_name",
                    "value": "output"
                  }
                ]
              }
            ],
            "inputsMetadata": [],
            "style": null,
            "config": null
          },
          {
            "id": "node_Constant_899",
            "label": "Constant",
            "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.0: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.0.self_attn: transformers.models.llama.modeling_llama.LlamaAttention/slice_28: aten.slice.Tensor",
            "subgraphIds": [],
            "attrs": [
              {
                "key": "__value",
                "value": "[3]"
              },
              {
                "key": "value",
                "value": "TensorProtoTensor<INT64,[1]>(array([3]), name='val_304')"
              }
            ],
            "incomingEdges": [],
            "outputsMetadata": [
              {
                "id": "0",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "val_304"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "INT64[1]"
                  },
                  {
                    "key": "param_name",
                    "value": "output"
                  }
                ]
              }
            ],
            "inputsMetadata": [],
            "style": null,
            "config": null
          },
          {
            "id": "node_Constant_410",
            "label": "Constant",
            "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.0: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.0.self_attn: transformers.models.llama.modeling_llama.LlamaAttention/slice_28: aten.slice.Tensor",
            "subgraphIds": [],
            "attrs": [
              {
                "key": "__value",
                "value": "[1]"
              },
              {
                "key": "value_ints",
                "value": "[1]"
              },
              {
                "key": "[metadata] namespace",
                "value": ": transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.0: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.0.self_attn: transformers.models.llama.modeling_llama.LlamaAttention/slice_28: aten.slice.Tensor"
              },
              {
                "key": "[metadata] pkg.torch.onnx.class_hierarchy",
                "value": "['transformers.models.llama.modeling_llama.LlamaForCausalLM', 'transformers.models.llama.modeling_llama.LlamaModel', 'transformers.models.llama.modeling_llama.LlamaDecoderLayer', 'transformers.models.llama.modeling_llama.LlamaAttention', 'aten.slice.Tensor']"
              },
              {
                "key": "[metadata] pkg.torch.onnx.fx_node",
                "value": "%slice_28 : [num_users=1] = call_function[target=torch.ops.aten.slice.Tensor](args = (%transpose_2, 3, 0, 32), kwargs = {})"
              },
              {
                "key": "[metadata] pkg.torch.onnx.name_scopes",
                "value": "['', 'model', 'model.layers.0', 'model.layers.0.self_attn', 'slice_28']"
              },
              {
                "key": "[metadata] pkg.torch.onnx.stack_trace",
                "value": "File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 688, in forward\n    outputs: BaseModelOutputWithPast = self.model(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 453, in forward\n    layer_outputs = decoder_layer(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 308, in forward\n    hidden_states, self_attn_weights = self.self_attn(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 247, in forward\n    query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)"
              }
            ],
            "incomingEdges": [],
            "outputsMetadata": [
              {
                "id": "0",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "val_305"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "INT64[1]"
                  },
                  {
                    "key": "param_name",
                    "value": "output"
                  }
                ]
              }
            ],
            "inputsMetadata": [],
            "style": null,
            "config": null
          },
          {
            "id": "node_Slice_411",
            "label": "Slice",
            "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.0: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.0.self_attn: transformers.models.llama.modeling_llama.LlamaAttention/slice_28: aten.slice.Tensor",
            "subgraphIds": [],
            "attrs": [
              {
                "key": "[metadata] namespace",
                "value": ": transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.0: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.0.self_attn: transformers.models.llama.modeling_llama.LlamaAttention/slice_28: aten.slice.Tensor"
              },
              {
                "key": "[metadata] pkg.torch.onnx.class_hierarchy",
                "value": "['transformers.models.llama.modeling_llama.LlamaForCausalLM', 'transformers.models.llama.modeling_llama.LlamaModel', 'transformers.models.llama.modeling_llama.LlamaDecoderLayer', 'transformers.models.llama.modeling_llama.LlamaAttention', 'aten.slice.Tensor']"
              },
              {
                "key": "[metadata] pkg.torch.onnx.fx_node",
                "value": "%slice_28 : [num_users=1] = call_function[target=torch.ops.aten.slice.Tensor](args = (%transpose_2, 3, 0, 32), kwargs = {})"
              },
              {
                "key": "[metadata] pkg.torch.onnx.name_scopes",
                "value": "['', 'model', 'model.layers.0', 'model.layers.0.self_attn', 'slice_28']"
              },
              {
                "key": "[metadata] pkg.torch.onnx.stack_trace",
                "value": "File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 688, in forward\n    outputs: BaseModelOutputWithPast = self.model(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 453, in forward\n    layer_outputs = decoder_layer(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 308, in forward\n    hidden_states, self_attn_weights = self.self_attn(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 247, in forward\n    query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)"
              }
            ],
            "incomingEdges": [
              {
                "sourceNodeId": "node_Transpose_359",
                "sourceNodeOutputId": "0",
                "targetNodeInputId": "0"
              },
              {
                "sourceNodeId": "node_Constant_893",
                "sourceNodeOutputId": "0",
                "targetNodeInputId": "1"
              },
              {
                "sourceNodeId": "node_Constant_896",
                "sourceNodeOutputId": "0",
                "targetNodeInputId": "2"
              },
              {
                "sourceNodeId": "node_Constant_899",
                "sourceNodeOutputId": "0",
                "targetNodeInputId": "3"
              },
              {
                "sourceNodeId": "node_Constant_410",
                "sourceNodeOutputId": "0",
                "targetNodeInputId": "4"
              }
            ],
            "outputsMetadata": [
              {
                "id": "0",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "slice_28"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "FLOAT[s34,32,s16,32]"
                  },
                  {
                    "key": "param_name",
                    "value": "output"
                  }
                ]
              }
            ],
            "inputsMetadata": [
              {
                "id": "0",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "transpose_2"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "FLOAT[s34,32,s16,64]"
                  },
                  {
                    "key": "param_name",
                    "value": "data"
                  }
                ]
              },
              {
                "id": "1",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "val_298"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "INT64[1]"
                  },
                  {
                    "key": "param_name",
                    "value": "starts"
                  }
                ]
              },
              {
                "id": "2",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "val_301"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "INT64[1]"
                  },
                  {
                    "key": "param_name",
                    "value": "ends"
                  }
                ]
              },
              {
                "id": "3",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "val_304"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "INT64[1]"
                  },
                  {
                    "key": "param_name",
                    "value": "axes"
                  }
                ]
              },
              {
                "id": "4",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "val_305"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "INT64[1]"
                  },
                  {
                    "key": "param_name",
                    "value": "steps"
                  }
                ]
              }
            ],
            "style": null,
            "config": null
          },
          {
            "id": "node_Constant_902",
            "label": "Constant",
            "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.0: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.0.self_attn: transformers.models.llama.modeling_llama.LlamaAttention/slice_29: aten.slice.Tensor",
            "subgraphIds": [],
            "attrs": [
              {
                "key": "__value",
                "value": "[32]"
              },
              {
                "key": "value",
                "value": "TensorProtoTensor<INT64,[1]>(array([32]), name='val_308')"
              }
            ],
            "incomingEdges": [],
            "outputsMetadata": [
              {
                "id": "0",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "val_308"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "INT64[1]"
                  },
                  {
                    "key": "param_name",
                    "value": "output"
                  }
                ]
              }
            ],
            "inputsMetadata": [],
            "style": null,
            "config": null
          },
          {
            "id": "node_Constant_905",
            "label": "Constant",
            "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.0: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.0.self_attn: transformers.models.llama.modeling_llama.LlamaAttention/slice_29: aten.slice.Tensor",
            "subgraphIds": [],
            "attrs": [
              {
                "key": "__value",
                "value": "[9223372036854775807]"
              },
              {
                "key": "value",
                "value": "TensorProtoTensor<INT64,[1]>(array([9223372036854775807]), name='val_311')"
              }
            ],
            "incomingEdges": [],
            "outputsMetadata": [
              {
                "id": "0",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "val_311"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "INT64[1]"
                  },
                  {
                    "key": "param_name",
                    "value": "output"
                  }
                ]
              }
            ],
            "inputsMetadata": [],
            "style": null,
            "config": null
          },
          {
            "id": "node_Constant_908",
            "label": "Constant",
            "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.0: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.0.self_attn: transformers.models.llama.modeling_llama.LlamaAttention/slice_29: aten.slice.Tensor",
            "subgraphIds": [],
            "attrs": [
              {
                "key": "__value",
                "value": "[3]"
              },
              {
                "key": "value",
                "value": "TensorProtoTensor<INT64,[1]>(array([3]), name='val_314')"
              }
            ],
            "incomingEdges": [],
            "outputsMetadata": [
              {
                "id": "0",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "val_314"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "INT64[1]"
                  },
                  {
                    "key": "param_name",
                    "value": "output"
                  }
                ]
              }
            ],
            "inputsMetadata": [],
            "style": null,
            "config": null
          },
          {
            "id": "node_Constant_421",
            "label": "Constant",
            "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.0: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.0.self_attn: transformers.models.llama.modeling_llama.LlamaAttention/slice_29: aten.slice.Tensor",
            "subgraphIds": [],
            "attrs": [
              {
                "key": "__value",
                "value": "[1]"
              },
              {
                "key": "value_ints",
                "value": "[1]"
              },
              {
                "key": "[metadata] namespace",
                "value": ": transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.0: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.0.self_attn: transformers.models.llama.modeling_llama.LlamaAttention/slice_29: aten.slice.Tensor"
              },
              {
                "key": "[metadata] pkg.torch.onnx.class_hierarchy",
                "value": "['transformers.models.llama.modeling_llama.LlamaForCausalLM', 'transformers.models.llama.modeling_llama.LlamaModel', 'transformers.models.llama.modeling_llama.LlamaDecoderLayer', 'transformers.models.llama.modeling_llama.LlamaAttention', 'aten.slice.Tensor']"
              },
              {
                "key": "[metadata] pkg.torch.onnx.fx_node",
                "value": "%slice_29 : [num_users=1] = call_function[target=torch.ops.aten.slice.Tensor](args = (%transpose_2, 3, 32, 9223372036854775807), kwargs = {})"
              },
              {
                "key": "[metadata] pkg.torch.onnx.name_scopes",
                "value": "['', 'model', 'model.layers.0', 'model.layers.0.self_attn', 'slice_29']"
              },
              {
                "key": "[metadata] pkg.torch.onnx.stack_trace",
                "value": "File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 688, in forward\n    outputs: BaseModelOutputWithPast = self.model(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 453, in forward\n    layer_outputs = decoder_layer(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 308, in forward\n    hidden_states, self_attn_weights = self.self_attn(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 247, in forward\n    query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)"
              }
            ],
            "incomingEdges": [],
            "outputsMetadata": [
              {
                "id": "0",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "val_315"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "INT64[1]"
                  },
                  {
                    "key": "param_name",
                    "value": "output"
                  }
                ]
              }
            ],
            "inputsMetadata": [],
            "style": null,
            "config": null
          },
          {
            "id": "node_Slice_422",
            "label": "Slice",
            "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.0: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.0.self_attn: transformers.models.llama.modeling_llama.LlamaAttention/slice_29: aten.slice.Tensor",
            "subgraphIds": [],
            "attrs": [
              {
                "key": "[metadata] namespace",
                "value": ": transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.0: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.0.self_attn: transformers.models.llama.modeling_llama.LlamaAttention/slice_29: aten.slice.Tensor"
              },
              {
                "key": "[metadata] pkg.torch.onnx.class_hierarchy",
                "value": "['transformers.models.llama.modeling_llama.LlamaForCausalLM', 'transformers.models.llama.modeling_llama.LlamaModel', 'transformers.models.llama.modeling_llama.LlamaDecoderLayer', 'transformers.models.llama.modeling_llama.LlamaAttention', 'aten.slice.Tensor']"
              },
              {
                "key": "[metadata] pkg.torch.onnx.fx_node",
                "value": "%slice_29 : [num_users=1] = call_function[target=torch.ops.aten.slice.Tensor](args = (%transpose_2, 3, 32, 9223372036854775807), kwargs = {})"
              },
              {
                "key": "[metadata] pkg.torch.onnx.name_scopes",
                "value": "['', 'model', 'model.layers.0', 'model.layers.0.self_attn', 'slice_29']"
              },
              {
                "key": "[metadata] pkg.torch.onnx.stack_trace",
                "value": "File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 688, in forward\n    outputs: BaseModelOutputWithPast = self.model(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 453, in forward\n    layer_outputs = decoder_layer(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 308, in forward\n    hidden_states, self_attn_weights = self.self_attn(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 247, in forward\n    query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)"
              }
            ],
            "incomingEdges": [
              {
                "sourceNodeId": "node_Transpose_359",
                "sourceNodeOutputId": "0",
                "targetNodeInputId": "0"
              },
              {
                "sourceNodeId": "node_Constant_902",
                "sourceNodeOutputId": "0",
                "targetNodeInputId": "1"
              },
              {
                "sourceNodeId": "node_Constant_905",
                "sourceNodeOutputId": "0",
                "targetNodeInputId": "2"
              },
              {
                "sourceNodeId": "node_Constant_908",
                "sourceNodeOutputId": "0",
                "targetNodeInputId": "3"
              },
              {
                "sourceNodeId": "node_Constant_421",
                "sourceNodeOutputId": "0",
                "targetNodeInputId": "4"
              }
            ],
            "outputsMetadata": [
              {
                "id": "0",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "slice_29"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "FLOAT[s34,32,s16,32]"
                  },
                  {
                    "key": "param_name",
                    "value": "output"
                  }
                ]
              }
            ],
            "inputsMetadata": [
              {
                "id": "0",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "transpose_2"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "FLOAT[s34,32,s16,64]"
                  },
                  {
                    "key": "param_name",
                    "value": "data"
                  }
                ]
              },
              {
                "id": "1",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "val_308"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "INT64[1]"
                  },
                  {
                    "key": "param_name",
                    "value": "starts"
                  }
                ]
              },
              {
                "id": "2",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "val_311"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "INT64[1]"
                  },
                  {
                    "key": "param_name",
                    "value": "ends"
                  }
                ]
              },
              {
                "id": "3",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "val_314"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "INT64[1]"
                  },
                  {
                    "key": "param_name",
                    "value": "axes"
                  }
                ]
              },
              {
                "id": "4",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "val_315"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "INT64[1]"
                  },
                  {
                    "key": "param_name",
                    "value": "steps"
                  }
                ]
              }
            ],
            "style": null,
            "config": null
          },
          {
            "id": "node_Neg_423",
            "label": "Neg",
            "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.0: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.0.self_attn: transformers.models.llama.modeling_llama.LlamaAttention/neg_1: aten.neg.default",
            "subgraphIds": [],
            "attrs": [
              {
                "key": "[metadata] namespace",
                "value": ": transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.0: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.0.self_attn: transformers.models.llama.modeling_llama.LlamaAttention/neg_1: aten.neg.default"
              },
              {
                "key": "[metadata] pkg.torch.onnx.class_hierarchy",
                "value": "['transformers.models.llama.modeling_llama.LlamaForCausalLM', 'transformers.models.llama.modeling_llama.LlamaModel', 'transformers.models.llama.modeling_llama.LlamaDecoderLayer', 'transformers.models.llama.modeling_llama.LlamaAttention', 'aten.neg.default']"
              },
              {
                "key": "[metadata] pkg.torch.onnx.fx_node",
                "value": "%neg_1 : [num_users=1] = call_function[target=torch.ops.aten.neg.default](args = (%slice_29,), kwargs = {})"
              },
              {
                "key": "[metadata] pkg.torch.onnx.name_scopes",
                "value": "['', 'model', 'model.layers.0', 'model.layers.0.self_attn', 'neg_1']"
              },
              {
                "key": "[metadata] pkg.torch.onnx.stack_trace",
                "value": "File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 688, in forward\n    outputs: BaseModelOutputWithPast = self.model(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 453, in forward\n    layer_outputs = decoder_layer(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 308, in forward\n    hidden_states, self_attn_weights = self.self_attn(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 247, in forward\n    query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)"
              }
            ],
            "incomingEdges": [
              {
                "sourceNodeId": "node_Slice_422",
                "sourceNodeOutputId": "0",
                "targetNodeInputId": "0"
              }
            ],
            "outputsMetadata": [
              {
                "id": "0",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "neg_1"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "FLOAT[s34,32,s16,32]"
                  },
                  {
                    "key": "param_name",
                    "value": "Y"
                  }
                ]
              }
            ],
            "inputsMetadata": [
              {
                "id": "0",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "slice_29"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "FLOAT[s34,32,s16,32]"
                  },
                  {
                    "key": "param_name",
                    "value": "X"
                  }
                ]
              }
            ],
            "style": null,
            "config": null
          },
          {
            "id": "node_Concat_424",
            "label": "Concat",
            "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.0: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.0.self_attn: transformers.models.llama.modeling_llama.LlamaAttention/cat_2: aten.cat.default",
            "subgraphIds": [],
            "attrs": [
              {
                "key": "axis",
                "value": "-1"
              },
              {
                "key": "[metadata] namespace",
                "value": ": transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.0: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.0.self_attn: transformers.models.llama.modeling_llama.LlamaAttention/cat_2: aten.cat.default"
              },
              {
                "key": "[metadata] pkg.torch.onnx.class_hierarchy",
                "value": "['transformers.models.llama.modeling_llama.LlamaForCausalLM', 'transformers.models.llama.modeling_llama.LlamaModel', 'transformers.models.llama.modeling_llama.LlamaDecoderLayer', 'transformers.models.llama.modeling_llama.LlamaAttention', 'aten.cat.default']"
              },
              {
                "key": "[metadata] pkg.torch.onnx.fx_node",
                "value": "%cat_2 : [num_users=1] = call_function[target=torch.ops.aten.cat.default](args = ([%neg_1, %slice_28], -1), kwargs = {})"
              },
              {
                "key": "[metadata] pkg.torch.onnx.name_scopes",
                "value": "['', 'model', 'model.layers.0', 'model.layers.0.self_attn', 'cat_2']"
              },
              {
                "key": "[metadata] pkg.torch.onnx.stack_trace",
                "value": "File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 688, in forward\n    outputs: BaseModelOutputWithPast = self.model(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 453, in forward\n    layer_outputs = decoder_layer(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 308, in forward\n    hidden_states, self_attn_weights = self.self_attn(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 247, in forward\n    query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)"
              }
            ],
            "incomingEdges": [
              {
                "sourceNodeId": "node_Neg_423",
                "sourceNodeOutputId": "0",
                "targetNodeInputId": "0"
              },
              {
                "sourceNodeId": "node_Slice_411",
                "sourceNodeOutputId": "0",
                "targetNodeInputId": "1"
              }
            ],
            "outputsMetadata": [
              {
                "id": "0",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "cat_2"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "FLOAT[s34,32,s16,64]"
                  },
                  {
                    "key": "param_name",
                    "value": "concat_result"
                  }
                ]
              }
            ],
            "inputsMetadata": [
              {
                "id": "0",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "neg_1"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "FLOAT[s34,32,s16,32]"
                  },
                  {
                    "key": "param_name",
                    "value": "inputs"
                  }
                ]
              },
              {
                "id": "1",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "slice_28"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "FLOAT[s34,32,s16,32]"
                  },
                  {
                    "key": "param_name",
                    "value": "inputs"
                  }
                ]
              }
            ],
            "style": null,
            "config": null
          },
          {
            "id": "node_Mul_425",
            "label": "Mul",
            "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.0: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.0.self_attn: transformers.models.llama.modeling_llama.LlamaAttention/mul_254: aten.mul.Tensor",
            "subgraphIds": [],
            "attrs": [
              {
                "key": "[metadata] namespace",
                "value": ": transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.0: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.0.self_attn: transformers.models.llama.modeling_llama.LlamaAttention/mul_254: aten.mul.Tensor"
              },
              {
                "key": "[metadata] pkg.torch.onnx.class_hierarchy",
                "value": "['transformers.models.llama.modeling_llama.LlamaForCausalLM', 'transformers.models.llama.modeling_llama.LlamaModel', 'transformers.models.llama.modeling_llama.LlamaDecoderLayer', 'transformers.models.llama.modeling_llama.LlamaAttention', 'aten.mul.Tensor']"
              },
              {
                "key": "[metadata] pkg.torch.onnx.fx_node",
                "value": "%mul_254 : [num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%cat_2, %unsqueeze_11), kwargs = {})"
              },
              {
                "key": "[metadata] pkg.torch.onnx.name_scopes",
                "value": "['', 'model', 'model.layers.0', 'model.layers.0.self_attn', 'mul_254']"
              },
              {
                "key": "[metadata] pkg.torch.onnx.stack_trace",
                "value": "File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 688, in forward\n    outputs: BaseModelOutputWithPast = self.model(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 453, in forward\n    layer_outputs = decoder_layer(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 308, in forward\n    hidden_states, self_attn_weights = self.self_attn(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 247, in forward\n    query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)"
              }
            ],
            "incomingEdges": [
              {
                "sourceNodeId": "node_Concat_424",
                "sourceNodeOutputId": "0",
                "targetNodeInputId": "0"
              },
              {
                "sourceNodeId": "node_Unsqueeze_371",
                "sourceNodeOutputId": "0",
                "targetNodeInputId": "1"
              }
            ],
            "outputsMetadata": [
              {
                "id": "0",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "mul_254"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "FLOAT[s34,32,s16,64]"
                  },
                  {
                    "key": "param_name",
                    "value": "C"
                  }
                ]
              }
            ],
            "inputsMetadata": [
              {
                "id": "0",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "cat_2"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "FLOAT[s34,32,s16,64]"
                  },
                  {
                    "key": "param_name",
                    "value": "A"
                  }
                ]
              },
              {
                "id": "1",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "unsqueeze_11"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "FLOAT[1,1,s16,64]"
                  },
                  {
                    "key": "param_name",
                    "value": "B"
                  }
                ]
              }
            ],
            "style": null,
            "config": null
          },
          {
            "id": "node_Add_426",
            "label": "Add",
            "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.0: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.0.self_attn: transformers.models.llama.modeling_llama.LlamaAttention/add_323: aten.add.Tensor",
            "subgraphIds": [],
            "attrs": [
              {
                "key": "[metadata] namespace",
                "value": ": transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.0: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.0.self_attn: transformers.models.llama.modeling_llama.LlamaAttention/add_323: aten.add.Tensor"
              },
              {
                "key": "[metadata] pkg.torch.onnx.class_hierarchy",
                "value": "['transformers.models.llama.modeling_llama.LlamaForCausalLM', 'transformers.models.llama.modeling_llama.LlamaModel', 'transformers.models.llama.modeling_llama.LlamaDecoderLayer', 'transformers.models.llama.modeling_llama.LlamaAttention', 'aten.add.Tensor']"
              },
              {
                "key": "[metadata] pkg.torch.onnx.fx_node",
                "value": "%add_323 : [num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%mul_237, %mul_254), kwargs = {})"
              },
              {
                "key": "[metadata] pkg.torch.onnx.name_scopes",
                "value": "['', 'model', 'model.layers.0', 'model.layers.0.self_attn', 'add_323']"
              },
              {
                "key": "[metadata] pkg.torch.onnx.stack_trace",
                "value": "File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 688, in forward\n    outputs: BaseModelOutputWithPast = self.model(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 453, in forward\n    layer_outputs = decoder_layer(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 308, in forward\n    hidden_states, self_attn_weights = self.self_attn(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 247, in forward\n    query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)"
              }
            ],
            "incomingEdges": [
              {
                "sourceNodeId": "node_Mul_400",
                "sourceNodeOutputId": "0",
                "targetNodeInputId": "0"
              },
              {
                "sourceNodeId": "node_Mul_425",
                "sourceNodeOutputId": "0",
                "targetNodeInputId": "1"
              }
            ],
            "outputsMetadata": [
              {
                "id": "0",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "add_323"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "FLOAT[s34,32,s16,64]"
                  },
                  {
                    "key": "param_name",
                    "value": "C"
                  }
                ]
              }
            ],
            "inputsMetadata": [
              {
                "id": "0",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "mul_237"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "FLOAT[s34,32,s16,64]"
                  },
                  {
                    "key": "param_name",
                    "value": "A"
                  }
                ]
              },
              {
                "id": "1",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "mul_254"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "FLOAT[s34,32,s16,64]"
                  },
                  {
                    "key": "param_name",
                    "value": "B"
                  }
                ]
              }
            ],
            "style": null,
            "config": null
          },
          {
            "id": "node_Concat_427",
            "label": "Concat",
            "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.0: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.0.self_attn: transformers.models.llama.modeling_llama.LlamaAttention/cat_3: aten.cat.default",
            "subgraphIds": [],
            "attrs": [
              {
                "key": "axis",
                "value": "-2"
              },
              {
                "key": "[metadata] namespace",
                "value": ": transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.0: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.0.self_attn: transformers.models.llama.modeling_llama.LlamaAttention/cat_3: aten.cat.default"
              },
              {
                "key": "[metadata] pkg.torch.onnx.class_hierarchy",
                "value": "['transformers.models.llama.modeling_llama.LlamaForCausalLM', 'transformers.models.llama.modeling_llama.LlamaModel', 'transformers.models.llama.modeling_llama.LlamaDecoderLayer', 'transformers.models.llama.modeling_llama.LlamaAttention', 'aten.cat.default']"
              },
              {
                "key": "[metadata] pkg.torch.onnx.fx_node",
                "value": "%cat_3 : [num_users=2] = call_function[target=torch.ops.aten.cat.default](args = ([%past_key_values_key_cache_0, %add_323], -2), kwargs = {})"
              },
              {
                "key": "[metadata] pkg.torch.onnx.name_scopes",
                "value": "['', 'model', 'model.layers.0', 'model.layers.0.self_attn', 'cat_3']"
              },
              {
                "key": "[metadata] pkg.torch.onnx.stack_trace",
                "value": "File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 688, in forward\n    outputs: BaseModelOutputWithPast = self.model(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 453, in forward\n    layer_outputs = decoder_layer(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 308, in forward\n    hidden_states, self_attn_weights = self.self_attn(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 252, in forward\n    key_states, value_states = past_key_value.update(key_states, value_states, self.layer_idx, cache_kwargs)"
              }
            ],
            "incomingEdges": [
              {
                "sourceNodeId": "[value] past_key_values_key_cache_0",
                "sourceNodeOutputId": "0",
                "targetNodeInputId": "0"
              },
              {
                "sourceNodeId": "node_Add_426",
                "sourceNodeOutputId": "0",
                "targetNodeInputId": "1"
              }
            ],
            "outputsMetadata": [
              {
                "id": "0",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "cat_3"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "FLOAT[s34,32,s16 + s17,64]"
                  },
                  {
                    "key": "[metadata] pkg.torch.export.graph_signature.OutputSpec.kind",
                    "value": "USER_OUTPUT"
                  },
                  {
                    "key": "param_name",
                    "value": "concat_result"
                  }
                ]
              }
            ],
            "inputsMetadata": [
              {
                "id": "0",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "past_key_values_key_cache_0"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "FLOAT[s34,32,s17,64]"
                  },
                  {
                    "key": "[metadata] pkg.torch.export.graph_signature.InputSpec.kind",
                    "value": "USER_INPUT"
                  },
                  {
                    "key": "[metadata] pkg.torch.export.graph_signature.InputSpec.persistent",
                    "value": "None"
                  },
                  {
                    "key": "param_name",
                    "value": "inputs"
                  }
                ]
              },
              {
                "id": "1",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "add_323"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "FLOAT[s34,32,s16,64]"
                  },
                  {
                    "key": "param_name",
                    "value": "inputs"
                  }
                ]
              }
            ],
            "style": null,
            "config": null
          },
          {
            "id": "node_Concat_428",
            "label": "Concat",
            "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.0: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.0.self_attn: transformers.models.llama.modeling_llama.LlamaAttention/cat_4: aten.cat.default",
            "subgraphIds": [],
            "attrs": [
              {
                "key": "axis",
                "value": "-2"
              },
              {
                "key": "[metadata] namespace",
                "value": ": transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.0: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.0.self_attn: transformers.models.llama.modeling_llama.LlamaAttention/cat_4: aten.cat.default"
              },
              {
                "key": "[metadata] pkg.torch.onnx.class_hierarchy",
                "value": "['transformers.models.llama.modeling_llama.LlamaForCausalLM', 'transformers.models.llama.modeling_llama.LlamaModel', 'transformers.models.llama.modeling_llama.LlamaDecoderLayer', 'transformers.models.llama.modeling_llama.LlamaAttention', 'aten.cat.default']"
              },
              {
                "key": "[metadata] pkg.torch.onnx.fx_node",
                "value": "%cat_4 : [num_users=2] = call_function[target=torch.ops.aten.cat.default](args = ([%past_key_values_value_cache_0, %transpose_3], -2), kwargs = {})"
              },
              {
                "key": "[metadata] pkg.torch.onnx.name_scopes",
                "value": "['', 'model', 'model.layers.0', 'model.layers.0.self_attn', 'cat_4']"
              },
              {
                "key": "[metadata] pkg.torch.onnx.stack_trace",
                "value": "File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 688, in forward\n    outputs: BaseModelOutputWithPast = self.model(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 453, in forward\n    layer_outputs = decoder_layer(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 308, in forward\n    hidden_states, self_attn_weights = self.self_attn(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 252, in forward\n    key_states, value_states = past_key_value.update(key_states, value_states, self.layer_idx, cache_kwargs)"
              }
            ],
            "incomingEdges": [
              {
                "sourceNodeId": "[value] past_key_values_value_cache_0",
                "sourceNodeOutputId": "0",
                "targetNodeInputId": "0"
              },
              {
                "sourceNodeId": "node_Transpose_369",
                "sourceNodeOutputId": "0",
                "targetNodeInputId": "1"
              }
            ],
            "outputsMetadata": [
              {
                "id": "0",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "cat_4"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "FLOAT[s34,32,s16 + s17,64]"
                  },
                  {
                    "key": "[metadata] pkg.torch.export.graph_signature.OutputSpec.kind",
                    "value": "USER_OUTPUT"
                  },
                  {
                    "key": "param_name",
                    "value": "concat_result"
                  }
                ]
              }
            ],
            "inputsMetadata": [
              {
                "id": "0",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "past_key_values_value_cache_0"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "FLOAT[s34,32,s17,64]"
                  },
                  {
                    "key": "[metadata] pkg.torch.export.graph_signature.InputSpec.kind",
                    "value": "USER_INPUT"
                  },
                  {
                    "key": "[metadata] pkg.torch.export.graph_signature.InputSpec.persistent",
                    "value": "None"
                  },
                  {
                    "key": "param_name",
                    "value": "inputs"
                  }
                ]
              },
              {
                "id": "1",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "transpose_3"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "FLOAT[s34,32,s16,64]"
                  },
                  {
                    "key": "param_name",
                    "value": "inputs"
                  }
                ]
              }
            ],
            "style": null,
            "config": null
          },
          {
            "id": "node_Transpose_429",
            "label": "Transpose",
            "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.0: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.0.self_attn: transformers.models.llama.modeling_llama.LlamaAttention/transpose_4: aten.transpose.int",
            "subgraphIds": [],
            "attrs": [
              {
                "key": "perm",
                "value": "[0, 1, 3, 2]"
              },
              {
                "key": "[metadata] namespace",
                "value": ": transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.0: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.0.self_attn: transformers.models.llama.modeling_llama.LlamaAttention/transpose_4: aten.transpose.int"
              },
              {
                "key": "[metadata] pkg.torch.onnx.class_hierarchy",
                "value": "['transformers.models.llama.modeling_llama.LlamaForCausalLM', 'transformers.models.llama.modeling_llama.LlamaModel', 'transformers.models.llama.modeling_llama.LlamaDecoderLayer', 'transformers.models.llama.modeling_llama.LlamaAttention', 'aten.transpose.int']"
              },
              {
                "key": "[metadata] pkg.torch.onnx.fx_node",
                "value": "%transpose_4 : [num_users=1] = call_function[target=torch.ops.aten.transpose.int](args = (%cat_3, 2, 3), kwargs = {})"
              },
              {
                "key": "[metadata] pkg.torch.onnx.name_scopes",
                "value": "['', 'model', 'model.layers.0', 'model.layers.0.self_attn', 'transpose_4']"
              },
              {
                "key": "[metadata] pkg.torch.onnx.stack_trace",
                "value": "File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 688, in forward\n    outputs: BaseModelOutputWithPast = self.model(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 453, in forward\n    layer_outputs = decoder_layer(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 308, in forward\n    hidden_states, self_attn_weights = self.self_attn(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 265, in forward\n    attn_output, attn_weights = attention_interface("
              }
            ],
            "incomingEdges": [
              {
                "sourceNodeId": "node_Concat_427",
                "sourceNodeOutputId": "0",
                "targetNodeInputId": "0"
              }
            ],
            "outputsMetadata": [
              {
                "id": "0",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "transpose_4"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "FLOAT[s34,32,64,s16 + s17]"
                  },
                  {
                    "key": "param_name",
                    "value": "transposed"
                  }
                ]
              }
            ],
            "inputsMetadata": [
              {
                "id": "0",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "cat_3"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "FLOAT[s34,32,s16 + s17,64]"
                  },
                  {
                    "key": "[metadata] pkg.torch.export.graph_signature.OutputSpec.kind",
                    "value": "USER_OUTPUT"
                  },
                  {
                    "key": "param_name",
                    "value": "data"
                  }
                ]
              }
            ],
            "style": null,
            "config": null
          },
          {
            "id": "node_MatMul_430",
            "label": "MatMul",
            "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.0: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.0.self_attn: transformers.models.llama.modeling_llama.LlamaAttention/matmul_1: aten.matmul.default",
            "subgraphIds": [],
            "attrs": [
              {
                "key": "[metadata] namespace",
                "value": ": transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.0: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.0.self_attn: transformers.models.llama.modeling_llama.LlamaAttention/matmul_1: aten.matmul.default"
              },
              {
                "key": "[metadata] pkg.torch.onnx.class_hierarchy",
                "value": "['transformers.models.llama.modeling_llama.LlamaForCausalLM', 'transformers.models.llama.modeling_llama.LlamaModel', 'transformers.models.llama.modeling_llama.LlamaDecoderLayer', 'transformers.models.llama.modeling_llama.LlamaAttention', 'aten.matmul.default']"
              },
              {
                "key": "[metadata] pkg.torch.onnx.fx_node",
                "value": "%matmul_1 : [num_users=1] = call_function[target=torch.ops.aten.matmul.default](args = (%add_287, %transpose_4), kwargs = {})"
              },
              {
                "key": "[metadata] pkg.torch.onnx.name_scopes",
                "value": "['', 'model', 'model.layers.0', 'model.layers.0.self_attn', 'matmul_1']"
              },
              {
                "key": "[metadata] pkg.torch.onnx.stack_trace",
                "value": "File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 688, in forward\n    outputs: BaseModelOutputWithPast = self.model(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 453, in forward\n    layer_outputs = decoder_layer(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 308, in forward\n    hidden_states, self_attn_weights = self.self_attn(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 265, in forward\n    attn_output, attn_weights = attention_interface("
              }
            ],
            "incomingEdges": [
              {
                "sourceNodeId": "node_Add_399",
                "sourceNodeOutputId": "0",
                "targetNodeInputId": "0"
              },
              {
                "sourceNodeId": "node_Transpose_429",
                "sourceNodeOutputId": "0",
                "targetNodeInputId": "1"
              }
            ],
            "outputsMetadata": [
              {
                "id": "0",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "matmul_1"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "FLOAT[s34,32,s16,s16 + s17]"
                  },
                  {
                    "key": "param_name",
                    "value": "Y"
                  }
                ]
              }
            ],
            "inputsMetadata": [
              {
                "id": "0",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "add_287"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "FLOAT[s34,32,s16,64]"
                  },
                  {
                    "key": "param_name",
                    "value": "A"
                  }
                ]
              },
              {
                "id": "1",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "transpose_4"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "FLOAT[s34,32,64,s16 + s17]"
                  },
                  {
                    "key": "param_name",
                    "value": "B"
                  }
                ]
              }
            ],
            "style": null,
            "config": null
          },
          {
            "id": "node_Constant_431",
            "label": "Constant",
            "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel",
            "subgraphIds": [],
            "attrs": [
              {
                "key": "__value",
                "value": "0.125"
              },
              {
                "key": "value",
                "value": "TensorProtoTensor<FLOAT,[]>(array(0.125, dtype=float32), name='')"
              },
              {
                "key": "[metadata] namespace",
                "value": ": transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.0: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.0.self_attn: transformers.models.llama.modeling_llama.LlamaAttention/mul_278: aten.mul.Tensor"
              },
              {
                "key": "[metadata] pkg.torch.onnx.class_hierarchy",
                "value": "['transformers.models.llama.modeling_llama.LlamaForCausalLM', 'transformers.models.llama.modeling_llama.LlamaModel', 'transformers.models.llama.modeling_llama.LlamaDecoderLayer', 'transformers.models.llama.modeling_llama.LlamaAttention', 'aten.mul.Tensor']"
              },
              {
                "key": "[metadata] pkg.torch.onnx.fx_node",
                "value": "%mul_278 : [num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%matmul_1, 0.125), kwargs = {})"
              },
              {
                "key": "[metadata] pkg.torch.onnx.name_scopes",
                "value": "['', 'model', 'model.layers.0', 'model.layers.0.self_attn', 'mul_278']"
              },
              {
                "key": "[metadata] pkg.torch.onnx.stack_trace",
                "value": "File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 688, in forward\n    outputs: BaseModelOutputWithPast = self.model(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 453, in forward\n    layer_outputs = decoder_layer(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 308, in forward\n    hidden_states, self_attn_weights = self.self_attn(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 265, in forward\n    attn_output, attn_weights = attention_interface("
              }
            ],
            "incomingEdges": [],
            "outputsMetadata": [
              {
                "id": "0",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "val_316"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "FLOAT[]"
                  },
                  {
                    "key": "param_name",
                    "value": "output"
                  }
                ]
              }
            ],
            "inputsMetadata": [],
            "style": null,
            "config": null
          },
          {
            "id": "node_Mul_432",
            "label": "Mul",
            "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.0: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.0.self_attn: transformers.models.llama.modeling_llama.LlamaAttention/mul_278: aten.mul.Tensor",
            "subgraphIds": [],
            "attrs": [
              {
                "key": "[metadata] namespace",
                "value": ": transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.0: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.0.self_attn: transformers.models.llama.modeling_llama.LlamaAttention/mul_278: aten.mul.Tensor"
              },
              {
                "key": "[metadata] pkg.torch.onnx.class_hierarchy",
                "value": "['transformers.models.llama.modeling_llama.LlamaForCausalLM', 'transformers.models.llama.modeling_llama.LlamaModel', 'transformers.models.llama.modeling_llama.LlamaDecoderLayer', 'transformers.models.llama.modeling_llama.LlamaAttention', 'aten.mul.Tensor']"
              },
              {
                "key": "[metadata] pkg.torch.onnx.fx_node",
                "value": "%mul_278 : [num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%matmul_1, 0.125), kwargs = {})"
              },
              {
                "key": "[metadata] pkg.torch.onnx.name_scopes",
                "value": "['', 'model', 'model.layers.0', 'model.layers.0.self_attn', 'mul_278']"
              },
              {
                "key": "[metadata] pkg.torch.onnx.stack_trace",
                "value": "File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 688, in forward\n    outputs: BaseModelOutputWithPast = self.model(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 453, in forward\n    layer_outputs = decoder_layer(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 308, in forward\n    hidden_states, self_attn_weights = self.self_attn(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 265, in forward\n    attn_output, attn_weights = attention_interface("
              }
            ],
            "incomingEdges": [
              {
                "sourceNodeId": "node_MatMul_430",
                "sourceNodeOutputId": "0",
                "targetNodeInputId": "0"
              },
              {
                "sourceNodeId": "node_Constant_431",
                "sourceNodeOutputId": "0",
                "targetNodeInputId": "1"
              }
            ],
            "outputsMetadata": [
              {
                "id": "0",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "mul_278"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "FLOAT[s34,32,s16,s16 + s17]"
                  },
                  {
                    "key": "param_name",
                    "value": "C"
                  }
                ]
              }
            ],
            "inputsMetadata": [
              {
                "id": "0",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "matmul_1"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "FLOAT[s34,32,s16,s16 + s17]"
                  },
                  {
                    "key": "param_name",
                    "value": "A"
                  }
                ]
              },
              {
                "id": "1",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "val_316"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "FLOAT[]"
                  },
                  {
                    "key": "param_name",
                    "value": "B"
                  }
                ]
              }
            ],
            "style": null,
            "config": null
          },
          {
            "id": "node_Constant_454",
            "label": "Constant",
            "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.0: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.0.self_attn: transformers.models.llama.modeling_llama.LlamaAttention/slice_37: aten.slice.Tensor",
            "subgraphIds": [],
            "attrs": [
              {
                "key": "__value",
                "value": "[0]"
              },
              {
                "key": "value_ints",
                "value": "[0]"
              },
              {
                "key": "[metadata] namespace",
                "value": ": transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.0: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.0.self_attn: transformers.models.llama.modeling_llama.LlamaAttention/slice_37: aten.slice.Tensor"
              },
              {
                "key": "[metadata] pkg.torch.onnx.class_hierarchy",
                "value": "['transformers.models.llama.modeling_llama.LlamaForCausalLM', 'transformers.models.llama.modeling_llama.LlamaModel', 'transformers.models.llama.modeling_llama.LlamaDecoderLayer', 'transformers.models.llama.modeling_llama.LlamaAttention', 'aten.slice.Tensor']"
              },
              {
                "key": "[metadata] pkg.torch.onnx.fx_node",
                "value": "%slice_37 : [num_users=1] = call_function[target=torch.ops.aten.slice.Tensor](args = (%slice_36, 3, None, %add_4), kwargs = {})"
              },
              {
                "key": "[metadata] pkg.torch.onnx.name_scopes",
                "value": "['', 'model', 'model.layers.0', 'model.layers.0.self_attn', 'slice_37']"
              },
              {
                "key": "[metadata] pkg.torch.onnx.stack_trace",
                "value": "File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 688, in forward\n    outputs: BaseModelOutputWithPast = self.model(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 453, in forward\n    layer_outputs = decoder_layer(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 308, in forward\n    hidden_states, self_attn_weights = self.self_attn(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 265, in forward\n    attn_output, attn_weights = attention_interface("
              }
            ],
            "incomingEdges": [],
            "outputsMetadata": [
              {
                "id": "0",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "val_335"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "INT64[1]"
                  },
                  {
                    "key": "param_name",
                    "value": "output"
                  }
                ]
              }
            ],
            "inputsMetadata": [],
            "style": null,
            "config": null
          },
          {
            "id": "node_Constant_456",
            "label": "Constant",
            "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.0: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.0.self_attn: transformers.models.llama.modeling_llama.LlamaAttention/slice_37: aten.slice.Tensor",
            "subgraphIds": [],
            "attrs": [
              {
                "key": "__value",
                "value": "[-1]"
              },
              {
                "key": "value_ints",
                "value": "[-1]"
              },
              {
                "key": "[metadata] namespace",
                "value": ": transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.0: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.0.self_attn: transformers.models.llama.modeling_llama.LlamaAttention/slice_37: aten.slice.Tensor"
              },
              {
                "key": "[metadata] pkg.torch.onnx.class_hierarchy",
                "value": "['transformers.models.llama.modeling_llama.LlamaForCausalLM', 'transformers.models.llama.modeling_llama.LlamaModel', 'transformers.models.llama.modeling_llama.LlamaDecoderLayer', 'transformers.models.llama.modeling_llama.LlamaAttention', 'aten.slice.Tensor']"
              },
              {
                "key": "[metadata] pkg.torch.onnx.fx_node",
                "value": "%slice_37 : [num_users=1] = call_function[target=torch.ops.aten.slice.Tensor](args = (%slice_36, 3, None, %add_4), kwargs = {})"
              },
              {
                "key": "[metadata] pkg.torch.onnx.name_scopes",
                "value": "['', 'model', 'model.layers.0', 'model.layers.0.self_attn', 'slice_37']"
              },
              {
                "key": "[metadata] pkg.torch.onnx.stack_trace",
                "value": "File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 688, in forward\n    outputs: BaseModelOutputWithPast = self.model(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 453, in forward\n    layer_outputs = decoder_layer(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 308, in forward\n    hidden_states, self_attn_weights = self.self_attn(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 265, in forward\n    attn_output, attn_weights = attention_interface("
              }
            ],
            "incomingEdges": [],
            "outputsMetadata": [
              {
                "id": "0",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "val_337"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "INT64[1]"
                  },
                  {
                    "key": "param_name",
                    "value": "output"
                  }
                ]
              }
            ],
            "inputsMetadata": [],
            "style": null,
            "config": null
          },
          {
            "id": "node_Reshape_457",
            "label": "Reshape",
            "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.0: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.0.self_attn: transformers.models.llama.modeling_llama.LlamaAttention/slice_37: aten.slice.Tensor",
            "subgraphIds": [],
            "attrs": [
              {
                "key": "allowzero",
                "value": "0"
              },
              {
                "key": "[metadata] namespace",
                "value": ": transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.0: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.0.self_attn: transformers.models.llama.modeling_llama.LlamaAttention/slice_37: aten.slice.Tensor"
              },
              {
                "key": "[metadata] pkg.torch.onnx.class_hierarchy",
                "value": "['transformers.models.llama.modeling_llama.LlamaForCausalLM', 'transformers.models.llama.modeling_llama.LlamaModel', 'transformers.models.llama.modeling_llama.LlamaDecoderLayer', 'transformers.models.llama.modeling_llama.LlamaAttention', 'aten.slice.Tensor']"
              },
              {
                "key": "[metadata] pkg.torch.onnx.fx_node",
                "value": "%slice_37 : [num_users=1] = call_function[target=torch.ops.aten.slice.Tensor](args = (%slice_36, 3, None, %add_4), kwargs = {})"
              },
              {
                "key": "[metadata] pkg.torch.onnx.name_scopes",
                "value": "['', 'model', 'model.layers.0', 'model.layers.0.self_attn', 'slice_37']"
              },
              {
                "key": "[metadata] pkg.torch.onnx.stack_trace",
                "value": "File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 688, in forward\n    outputs: BaseModelOutputWithPast = self.model(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 453, in forward\n    layer_outputs = decoder_layer(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 308, in forward\n    hidden_states, self_attn_weights = self.self_attn(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 265, in forward\n    attn_output, attn_weights = attention_interface("
              }
            ],
            "incomingEdges": [
              {
                "sourceNodeId": "node_Add_33",
                "sourceNodeOutputId": "0",
                "targetNodeInputId": "0"
              },
              {
                "sourceNodeId": "node_Constant_456",
                "sourceNodeOutputId": "0",
                "targetNodeInputId": "1"
              }
            ],
            "outputsMetadata": [
              {
                "id": "0",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "val_338"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "INT64[1]"
                  },
                  {
                    "key": "param_name",
                    "value": "reshaped"
                  }
                ]
              }
            ],
            "inputsMetadata": [
              {
                "id": "0",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "add_4"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "INT64[]"
                  },
                  {
                    "key": "param_name",
                    "value": "data"
                  }
                ]
              },
              {
                "id": "1",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "val_337"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "INT64[1]"
                  },
                  {
                    "key": "param_name",
                    "value": "shape"
                  }
                ]
              }
            ],
            "style": null,
            "config": null
          },
          {
            "id": "node_Constant_921",
            "label": "Constant",
            "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.0: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.0.self_attn: transformers.models.llama.modeling_llama.LlamaAttention/slice_37: aten.slice.Tensor",
            "subgraphIds": [],
            "attrs": [
              {
                "key": "__value",
                "value": "[3]"
              },
              {
                "key": "value",
                "value": "TensorProtoTensor<INT64,[1]>(array([3]), name='val_341')"
              }
            ],
            "incomingEdges": [],
            "outputsMetadata": [
              {
                "id": "0",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "val_341"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "INT64[1]"
                  },
                  {
                    "key": "param_name",
                    "value": "output"
                  }
                ]
              }
            ],
            "inputsMetadata": [],
            "style": null,
            "config": null
          },
          {
            "id": "node_Constant_461",
            "label": "Constant",
            "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.0: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.0.self_attn: transformers.models.llama.modeling_llama.LlamaAttention/slice_37: aten.slice.Tensor",
            "subgraphIds": [],
            "attrs": [
              {
                "key": "__value",
                "value": "[1]"
              },
              {
                "key": "value_ints",
                "value": "[1]"
              },
              {
                "key": "[metadata] namespace",
                "value": ": transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.0: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.0.self_attn: transformers.models.llama.modeling_llama.LlamaAttention/slice_37: aten.slice.Tensor"
              },
              {
                "key": "[metadata] pkg.torch.onnx.class_hierarchy",
                "value": "['transformers.models.llama.modeling_llama.LlamaForCausalLM', 'transformers.models.llama.modeling_llama.LlamaModel', 'transformers.models.llama.modeling_llama.LlamaDecoderLayer', 'transformers.models.llama.modeling_llama.LlamaAttention', 'aten.slice.Tensor']"
              },
              {
                "key": "[metadata] pkg.torch.onnx.fx_node",
                "value": "%slice_37 : [num_users=1] = call_function[target=torch.ops.aten.slice.Tensor](args = (%slice_36, 3, None, %add_4), kwargs = {})"
              },
              {
                "key": "[metadata] pkg.torch.onnx.name_scopes",
                "value": "['', 'model', 'model.layers.0', 'model.layers.0.self_attn', 'slice_37']"
              },
              {
                "key": "[metadata] pkg.torch.onnx.stack_trace",
                "value": "File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 688, in forward\n    outputs: BaseModelOutputWithPast = self.model(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 453, in forward\n    layer_outputs = decoder_layer(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 308, in forward\n    hidden_states, self_attn_weights = self.self_attn(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 265, in forward\n    attn_output, attn_weights = attention_interface("
              }
            ],
            "incomingEdges": [],
            "outputsMetadata": [
              {
                "id": "0",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "val_342"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "INT64[1]"
                  },
                  {
                    "key": "param_name",
                    "value": "output"
                  }
                ]
              }
            ],
            "inputsMetadata": [],
            "style": null,
            "config": null
          },
          {
            "id": "node_Slice_462",
            "label": "Slice",
            "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.0: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.0.self_attn: transformers.models.llama.modeling_llama.LlamaAttention/slice_37: aten.slice.Tensor",
            "subgraphIds": [],
            "attrs": [
              {
                "key": "[metadata] namespace",
                "value": ": transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.0: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.0.self_attn: transformers.models.llama.modeling_llama.LlamaAttention/slice_37: aten.slice.Tensor"
              },
              {
                "key": "[metadata] pkg.torch.onnx.class_hierarchy",
                "value": "['transformers.models.llama.modeling_llama.LlamaForCausalLM', 'transformers.models.llama.modeling_llama.LlamaModel', 'transformers.models.llama.modeling_llama.LlamaDecoderLayer', 'transformers.models.llama.modeling_llama.LlamaAttention', 'aten.slice.Tensor']"
              },
              {
                "key": "[metadata] pkg.torch.onnx.fx_node",
                "value": "%slice_37 : [num_users=1] = call_function[target=torch.ops.aten.slice.Tensor](args = (%slice_36, 3, None, %add_4), kwargs = {})"
              },
              {
                "key": "[metadata] pkg.torch.onnx.name_scopes",
                "value": "['', 'model', 'model.layers.0', 'model.layers.0.self_attn', 'slice_37']"
              },
              {
                "key": "[metadata] pkg.torch.onnx.stack_trace",
                "value": "File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 688, in forward\n    outputs: BaseModelOutputWithPast = self.model(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 453, in forward\n    layer_outputs = decoder_layer(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 308, in forward\n    hidden_states, self_attn_weights = self.self_attn(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 265, in forward\n    attn_output, attn_weights = attention_interface("
              }
            ],
            "incomingEdges": [
              {
                "sourceNodeId": "node_ScatterND_279",
                "sourceNodeOutputId": "0",
                "targetNodeInputId": "0"
              },
              {
                "sourceNodeId": "node_Constant_454",
                "sourceNodeOutputId": "0",
                "targetNodeInputId": "1"
              },
              {
                "sourceNodeId": "node_Reshape_457",
                "sourceNodeOutputId": "0",
                "targetNodeInputId": "2"
              },
              {
                "sourceNodeId": "node_Constant_921",
                "sourceNodeOutputId": "0",
                "targetNodeInputId": "3"
              },
              {
                "sourceNodeId": "node_Constant_461",
                "sourceNodeOutputId": "0",
                "targetNodeInputId": "4"
              }
            ],
            "outputsMetadata": [
              {
                "id": "0",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "slice_37"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "FLOAT[s34,1,s16,s16 + s17]"
                  },
                  {
                    "key": "param_name",
                    "value": "output"
                  }
                ]
              }
            ],
            "inputsMetadata": [
              {
                "id": "0",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "slice_scatter_2"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "FLOAT[s34,1,s16,s16 + s17]"
                  },
                  {
                    "key": "param_name",
                    "value": "data"
                  }
                ]
              },
              {
                "id": "1",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "val_335"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "INT64[1]"
                  },
                  {
                    "key": "param_name",
                    "value": "starts"
                  }
                ]
              },
              {
                "id": "2",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "val_338"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "INT64[1]"
                  },
                  {
                    "key": "param_name",
                    "value": "ends"
                  }
                ]
              },
              {
                "id": "3",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "val_341"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "INT64[1]"
                  },
                  {
                    "key": "param_name",
                    "value": "axes"
                  }
                ]
              },
              {
                "id": "4",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "val_342"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "INT64[1]"
                  },
                  {
                    "key": "param_name",
                    "value": "steps"
                  }
                ]
              }
            ],
            "style": null,
            "config": null
          },
          {
            "id": "node_Add_463",
            "label": "Add",
            "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.0: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.0.self_attn: transformers.models.llama.modeling_llama.LlamaAttention/add_374: aten.add.Tensor",
            "subgraphIds": [],
            "attrs": [
              {
                "key": "[metadata] namespace",
                "value": ": transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.0: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.0.self_attn: transformers.models.llama.modeling_llama.LlamaAttention/add_374: aten.add.Tensor"
              },
              {
                "key": "[metadata] pkg.torch.onnx.class_hierarchy",
                "value": "['transformers.models.llama.modeling_llama.LlamaForCausalLM', 'transformers.models.llama.modeling_llama.LlamaModel', 'transformers.models.llama.modeling_llama.LlamaDecoderLayer', 'transformers.models.llama.modeling_llama.LlamaAttention', 'aten.add.Tensor']"
              },
              {
                "key": "[metadata] pkg.torch.onnx.fx_node",
                "value": "%add_374 : [num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%mul_278, %slice_37), kwargs = {})"
              },
              {
                "key": "[metadata] pkg.torch.onnx.name_scopes",
                "value": "['', 'model', 'model.layers.0', 'model.layers.0.self_attn', 'add_374']"
              },
              {
                "key": "[metadata] pkg.torch.onnx.stack_trace",
                "value": "File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 688, in forward\n    outputs: BaseModelOutputWithPast = self.model(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 453, in forward\n    layer_outputs = decoder_layer(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 308, in forward\n    hidden_states, self_attn_weights = self.self_attn(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 265, in forward\n    attn_output, attn_weights = attention_interface("
              }
            ],
            "incomingEdges": [
              {
                "sourceNodeId": "node_Mul_432",
                "sourceNodeOutputId": "0",
                "targetNodeInputId": "0"
              },
              {
                "sourceNodeId": "node_Slice_462",
                "sourceNodeOutputId": "0",
                "targetNodeInputId": "1"
              }
            ],
            "outputsMetadata": [
              {
                "id": "0",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "add_374"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "FLOAT[s34,32,s16,s16 + s17]"
                  },
                  {
                    "key": "param_name",
                    "value": "C"
                  }
                ]
              }
            ],
            "inputsMetadata": [
              {
                "id": "0",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "mul_278"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "FLOAT[s34,32,s16,s16 + s17]"
                  },
                  {
                    "key": "param_name",
                    "value": "A"
                  }
                ]
              },
              {
                "id": "1",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "slice_37"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "FLOAT[s34,1,s16,s16 + s17]"
                  },
                  {
                    "key": "param_name",
                    "value": "B"
                  }
                ]
              }
            ],
            "style": null,
            "config": null
          },
          {
            "id": "node_Softmax_464",
            "label": "Softmax",
            "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.0: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.0.self_attn: transformers.models.llama.modeling_llama.LlamaAttention/softmax: aten.softmax.int",
            "subgraphIds": [],
            "attrs": [
              {
                "key": "axis",
                "value": "-1"
              },
              {
                "key": "[metadata] namespace",
                "value": ": transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.0: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.0.self_attn: transformers.models.llama.modeling_llama.LlamaAttention/softmax: aten.softmax.int"
              },
              {
                "key": "[metadata] pkg.torch.onnx.class_hierarchy",
                "value": "['transformers.models.llama.modeling_llama.LlamaForCausalLM', 'transformers.models.llama.modeling_llama.LlamaModel', 'transformers.models.llama.modeling_llama.LlamaDecoderLayer', 'transformers.models.llama.modeling_llama.LlamaAttention', 'aten.softmax.int']"
              },
              {
                "key": "[metadata] pkg.torch.onnx.fx_node",
                "value": "%softmax : [num_users=1] = call_function[target=torch.ops.aten.softmax.int](args = (%add_374, -1, torch.float32), kwargs = {})"
              },
              {
                "key": "[metadata] pkg.torch.onnx.name_scopes",
                "value": "['', 'model', 'model.layers.0', 'model.layers.0.self_attn', 'softmax']"
              },
              {
                "key": "[metadata] pkg.torch.onnx.stack_trace",
                "value": "File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 688, in forward\n    outputs: BaseModelOutputWithPast = self.model(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 453, in forward\n    layer_outputs = decoder_layer(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 308, in forward\n    hidden_states, self_attn_weights = self.self_attn(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 265, in forward\n    attn_output, attn_weights = attention_interface("
              }
            ],
            "incomingEdges": [
              {
                "sourceNodeId": "node_Add_463",
                "sourceNodeOutputId": "0",
                "targetNodeInputId": "0"
              }
            ],
            "outputsMetadata": [
              {
                "id": "0",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "val_343"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "FLOAT[s34,32,s16,s16 + s17]"
                  },
                  {
                    "key": "param_name",
                    "value": "output"
                  }
                ]
              }
            ],
            "inputsMetadata": [
              {
                "id": "0",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "add_374"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "FLOAT[s34,32,s16,s16 + s17]"
                  },
                  {
                    "key": "param_name",
                    "value": "input"
                  }
                ]
              }
            ],
            "style": null,
            "config": null
          },
          {
            "id": "node_MatMul_467",
            "label": "MatMul",
            "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.0: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.0.self_attn: transformers.models.llama.modeling_llama.LlamaAttention/matmul_2: aten.matmul.default",
            "subgraphIds": [],
            "attrs": [
              {
                "key": "[metadata] namespace",
                "value": ": transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.0: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.0.self_attn: transformers.models.llama.modeling_llama.LlamaAttention/matmul_2: aten.matmul.default"
              },
              {
                "key": "[metadata] pkg.torch.onnx.class_hierarchy",
                "value": "['transformers.models.llama.modeling_llama.LlamaForCausalLM', 'transformers.models.llama.modeling_llama.LlamaModel', 'transformers.models.llama.modeling_llama.LlamaDecoderLayer', 'transformers.models.llama.modeling_llama.LlamaAttention', 'aten.matmul.default']"
              },
              {
                "key": "[metadata] pkg.torch.onnx.fx_node",
                "value": "%matmul_2 : [num_users=1] = call_function[target=torch.ops.aten.matmul.default](args = (%clone_1, %cat_4), kwargs = {})"
              },
              {
                "key": "[metadata] pkg.torch.onnx.name_scopes",
                "value": "['', 'model', 'model.layers.0', 'model.layers.0.self_attn', 'matmul_2']"
              },
              {
                "key": "[metadata] pkg.torch.onnx.stack_trace",
                "value": "File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 688, in forward\n    outputs: BaseModelOutputWithPast = self.model(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 453, in forward\n    layer_outputs = decoder_layer(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 308, in forward\n    hidden_states, self_attn_weights = self.self_attn(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 265, in forward\n    attn_output, attn_weights = attention_interface("
              }
            ],
            "incomingEdges": [
              {
                "sourceNodeId": "node_Softmax_464",
                "sourceNodeOutputId": "0",
                "targetNodeInputId": "0"
              },
              {
                "sourceNodeId": "node_Concat_428",
                "sourceNodeOutputId": "0",
                "targetNodeInputId": "1"
              }
            ],
            "outputsMetadata": [
              {
                "id": "0",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "matmul_2"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "FLOAT[s34,32,s16,64]"
                  },
                  {
                    "key": "param_name",
                    "value": "Y"
                  }
                ]
              }
            ],
            "inputsMetadata": [
              {
                "id": "0",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "val_343"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "FLOAT[s34,32,s16,s16 + s17]"
                  },
                  {
                    "key": "param_name",
                    "value": "A"
                  }
                ]
              },
              {
                "id": "1",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "cat_4"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "FLOAT[s34,32,s16 + s17,64]"
                  },
                  {
                    "key": "[metadata] pkg.torch.export.graph_signature.OutputSpec.kind",
                    "value": "USER_OUTPUT"
                  },
                  {
                    "key": "param_name",
                    "value": "B"
                  }
                ]
              }
            ],
            "style": null,
            "config": null
          },
          {
            "id": "node_Transpose_468",
            "label": "Transpose",
            "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.0: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.0.self_attn: transformers.models.llama.modeling_llama.LlamaAttention/transpose_5: aten.transpose.int",
            "subgraphIds": [],
            "attrs": [
              {
                "key": "perm",
                "value": "[0, 2, 1, 3]"
              },
              {
                "key": "[metadata] namespace",
                "value": ": transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.0: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.0.self_attn: transformers.models.llama.modeling_llama.LlamaAttention/transpose_5: aten.transpose.int"
              },
              {
                "key": "[metadata] pkg.torch.onnx.class_hierarchy",
                "value": "['transformers.models.llama.modeling_llama.LlamaForCausalLM', 'transformers.models.llama.modeling_llama.LlamaModel', 'transformers.models.llama.modeling_llama.LlamaDecoderLayer', 'transformers.models.llama.modeling_llama.LlamaAttention', 'aten.transpose.int']"
              },
              {
                "key": "[metadata] pkg.torch.onnx.fx_node",
                "value": "%transpose_5 : [num_users=1] = call_function[target=torch.ops.aten.transpose.int](args = (%matmul_2, 1, 2), kwargs = {})"
              },
              {
                "key": "[metadata] pkg.torch.onnx.name_scopes",
                "value": "['', 'model', 'model.layers.0', 'model.layers.0.self_attn', 'transpose_5']"
              },
              {
                "key": "[metadata] pkg.torch.onnx.stack_trace",
                "value": "File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 688, in forward\n    outputs: BaseModelOutputWithPast = self.model(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 453, in forward\n    layer_outputs = decoder_layer(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 308, in forward\n    hidden_states, self_attn_weights = self.self_attn(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 265, in forward\n    attn_output, attn_weights = attention_interface("
              }
            ],
            "incomingEdges": [
              {
                "sourceNodeId": "node_MatMul_467",
                "sourceNodeOutputId": "0",
                "targetNodeInputId": "0"
              }
            ],
            "outputsMetadata": [
              {
                "id": "0",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "transpose_5"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "FLOAT[s34,s16,32,64]"
                  },
                  {
                    "key": "param_name",
                    "value": "transposed"
                  }
                ]
              }
            ],
            "inputsMetadata": [
              {
                "id": "0",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "matmul_2"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "FLOAT[s34,32,s16,64]"
                  },
                  {
                    "key": "param_name",
                    "value": "data"
                  }
                ]
              }
            ],
            "style": null,
            "config": null
          },
          {
            "id": "node_Concat_474",
            "label": "Concat",
            "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.0: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.0.self_attn: transformers.models.llama.modeling_llama.LlamaAttention/view_4: aten.view.default",
            "subgraphIds": [],
            "attrs": [
              {
                "key": "axis",
                "value": "0"
              },
              {
                "key": "[metadata] namespace",
                "value": ": transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.0: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.0.self_attn: transformers.models.llama.modeling_llama.LlamaAttention/view_4: aten.view.default"
              },
              {
                "key": "[metadata] pkg.torch.onnx.class_hierarchy",
                "value": "['transformers.models.llama.modeling_llama.LlamaForCausalLM', 'transformers.models.llama.modeling_llama.LlamaModel', 'transformers.models.llama.modeling_llama.LlamaDecoderLayer', 'transformers.models.llama.modeling_llama.LlamaAttention', 'aten.view.default']"
              },
              {
                "key": "[metadata] pkg.torch.onnx.fx_node",
                "value": "%view_4 : [num_users=1] = call_function[target=torch.ops.aten.view.default](args = (%clone_2, [%sym_size_int_68, %sym_size_int_61, -1]), kwargs = {})"
              },
              {
                "key": "[metadata] pkg.torch.onnx.name_scopes",
                "value": "['', 'model', 'model.layers.0', 'model.layers.0.self_attn', 'view_4']"
              },
              {
                "key": "[metadata] pkg.torch.onnx.stack_trace",
                "value": "File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 688, in forward\n    outputs: BaseModelOutputWithPast = self.model(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 453, in forward\n    layer_outputs = decoder_layer(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 308, in forward\n    hidden_states, self_attn_weights = self.self_attn(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 276, in forward\n    attn_output = attn_output.reshape(*input_shape, -1).contiguous()"
              }
            ],
            "incomingEdges": [
              {
                "sourceNodeId": "node_Shape_12",
                "sourceNodeOutputId": "0",
                "targetNodeInputId": "0"
              },
              {
                "sourceNodeId": "node_Shape_2",
                "sourceNodeOutputId": "0",
                "targetNodeInputId": "1"
              },
              {
                "sourceNodeId": "node_Constant_90",
                "sourceNodeOutputId": "0",
                "targetNodeInputId": "2"
              }
            ],
            "outputsMetadata": [
              {
                "id": "0",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "val_348"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "INT64[3]"
                  },
                  {
                    "key": "param_name",
                    "value": "concat_result"
                  }
                ]
              }
            ],
            "inputsMetadata": [
              {
                "id": "0",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "val_6"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "INT64[1]"
                  },
                  {
                    "key": "param_name",
                    "value": "inputs"
                  }
                ]
              },
              {
                "id": "1",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "val_1"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "INT64[1]"
                  },
                  {
                    "key": "param_name",
                    "value": "inputs"
                  }
                ]
              },
              {
                "id": "2",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "val_53"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "INT64[1]"
                  },
                  {
                    "key": "param_name",
                    "value": "inputs"
                  }
                ]
              }
            ],
            "style": null,
            "config": null
          },
          {
            "id": "node_Reshape_476",
            "label": "Reshape",
            "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.0: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.0.self_attn: transformers.models.llama.modeling_llama.LlamaAttention/view_4: aten.view.default",
            "subgraphIds": [],
            "attrs": [
              {
                "key": "allowzero",
                "value": "1"
              },
              {
                "key": "[metadata] namespace",
                "value": ": transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.0: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.0.self_attn: transformers.models.llama.modeling_llama.LlamaAttention/view_4: aten.view.default"
              },
              {
                "key": "[metadata] pkg.torch.onnx.class_hierarchy",
                "value": "['transformers.models.llama.modeling_llama.LlamaForCausalLM', 'transformers.models.llama.modeling_llama.LlamaModel', 'transformers.models.llama.modeling_llama.LlamaDecoderLayer', 'transformers.models.llama.modeling_llama.LlamaAttention', 'aten.view.default']"
              },
              {
                "key": "[metadata] pkg.torch.onnx.fx_node",
                "value": "%view_4 : [num_users=1] = call_function[target=torch.ops.aten.view.default](args = (%clone_2, [%sym_size_int_68, %sym_size_int_61, -1]), kwargs = {})"
              },
              {
                "key": "[metadata] pkg.torch.onnx.name_scopes",
                "value": "['', 'model', 'model.layers.0', 'model.layers.0.self_attn', 'view_4']"
              },
              {
                "key": "[metadata] pkg.torch.onnx.stack_trace",
                "value": "File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 688, in forward\n    outputs: BaseModelOutputWithPast = self.model(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 453, in forward\n    layer_outputs = decoder_layer(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 308, in forward\n    hidden_states, self_attn_weights = self.self_attn(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 276, in forward\n    attn_output = attn_output.reshape(*input_shape, -1).contiguous()"
              }
            ],
            "incomingEdges": [
              {
                "sourceNodeId": "node_Transpose_468",
                "sourceNodeOutputId": "0",
                "targetNodeInputId": "0"
              },
              {
                "sourceNodeId": "node_Concat_474",
                "sourceNodeOutputId": "0",
                "targetNodeInputId": "1"
              }
            ],
            "outputsMetadata": [
              {
                "id": "0",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "view_4"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "FLOAT[s34,s16,2048]"
                  },
                  {
                    "key": "param_name",
                    "value": "reshaped"
                  }
                ]
              }
            ],
            "inputsMetadata": [
              {
                "id": "0",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "transpose_5"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "FLOAT[s34,s16,32,64]"
                  },
                  {
                    "key": "param_name",
                    "value": "data"
                  }
                ]
              },
              {
                "id": "1",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "val_348"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "INT64[3]"
                  },
                  {
                    "key": "param_name",
                    "value": "shape"
                  }
                ]
              }
            ],
            "style": null,
            "config": null
          },
          {
            "id": "node_Transpose_477",
            "label": "Transpose",
            "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.0: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.0.self_attn: transformers.models.llama.modeling_llama.LlamaAttention/model.layers.0.self_attn.o_proj: torch.nn.modules.linear.Linear/linear_3: aten.linear.default",
            "subgraphIds": [],
            "attrs": [
              {
                "key": "perm",
                "value": "[1, 0]"
              },
              {
                "key": "[metadata] namespace",
                "value": ": transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.0: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.0.self_attn: transformers.models.llama.modeling_llama.LlamaAttention/model.layers.0.self_attn.o_proj: torch.nn.modules.linear.Linear/linear_3: aten.linear.default"
              },
              {
                "key": "[metadata] pkg.torch.onnx.class_hierarchy",
                "value": "['transformers.models.llama.modeling_llama.LlamaForCausalLM', 'transformers.models.llama.modeling_llama.LlamaModel', 'transformers.models.llama.modeling_llama.LlamaDecoderLayer', 'transformers.models.llama.modeling_llama.LlamaAttention', 'torch.nn.modules.linear.Linear', 'aten.linear.default']"
              },
              {
                "key": "[metadata] pkg.torch.onnx.fx_node",
                "value": "%linear_3 : [num_users=1] = call_function[target=torch.ops.aten.linear.default](args = (%view_4, %p_model_layers_0_self_attn_o_proj_weight), kwargs = {})"
              },
              {
                "key": "[metadata] pkg.torch.onnx.name_scopes",
                "value": "['', 'model', 'model.layers.0', 'model.layers.0.self_attn', 'model.layers.0.self_attn.o_proj', 'linear_3']"
              },
              {
                "key": "[metadata] pkg.torch.onnx.stack_trace",
                "value": "File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 688, in forward\n    outputs: BaseModelOutputWithPast = self.model(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 453, in forward\n    layer_outputs = decoder_layer(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 308, in forward\n    hidden_states, self_attn_weights = self.self_attn(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 277, in forward\n    attn_output = self.o_proj(attn_output)\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/torch/nn/modules/linear.py\", line 125, in forward\n    return F.linear(input, self.weight, self.bias)"
              }
            ],
            "incomingEdges": [
              {
                "sourceNodeId": "[value] model.layers.0.self_attn.o_proj.weight",
                "sourceNodeOutputId": "0",
                "targetNodeInputId": "0"
              }
            ],
            "outputsMetadata": [
              {
                "id": "0",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "val_350"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "FLOAT[2048,2048]"
                  },
                  {
                    "key": "param_name",
                    "value": "transposed"
                  }
                ]
              }
            ],
            "inputsMetadata": [
              {
                "id": "0",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "model.layers.0.self_attn.o_proj.weight"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "FLOAT[2048,2048]"
                  },
                  {
                    "key": "param_name",
                    "value": "data"
                  }
                ]
              }
            ],
            "style": null,
            "config": null
          },
          {
            "id": "node_MatMul_478",
            "label": "MatMul",
            "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.0: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.0.self_attn: transformers.models.llama.modeling_llama.LlamaAttention/model.layers.0.self_attn.o_proj: torch.nn.modules.linear.Linear/linear_3: aten.linear.default",
            "subgraphIds": [],
            "attrs": [
              {
                "key": "[metadata] namespace",
                "value": ": transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.0: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.0.self_attn: transformers.models.llama.modeling_llama.LlamaAttention/model.layers.0.self_attn.o_proj: torch.nn.modules.linear.Linear/linear_3: aten.linear.default"
              },
              {
                "key": "[metadata] pkg.torch.onnx.class_hierarchy",
                "value": "['transformers.models.llama.modeling_llama.LlamaForCausalLM', 'transformers.models.llama.modeling_llama.LlamaModel', 'transformers.models.llama.modeling_llama.LlamaDecoderLayer', 'transformers.models.llama.modeling_llama.LlamaAttention', 'torch.nn.modules.linear.Linear', 'aten.linear.default']"
              },
              {
                "key": "[metadata] pkg.torch.onnx.fx_node",
                "value": "%linear_3 : [num_users=1] = call_function[target=torch.ops.aten.linear.default](args = (%view_4, %p_model_layers_0_self_attn_o_proj_weight), kwargs = {})"
              },
              {
                "key": "[metadata] pkg.torch.onnx.name_scopes",
                "value": "['', 'model', 'model.layers.0', 'model.layers.0.self_attn', 'model.layers.0.self_attn.o_proj', 'linear_3']"
              },
              {
                "key": "[metadata] pkg.torch.onnx.stack_trace",
                "value": "File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 688, in forward\n    outputs: BaseModelOutputWithPast = self.model(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 453, in forward\n    layer_outputs = decoder_layer(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 308, in forward\n    hidden_states, self_attn_weights = self.self_attn(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 277, in forward\n    attn_output = self.o_proj(attn_output)\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/torch/nn/modules/linear.py\", line 125, in forward\n    return F.linear(input, self.weight, self.bias)"
              }
            ],
            "incomingEdges": [
              {
                "sourceNodeId": "node_Reshape_476",
                "sourceNodeOutputId": "0",
                "targetNodeInputId": "0"
              },
              {
                "sourceNodeId": "node_Transpose_477",
                "sourceNodeOutputId": "0",
                "targetNodeInputId": "1"
              }
            ],
            "outputsMetadata": [
              {
                "id": "0",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "linear_3"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "FLOAT[s34,s16,2048]"
                  },
                  {
                    "key": "param_name",
                    "value": "Y"
                  }
                ]
              }
            ],
            "inputsMetadata": [
              {
                "id": "0",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "view_4"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "FLOAT[s34,s16,2048]"
                  },
                  {
                    "key": "param_name",
                    "value": "A"
                  }
                ]
              },
              {
                "id": "1",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "val_350"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "FLOAT[2048,2048]"
                  },
                  {
                    "key": "param_name",
                    "value": "B"
                  }
                ]
              }
            ],
            "style": null,
            "config": null
          },
          {
            "id": "node_Add_479",
            "label": "Add",
            "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.0: transformers.models.llama.modeling_llama.LlamaDecoderLayer/add_413: aten.add.Tensor",
            "subgraphIds": [],
            "attrs": [
              {
                "key": "[metadata] namespace",
                "value": ": transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.0: transformers.models.llama.modeling_llama.LlamaDecoderLayer/add_413: aten.add.Tensor"
              },
              {
                "key": "[metadata] pkg.torch.onnx.class_hierarchy",
                "value": "['transformers.models.llama.modeling_llama.LlamaForCausalLM', 'transformers.models.llama.modeling_llama.LlamaModel', 'transformers.models.llama.modeling_llama.LlamaDecoderLayer', 'aten.add.Tensor']"
              },
              {
                "key": "[metadata] pkg.torch.onnx.fx_node",
                "value": "%add_413 : [num_users=3] = call_function[target=torch.ops.aten.add.Tensor](args = (%embedding, %linear_3), kwargs = {})"
              },
              {
                "key": "[metadata] pkg.torch.onnx.name_scopes",
                "value": "['', 'model', 'model.layers.0', 'add_413']"
              },
              {
                "key": "[metadata] pkg.torch.onnx.stack_trace",
                "value": "File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 688, in forward\n    outputs: BaseModelOutputWithPast = self.model(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 453, in forward\n    layer_outputs = decoder_layer(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 319, in forward\n    hidden_states = residual + hidden_states"
              }
            ],
            "incomingEdges": [
              {
                "sourceNodeId": "node_Gather_20",
                "sourceNodeOutputId": "0",
                "targetNodeInputId": "0"
              },
              {
                "sourceNodeId": "node_MatMul_478",
                "sourceNodeOutputId": "0",
                "targetNodeInputId": "1"
              }
            ],
            "outputsMetadata": [
              {
                "id": "0",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "add_413"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "FLOAT[s34,s16,2048]"
                  },
                  {
                    "key": "param_name",
                    "value": "C"
                  }
                ]
              }
            ],
            "inputsMetadata": [
              {
                "id": "0",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "embedding"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "FLOAT[s34,s16,2048]"
                  },
                  {
                    "key": "param_name",
                    "value": "A"
                  }
                ]
              },
              {
                "id": "1",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "linear_3"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "FLOAT[s34,s16,2048]"
                  },
                  {
                    "key": "param_name",
                    "value": "B"
                  }
                ]
              }
            ],
            "style": null,
            "config": null
          },
          {
            "id": "node_Constant_480",
            "label": "Constant",
            "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.0: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.0.post_attention_layernorm: transformers.models.llama.modeling_llama.LlamaRMSNorm/pow_2: aten.pow.Tensor_Scalar",
            "subgraphIds": [],
            "attrs": [
              {
                "key": "__value",
                "value": "2.0"
              },
              {
                "key": "value",
                "value": "TensorProtoTensor<FLOAT,[]>(array(2., dtype=float32), name='')"
              },
              {
                "key": "[metadata] namespace",
                "value": ": transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.0: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.0.post_attention_layernorm: transformers.models.llama.modeling_llama.LlamaRMSNorm/pow_2: aten.pow.Tensor_Scalar"
              },
              {
                "key": "[metadata] pkg.torch.onnx.class_hierarchy",
                "value": "['transformers.models.llama.modeling_llama.LlamaForCausalLM', 'transformers.models.llama.modeling_llama.LlamaModel', 'transformers.models.llama.modeling_llama.LlamaDecoderLayer', 'transformers.models.llama.modeling_llama.LlamaRMSNorm', 'aten.pow.Tensor_Scalar']"
              },
              {
                "key": "[metadata] pkg.torch.onnx.fx_node",
                "value": "%pow_2 : [num_users=1] = call_function[target=torch.ops.aten.pow.Tensor_Scalar](args = (%add_413, 2), kwargs = {})"
              },
              {
                "key": "[metadata] pkg.torch.onnx.name_scopes",
                "value": "['', 'model', 'model.layers.0', 'model.layers.0.post_attention_layernorm', 'pow_2']"
              },
              {
                "key": "[metadata] pkg.torch.onnx.stack_trace",
                "value": "File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 688, in forward\n    outputs: BaseModelOutputWithPast = self.model(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 453, in forward\n    layer_outputs = decoder_layer(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 323, in forward\n    hidden_states = self.post_attention_layernorm(hidden_states)\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 71, in forward\n    variance = hidden_states.pow(2).mean(-1, keepdim=True)"
              }
            ],
            "incomingEdges": [],
            "outputsMetadata": [
              {
                "id": "0",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "val_351"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "FLOAT[]"
                  },
                  {
                    "key": "param_name",
                    "value": "output"
                  }
                ]
              }
            ],
            "inputsMetadata": [],
            "style": null,
            "config": null
          },
          {
            "id": "node_Pow_481",
            "label": "Pow",
            "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.0: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.0.post_attention_layernorm: transformers.models.llama.modeling_llama.LlamaRMSNorm/pow_2: aten.pow.Tensor_Scalar",
            "subgraphIds": [],
            "attrs": [
              {
                "key": "[metadata] namespace",
                "value": ": transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.0: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.0.post_attention_layernorm: transformers.models.llama.modeling_llama.LlamaRMSNorm/pow_2: aten.pow.Tensor_Scalar"
              },
              {
                "key": "[metadata] pkg.torch.onnx.class_hierarchy",
                "value": "['transformers.models.llama.modeling_llama.LlamaForCausalLM', 'transformers.models.llama.modeling_llama.LlamaModel', 'transformers.models.llama.modeling_llama.LlamaDecoderLayer', 'transformers.models.llama.modeling_llama.LlamaRMSNorm', 'aten.pow.Tensor_Scalar']"
              },
              {
                "key": "[metadata] pkg.torch.onnx.fx_node",
                "value": "%pow_2 : [num_users=1] = call_function[target=torch.ops.aten.pow.Tensor_Scalar](args = (%add_413, 2), kwargs = {})"
              },
              {
                "key": "[metadata] pkg.torch.onnx.name_scopes",
                "value": "['', 'model', 'model.layers.0', 'model.layers.0.post_attention_layernorm', 'pow_2']"
              },
              {
                "key": "[metadata] pkg.torch.onnx.stack_trace",
                "value": "File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 688, in forward\n    outputs: BaseModelOutputWithPast = self.model(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 453, in forward\n    layer_outputs = decoder_layer(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 323, in forward\n    hidden_states = self.post_attention_layernorm(hidden_states)\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 71, in forward\n    variance = hidden_states.pow(2).mean(-1, keepdim=True)"
              }
            ],
            "incomingEdges": [
              {
                "sourceNodeId": "node_Add_479",
                "sourceNodeOutputId": "0",
                "targetNodeInputId": "0"
              },
              {
                "sourceNodeId": "node_Constant_480",
                "sourceNodeOutputId": "0",
                "targetNodeInputId": "1"
              }
            ],
            "outputsMetadata": [
              {
                "id": "0",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "pow_2"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "FLOAT[s34,s16,2048]"
                  },
                  {
                    "key": "param_name",
                    "value": "Z"
                  }
                ]
              }
            ],
            "inputsMetadata": [
              {
                "id": "0",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "add_413"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "FLOAT[s34,s16,2048]"
                  },
                  {
                    "key": "param_name",
                    "value": "X"
                  }
                ]
              },
              {
                "id": "1",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "val_351"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "FLOAT[]"
                  },
                  {
                    "key": "param_name",
                    "value": "Y"
                  }
                ]
              }
            ],
            "style": null,
            "config": null
          },
          {
            "id": "node_Constant_924",
            "label": "Constant",
            "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.0: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.0.post_attention_layernorm: transformers.models.llama.modeling_llama.LlamaRMSNorm/mean_1: aten.mean.dim",
            "subgraphIds": [],
            "attrs": [
              {
                "key": "__value",
                "value": "[-1]"
              },
              {
                "key": "value",
                "value": "TensorProtoTensor<INT64,[1]>(array([-1]), name='val_353')"
              }
            ],
            "incomingEdges": [],
            "outputsMetadata": [
              {
                "id": "0",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "val_353"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "INT64[1]"
                  },
                  {
                    "key": "param_name",
                    "value": "output"
                  }
                ]
              }
            ],
            "inputsMetadata": [],
            "style": null,
            "config": null
          },
          {
            "id": "node_ReduceMean_484",
            "label": "ReduceMean",
            "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.0: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.0.post_attention_layernorm: transformers.models.llama.modeling_llama.LlamaRMSNorm/mean_1: aten.mean.dim",
            "subgraphIds": [],
            "attrs": [
              {
                "key": "noop_with_empty_axes",
                "value": "0"
              },
              {
                "key": "keepdims",
                "value": "1"
              },
              {
                "key": "[metadata] namespace",
                "value": ": transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.0: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.0.post_attention_layernorm: transformers.models.llama.modeling_llama.LlamaRMSNorm/mean_1: aten.mean.dim"
              },
              {
                "key": "[metadata] pkg.torch.onnx.class_hierarchy",
                "value": "['transformers.models.llama.modeling_llama.LlamaForCausalLM', 'transformers.models.llama.modeling_llama.LlamaModel', 'transformers.models.llama.modeling_llama.LlamaDecoderLayer', 'transformers.models.llama.modeling_llama.LlamaRMSNorm', 'aten.mean.dim']"
              },
              {
                "key": "[metadata] pkg.torch.onnx.fx_node",
                "value": "%mean_1 : [num_users=1] = call_function[target=torch.ops.aten.mean.dim](args = (%pow_2, [-1], True), kwargs = {})"
              },
              {
                "key": "[metadata] pkg.torch.onnx.name_scopes",
                "value": "['', 'model', 'model.layers.0', 'model.layers.0.post_attention_layernorm', 'mean_1']"
              },
              {
                "key": "[metadata] pkg.torch.onnx.stack_trace",
                "value": "File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 688, in forward\n    outputs: BaseModelOutputWithPast = self.model(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 453, in forward\n    layer_outputs = decoder_layer(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 323, in forward\n    hidden_states = self.post_attention_layernorm(hidden_states)\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 71, in forward\n    variance = hidden_states.pow(2).mean(-1, keepdim=True)"
              }
            ],
            "incomingEdges": [
              {
                "sourceNodeId": "node_Pow_481",
                "sourceNodeOutputId": "0",
                "targetNodeInputId": "0"
              },
              {
                "sourceNodeId": "node_Constant_924",
                "sourceNodeOutputId": "0",
                "targetNodeInputId": "1"
              }
            ],
            "outputsMetadata": [
              {
                "id": "0",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "mean_1"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "FLOAT[s34,s16,1]"
                  },
                  {
                    "key": "param_name",
                    "value": "reduced"
                  }
                ]
              }
            ],
            "inputsMetadata": [
              {
                "id": "0",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "pow_2"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "FLOAT[s34,s16,2048]"
                  },
                  {
                    "key": "param_name",
                    "value": "data"
                  }
                ]
              },
              {
                "id": "1",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "val_353"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "INT64[1]"
                  },
                  {
                    "key": "param_name",
                    "value": "axes"
                  }
                ]
              }
            ],
            "style": null,
            "config": null
          },
          {
            "id": "node_Add_485",
            "label": "Add",
            "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.0: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.0.post_attention_layernorm: transformers.models.llama.modeling_llama.LlamaRMSNorm/add_426: aten.add.Tensor",
            "subgraphIds": [],
            "attrs": [
              {
                "key": "[metadata] namespace",
                "value": ": transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.0: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.0.post_attention_layernorm: transformers.models.llama.modeling_llama.LlamaRMSNorm/add_426: aten.add.Tensor"
              },
              {
                "key": "[metadata] pkg.torch.onnx.class_hierarchy",
                "value": "['transformers.models.llama.modeling_llama.LlamaForCausalLM', 'transformers.models.llama.modeling_llama.LlamaModel', 'transformers.models.llama.modeling_llama.LlamaDecoderLayer', 'transformers.models.llama.modeling_llama.LlamaRMSNorm', 'aten.add.Tensor']"
              },
              {
                "key": "[metadata] pkg.torch.onnx.fx_node",
                "value": "%add_426 : [num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%mean_1, 1e-05), kwargs = {})"
              },
              {
                "key": "[metadata] pkg.torch.onnx.name_scopes",
                "value": "['', 'model', 'model.layers.0', 'model.layers.0.post_attention_layernorm', 'add_426']"
              },
              {
                "key": "[metadata] pkg.torch.onnx.stack_trace",
                "value": "File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 688, in forward\n    outputs: BaseModelOutputWithPast = self.model(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 453, in forward\n    layer_outputs = decoder_layer(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 323, in forward\n    hidden_states = self.post_attention_layernorm(hidden_states)\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 72, in forward\n    hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)"
              }
            ],
            "incomingEdges": [
              {
                "sourceNodeId": "node_ReduceMean_484",
                "sourceNodeOutputId": "0",
                "targetNodeInputId": "0"
              },
              {
                "sourceNodeId": "node_Constant_333",
                "sourceNodeOutputId": "0",
                "targetNodeInputId": "1"
              }
            ],
            "outputsMetadata": [
              {
                "id": "0",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "add_426"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "FLOAT[s34,s16,1]"
                  },
                  {
                    "key": "param_name",
                    "value": "C"
                  }
                ]
              }
            ],
            "inputsMetadata": [
              {
                "id": "0",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "mean_1"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "FLOAT[s34,s16,1]"
                  },
                  {
                    "key": "param_name",
                    "value": "A"
                  }
                ]
              },
              {
                "id": "1",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "val_251"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "FLOAT[]"
                  },
                  {
                    "key": "param_name",
                    "value": "B"
                  }
                ]
              }
            ],
            "style": null,
            "config": null
          },
          {
            "id": "node_Sqrt_486",
            "label": "Sqrt",
            "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.0: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.0.post_attention_layernorm: transformers.models.llama.modeling_llama.LlamaRMSNorm/rsqrt_1: aten.rsqrt.default",
            "subgraphIds": [],
            "attrs": [
              {
                "key": "[metadata] namespace",
                "value": ": transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.0: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.0.post_attention_layernorm: transformers.models.llama.modeling_llama.LlamaRMSNorm/rsqrt_1: aten.rsqrt.default"
              },
              {
                "key": "[metadata] pkg.torch.onnx.class_hierarchy",
                "value": "['transformers.models.llama.modeling_llama.LlamaForCausalLM', 'transformers.models.llama.modeling_llama.LlamaModel', 'transformers.models.llama.modeling_llama.LlamaDecoderLayer', 'transformers.models.llama.modeling_llama.LlamaRMSNorm', 'aten.rsqrt.default']"
              },
              {
                "key": "[metadata] pkg.torch.onnx.fx_node",
                "value": "%rsqrt_1 : [num_users=1] = call_function[target=torch.ops.aten.rsqrt.default](args = (%add_426,), kwargs = {})"
              },
              {
                "key": "[metadata] pkg.torch.onnx.name_scopes",
                "value": "['', 'model', 'model.layers.0', 'model.layers.0.post_attention_layernorm', 'rsqrt_1']"
              },
              {
                "key": "[metadata] pkg.torch.onnx.stack_trace",
                "value": "File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 688, in forward\n    outputs: BaseModelOutputWithPast = self.model(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 453, in forward\n    layer_outputs = decoder_layer(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 323, in forward\n    hidden_states = self.post_attention_layernorm(hidden_states)\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 72, in forward\n    hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)"
              }
            ],
            "incomingEdges": [
              {
                "sourceNodeId": "node_Add_485",
                "sourceNodeOutputId": "0",
                "targetNodeInputId": "0"
              }
            ],
            "outputsMetadata": [
              {
                "id": "0",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "val_354"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "FLOAT[s34,s16,1]"
                  },
                  {
                    "key": "param_name",
                    "value": "Y"
                  }
                ]
              }
            ],
            "inputsMetadata": [
              {
                "id": "0",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "add_426"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "FLOAT[s34,s16,1]"
                  },
                  {
                    "key": "param_name",
                    "value": "X"
                  }
                ]
              }
            ],
            "style": null,
            "config": null
          },
          {
            "id": "node_Reciprocal_487",
            "label": "Reciprocal",
            "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.0: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.0.post_attention_layernorm: transformers.models.llama.modeling_llama.LlamaRMSNorm/rsqrt_1: aten.rsqrt.default",
            "subgraphIds": [],
            "attrs": [
              {
                "key": "[metadata] namespace",
                "value": ": transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.0: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.0.post_attention_layernorm: transformers.models.llama.modeling_llama.LlamaRMSNorm/rsqrt_1: aten.rsqrt.default"
              },
              {
                "key": "[metadata] pkg.torch.onnx.class_hierarchy",
                "value": "['transformers.models.llama.modeling_llama.LlamaForCausalLM', 'transformers.models.llama.modeling_llama.LlamaModel', 'transformers.models.llama.modeling_llama.LlamaDecoderLayer', 'transformers.models.llama.modeling_llama.LlamaRMSNorm', 'aten.rsqrt.default']"
              },
              {
                "key": "[metadata] pkg.torch.onnx.fx_node",
                "value": "%rsqrt_1 : [num_users=1] = call_function[target=torch.ops.aten.rsqrt.default](args = (%add_426,), kwargs = {})"
              },
              {
                "key": "[metadata] pkg.torch.onnx.name_scopes",
                "value": "['', 'model', 'model.layers.0', 'model.layers.0.post_attention_layernorm', 'rsqrt_1']"
              },
              {
                "key": "[metadata] pkg.torch.onnx.stack_trace",
                "value": "File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 688, in forward\n    outputs: BaseModelOutputWithPast = self.model(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 453, in forward\n    layer_outputs = decoder_layer(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 323, in forward\n    hidden_states = self.post_attention_layernorm(hidden_states)\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 72, in forward\n    hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)"
              }
            ],
            "incomingEdges": [
              {
                "sourceNodeId": "node_Sqrt_486",
                "sourceNodeOutputId": "0",
                "targetNodeInputId": "0"
              }
            ],
            "outputsMetadata": [
              {
                "id": "0",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "rsqrt_1"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "FLOAT[s34,s16,1]"
                  },
                  {
                    "key": "param_name",
                    "value": "Y"
                  }
                ]
              }
            ],
            "inputsMetadata": [
              {
                "id": "0",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "val_354"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "FLOAT[s34,s16,1]"
                  },
                  {
                    "key": "param_name",
                    "value": "X"
                  }
                ]
              }
            ],
            "style": null,
            "config": null
          },
          {
            "id": "node_Mul_488",
            "label": "Mul",
            "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.0: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.0.post_attention_layernorm: transformers.models.llama.modeling_llama.LlamaRMSNorm/mul_348: aten.mul.Tensor",
            "subgraphIds": [],
            "attrs": [
              {
                "key": "[metadata] namespace",
                "value": ": transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.0: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.0.post_attention_layernorm: transformers.models.llama.modeling_llama.LlamaRMSNorm/mul_348: aten.mul.Tensor"
              },
              {
                "key": "[metadata] pkg.torch.onnx.class_hierarchy",
                "value": "['transformers.models.llama.modeling_llama.LlamaForCausalLM', 'transformers.models.llama.modeling_llama.LlamaModel', 'transformers.models.llama.modeling_llama.LlamaDecoderLayer', 'transformers.models.llama.modeling_llama.LlamaRMSNorm', 'aten.mul.Tensor']"
              },
              {
                "key": "[metadata] pkg.torch.onnx.fx_node",
                "value": "%mul_348 : [num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%add_413, %rsqrt_1), kwargs = {})"
              },
              {
                "key": "[metadata] pkg.torch.onnx.name_scopes",
                "value": "['', 'model', 'model.layers.0', 'model.layers.0.post_attention_layernorm', 'mul_348']"
              },
              {
                "key": "[metadata] pkg.torch.onnx.stack_trace",
                "value": "File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 688, in forward\n    outputs: BaseModelOutputWithPast = self.model(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 453, in forward\n    layer_outputs = decoder_layer(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 323, in forward\n    hidden_states = self.post_attention_layernorm(hidden_states)\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 72, in forward\n    hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)"
              }
            ],
            "incomingEdges": [
              {
                "sourceNodeId": "node_Add_479",
                "sourceNodeOutputId": "0",
                "targetNodeInputId": "0"
              },
              {
                "sourceNodeId": "node_Reciprocal_487",
                "sourceNodeOutputId": "0",
                "targetNodeInputId": "1"
              }
            ],
            "outputsMetadata": [
              {
                "id": "0",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "mul_348"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "FLOAT[s34,s16,2048]"
                  },
                  {
                    "key": "param_name",
                    "value": "C"
                  }
                ]
              }
            ],
            "inputsMetadata": [
              {
                "id": "0",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "add_413"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "FLOAT[s34,s16,2048]"
                  },
                  {
                    "key": "param_name",
                    "value": "A"
                  }
                ]
              },
              {
                "id": "1",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "rsqrt_1"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "FLOAT[s34,s16,1]"
                  },
                  {
                    "key": "param_name",
                    "value": "B"
                  }
                ]
              }
            ],
            "style": null,
            "config": null
          },
          {
            "id": "node_Mul_489",
            "label": "Mul",
            "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.0: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.0.post_attention_layernorm: transformers.models.llama.modeling_llama.LlamaRMSNorm/mul_352: aten.mul.Tensor",
            "subgraphIds": [],
            "attrs": [
              {
                "key": "[metadata] namespace",
                "value": ": transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.0: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.0.post_attention_layernorm: transformers.models.llama.modeling_llama.LlamaRMSNorm/mul_352: aten.mul.Tensor"
              },
              {
                "key": "[metadata] pkg.torch.onnx.class_hierarchy",
                "value": "['transformers.models.llama.modeling_llama.LlamaForCausalLM', 'transformers.models.llama.modeling_llama.LlamaModel', 'transformers.models.llama.modeling_llama.LlamaDecoderLayer', 'transformers.models.llama.modeling_llama.LlamaRMSNorm', 'aten.mul.Tensor']"
              },
              {
                "key": "[metadata] pkg.torch.onnx.fx_node",
                "value": "%mul_352 : [num_users=2] = call_function[target=torch.ops.aten.mul.Tensor](args = (%p_model_layers_0_post_attention_layernorm_weight, %mul_348), kwargs = {})"
              },
              {
                "key": "[metadata] pkg.torch.onnx.name_scopes",
                "value": "['', 'model', 'model.layers.0', 'model.layers.0.post_attention_layernorm', 'mul_352']"
              },
              {
                "key": "[metadata] pkg.torch.onnx.stack_trace",
                "value": "File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 688, in forward\n    outputs: BaseModelOutputWithPast = self.model(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 453, in forward\n    layer_outputs = decoder_layer(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 323, in forward\n    hidden_states = self.post_attention_layernorm(hidden_states)\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 73, in forward\n    return self.weight * hidden_states.to(input_dtype)"
              }
            ],
            "incomingEdges": [
              {
                "sourceNodeId": "[value] model.layers.0.post_attention_layernorm.weight",
                "sourceNodeOutputId": "0",
                "targetNodeInputId": "0"
              },
              {
                "sourceNodeId": "node_Mul_488",
                "sourceNodeOutputId": "0",
                "targetNodeInputId": "1"
              }
            ],
            "outputsMetadata": [
              {
                "id": "0",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "mul_352"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "FLOAT[s34,s16,2048]"
                  },
                  {
                    "key": "param_name",
                    "value": "C"
                  }
                ]
              }
            ],
            "inputsMetadata": [
              {
                "id": "0",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "model.layers.0.post_attention_layernorm.weight"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "FLOAT[2048]"
                  },
                  {
                    "key": "param_name",
                    "value": "A"
                  }
                ]
              },
              {
                "id": "1",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "mul_348"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "FLOAT[s34,s16,2048]"
                  },
                  {
                    "key": "param_name",
                    "value": "B"
                  }
                ]
              }
            ],
            "style": null,
            "config": null
          },
          {
            "id": "node_Transpose_490",
            "label": "Transpose",
            "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.0: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.0.mlp: transformers.models.llama.modeling_llama.LlamaMLP/model.layers.0.mlp.gate_proj: torch.nn.modules.linear.Linear/linear_4: aten.linear.default",
            "subgraphIds": [],
            "attrs": [
              {
                "key": "perm",
                "value": "[1, 0]"
              },
              {
                "key": "[metadata] namespace",
                "value": ": transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.0: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.0.mlp: transformers.models.llama.modeling_llama.LlamaMLP/model.layers.0.mlp.gate_proj: torch.nn.modules.linear.Linear/linear_4: aten.linear.default"
              },
              {
                "key": "[metadata] pkg.torch.onnx.class_hierarchy",
                "value": "['transformers.models.llama.modeling_llama.LlamaForCausalLM', 'transformers.models.llama.modeling_llama.LlamaModel', 'transformers.models.llama.modeling_llama.LlamaDecoderLayer', 'transformers.models.llama.modeling_llama.LlamaMLP', 'torch.nn.modules.linear.Linear', 'aten.linear.default']"
              },
              {
                "key": "[metadata] pkg.torch.onnx.fx_node",
                "value": "%linear_4 : [num_users=1] = call_function[target=torch.ops.aten.linear.default](args = (%mul_352, %p_model_layers_0_mlp_gate_proj_weight), kwargs = {})"
              },
              {
                "key": "[metadata] pkg.torch.onnx.name_scopes",
                "value": "['', 'model', 'model.layers.0', 'model.layers.0.mlp', 'model.layers.0.mlp.gate_proj', 'linear_4']"
              },
              {
                "key": "[metadata] pkg.torch.onnx.stack_trace",
                "value": "File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 688, in forward\n    outputs: BaseModelOutputWithPast = self.model(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 453, in forward\n    layer_outputs = decoder_layer(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 324, in forward\n    hidden_states = self.mlp(hidden_states)\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 162, in forward\n    down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/torch/nn/modules/linear.py\", line 125, in forward\n    return F.linear(input, self.weight, self.bias)"
              }
            ],
            "incomingEdges": [
              {
                "sourceNodeId": "[value] model.layers.0.mlp.gate_proj.weight",
                "sourceNodeOutputId": "0",
                "targetNodeInputId": "0"
              }
            ],
            "outputsMetadata": [
              {
                "id": "0",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "val_355"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "FLOAT[2048,8192]"
                  },
                  {
                    "key": "param_name",
                    "value": "transposed"
                  }
                ]
              }
            ],
            "inputsMetadata": [
              {
                "id": "0",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "model.layers.0.mlp.gate_proj.weight"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "FLOAT[8192,2048]"
                  },
                  {
                    "key": "param_name",
                    "value": "data"
                  }
                ]
              }
            ],
            "style": null,
            "config": null
          },
          {
            "id": "node_MatMul_491",
            "label": "MatMul",
            "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.0: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.0.mlp: transformers.models.llama.modeling_llama.LlamaMLP/model.layers.0.mlp.gate_proj: torch.nn.modules.linear.Linear/linear_4: aten.linear.default",
            "subgraphIds": [],
            "attrs": [
              {
                "key": "[metadata] namespace",
                "value": ": transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.0: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.0.mlp: transformers.models.llama.modeling_llama.LlamaMLP/model.layers.0.mlp.gate_proj: torch.nn.modules.linear.Linear/linear_4: aten.linear.default"
              },
              {
                "key": "[metadata] pkg.torch.onnx.class_hierarchy",
                "value": "['transformers.models.llama.modeling_llama.LlamaForCausalLM', 'transformers.models.llama.modeling_llama.LlamaModel', 'transformers.models.llama.modeling_llama.LlamaDecoderLayer', 'transformers.models.llama.modeling_llama.LlamaMLP', 'torch.nn.modules.linear.Linear', 'aten.linear.default']"
              },
              {
                "key": "[metadata] pkg.torch.onnx.fx_node",
                "value": "%linear_4 : [num_users=1] = call_function[target=torch.ops.aten.linear.default](args = (%mul_352, %p_model_layers_0_mlp_gate_proj_weight), kwargs = {})"
              },
              {
                "key": "[metadata] pkg.torch.onnx.name_scopes",
                "value": "['', 'model', 'model.layers.0', 'model.layers.0.mlp', 'model.layers.0.mlp.gate_proj', 'linear_4']"
              },
              {
                "key": "[metadata] pkg.torch.onnx.stack_trace",
                "value": "File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 688, in forward\n    outputs: BaseModelOutputWithPast = self.model(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 453, in forward\n    layer_outputs = decoder_layer(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 324, in forward\n    hidden_states = self.mlp(hidden_states)\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 162, in forward\n    down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/torch/nn/modules/linear.py\", line 125, in forward\n    return F.linear(input, self.weight, self.bias)"
              }
            ],
            "incomingEdges": [
              {
                "sourceNodeId": "node_Mul_489",
                "sourceNodeOutputId": "0",
                "targetNodeInputId": "0"
              },
              {
                "sourceNodeId": "node_Transpose_490",
                "sourceNodeOutputId": "0",
                "targetNodeInputId": "1"
              }
            ],
            "outputsMetadata": [
              {
                "id": "0",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "linear_4"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "FLOAT[s34,s16,8192]"
                  },
                  {
                    "key": "param_name",
                    "value": "Y"
                  }
                ]
              }
            ],
            "inputsMetadata": [
              {
                "id": "0",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "mul_352"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "FLOAT[s34,s16,2048]"
                  },
                  {
                    "key": "param_name",
                    "value": "A"
                  }
                ]
              },
              {
                "id": "1",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "val_355"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "FLOAT[2048,8192]"
                  },
                  {
                    "key": "param_name",
                    "value": "B"
                  }
                ]
              }
            ],
            "style": null,
            "config": null
          },
          {
            "id": "node_Sigmoid_492",
            "label": "Sigmoid",
            "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.0: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.0.mlp: transformers.models.llama.modeling_llama.LlamaMLP/model.layers.0.mlp.act_fn: torch.nn.modules.activation.SiLU/silu: aten.silu.default",
            "subgraphIds": [],
            "attrs": [
              {
                "key": "[metadata] namespace",
                "value": ": transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.0: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.0.mlp: transformers.models.llama.modeling_llama.LlamaMLP/model.layers.0.mlp.act_fn: torch.nn.modules.activation.SiLU/silu: aten.silu.default"
              },
              {
                "key": "[metadata] pkg.torch.onnx.class_hierarchy",
                "value": "['transformers.models.llama.modeling_llama.LlamaForCausalLM', 'transformers.models.llama.modeling_llama.LlamaModel', 'transformers.models.llama.modeling_llama.LlamaDecoderLayer', 'transformers.models.llama.modeling_llama.LlamaMLP', 'torch.nn.modules.activation.SiLU', 'aten.silu.default']"
              },
              {
                "key": "[metadata] pkg.torch.onnx.fx_node",
                "value": "%silu : [num_users=1] = call_function[target=torch.ops.aten.silu.default](args = (%linear_4,), kwargs = {})"
              },
              {
                "key": "[metadata] pkg.torch.onnx.name_scopes",
                "value": "['', 'model', 'model.layers.0', 'model.layers.0.mlp', 'model.layers.0.mlp.act_fn', 'silu']"
              },
              {
                "key": "[metadata] pkg.torch.onnx.stack_trace",
                "value": "File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 688, in forward\n    outputs: BaseModelOutputWithPast = self.model(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 453, in forward\n    layer_outputs = decoder_layer(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 324, in forward\n    hidden_states = self.mlp(hidden_states)\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 162, in forward\n    down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/torch/nn/modules/activation.py\", line 434, in forward\n    return F.silu(input, inplace=self.inplace)"
              }
            ],
            "incomingEdges": [
              {
                "sourceNodeId": "node_MatMul_491",
                "sourceNodeOutputId": "0",
                "targetNodeInputId": "0"
              }
            ],
            "outputsMetadata": [
              {
                "id": "0",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "val_356"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "FLOAT[s34,s16,8192]"
                  },
                  {
                    "key": "param_name",
                    "value": "Y"
                  }
                ]
              }
            ],
            "inputsMetadata": [
              {
                "id": "0",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "linear_4"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "FLOAT[s34,s16,8192]"
                  },
                  {
                    "key": "param_name",
                    "value": "X"
                  }
                ]
              }
            ],
            "style": null,
            "config": null
          },
          {
            "id": "node_Mul_493",
            "label": "Mul",
            "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.0: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.0.mlp: transformers.models.llama.modeling_llama.LlamaMLP/model.layers.0.mlp.act_fn: torch.nn.modules.activation.SiLU/silu: aten.silu.default",
            "subgraphIds": [],
            "attrs": [
              {
                "key": "[metadata] namespace",
                "value": ": transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.0: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.0.mlp: transformers.models.llama.modeling_llama.LlamaMLP/model.layers.0.mlp.act_fn: torch.nn.modules.activation.SiLU/silu: aten.silu.default"
              },
              {
                "key": "[metadata] pkg.torch.onnx.class_hierarchy",
                "value": "['transformers.models.llama.modeling_llama.LlamaForCausalLM', 'transformers.models.llama.modeling_llama.LlamaModel', 'transformers.models.llama.modeling_llama.LlamaDecoderLayer', 'transformers.models.llama.modeling_llama.LlamaMLP', 'torch.nn.modules.activation.SiLU', 'aten.silu.default']"
              },
              {
                "key": "[metadata] pkg.torch.onnx.fx_node",
                "value": "%silu : [num_users=1] = call_function[target=torch.ops.aten.silu.default](args = (%linear_4,), kwargs = {})"
              },
              {
                "key": "[metadata] pkg.torch.onnx.name_scopes",
                "value": "['', 'model', 'model.layers.0', 'model.layers.0.mlp', 'model.layers.0.mlp.act_fn', 'silu']"
              },
              {
                "key": "[metadata] pkg.torch.onnx.stack_trace",
                "value": "File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 688, in forward\n    outputs: BaseModelOutputWithPast = self.model(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 453, in forward\n    layer_outputs = decoder_layer(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 324, in forward\n    hidden_states = self.mlp(hidden_states)\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 162, in forward\n    down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/torch/nn/modules/activation.py\", line 434, in forward\n    return F.silu(input, inplace=self.inplace)"
              }
            ],
            "incomingEdges": [
              {
                "sourceNodeId": "node_MatMul_491",
                "sourceNodeOutputId": "0",
                "targetNodeInputId": "0"
              },
              {
                "sourceNodeId": "node_Sigmoid_492",
                "sourceNodeOutputId": "0",
                "targetNodeInputId": "1"
              }
            ],
            "outputsMetadata": [
              {
                "id": "0",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "silu"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "FLOAT[s34,s16,8192]"
                  },
                  {
                    "key": "param_name",
                    "value": "C"
                  }
                ]
              }
            ],
            "inputsMetadata": [
              {
                "id": "0",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "linear_4"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "FLOAT[s34,s16,8192]"
                  },
                  {
                    "key": "param_name",
                    "value": "A"
                  }
                ]
              },
              {
                "id": "1",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "val_356"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "FLOAT[s34,s16,8192]"
                  },
                  {
                    "key": "param_name",
                    "value": "B"
                  }
                ]
              }
            ],
            "style": null,
            "config": null
          },
          {
            "id": "node_Transpose_494",
            "label": "Transpose",
            "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.0: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.0.mlp: transformers.models.llama.modeling_llama.LlamaMLP/model.layers.0.mlp.up_proj: torch.nn.modules.linear.Linear/linear_5: aten.linear.default",
            "subgraphIds": [],
            "attrs": [
              {
                "key": "perm",
                "value": "[1, 0]"
              },
              {
                "key": "[metadata] namespace",
                "value": ": transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.0: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.0.mlp: transformers.models.llama.modeling_llama.LlamaMLP/model.layers.0.mlp.up_proj: torch.nn.modules.linear.Linear/linear_5: aten.linear.default"
              },
              {
                "key": "[metadata] pkg.torch.onnx.class_hierarchy",
                "value": "['transformers.models.llama.modeling_llama.LlamaForCausalLM', 'transformers.models.llama.modeling_llama.LlamaModel', 'transformers.models.llama.modeling_llama.LlamaDecoderLayer', 'transformers.models.llama.modeling_llama.LlamaMLP', 'torch.nn.modules.linear.Linear', 'aten.linear.default']"
              },
              {
                "key": "[metadata] pkg.torch.onnx.fx_node",
                "value": "%linear_5 : [num_users=1] = call_function[target=torch.ops.aten.linear.default](args = (%mul_352, %p_model_layers_0_mlp_up_proj_weight), kwargs = {})"
              },
              {
                "key": "[metadata] pkg.torch.onnx.name_scopes",
                "value": "['', 'model', 'model.layers.0', 'model.layers.0.mlp', 'model.layers.0.mlp.up_proj', 'linear_5']"
              },
              {
                "key": "[metadata] pkg.torch.onnx.stack_trace",
                "value": "File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 688, in forward\n    outputs: BaseModelOutputWithPast = self.model(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 453, in forward\n    layer_outputs = decoder_layer(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 324, in forward\n    hidden_states = self.mlp(hidden_states)\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 162, in forward\n    down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/torch/nn/modules/linear.py\", line 125, in forward\n    return F.linear(input, self.weight, self.bias)"
              }
            ],
            "incomingEdges": [
              {
                "sourceNodeId": "[value] model.layers.0.mlp.up_proj.weight",
                "sourceNodeOutputId": "0",
                "targetNodeInputId": "0"
              }
            ],
            "outputsMetadata": [
              {
                "id": "0",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "val_357"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "FLOAT[2048,8192]"
                  },
                  {
                    "key": "param_name",
                    "value": "transposed"
                  }
                ]
              }
            ],
            "inputsMetadata": [
              {
                "id": "0",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "model.layers.0.mlp.up_proj.weight"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "FLOAT[8192,2048]"
                  },
                  {
                    "key": "param_name",
                    "value": "data"
                  }
                ]
              }
            ],
            "style": null,
            "config": null
          },
          {
            "id": "node_MatMul_495",
            "label": "MatMul",
            "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.0: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.0.mlp: transformers.models.llama.modeling_llama.LlamaMLP/model.layers.0.mlp.up_proj: torch.nn.modules.linear.Linear/linear_5: aten.linear.default",
            "subgraphIds": [],
            "attrs": [
              {
                "key": "[metadata] namespace",
                "value": ": transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.0: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.0.mlp: transformers.models.llama.modeling_llama.LlamaMLP/model.layers.0.mlp.up_proj: torch.nn.modules.linear.Linear/linear_5: aten.linear.default"
              },
              {
                "key": "[metadata] pkg.torch.onnx.class_hierarchy",
                "value": "['transformers.models.llama.modeling_llama.LlamaForCausalLM', 'transformers.models.llama.modeling_llama.LlamaModel', 'transformers.models.llama.modeling_llama.LlamaDecoderLayer', 'transformers.models.llama.modeling_llama.LlamaMLP', 'torch.nn.modules.linear.Linear', 'aten.linear.default']"
              },
              {
                "key": "[metadata] pkg.torch.onnx.fx_node",
                "value": "%linear_5 : [num_users=1] = call_function[target=torch.ops.aten.linear.default](args = (%mul_352, %p_model_layers_0_mlp_up_proj_weight), kwargs = {})"
              },
              {
                "key": "[metadata] pkg.torch.onnx.name_scopes",
                "value": "['', 'model', 'model.layers.0', 'model.layers.0.mlp', 'model.layers.0.mlp.up_proj', 'linear_5']"
              },
              {
                "key": "[metadata] pkg.torch.onnx.stack_trace",
                "value": "File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 688, in forward\n    outputs: BaseModelOutputWithPast = self.model(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 453, in forward\n    layer_outputs = decoder_layer(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 324, in forward\n    hidden_states = self.mlp(hidden_states)\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 162, in forward\n    down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/torch/nn/modules/linear.py\", line 125, in forward\n    return F.linear(input, self.weight, self.bias)"
              }
            ],
            "incomingEdges": [
              {
                "sourceNodeId": "node_Mul_489",
                "sourceNodeOutputId": "0",
                "targetNodeInputId": "0"
              },
              {
                "sourceNodeId": "node_Transpose_494",
                "sourceNodeOutputId": "0",
                "targetNodeInputId": "1"
              }
            ],
            "outputsMetadata": [
              {
                "id": "0",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "linear_5"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "FLOAT[s34,s16,8192]"
                  },
                  {
                    "key": "param_name",
                    "value": "Y"
                  }
                ]
              }
            ],
            "inputsMetadata": [
              {
                "id": "0",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "mul_352"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "FLOAT[s34,s16,2048]"
                  },
                  {
                    "key": "param_name",
                    "value": "A"
                  }
                ]
              },
              {
                "id": "1",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "val_357"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "FLOAT[2048,8192]"
                  },
                  {
                    "key": "param_name",
                    "value": "B"
                  }
                ]
              }
            ],
            "style": null,
            "config": null
          },
          {
            "id": "node_Mul_496",
            "label": "Mul",
            "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.0: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.0.mlp: transformers.models.llama.modeling_llama.LlamaMLP/mul_365: aten.mul.Tensor",
            "subgraphIds": [],
            "attrs": [
              {
                "key": "[metadata] namespace",
                "value": ": transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.0: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.0.mlp: transformers.models.llama.modeling_llama.LlamaMLP/mul_365: aten.mul.Tensor"
              },
              {
                "key": "[metadata] pkg.torch.onnx.class_hierarchy",
                "value": "['transformers.models.llama.modeling_llama.LlamaForCausalLM', 'transformers.models.llama.modeling_llama.LlamaModel', 'transformers.models.llama.modeling_llama.LlamaDecoderLayer', 'transformers.models.llama.modeling_llama.LlamaMLP', 'aten.mul.Tensor']"
              },
              {
                "key": "[metadata] pkg.torch.onnx.fx_node",
                "value": "%mul_365 : [num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%silu, %linear_5), kwargs = {})"
              },
              {
                "key": "[metadata] pkg.torch.onnx.name_scopes",
                "value": "['', 'model', 'model.layers.0', 'model.layers.0.mlp', 'mul_365']"
              },
              {
                "key": "[metadata] pkg.torch.onnx.stack_trace",
                "value": "File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 688, in forward\n    outputs: BaseModelOutputWithPast = self.model(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 453, in forward\n    layer_outputs = decoder_layer(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 324, in forward\n    hidden_states = self.mlp(hidden_states)\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 162, in forward\n    down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))"
              }
            ],
            "incomingEdges": [
              {
                "sourceNodeId": "node_Mul_493",
                "sourceNodeOutputId": "0",
                "targetNodeInputId": "0"
              },
              {
                "sourceNodeId": "node_MatMul_495",
                "sourceNodeOutputId": "0",
                "targetNodeInputId": "1"
              }
            ],
            "outputsMetadata": [
              {
                "id": "0",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "mul_365"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "FLOAT[s34,s16,8192]"
                  },
                  {
                    "key": "param_name",
                    "value": "C"
                  }
                ]
              }
            ],
            "inputsMetadata": [
              {
                "id": "0",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "silu"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "FLOAT[s34,s16,8192]"
                  },
                  {
                    "key": "param_name",
                    "value": "A"
                  }
                ]
              },
              {
                "id": "1",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "linear_5"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "FLOAT[s34,s16,8192]"
                  },
                  {
                    "key": "param_name",
                    "value": "B"
                  }
                ]
              }
            ],
            "style": null,
            "config": null
          },
          {
            "id": "node_Transpose_497",
            "label": "Transpose",
            "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.0: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.0.mlp: transformers.models.llama.modeling_llama.LlamaMLP/model.layers.0.mlp.down_proj: torch.nn.modules.linear.Linear/linear_6: aten.linear.default",
            "subgraphIds": [],
            "attrs": [
              {
                "key": "perm",
                "value": "[1, 0]"
              },
              {
                "key": "[metadata] namespace",
                "value": ": transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.0: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.0.mlp: transformers.models.llama.modeling_llama.LlamaMLP/model.layers.0.mlp.down_proj: torch.nn.modules.linear.Linear/linear_6: aten.linear.default"
              },
              {
                "key": "[metadata] pkg.torch.onnx.class_hierarchy",
                "value": "['transformers.models.llama.modeling_llama.LlamaForCausalLM', 'transformers.models.llama.modeling_llama.LlamaModel', 'transformers.models.llama.modeling_llama.LlamaDecoderLayer', 'transformers.models.llama.modeling_llama.LlamaMLP', 'torch.nn.modules.linear.Linear', 'aten.linear.default']"
              },
              {
                "key": "[metadata] pkg.torch.onnx.fx_node",
                "value": "%linear_6 : [num_users=1] = call_function[target=torch.ops.aten.linear.default](args = (%mul_365, %p_model_layers_0_mlp_down_proj_weight), kwargs = {})"
              },
              {
                "key": "[metadata] pkg.torch.onnx.name_scopes",
                "value": "['', 'model', 'model.layers.0', 'model.layers.0.mlp', 'model.layers.0.mlp.down_proj', 'linear_6']"
              },
              {
                "key": "[metadata] pkg.torch.onnx.stack_trace",
                "value": "File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 688, in forward\n    outputs: BaseModelOutputWithPast = self.model(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 453, in forward\n    layer_outputs = decoder_layer(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 324, in forward\n    hidden_states = self.mlp(hidden_states)\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 162, in forward\n    down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/torch/nn/modules/linear.py\", line 125, in forward\n    return F.linear(input, self.weight, self.bias)"
              }
            ],
            "incomingEdges": [
              {
                "sourceNodeId": "[value] model.layers.0.mlp.down_proj.weight",
                "sourceNodeOutputId": "0",
                "targetNodeInputId": "0"
              }
            ],
            "outputsMetadata": [
              {
                "id": "0",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "val_358"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "FLOAT[8192,2048]"
                  },
                  {
                    "key": "param_name",
                    "value": "transposed"
                  }
                ]
              }
            ],
            "inputsMetadata": [
              {
                "id": "0",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "model.layers.0.mlp.down_proj.weight"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "FLOAT[2048,8192]"
                  },
                  {
                    "key": "param_name",
                    "value": "data"
                  }
                ]
              }
            ],
            "style": null,
            "config": null
          },
          {
            "id": "node_MatMul_498",
            "label": "MatMul",
            "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.0: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.0.mlp: transformers.models.llama.modeling_llama.LlamaMLP/model.layers.0.mlp.down_proj: torch.nn.modules.linear.Linear/linear_6: aten.linear.default",
            "subgraphIds": [],
            "attrs": [
              {
                "key": "[metadata] namespace",
                "value": ": transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.0: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.0.mlp: transformers.models.llama.modeling_llama.LlamaMLP/model.layers.0.mlp.down_proj: torch.nn.modules.linear.Linear/linear_6: aten.linear.default"
              },
              {
                "key": "[metadata] pkg.torch.onnx.class_hierarchy",
                "value": "['transformers.models.llama.modeling_llama.LlamaForCausalLM', 'transformers.models.llama.modeling_llama.LlamaModel', 'transformers.models.llama.modeling_llama.LlamaDecoderLayer', 'transformers.models.llama.modeling_llama.LlamaMLP', 'torch.nn.modules.linear.Linear', 'aten.linear.default']"
              },
              {
                "key": "[metadata] pkg.torch.onnx.fx_node",
                "value": "%linear_6 : [num_users=1] = call_function[target=torch.ops.aten.linear.default](args = (%mul_365, %p_model_layers_0_mlp_down_proj_weight), kwargs = {})"
              },
              {
                "key": "[metadata] pkg.torch.onnx.name_scopes",
                "value": "['', 'model', 'model.layers.0', 'model.layers.0.mlp', 'model.layers.0.mlp.down_proj', 'linear_6']"
              },
              {
                "key": "[metadata] pkg.torch.onnx.stack_trace",
                "value": "File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 688, in forward\n    outputs: BaseModelOutputWithPast = self.model(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 453, in forward\n    layer_outputs = decoder_layer(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 324, in forward\n    hidden_states = self.mlp(hidden_states)\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 162, in forward\n    down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/torch/nn/modules/linear.py\", line 125, in forward\n    return F.linear(input, self.weight, self.bias)"
              }
            ],
            "incomingEdges": [
              {
                "sourceNodeId": "node_Mul_496",
                "sourceNodeOutputId": "0",
                "targetNodeInputId": "0"
              },
              {
                "sourceNodeId": "node_Transpose_497",
                "sourceNodeOutputId": "0",
                "targetNodeInputId": "1"
              }
            ],
            "outputsMetadata": [
              {
                "id": "0",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "linear_6"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "FLOAT[s34,s16,2048]"
                  },
                  {
                    "key": "param_name",
                    "value": "Y"
                  }
                ]
              }
            ],
            "inputsMetadata": [
              {
                "id": "0",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "mul_365"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "FLOAT[s34,s16,8192]"
                  },
                  {
                    "key": "param_name",
                    "value": "A"
                  }
                ]
              },
              {
                "id": "1",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "val_358"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "FLOAT[8192,2048]"
                  },
                  {
                    "key": "param_name",
                    "value": "B"
                  }
                ]
              }
            ],
            "style": null,
            "config": null
          },
          {
            "id": "node_Add_499",
            "label": "Add",
            "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.0: transformers.models.llama.modeling_llama.LlamaDecoderLayer/add_463: aten.add.Tensor",
            "subgraphIds": [],
            "attrs": [
              {
                "key": "[metadata] namespace",
                "value": ": transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.0: transformers.models.llama.modeling_llama.LlamaDecoderLayer/add_463: aten.add.Tensor"
              },
              {
                "key": "[metadata] pkg.torch.onnx.class_hierarchy",
                "value": "['transformers.models.llama.modeling_llama.LlamaForCausalLM', 'transformers.models.llama.modeling_llama.LlamaModel', 'transformers.models.llama.modeling_llama.LlamaDecoderLayer', 'aten.add.Tensor']"
              },
              {
                "key": "[metadata] pkg.torch.onnx.fx_node",
                "value": "%add_463 : [num_users=3] = call_function[target=torch.ops.aten.add.Tensor](args = (%add_413, %linear_6), kwargs = {})"
              },
              {
                "key": "[metadata] pkg.torch.onnx.name_scopes",
                "value": "['', 'model', 'model.layers.0', 'add_463']"
              },
              {
                "key": "[metadata] pkg.torch.onnx.stack_trace",
                "value": "File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 688, in forward\n    outputs: BaseModelOutputWithPast = self.model(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 453, in forward\n    layer_outputs = decoder_layer(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 325, in forward\n    hidden_states = residual + hidden_states"
              }
            ],
            "incomingEdges": [
              {
                "sourceNodeId": "node_Add_479",
                "sourceNodeOutputId": "0",
                "targetNodeInputId": "0"
              },
              {
                "sourceNodeId": "node_MatMul_498",
                "sourceNodeOutputId": "0",
                "targetNodeInputId": "1"
              }
            ],
            "outputsMetadata": [
              {
                "id": "0",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "add_463"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "FLOAT[s34,s16,2048]"
                  },
                  {
                    "key": "param_name",
                    "value": "C"
                  }
                ]
              }
            ],
            "inputsMetadata": [
              {
                "id": "0",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "add_413"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "FLOAT[s34,s16,2048]"
                  },
                  {
                    "key": "param_name",
                    "value": "A"
                  }
                ]
              },
              {
                "id": "1",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "linear_6"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "FLOAT[s34,s16,2048]"
                  },
                  {
                    "key": "param_name",
                    "value": "B"
                  }
                ]
              }
            ],
            "style": null,
            "config": null
          },
          {
            "id": "node_Constant_500",
            "label": "Constant",
            "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.1: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.1.input_layernorm: transformers.models.llama.modeling_llama.LlamaRMSNorm/pow_3: aten.pow.Tensor_Scalar",
            "subgraphIds": [],
            "attrs": [
              {
                "key": "__value",
                "value": "2.0"
              },
              {
                "key": "value",
                "value": "TensorProtoTensor<FLOAT,[]>(array(2., dtype=float32), name='')"
              },
              {
                "key": "[metadata] namespace",
                "value": ": transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.1: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.1.input_layernorm: transformers.models.llama.modeling_llama.LlamaRMSNorm/pow_3: aten.pow.Tensor_Scalar"
              },
              {
                "key": "[metadata] pkg.torch.onnx.class_hierarchy",
                "value": "['transformers.models.llama.modeling_llama.LlamaForCausalLM', 'transformers.models.llama.modeling_llama.LlamaModel', 'transformers.models.llama.modeling_llama.LlamaDecoderLayer', 'transformers.models.llama.modeling_llama.LlamaRMSNorm', 'aten.pow.Tensor_Scalar']"
              },
              {
                "key": "[metadata] pkg.torch.onnx.fx_node",
                "value": "%pow_3 : [num_users=1] = call_function[target=torch.ops.aten.pow.Tensor_Scalar](args = (%add_463, 2), kwargs = {})"
              },
              {
                "key": "[metadata] pkg.torch.onnx.name_scopes",
                "value": "['', 'model', 'model.layers.1', 'model.layers.1.input_layernorm', 'pow_3']"
              },
              {
                "key": "[metadata] pkg.torch.onnx.stack_trace",
                "value": "File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 688, in forward\n    outputs: BaseModelOutputWithPast = self.model(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 453, in forward\n    layer_outputs = decoder_layer(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 305, in forward\n    hidden_states = self.input_layernorm(hidden_states)\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 71, in forward\n    variance = hidden_states.pow(2).mean(-1, keepdim=True)"
              }
            ],
            "incomingEdges": [],
            "outputsMetadata": [
              {
                "id": "0",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "val_359"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "FLOAT[]"
                  },
                  {
                    "key": "param_name",
                    "value": "output"
                  }
                ]
              }
            ],
            "inputsMetadata": [],
            "style": null,
            "config": null
          },
          {
            "id": "node_Pow_501",
            "label": "Pow",
            "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.1: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.1.input_layernorm: transformers.models.llama.modeling_llama.LlamaRMSNorm/pow_3: aten.pow.Tensor_Scalar",
            "subgraphIds": [],
            "attrs": [
              {
                "key": "[metadata] namespace",
                "value": ": transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.1: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.1.input_layernorm: transformers.models.llama.modeling_llama.LlamaRMSNorm/pow_3: aten.pow.Tensor_Scalar"
              },
              {
                "key": "[metadata] pkg.torch.onnx.class_hierarchy",
                "value": "['transformers.models.llama.modeling_llama.LlamaForCausalLM', 'transformers.models.llama.modeling_llama.LlamaModel', 'transformers.models.llama.modeling_llama.LlamaDecoderLayer', 'transformers.models.llama.modeling_llama.LlamaRMSNorm', 'aten.pow.Tensor_Scalar']"
              },
              {
                "key": "[metadata] pkg.torch.onnx.fx_node",
                "value": "%pow_3 : [num_users=1] = call_function[target=torch.ops.aten.pow.Tensor_Scalar](args = (%add_463, 2), kwargs = {})"
              },
              {
                "key": "[metadata] pkg.torch.onnx.name_scopes",
                "value": "['', 'model', 'model.layers.1', 'model.layers.1.input_layernorm', 'pow_3']"
              },
              {
                "key": "[metadata] pkg.torch.onnx.stack_trace",
                "value": "File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 688, in forward\n    outputs: BaseModelOutputWithPast = self.model(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 453, in forward\n    layer_outputs = decoder_layer(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 305, in forward\n    hidden_states = self.input_layernorm(hidden_states)\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 71, in forward\n    variance = hidden_states.pow(2).mean(-1, keepdim=True)"
              }
            ],
            "incomingEdges": [
              {
                "sourceNodeId": "node_Add_499",
                "sourceNodeOutputId": "0",
                "targetNodeInputId": "0"
              },
              {
                "sourceNodeId": "node_Constant_500",
                "sourceNodeOutputId": "0",
                "targetNodeInputId": "1"
              }
            ],
            "outputsMetadata": [
              {
                "id": "0",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "pow_3"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "FLOAT[s34,s16,2048]"
                  },
                  {
                    "key": "param_name",
                    "value": "Z"
                  }
                ]
              }
            ],
            "inputsMetadata": [
              {
                "id": "0",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "add_463"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "FLOAT[s34,s16,2048]"
                  },
                  {
                    "key": "param_name",
                    "value": "X"
                  }
                ]
              },
              {
                "id": "1",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "val_359"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "FLOAT[]"
                  },
                  {
                    "key": "param_name",
                    "value": "Y"
                  }
                ]
              }
            ],
            "style": null,
            "config": null
          },
          {
            "id": "node_Constant_925",
            "label": "Constant",
            "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.1: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.1.input_layernorm: transformers.models.llama.modeling_llama.LlamaRMSNorm/mean_2: aten.mean.dim",
            "subgraphIds": [],
            "attrs": [
              {
                "key": "__value",
                "value": "[-1]"
              },
              {
                "key": "value",
                "value": "TensorProtoTensor<INT64,[1]>(array([-1]), name='val_361')"
              }
            ],
            "incomingEdges": [],
            "outputsMetadata": [
              {
                "id": "0",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "val_361"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "INT64[1]"
                  },
                  {
                    "key": "param_name",
                    "value": "output"
                  }
                ]
              }
            ],
            "inputsMetadata": [],
            "style": null,
            "config": null
          },
          {
            "id": "node_ReduceMean_504",
            "label": "ReduceMean",
            "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.1: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.1.input_layernorm: transformers.models.llama.modeling_llama.LlamaRMSNorm/mean_2: aten.mean.dim",
            "subgraphIds": [],
            "attrs": [
              {
                "key": "noop_with_empty_axes",
                "value": "0"
              },
              {
                "key": "keepdims",
                "value": "1"
              },
              {
                "key": "[metadata] namespace",
                "value": ": transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.1: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.1.input_layernorm: transformers.models.llama.modeling_llama.LlamaRMSNorm/mean_2: aten.mean.dim"
              },
              {
                "key": "[metadata] pkg.torch.onnx.class_hierarchy",
                "value": "['transformers.models.llama.modeling_llama.LlamaForCausalLM', 'transformers.models.llama.modeling_llama.LlamaModel', 'transformers.models.llama.modeling_llama.LlamaDecoderLayer', 'transformers.models.llama.modeling_llama.LlamaRMSNorm', 'aten.mean.dim']"
              },
              {
                "key": "[metadata] pkg.torch.onnx.fx_node",
                "value": "%mean_2 : [num_users=1] = call_function[target=torch.ops.aten.mean.dim](args = (%pow_3, [-1], True), kwargs = {})"
              },
              {
                "key": "[metadata] pkg.torch.onnx.name_scopes",
                "value": "['', 'model', 'model.layers.1', 'model.layers.1.input_layernorm', 'mean_2']"
              },
              {
                "key": "[metadata] pkg.torch.onnx.stack_trace",
                "value": "File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 688, in forward\n    outputs: BaseModelOutputWithPast = self.model(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 453, in forward\n    layer_outputs = decoder_layer(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 305, in forward\n    hidden_states = self.input_layernorm(hidden_states)\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 71, in forward\n    variance = hidden_states.pow(2).mean(-1, keepdim=True)"
              }
            ],
            "incomingEdges": [
              {
                "sourceNodeId": "node_Pow_501",
                "sourceNodeOutputId": "0",
                "targetNodeInputId": "0"
              },
              {
                "sourceNodeId": "node_Constant_925",
                "sourceNodeOutputId": "0",
                "targetNodeInputId": "1"
              }
            ],
            "outputsMetadata": [
              {
                "id": "0",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "mean_2"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "FLOAT[s34,s16,1]"
                  },
                  {
                    "key": "param_name",
                    "value": "reduced"
                  }
                ]
              }
            ],
            "inputsMetadata": [
              {
                "id": "0",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "pow_3"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "FLOAT[s34,s16,2048]"
                  },
                  {
                    "key": "param_name",
                    "value": "data"
                  }
                ]
              },
              {
                "id": "1",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "val_361"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "INT64[1]"
                  },
                  {
                    "key": "param_name",
                    "value": "axes"
                  }
                ]
              }
            ],
            "style": null,
            "config": null
          },
          {
            "id": "node_Add_505",
            "label": "Add",
            "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.1: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.1.input_layernorm: transformers.models.llama.modeling_llama.LlamaRMSNorm/add_476: aten.add.Tensor",
            "subgraphIds": [],
            "attrs": [
              {
                "key": "[metadata] namespace",
                "value": ": transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.1: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.1.input_layernorm: transformers.models.llama.modeling_llama.LlamaRMSNorm/add_476: aten.add.Tensor"
              },
              {
                "key": "[metadata] pkg.torch.onnx.class_hierarchy",
                "value": "['transformers.models.llama.modeling_llama.LlamaForCausalLM', 'transformers.models.llama.modeling_llama.LlamaModel', 'transformers.models.llama.modeling_llama.LlamaDecoderLayer', 'transformers.models.llama.modeling_llama.LlamaRMSNorm', 'aten.add.Tensor']"
              },
              {
                "key": "[metadata] pkg.torch.onnx.fx_node",
                "value": "%add_476 : [num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%mean_2, 1e-05), kwargs = {})"
              },
              {
                "key": "[metadata] pkg.torch.onnx.name_scopes",
                "value": "['', 'model', 'model.layers.1', 'model.layers.1.input_layernorm', 'add_476']"
              },
              {
                "key": "[metadata] pkg.torch.onnx.stack_trace",
                "value": "File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 688, in forward\n    outputs: BaseModelOutputWithPast = self.model(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 453, in forward\n    layer_outputs = decoder_layer(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 305, in forward\n    hidden_states = self.input_layernorm(hidden_states)\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 72, in forward\n    hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)"
              }
            ],
            "incomingEdges": [
              {
                "sourceNodeId": "node_ReduceMean_504",
                "sourceNodeOutputId": "0",
                "targetNodeInputId": "0"
              },
              {
                "sourceNodeId": "node_Constant_333",
                "sourceNodeOutputId": "0",
                "targetNodeInputId": "1"
              }
            ],
            "outputsMetadata": [
              {
                "id": "0",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "add_476"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "FLOAT[s34,s16,1]"
                  },
                  {
                    "key": "param_name",
                    "value": "C"
                  }
                ]
              }
            ],
            "inputsMetadata": [
              {
                "id": "0",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "mean_2"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "FLOAT[s34,s16,1]"
                  },
                  {
                    "key": "param_name",
                    "value": "A"
                  }
                ]
              },
              {
                "id": "1",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "val_251"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "FLOAT[]"
                  },
                  {
                    "key": "param_name",
                    "value": "B"
                  }
                ]
              }
            ],
            "style": null,
            "config": null
          },
          {
            "id": "node_Sqrt_506",
            "label": "Sqrt",
            "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.1: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.1.input_layernorm: transformers.models.llama.modeling_llama.LlamaRMSNorm/rsqrt_2: aten.rsqrt.default",
            "subgraphIds": [],
            "attrs": [
              {
                "key": "[metadata] namespace",
                "value": ": transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.1: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.1.input_layernorm: transformers.models.llama.modeling_llama.LlamaRMSNorm/rsqrt_2: aten.rsqrt.default"
              },
              {
                "key": "[metadata] pkg.torch.onnx.class_hierarchy",
                "value": "['transformers.models.llama.modeling_llama.LlamaForCausalLM', 'transformers.models.llama.modeling_llama.LlamaModel', 'transformers.models.llama.modeling_llama.LlamaDecoderLayer', 'transformers.models.llama.modeling_llama.LlamaRMSNorm', 'aten.rsqrt.default']"
              },
              {
                "key": "[metadata] pkg.torch.onnx.fx_node",
                "value": "%rsqrt_2 : [num_users=1] = call_function[target=torch.ops.aten.rsqrt.default](args = (%add_476,), kwargs = {})"
              },
              {
                "key": "[metadata] pkg.torch.onnx.name_scopes",
                "value": "['', 'model', 'model.layers.1', 'model.layers.1.input_layernorm', 'rsqrt_2']"
              },
              {
                "key": "[metadata] pkg.torch.onnx.stack_trace",
                "value": "File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 688, in forward\n    outputs: BaseModelOutputWithPast = self.model(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 453, in forward\n    layer_outputs = decoder_layer(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 305, in forward\n    hidden_states = self.input_layernorm(hidden_states)\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 72, in forward\n    hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)"
              }
            ],
            "incomingEdges": [
              {
                "sourceNodeId": "node_Add_505",
                "sourceNodeOutputId": "0",
                "targetNodeInputId": "0"
              }
            ],
            "outputsMetadata": [
              {
                "id": "0",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "val_362"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "FLOAT[s34,s16,1]"
                  },
                  {
                    "key": "param_name",
                    "value": "Y"
                  }
                ]
              }
            ],
            "inputsMetadata": [
              {
                "id": "0",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "add_476"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "FLOAT[s34,s16,1]"
                  },
                  {
                    "key": "param_name",
                    "value": "X"
                  }
                ]
              }
            ],
            "style": null,
            "config": null
          },
          {
            "id": "node_Reciprocal_507",
            "label": "Reciprocal",
            "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.1: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.1.input_layernorm: transformers.models.llama.modeling_llama.LlamaRMSNorm/rsqrt_2: aten.rsqrt.default",
            "subgraphIds": [],
            "attrs": [
              {
                "key": "[metadata] namespace",
                "value": ": transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.1: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.1.input_layernorm: transformers.models.llama.modeling_llama.LlamaRMSNorm/rsqrt_2: aten.rsqrt.default"
              },
              {
                "key": "[metadata] pkg.torch.onnx.class_hierarchy",
                "value": "['transformers.models.llama.modeling_llama.LlamaForCausalLM', 'transformers.models.llama.modeling_llama.LlamaModel', 'transformers.models.llama.modeling_llama.LlamaDecoderLayer', 'transformers.models.llama.modeling_llama.LlamaRMSNorm', 'aten.rsqrt.default']"
              },
              {
                "key": "[metadata] pkg.torch.onnx.fx_node",
                "value": "%rsqrt_2 : [num_users=1] = call_function[target=torch.ops.aten.rsqrt.default](args = (%add_476,), kwargs = {})"
              },
              {
                "key": "[metadata] pkg.torch.onnx.name_scopes",
                "value": "['', 'model', 'model.layers.1', 'model.layers.1.input_layernorm', 'rsqrt_2']"
              },
              {
                "key": "[metadata] pkg.torch.onnx.stack_trace",
                "value": "File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 688, in forward\n    outputs: BaseModelOutputWithPast = self.model(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 453, in forward\n    layer_outputs = decoder_layer(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 305, in forward\n    hidden_states = self.input_layernorm(hidden_states)\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 72, in forward\n    hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)"
              }
            ],
            "incomingEdges": [
              {
                "sourceNodeId": "node_Sqrt_506",
                "sourceNodeOutputId": "0",
                "targetNodeInputId": "0"
              }
            ],
            "outputsMetadata": [
              {
                "id": "0",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "rsqrt_2"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "FLOAT[s34,s16,1]"
                  },
                  {
                    "key": "param_name",
                    "value": "Y"
                  }
                ]
              }
            ],
            "inputsMetadata": [
              {
                "id": "0",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "val_362"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "FLOAT[s34,s16,1]"
                  },
                  {
                    "key": "param_name",
                    "value": "X"
                  }
                ]
              }
            ],
            "style": null,
            "config": null
          },
          {
            "id": "node_Mul_508",
            "label": "Mul",
            "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.1: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.1.input_layernorm: transformers.models.llama.modeling_llama.LlamaRMSNorm/mul_384: aten.mul.Tensor",
            "subgraphIds": [],
            "attrs": [
              {
                "key": "[metadata] namespace",
                "value": ": transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.1: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.1.input_layernorm: transformers.models.llama.modeling_llama.LlamaRMSNorm/mul_384: aten.mul.Tensor"
              },
              {
                "key": "[metadata] pkg.torch.onnx.class_hierarchy",
                "value": "['transformers.models.llama.modeling_llama.LlamaForCausalLM', 'transformers.models.llama.modeling_llama.LlamaModel', 'transformers.models.llama.modeling_llama.LlamaDecoderLayer', 'transformers.models.llama.modeling_llama.LlamaRMSNorm', 'aten.mul.Tensor']"
              },
              {
                "key": "[metadata] pkg.torch.onnx.fx_node",
                "value": "%mul_384 : [num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%add_463, %rsqrt_2), kwargs = {})"
              },
              {
                "key": "[metadata] pkg.torch.onnx.name_scopes",
                "value": "['', 'model', 'model.layers.1', 'model.layers.1.input_layernorm', 'mul_384']"
              },
              {
                "key": "[metadata] pkg.torch.onnx.stack_trace",
                "value": "File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 688, in forward\n    outputs: BaseModelOutputWithPast = self.model(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 453, in forward\n    layer_outputs = decoder_layer(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 305, in forward\n    hidden_states = self.input_layernorm(hidden_states)\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 72, in forward\n    hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)"
              }
            ],
            "incomingEdges": [
              {
                "sourceNodeId": "node_Add_499",
                "sourceNodeOutputId": "0",
                "targetNodeInputId": "0"
              },
              {
                "sourceNodeId": "node_Reciprocal_507",
                "sourceNodeOutputId": "0",
                "targetNodeInputId": "1"
              }
            ],
            "outputsMetadata": [
              {
                "id": "0",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "mul_384"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "FLOAT[s34,s16,2048]"
                  },
                  {
                    "key": "param_name",
                    "value": "C"
                  }
                ]
              }
            ],
            "inputsMetadata": [
              {
                "id": "0",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "add_463"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "FLOAT[s34,s16,2048]"
                  },
                  {
                    "key": "param_name",
                    "value": "A"
                  }
                ]
              },
              {
                "id": "1",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "rsqrt_2"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "FLOAT[s34,s16,1]"
                  },
                  {
                    "key": "param_name",
                    "value": "B"
                  }
                ]
              }
            ],
            "style": null,
            "config": null
          },
          {
            "id": "node_Mul_509",
            "label": "Mul",
            "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.1: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.1.input_layernorm: transformers.models.llama.modeling_llama.LlamaRMSNorm/mul_388: aten.mul.Tensor",
            "subgraphIds": [],
            "attrs": [
              {
                "key": "[metadata] namespace",
                "value": ": transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.1: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.1.input_layernorm: transformers.models.llama.modeling_llama.LlamaRMSNorm/mul_388: aten.mul.Tensor"
              },
              {
                "key": "[metadata] pkg.torch.onnx.class_hierarchy",
                "value": "['transformers.models.llama.modeling_llama.LlamaForCausalLM', 'transformers.models.llama.modeling_llama.LlamaModel', 'transformers.models.llama.modeling_llama.LlamaDecoderLayer', 'transformers.models.llama.modeling_llama.LlamaRMSNorm', 'aten.mul.Tensor']"
              },
              {
                "key": "[metadata] pkg.torch.onnx.fx_node",
                "value": "%mul_388 : [num_users=3] = call_function[target=torch.ops.aten.mul.Tensor](args = (%p_model_layers_1_input_layernorm_weight, %mul_384), kwargs = {})"
              },
              {
                "key": "[metadata] pkg.torch.onnx.name_scopes",
                "value": "['', 'model', 'model.layers.1', 'model.layers.1.input_layernorm', 'mul_388']"
              },
              {
                "key": "[metadata] pkg.torch.onnx.stack_trace",
                "value": "File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 688, in forward\n    outputs: BaseModelOutputWithPast = self.model(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 453, in forward\n    layer_outputs = decoder_layer(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 305, in forward\n    hidden_states = self.input_layernorm(hidden_states)\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 73, in forward\n    return self.weight * hidden_states.to(input_dtype)"
              }
            ],
            "incomingEdges": [
              {
                "sourceNodeId": "[value] model.layers.1.input_layernorm.weight",
                "sourceNodeOutputId": "0",
                "targetNodeInputId": "0"
              },
              {
                "sourceNodeId": "node_Mul_508",
                "sourceNodeOutputId": "0",
                "targetNodeInputId": "1"
              }
            ],
            "outputsMetadata": [
              {
                "id": "0",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "mul_388"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "FLOAT[s34,s16,2048]"
                  },
                  {
                    "key": "param_name",
                    "value": "C"
                  }
                ]
              }
            ],
            "inputsMetadata": [
              {
                "id": "0",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "model.layers.1.input_layernorm.weight"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "FLOAT[2048]"
                  },
                  {
                    "key": "param_name",
                    "value": "A"
                  }
                ]
              },
              {
                "id": "1",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "mul_384"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "FLOAT[s34,s16,2048]"
                  },
                  {
                    "key": "param_name",
                    "value": "B"
                  }
                ]
              }
            ],
            "style": null,
            "config": null
          },
          {
            "id": "node_Transpose_510",
            "label": "Transpose",
            "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.1: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.1.self_attn: transformers.models.llama.modeling_llama.LlamaAttention/model.layers.1.self_attn.q_proj: torch.nn.modules.linear.Linear/linear_7: aten.linear.default",
            "subgraphIds": [],
            "attrs": [
              {
                "key": "perm",
                "value": "[1, 0]"
              },
              {
                "key": "[metadata] namespace",
                "value": ": transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.1: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.1.self_attn: transformers.models.llama.modeling_llama.LlamaAttention/model.layers.1.self_attn.q_proj: torch.nn.modules.linear.Linear/linear_7: aten.linear.default"
              },
              {
                "key": "[metadata] pkg.torch.onnx.class_hierarchy",
                "value": "['transformers.models.llama.modeling_llama.LlamaForCausalLM', 'transformers.models.llama.modeling_llama.LlamaModel', 'transformers.models.llama.modeling_llama.LlamaDecoderLayer', 'transformers.models.llama.modeling_llama.LlamaAttention', 'torch.nn.modules.linear.Linear', 'aten.linear.default']"
              },
              {
                "key": "[metadata] pkg.torch.onnx.fx_node",
                "value": "%linear_7 : [num_users=1] = call_function[target=torch.ops.aten.linear.default](args = (%mul_388, %p_model_layers_1_self_attn_q_proj_weight), kwargs = {})"
              },
              {
                "key": "[metadata] pkg.torch.onnx.name_scopes",
                "value": "['', 'model', 'model.layers.1', 'model.layers.1.self_attn', 'model.layers.1.self_attn.q_proj', 'linear_7']"
              },
              {
                "key": "[metadata] pkg.torch.onnx.stack_trace",
                "value": "File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 688, in forward\n    outputs: BaseModelOutputWithPast = self.model(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 453, in forward\n    layer_outputs = decoder_layer(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 308, in forward\n    hidden_states, self_attn_weights = self.self_attn(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 242, in forward\n    query_states = self.q_proj(hidden_states).view(hidden_shape).transpose(1, 2)\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/torch/nn/modules/linear.py\", line 125, in forward\n    return F.linear(input, self.weight, self.bias)"
              }
            ],
            "incomingEdges": [
              {
                "sourceNodeId": "[value] model.layers.1.self_attn.q_proj.weight",
                "sourceNodeOutputId": "0",
                "targetNodeInputId": "0"
              }
            ],
            "outputsMetadata": [
              {
                "id": "0",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "val_363"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "FLOAT[2048,2048]"
                  },
                  {
                    "key": "param_name",
                    "value": "transposed"
                  }
                ]
              }
            ],
            "inputsMetadata": [
              {
                "id": "0",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "model.layers.1.self_attn.q_proj.weight"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "FLOAT[2048,2048]"
                  },
                  {
                    "key": "param_name",
                    "value": "data"
                  }
                ]
              }
            ],
            "style": null,
            "config": null
          },
          {
            "id": "node_MatMul_511",
            "label": "MatMul",
            "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.1: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.1.self_attn: transformers.models.llama.modeling_llama.LlamaAttention/model.layers.1.self_attn.q_proj: torch.nn.modules.linear.Linear/linear_7: aten.linear.default",
            "subgraphIds": [],
            "attrs": [
              {
                "key": "[metadata] namespace",
                "value": ": transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.1: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.1.self_attn: transformers.models.llama.modeling_llama.LlamaAttention/model.layers.1.self_attn.q_proj: torch.nn.modules.linear.Linear/linear_7: aten.linear.default"
              },
              {
                "key": "[metadata] pkg.torch.onnx.class_hierarchy",
                "value": "['transformers.models.llama.modeling_llama.LlamaForCausalLM', 'transformers.models.llama.modeling_llama.LlamaModel', 'transformers.models.llama.modeling_llama.LlamaDecoderLayer', 'transformers.models.llama.modeling_llama.LlamaAttention', 'torch.nn.modules.linear.Linear', 'aten.linear.default']"
              },
              {
                "key": "[metadata] pkg.torch.onnx.fx_node",
                "value": "%linear_7 : [num_users=1] = call_function[target=torch.ops.aten.linear.default](args = (%mul_388, %p_model_layers_1_self_attn_q_proj_weight), kwargs = {})"
              },
              {
                "key": "[metadata] pkg.torch.onnx.name_scopes",
                "value": "['', 'model', 'model.layers.1', 'model.layers.1.self_attn', 'model.layers.1.self_attn.q_proj', 'linear_7']"
              },
              {
                "key": "[metadata] pkg.torch.onnx.stack_trace",
                "value": "File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 688, in forward\n    outputs: BaseModelOutputWithPast = self.model(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 453, in forward\n    layer_outputs = decoder_layer(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 308, in forward\n    hidden_states, self_attn_weights = self.self_attn(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 242, in forward\n    query_states = self.q_proj(hidden_states).view(hidden_shape).transpose(1, 2)\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/torch/nn/modules/linear.py\", line 125, in forward\n    return F.linear(input, self.weight, self.bias)"
              }
            ],
            "incomingEdges": [
              {
                "sourceNodeId": "node_Mul_509",
                "sourceNodeOutputId": "0",
                "targetNodeInputId": "0"
              },
              {
                "sourceNodeId": "node_Transpose_510",
                "sourceNodeOutputId": "0",
                "targetNodeInputId": "1"
              }
            ],
            "outputsMetadata": [
              {
                "id": "0",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "linear_7"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "FLOAT[s34,s16,2048]"
                  },
                  {
                    "key": "param_name",
                    "value": "Y"
                  }
                ]
              }
            ],
            "inputsMetadata": [
              {
                "id": "0",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "mul_388"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "FLOAT[s34,s16,2048]"
                  },
                  {
                    "key": "param_name",
                    "value": "A"
                  }
                ]
              },
              {
                "id": "1",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "val_363"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "FLOAT[2048,2048]"
                  },
                  {
                    "key": "param_name",
                    "value": "B"
                  }
                ]
              }
            ],
            "style": null,
            "config": null
          },
          {
            "id": "node_Concat_516",
            "label": "Concat",
            "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.1: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.1.self_attn: transformers.models.llama.modeling_llama.LlamaAttention/view_5: aten.view.default",
            "subgraphIds": [],
            "attrs": [
              {
                "key": "axis",
                "value": "0"
              },
              {
                "key": "[metadata] namespace",
                "value": ": transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.1: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.1.self_attn: transformers.models.llama.modeling_llama.LlamaAttention/view_5: aten.view.default"
              },
              {
                "key": "[metadata] pkg.torch.onnx.class_hierarchy",
                "value": "['transformers.models.llama.modeling_llama.LlamaForCausalLM', 'transformers.models.llama.modeling_llama.LlamaModel', 'transformers.models.llama.modeling_llama.LlamaDecoderLayer', 'transformers.models.llama.modeling_llama.LlamaAttention', 'aten.view.default']"
              },
              {
                "key": "[metadata] pkg.torch.onnx.fx_node",
                "value": "%view_5 : [num_users=1] = call_function[target=torch.ops.aten.view.default](args = (%linear_7, [%sym_size_int_68, %sym_size_int_61, -1, 64]), kwargs = {})"
              },
              {
                "key": "[metadata] pkg.torch.onnx.name_scopes",
                "value": "['', 'model', 'model.layers.1', 'model.layers.1.self_attn', 'view_5']"
              },
              {
                "key": "[metadata] pkg.torch.onnx.stack_trace",
                "value": "File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 688, in forward\n    outputs: BaseModelOutputWithPast = self.model(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 453, in forward\n    layer_outputs = decoder_layer(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 308, in forward\n    hidden_states, self_attn_weights = self.self_attn(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 242, in forward\n    query_states = self.q_proj(hidden_states).view(hidden_shape).transpose(1, 2)"
              }
            ],
            "incomingEdges": [
              {
                "sourceNodeId": "node_Shape_12",
                "sourceNodeOutputId": "0",
                "targetNodeInputId": "0"
              },
              {
                "sourceNodeId": "node_Shape_2",
                "sourceNodeOutputId": "0",
                "targetNodeInputId": "1"
              },
              {
                "sourceNodeId": "node_Constant_90",
                "sourceNodeOutputId": "0",
                "targetNodeInputId": "2"
              },
              {
                "sourceNodeId": "node_Constant_345",
                "sourceNodeOutputId": "0",
                "targetNodeInputId": "3"
              }
            ],
            "outputsMetadata": [
              {
                "id": "0",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "val_368"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "INT64[4]"
                  },
                  {
                    "key": "param_name",
                    "value": "concat_result"
                  }
                ]
              }
            ],
            "inputsMetadata": [
              {
                "id": "0",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "val_6"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "INT64[1]"
                  },
                  {
                    "key": "param_name",
                    "value": "inputs"
                  }
                ]
              },
              {
                "id": "1",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "val_1"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "INT64[1]"
                  },
                  {
                    "key": "param_name",
                    "value": "inputs"
                  }
                ]
              },
              {
                "id": "2",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "val_53"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "INT64[1]"
                  },
                  {
                    "key": "param_name",
                    "value": "inputs"
                  }
                ]
              },
              {
                "id": "3",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "val_258"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "INT64[1]"
                  },
                  {
                    "key": "param_name",
                    "value": "inputs"
                  }
                ]
              }
            ],
            "style": null,
            "config": null
          },
          {
            "id": "node_Reshape_518",
            "label": "Reshape",
            "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.1: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.1.self_attn: transformers.models.llama.modeling_llama.LlamaAttention/view_5: aten.view.default",
            "subgraphIds": [],
            "attrs": [
              {
                "key": "allowzero",
                "value": "1"
              },
              {
                "key": "[metadata] namespace",
                "value": ": transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.1: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.1.self_attn: transformers.models.llama.modeling_llama.LlamaAttention/view_5: aten.view.default"
              },
              {
                "key": "[metadata] pkg.torch.onnx.class_hierarchy",
                "value": "['transformers.models.llama.modeling_llama.LlamaForCausalLM', 'transformers.models.llama.modeling_llama.LlamaModel', 'transformers.models.llama.modeling_llama.LlamaDecoderLayer', 'transformers.models.llama.modeling_llama.LlamaAttention', 'aten.view.default']"
              },
              {
                "key": "[metadata] pkg.torch.onnx.fx_node",
                "value": "%view_5 : [num_users=1] = call_function[target=torch.ops.aten.view.default](args = (%linear_7, [%sym_size_int_68, %sym_size_int_61, -1, 64]), kwargs = {})"
              },
              {
                "key": "[metadata] pkg.torch.onnx.name_scopes",
                "value": "['', 'model', 'model.layers.1', 'model.layers.1.self_attn', 'view_5']"
              },
              {
                "key": "[metadata] pkg.torch.onnx.stack_trace",
                "value": "File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 688, in forward\n    outputs: BaseModelOutputWithPast = self.model(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 453, in forward\n    layer_outputs = decoder_layer(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 308, in forward\n    hidden_states, self_attn_weights = self.self_attn(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 242, in forward\n    query_states = self.q_proj(hidden_states).view(hidden_shape).transpose(1, 2)"
              }
            ],
            "incomingEdges": [
              {
                "sourceNodeId": "node_MatMul_511",
                "sourceNodeOutputId": "0",
                "targetNodeInputId": "0"
              },
              {
                "sourceNodeId": "node_Concat_516",
                "sourceNodeOutputId": "0",
                "targetNodeInputId": "1"
              }
            ],
            "outputsMetadata": [
              {
                "id": "0",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "view_5"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "FLOAT[s34,s16,32,64]"
                  },
                  {
                    "key": "param_name",
                    "value": "reshaped"
                  }
                ]
              }
            ],
            "inputsMetadata": [
              {
                "id": "0",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "linear_7"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "FLOAT[s34,s16,2048]"
                  },
                  {
                    "key": "param_name",
                    "value": "data"
                  }
                ]
              },
              {
                "id": "1",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "val_368"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "INT64[4]"
                  },
                  {
                    "key": "param_name",
                    "value": "shape"
                  }
                ]
              }
            ],
            "style": null,
            "config": null
          },
          {
            "id": "node_Transpose_519",
            "label": "Transpose",
            "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.1: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.1.self_attn: transformers.models.llama.modeling_llama.LlamaAttention/transpose_6: aten.transpose.int",
            "subgraphIds": [],
            "attrs": [
              {
                "key": "perm",
                "value": "[0, 2, 1, 3]"
              },
              {
                "key": "[metadata] namespace",
                "value": ": transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.1: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.1.self_attn: transformers.models.llama.modeling_llama.LlamaAttention/transpose_6: aten.transpose.int"
              },
              {
                "key": "[metadata] pkg.torch.onnx.class_hierarchy",
                "value": "['transformers.models.llama.modeling_llama.LlamaForCausalLM', 'transformers.models.llama.modeling_llama.LlamaModel', 'transformers.models.llama.modeling_llama.LlamaDecoderLayer', 'transformers.models.llama.modeling_llama.LlamaAttention', 'aten.transpose.int']"
              },
              {
                "key": "[metadata] pkg.torch.onnx.fx_node",
                "value": "%transpose_6 : [num_users=3] = call_function[target=torch.ops.aten.transpose.int](args = (%view_5, 1, 2), kwargs = {})"
              },
              {
                "key": "[metadata] pkg.torch.onnx.name_scopes",
                "value": "['', 'model', 'model.layers.1', 'model.layers.1.self_attn', 'transpose_6']"
              },
              {
                "key": "[metadata] pkg.torch.onnx.stack_trace",
                "value": "File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 688, in forward\n    outputs: BaseModelOutputWithPast = self.model(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 453, in forward\n    layer_outputs = decoder_layer(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 308, in forward\n    hidden_states, self_attn_weights = self.self_attn(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 242, in forward\n    query_states = self.q_proj(hidden_states).view(hidden_shape).transpose(1, 2)"
              }
            ],
            "incomingEdges": [
              {
                "sourceNodeId": "node_Reshape_518",
                "sourceNodeOutputId": "0",
                "targetNodeInputId": "0"
              }
            ],
            "outputsMetadata": [
              {
                "id": "0",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "transpose_6"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "FLOAT[s34,32,s16,64]"
                  },
                  {
                    "key": "param_name",
                    "value": "transposed"
                  }
                ]
              }
            ],
            "inputsMetadata": [
              {
                "id": "0",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "view_5"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "FLOAT[s34,s16,32,64]"
                  },
                  {
                    "key": "param_name",
                    "value": "data"
                  }
                ]
              }
            ],
            "style": null,
            "config": null
          },
          {
            "id": "node_Transpose_520",
            "label": "Transpose",
            "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.1: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.1.self_attn: transformers.models.llama.modeling_llama.LlamaAttention/model.layers.1.self_attn.k_proj: torch.nn.modules.linear.Linear/linear_8: aten.linear.default",
            "subgraphIds": [],
            "attrs": [
              {
                "key": "perm",
                "value": "[1, 0]"
              },
              {
                "key": "[metadata] namespace",
                "value": ": transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.1: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.1.self_attn: transformers.models.llama.modeling_llama.LlamaAttention/model.layers.1.self_attn.k_proj: torch.nn.modules.linear.Linear/linear_8: aten.linear.default"
              },
              {
                "key": "[metadata] pkg.torch.onnx.class_hierarchy",
                "value": "['transformers.models.llama.modeling_llama.LlamaForCausalLM', 'transformers.models.llama.modeling_llama.LlamaModel', 'transformers.models.llama.modeling_llama.LlamaDecoderLayer', 'transformers.models.llama.modeling_llama.LlamaAttention', 'torch.nn.modules.linear.Linear', 'aten.linear.default']"
              },
              {
                "key": "[metadata] pkg.torch.onnx.fx_node",
                "value": "%linear_8 : [num_users=1] = call_function[target=torch.ops.aten.linear.default](args = (%mul_388, %p_model_layers_1_self_attn_k_proj_weight), kwargs = {})"
              },
              {
                "key": "[metadata] pkg.torch.onnx.name_scopes",
                "value": "['', 'model', 'model.layers.1', 'model.layers.1.self_attn', 'model.layers.1.self_attn.k_proj', 'linear_8']"
              },
              {
                "key": "[metadata] pkg.torch.onnx.stack_trace",
                "value": "File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 688, in forward\n    outputs: BaseModelOutputWithPast = self.model(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 453, in forward\n    layer_outputs = decoder_layer(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 308, in forward\n    hidden_states, self_attn_weights = self.self_attn(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 243, in forward\n    key_states = self.k_proj(hidden_states).view(hidden_shape).transpose(1, 2)\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/torch/nn/modules/linear.py\", line 125, in forward\n    return F.linear(input, self.weight, self.bias)"
              }
            ],
            "incomingEdges": [
              {
                "sourceNodeId": "[value] model.layers.1.self_attn.k_proj.weight",
                "sourceNodeOutputId": "0",
                "targetNodeInputId": "0"
              }
            ],
            "outputsMetadata": [
              {
                "id": "0",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "val_370"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "FLOAT[2048,2048]"
                  },
                  {
                    "key": "param_name",
                    "value": "transposed"
                  }
                ]
              }
            ],
            "inputsMetadata": [
              {
                "id": "0",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "model.layers.1.self_attn.k_proj.weight"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "FLOAT[2048,2048]"
                  },
                  {
                    "key": "param_name",
                    "value": "data"
                  }
                ]
              }
            ],
            "style": null,
            "config": null
          },
          {
            "id": "node_MatMul_521",
            "label": "MatMul",
            "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.1: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.1.self_attn: transformers.models.llama.modeling_llama.LlamaAttention/model.layers.1.self_attn.k_proj: torch.nn.modules.linear.Linear/linear_8: aten.linear.default",
            "subgraphIds": [],
            "attrs": [
              {
                "key": "[metadata] namespace",
                "value": ": transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.1: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.1.self_attn: transformers.models.llama.modeling_llama.LlamaAttention/model.layers.1.self_attn.k_proj: torch.nn.modules.linear.Linear/linear_8: aten.linear.default"
              },
              {
                "key": "[metadata] pkg.torch.onnx.class_hierarchy",
                "value": "['transformers.models.llama.modeling_llama.LlamaForCausalLM', 'transformers.models.llama.modeling_llama.LlamaModel', 'transformers.models.llama.modeling_llama.LlamaDecoderLayer', 'transformers.models.llama.modeling_llama.LlamaAttention', 'torch.nn.modules.linear.Linear', 'aten.linear.default']"
              },
              {
                "key": "[metadata] pkg.torch.onnx.fx_node",
                "value": "%linear_8 : [num_users=1] = call_function[target=torch.ops.aten.linear.default](args = (%mul_388, %p_model_layers_1_self_attn_k_proj_weight), kwargs = {})"
              },
              {
                "key": "[metadata] pkg.torch.onnx.name_scopes",
                "value": "['', 'model', 'model.layers.1', 'model.layers.1.self_attn', 'model.layers.1.self_attn.k_proj', 'linear_8']"
              },
              {
                "key": "[metadata] pkg.torch.onnx.stack_trace",
                "value": "File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 688, in forward\n    outputs: BaseModelOutputWithPast = self.model(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 453, in forward\n    layer_outputs = decoder_layer(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 308, in forward\n    hidden_states, self_attn_weights = self.self_attn(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 243, in forward\n    key_states = self.k_proj(hidden_states).view(hidden_shape).transpose(1, 2)\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/torch/nn/modules/linear.py\", line 125, in forward\n    return F.linear(input, self.weight, self.bias)"
              }
            ],
            "incomingEdges": [
              {
                "sourceNodeId": "node_Mul_509",
                "sourceNodeOutputId": "0",
                "targetNodeInputId": "0"
              },
              {
                "sourceNodeId": "node_Transpose_520",
                "sourceNodeOutputId": "0",
                "targetNodeInputId": "1"
              }
            ],
            "outputsMetadata": [
              {
                "id": "0",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "linear_8"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "FLOAT[s34,s16,2048]"
                  },
                  {
                    "key": "param_name",
                    "value": "Y"
                  }
                ]
              }
            ],
            "inputsMetadata": [
              {
                "id": "0",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "mul_388"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "FLOAT[s34,s16,2048]"
                  },
                  {
                    "key": "param_name",
                    "value": "A"
                  }
                ]
              },
              {
                "id": "1",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "val_370"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "FLOAT[2048,2048]"
                  },
                  {
                    "key": "param_name",
                    "value": "B"
                  }
                ]
              }
            ],
            "style": null,
            "config": null
          },
          {
            "id": "node_Concat_526",
            "label": "Concat",
            "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.1: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.1.self_attn: transformers.models.llama.modeling_llama.LlamaAttention/view_6: aten.view.default",
            "subgraphIds": [],
            "attrs": [
              {
                "key": "axis",
                "value": "0"
              },
              {
                "key": "[metadata] namespace",
                "value": ": transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.1: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.1.self_attn: transformers.models.llama.modeling_llama.LlamaAttention/view_6: aten.view.default"
              },
              {
                "key": "[metadata] pkg.torch.onnx.class_hierarchy",
                "value": "['transformers.models.llama.modeling_llama.LlamaForCausalLM', 'transformers.models.llama.modeling_llama.LlamaModel', 'transformers.models.llama.modeling_llama.LlamaDecoderLayer', 'transformers.models.llama.modeling_llama.LlamaAttention', 'aten.view.default']"
              },
              {
                "key": "[metadata] pkg.torch.onnx.fx_node",
                "value": "%view_6 : [num_users=1] = call_function[target=torch.ops.aten.view.default](args = (%linear_8, [%sym_size_int_68, %sym_size_int_61, -1, 64]), kwargs = {})"
              },
              {
                "key": "[metadata] pkg.torch.onnx.name_scopes",
                "value": "['', 'model', 'model.layers.1', 'model.layers.1.self_attn', 'view_6']"
              },
              {
                "key": "[metadata] pkg.torch.onnx.stack_trace",
                "value": "File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 688, in forward\n    outputs: BaseModelOutputWithPast = self.model(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 453, in forward\n    layer_outputs = decoder_layer(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 308, in forward\n    hidden_states, self_attn_weights = self.self_attn(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 243, in forward\n    key_states = self.k_proj(hidden_states).view(hidden_shape).transpose(1, 2)"
              }
            ],
            "incomingEdges": [
              {
                "sourceNodeId": "node_Shape_12",
                "sourceNodeOutputId": "0",
                "targetNodeInputId": "0"
              },
              {
                "sourceNodeId": "node_Shape_2",
                "sourceNodeOutputId": "0",
                "targetNodeInputId": "1"
              },
              {
                "sourceNodeId": "node_Constant_90",
                "sourceNodeOutputId": "0",
                "targetNodeInputId": "2"
              },
              {
                "sourceNodeId": "node_Constant_345",
                "sourceNodeOutputId": "0",
                "targetNodeInputId": "3"
              }
            ],
            "outputsMetadata": [
              {
                "id": "0",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "val_375"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "INT64[4]"
                  },
                  {
                    "key": "param_name",
                    "value": "concat_result"
                  }
                ]
              }
            ],
            "inputsMetadata": [
              {
                "id": "0",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "val_6"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "INT64[1]"
                  },
                  {
                    "key": "param_name",
                    "value": "inputs"
                  }
                ]
              },
              {
                "id": "1",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "val_1"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "INT64[1]"
                  },
                  {
                    "key": "param_name",
                    "value": "inputs"
                  }
                ]
              },
              {
                "id": "2",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "val_53"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "INT64[1]"
                  },
                  {
                    "key": "param_name",
                    "value": "inputs"
                  }
                ]
              },
              {
                "id": "3",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "val_258"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "INT64[1]"
                  },
                  {
                    "key": "param_name",
                    "value": "inputs"
                  }
                ]
              }
            ],
            "style": null,
            "config": null
          },
          {
            "id": "node_Reshape_528",
            "label": "Reshape",
            "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.1: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.1.self_attn: transformers.models.llama.modeling_llama.LlamaAttention/view_6: aten.view.default",
            "subgraphIds": [],
            "attrs": [
              {
                "key": "allowzero",
                "value": "1"
              },
              {
                "key": "[metadata] namespace",
                "value": ": transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.1: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.1.self_attn: transformers.models.llama.modeling_llama.LlamaAttention/view_6: aten.view.default"
              },
              {
                "key": "[metadata] pkg.torch.onnx.class_hierarchy",
                "value": "['transformers.models.llama.modeling_llama.LlamaForCausalLM', 'transformers.models.llama.modeling_llama.LlamaModel', 'transformers.models.llama.modeling_llama.LlamaDecoderLayer', 'transformers.models.llama.modeling_llama.LlamaAttention', 'aten.view.default']"
              },
              {
                "key": "[metadata] pkg.torch.onnx.fx_node",
                "value": "%view_6 : [num_users=1] = call_function[target=torch.ops.aten.view.default](args = (%linear_8, [%sym_size_int_68, %sym_size_int_61, -1, 64]), kwargs = {})"
              },
              {
                "key": "[metadata] pkg.torch.onnx.name_scopes",
                "value": "['', 'model', 'model.layers.1', 'model.layers.1.self_attn', 'view_6']"
              },
              {
                "key": "[metadata] pkg.torch.onnx.stack_trace",
                "value": "File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 688, in forward\n    outputs: BaseModelOutputWithPast = self.model(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 453, in forward\n    layer_outputs = decoder_layer(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 308, in forward\n    hidden_states, self_attn_weights = self.self_attn(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 243, in forward\n    key_states = self.k_proj(hidden_states).view(hidden_shape).transpose(1, 2)"
              }
            ],
            "incomingEdges": [
              {
                "sourceNodeId": "node_MatMul_521",
                "sourceNodeOutputId": "0",
                "targetNodeInputId": "0"
              },
              {
                "sourceNodeId": "node_Concat_526",
                "sourceNodeOutputId": "0",
                "targetNodeInputId": "1"
              }
            ],
            "outputsMetadata": [
              {
                "id": "0",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "view_6"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "FLOAT[s34,s16,32,64]"
                  },
                  {
                    "key": "param_name",
                    "value": "reshaped"
                  }
                ]
              }
            ],
            "inputsMetadata": [
              {
                "id": "0",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "linear_8"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "FLOAT[s34,s16,2048]"
                  },
                  {
                    "key": "param_name",
                    "value": "data"
                  }
                ]
              },
              {
                "id": "1",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "val_375"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "INT64[4]"
                  },
                  {
                    "key": "param_name",
                    "value": "shape"
                  }
                ]
              }
            ],
            "style": null,
            "config": null
          },
          {
            "id": "node_Transpose_529",
            "label": "Transpose",
            "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.1: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.1.self_attn: transformers.models.llama.modeling_llama.LlamaAttention/transpose_7: aten.transpose.int",
            "subgraphIds": [],
            "attrs": [
              {
                "key": "perm",
                "value": "[0, 2, 1, 3]"
              },
              {
                "key": "[metadata] namespace",
                "value": ": transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.1: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.1.self_attn: transformers.models.llama.modeling_llama.LlamaAttention/transpose_7: aten.transpose.int"
              },
              {
                "key": "[metadata] pkg.torch.onnx.class_hierarchy",
                "value": "['transformers.models.llama.modeling_llama.LlamaForCausalLM', 'transformers.models.llama.modeling_llama.LlamaModel', 'transformers.models.llama.modeling_llama.LlamaDecoderLayer', 'transformers.models.llama.modeling_llama.LlamaAttention', 'aten.transpose.int']"
              },
              {
                "key": "[metadata] pkg.torch.onnx.fx_node",
                "value": "%transpose_7 : [num_users=3] = call_function[target=torch.ops.aten.transpose.int](args = (%view_6, 1, 2), kwargs = {})"
              },
              {
                "key": "[metadata] pkg.torch.onnx.name_scopes",
                "value": "['', 'model', 'model.layers.1', 'model.layers.1.self_attn', 'transpose_7']"
              },
              {
                "key": "[metadata] pkg.torch.onnx.stack_trace",
                "value": "File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 688, in forward\n    outputs: BaseModelOutputWithPast = self.model(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 453, in forward\n    layer_outputs = decoder_layer(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 308, in forward\n    hidden_states, self_attn_weights = self.self_attn(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 243, in forward\n    key_states = self.k_proj(hidden_states).view(hidden_shape).transpose(1, 2)"
              }
            ],
            "incomingEdges": [
              {
                "sourceNodeId": "node_Reshape_528",
                "sourceNodeOutputId": "0",
                "targetNodeInputId": "0"
              }
            ],
            "outputsMetadata": [
              {
                "id": "0",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "transpose_7"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "FLOAT[s34,32,s16,64]"
                  },
                  {
                    "key": "param_name",
                    "value": "transposed"
                  }
                ]
              }
            ],
            "inputsMetadata": [
              {
                "id": "0",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "view_6"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "FLOAT[s34,s16,32,64]"
                  },
                  {
                    "key": "param_name",
                    "value": "data"
                  }
                ]
              }
            ],
            "style": null,
            "config": null
          },
          {
            "id": "node_Transpose_530",
            "label": "Transpose",
            "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.1: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.1.self_attn: transformers.models.llama.modeling_llama.LlamaAttention/model.layers.1.self_attn.v_proj: torch.nn.modules.linear.Linear/linear_9: aten.linear.default",
            "subgraphIds": [],
            "attrs": [
              {
                "key": "perm",
                "value": "[1, 0]"
              },
              {
                "key": "[metadata] namespace",
                "value": ": transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.1: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.1.self_attn: transformers.models.llama.modeling_llama.LlamaAttention/model.layers.1.self_attn.v_proj: torch.nn.modules.linear.Linear/linear_9: aten.linear.default"
              },
              {
                "key": "[metadata] pkg.torch.onnx.class_hierarchy",
                "value": "['transformers.models.llama.modeling_llama.LlamaForCausalLM', 'transformers.models.llama.modeling_llama.LlamaModel', 'transformers.models.llama.modeling_llama.LlamaDecoderLayer', 'transformers.models.llama.modeling_llama.LlamaAttention', 'torch.nn.modules.linear.Linear', 'aten.linear.default']"
              },
              {
                "key": "[metadata] pkg.torch.onnx.fx_node",
                "value": "%linear_9 : [num_users=1] = call_function[target=torch.ops.aten.linear.default](args = (%mul_388, %p_model_layers_1_self_attn_v_proj_weight), kwargs = {})"
              },
              {
                "key": "[metadata] pkg.torch.onnx.name_scopes",
                "value": "['', 'model', 'model.layers.1', 'model.layers.1.self_attn', 'model.layers.1.self_attn.v_proj', 'linear_9']"
              },
              {
                "key": "[metadata] pkg.torch.onnx.stack_trace",
                "value": "File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 688, in forward\n    outputs: BaseModelOutputWithPast = self.model(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 453, in forward\n    layer_outputs = decoder_layer(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 308, in forward\n    hidden_states, self_attn_weights = self.self_attn(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 244, in forward\n    value_states = self.v_proj(hidden_states).view(hidden_shape).transpose(1, 2)\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/torch/nn/modules/linear.py\", line 125, in forward\n    return F.linear(input, self.weight, self.bias)"
              }
            ],
            "incomingEdges": [
              {
                "sourceNodeId": "[value] model.layers.1.self_attn.v_proj.weight",
                "sourceNodeOutputId": "0",
                "targetNodeInputId": "0"
              }
            ],
            "outputsMetadata": [
              {
                "id": "0",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "val_377"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "FLOAT[2048,2048]"
                  },
                  {
                    "key": "param_name",
                    "value": "transposed"
                  }
                ]
              }
            ],
            "inputsMetadata": [
              {
                "id": "0",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "model.layers.1.self_attn.v_proj.weight"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "FLOAT[2048,2048]"
                  },
                  {
                    "key": "param_name",
                    "value": "data"
                  }
                ]
              }
            ],
            "style": null,
            "config": null
          },
          {
            "id": "node_MatMul_531",
            "label": "MatMul",
            "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.1: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.1.self_attn: transformers.models.llama.modeling_llama.LlamaAttention/model.layers.1.self_attn.v_proj: torch.nn.modules.linear.Linear/linear_9: aten.linear.default",
            "subgraphIds": [],
            "attrs": [
              {
                "key": "[metadata] namespace",
                "value": ": transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.1: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.1.self_attn: transformers.models.llama.modeling_llama.LlamaAttention/model.layers.1.self_attn.v_proj: torch.nn.modules.linear.Linear/linear_9: aten.linear.default"
              },
              {
                "key": "[metadata] pkg.torch.onnx.class_hierarchy",
                "value": "['transformers.models.llama.modeling_llama.LlamaForCausalLM', 'transformers.models.llama.modeling_llama.LlamaModel', 'transformers.models.llama.modeling_llama.LlamaDecoderLayer', 'transformers.models.llama.modeling_llama.LlamaAttention', 'torch.nn.modules.linear.Linear', 'aten.linear.default']"
              },
              {
                "key": "[metadata] pkg.torch.onnx.fx_node",
                "value": "%linear_9 : [num_users=1] = call_function[target=torch.ops.aten.linear.default](args = (%mul_388, %p_model_layers_1_self_attn_v_proj_weight), kwargs = {})"
              },
              {
                "key": "[metadata] pkg.torch.onnx.name_scopes",
                "value": "['', 'model', 'model.layers.1', 'model.layers.1.self_attn', 'model.layers.1.self_attn.v_proj', 'linear_9']"
              },
              {
                "key": "[metadata] pkg.torch.onnx.stack_trace",
                "value": "File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 688, in forward\n    outputs: BaseModelOutputWithPast = self.model(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 453, in forward\n    layer_outputs = decoder_layer(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 308, in forward\n    hidden_states, self_attn_weights = self.self_attn(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 244, in forward\n    value_states = self.v_proj(hidden_states).view(hidden_shape).transpose(1, 2)\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/torch/nn/modules/linear.py\", line 125, in forward\n    return F.linear(input, self.weight, self.bias)"
              }
            ],
            "incomingEdges": [
              {
                "sourceNodeId": "node_Mul_509",
                "sourceNodeOutputId": "0",
                "targetNodeInputId": "0"
              },
              {
                "sourceNodeId": "node_Transpose_530",
                "sourceNodeOutputId": "0",
                "targetNodeInputId": "1"
              }
            ],
            "outputsMetadata": [
              {
                "id": "0",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "linear_9"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "FLOAT[s34,s16,2048]"
                  },
                  {
                    "key": "param_name",
                    "value": "Y"
                  }
                ]
              }
            ],
            "inputsMetadata": [
              {
                "id": "0",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "mul_388"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "FLOAT[s34,s16,2048]"
                  },
                  {
                    "key": "param_name",
                    "value": "A"
                  }
                ]
              },
              {
                "id": "1",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "val_377"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "FLOAT[2048,2048]"
                  },
                  {
                    "key": "param_name",
                    "value": "B"
                  }
                ]
              }
            ],
            "style": null,
            "config": null
          },
          {
            "id": "node_Concat_536",
            "label": "Concat",
            "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.1: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.1.self_attn: transformers.models.llama.modeling_llama.LlamaAttention/view_7: aten.view.default",
            "subgraphIds": [],
            "attrs": [
              {
                "key": "axis",
                "value": "0"
              },
              {
                "key": "[metadata] namespace",
                "value": ": transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.1: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.1.self_attn: transformers.models.llama.modeling_llama.LlamaAttention/view_7: aten.view.default"
              },
              {
                "key": "[metadata] pkg.torch.onnx.class_hierarchy",
                "value": "['transformers.models.llama.modeling_llama.LlamaForCausalLM', 'transformers.models.llama.modeling_llama.LlamaModel', 'transformers.models.llama.modeling_llama.LlamaDecoderLayer', 'transformers.models.llama.modeling_llama.LlamaAttention', 'aten.view.default']"
              },
              {
                "key": "[metadata] pkg.torch.onnx.fx_node",
                "value": "%view_7 : [num_users=1] = call_function[target=torch.ops.aten.view.default](args = (%linear_9, [%sym_size_int_68, %sym_size_int_61, -1, 64]), kwargs = {})"
              },
              {
                "key": "[metadata] pkg.torch.onnx.name_scopes",
                "value": "['', 'model', 'model.layers.1', 'model.layers.1.self_attn', 'view_7']"
              },
              {
                "key": "[metadata] pkg.torch.onnx.stack_trace",
                "value": "File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 688, in forward\n    outputs: BaseModelOutputWithPast = self.model(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 453, in forward\n    layer_outputs = decoder_layer(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 308, in forward\n    hidden_states, self_attn_weights = self.self_attn(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 244, in forward\n    value_states = self.v_proj(hidden_states).view(hidden_shape).transpose(1, 2)"
              }
            ],
            "incomingEdges": [
              {
                "sourceNodeId": "node_Shape_12",
                "sourceNodeOutputId": "0",
                "targetNodeInputId": "0"
              },
              {
                "sourceNodeId": "node_Shape_2",
                "sourceNodeOutputId": "0",
                "targetNodeInputId": "1"
              },
              {
                "sourceNodeId": "node_Constant_90",
                "sourceNodeOutputId": "0",
                "targetNodeInputId": "2"
              },
              {
                "sourceNodeId": "node_Constant_345",
                "sourceNodeOutputId": "0",
                "targetNodeInputId": "3"
              }
            ],
            "outputsMetadata": [
              {
                "id": "0",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "val_382"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "INT64[4]"
                  },
                  {
                    "key": "param_name",
                    "value": "concat_result"
                  }
                ]
              }
            ],
            "inputsMetadata": [
              {
                "id": "0",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "val_6"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "INT64[1]"
                  },
                  {
                    "key": "param_name",
                    "value": "inputs"
                  }
                ]
              },
              {
                "id": "1",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "val_1"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "INT64[1]"
                  },
                  {
                    "key": "param_name",
                    "value": "inputs"
                  }
                ]
              },
              {
                "id": "2",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "val_53"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "INT64[1]"
                  },
                  {
                    "key": "param_name",
                    "value": "inputs"
                  }
                ]
              },
              {
                "id": "3",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "val_258"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "INT64[1]"
                  },
                  {
                    "key": "param_name",
                    "value": "inputs"
                  }
                ]
              }
            ],
            "style": null,
            "config": null
          },
          {
            "id": "node_Reshape_538",
            "label": "Reshape",
            "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.1: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.1.self_attn: transformers.models.llama.modeling_llama.LlamaAttention/view_7: aten.view.default",
            "subgraphIds": [],
            "attrs": [
              {
                "key": "allowzero",
                "value": "1"
              },
              {
                "key": "[metadata] namespace",
                "value": ": transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.1: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.1.self_attn: transformers.models.llama.modeling_llama.LlamaAttention/view_7: aten.view.default"
              },
              {
                "key": "[metadata] pkg.torch.onnx.class_hierarchy",
                "value": "['transformers.models.llama.modeling_llama.LlamaForCausalLM', 'transformers.models.llama.modeling_llama.LlamaModel', 'transformers.models.llama.modeling_llama.LlamaDecoderLayer', 'transformers.models.llama.modeling_llama.LlamaAttention', 'aten.view.default']"
              },
              {
                "key": "[metadata] pkg.torch.onnx.fx_node",
                "value": "%view_7 : [num_users=1] = call_function[target=torch.ops.aten.view.default](args = (%linear_9, [%sym_size_int_68, %sym_size_int_61, -1, 64]), kwargs = {})"
              },
              {
                "key": "[metadata] pkg.torch.onnx.name_scopes",
                "value": "['', 'model', 'model.layers.1', 'model.layers.1.self_attn', 'view_7']"
              },
              {
                "key": "[metadata] pkg.torch.onnx.stack_trace",
                "value": "File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 688, in forward\n    outputs: BaseModelOutputWithPast = self.model(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 453, in forward\n    layer_outputs = decoder_layer(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 308, in forward\n    hidden_states, self_attn_weights = self.self_attn(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 244, in forward\n    value_states = self.v_proj(hidden_states).view(hidden_shape).transpose(1, 2)"
              }
            ],
            "incomingEdges": [
              {
                "sourceNodeId": "node_MatMul_531",
                "sourceNodeOutputId": "0",
                "targetNodeInputId": "0"
              },
              {
                "sourceNodeId": "node_Concat_536",
                "sourceNodeOutputId": "0",
                "targetNodeInputId": "1"
              }
            ],
            "outputsMetadata": [
              {
                "id": "0",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "view_7"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "FLOAT[s34,s16,32,64]"
                  },
                  {
                    "key": "param_name",
                    "value": "reshaped"
                  }
                ]
              }
            ],
            "inputsMetadata": [
              {
                "id": "0",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "linear_9"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "FLOAT[s34,s16,2048]"
                  },
                  {
                    "key": "param_name",
                    "value": "data"
                  }
                ]
              },
              {
                "id": "1",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "val_382"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "INT64[4]"
                  },
                  {
                    "key": "param_name",
                    "value": "shape"
                  }
                ]
              }
            ],
            "style": null,
            "config": null
          },
          {
            "id": "node_Transpose_539",
            "label": "Transpose",
            "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.1: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.1.self_attn: transformers.models.llama.modeling_llama.LlamaAttention/transpose_8: aten.transpose.int",
            "subgraphIds": [],
            "attrs": [
              {
                "key": "perm",
                "value": "[0, 2, 1, 3]"
              },
              {
                "key": "[metadata] namespace",
                "value": ": transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.1: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.1.self_attn: transformers.models.llama.modeling_llama.LlamaAttention/transpose_8: aten.transpose.int"
              },
              {
                "key": "[metadata] pkg.torch.onnx.class_hierarchy",
                "value": "['transformers.models.llama.modeling_llama.LlamaForCausalLM', 'transformers.models.llama.modeling_llama.LlamaModel', 'transformers.models.llama.modeling_llama.LlamaDecoderLayer', 'transformers.models.llama.modeling_llama.LlamaAttention', 'aten.transpose.int']"
              },
              {
                "key": "[metadata] pkg.torch.onnx.fx_node",
                "value": "%transpose_8 : [num_users=1] = call_function[target=torch.ops.aten.transpose.int](args = (%view_7, 1, 2), kwargs = {})"
              },
              {
                "key": "[metadata] pkg.torch.onnx.name_scopes",
                "value": "['', 'model', 'model.layers.1', 'model.layers.1.self_attn', 'transpose_8']"
              },
              {
                "key": "[metadata] pkg.torch.onnx.stack_trace",
                "value": "File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 688, in forward\n    outputs: BaseModelOutputWithPast = self.model(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 453, in forward\n    layer_outputs = decoder_layer(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 308, in forward\n    hidden_states, self_attn_weights = self.self_attn(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 244, in forward\n    value_states = self.v_proj(hidden_states).view(hidden_shape).transpose(1, 2)"
              }
            ],
            "incomingEdges": [
              {
                "sourceNodeId": "node_Reshape_538",
                "sourceNodeOutputId": "0",
                "targetNodeInputId": "0"
              }
            ],
            "outputsMetadata": [
              {
                "id": "0",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "transpose_8"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "FLOAT[s34,32,s16,64]"
                  },
                  {
                    "key": "param_name",
                    "value": "transposed"
                  }
                ]
              }
            ],
            "inputsMetadata": [
              {
                "id": "0",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "view_7"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "FLOAT[s34,s16,32,64]"
                  },
                  {
                    "key": "param_name",
                    "value": "data"
                  }
                ]
              }
            ],
            "style": null,
            "config": null
          },
          {
            "id": "node_Unsqueeze_540",
            "label": "Unsqueeze",
            "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.1: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.1.self_attn: transformers.models.llama.modeling_llama.LlamaAttention/unsqueeze_12: aten.unsqueeze.default",
            "subgraphIds": [],
            "attrs": [
              {
                "key": "[metadata] namespace",
                "value": ": transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.1: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.1.self_attn: transformers.models.llama.modeling_llama.LlamaAttention/unsqueeze_12: aten.unsqueeze.default"
              },
              {
                "key": "[metadata] pkg.torch.onnx.class_hierarchy",
                "value": "['transformers.models.llama.modeling_llama.LlamaForCausalLM', 'transformers.models.llama.modeling_llama.LlamaModel', 'transformers.models.llama.modeling_llama.LlamaDecoderLayer', 'transformers.models.llama.modeling_llama.LlamaAttention', 'aten.unsqueeze.default']"
              },
              {
                "key": "[metadata] pkg.torch.onnx.fx_node",
                "value": "%unsqueeze_12 : [num_users=2] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%mul_149, 1), kwargs = {})"
              },
              {
                "key": "[metadata] pkg.torch.onnx.name_scopes",
                "value": "['', 'model', 'model.layers.1', 'model.layers.1.self_attn', 'unsqueeze_12']"
              },
              {
                "key": "[metadata] pkg.torch.onnx.stack_trace",
                "value": "File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 688, in forward\n    outputs: BaseModelOutputWithPast = self.model(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 453, in forward\n    layer_outputs = decoder_layer(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 308, in forward\n    hidden_states, self_attn_weights = self.self_attn(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 247, in forward\n    query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)"
              }
            ],
            "incomingEdges": [
              {
                "sourceNodeId": "node_Cos_324",
                "sourceNodeOutputId": "0",
                "targetNodeInputId": "0"
              },
              {
                "sourceNodeId": "node_Constant_60",
                "sourceNodeOutputId": "0",
                "targetNodeInputId": "1"
              }
            ],
            "outputsMetadata": [
              {
                "id": "0",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "unsqueeze_12"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "FLOAT[1,1,s16,64]"
                  },
                  {
                    "key": "param_name",
                    "value": "expanded"
                  }
                ]
              }
            ],
            "inputsMetadata": [
              {
                "id": "0",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "cos"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "FLOAT[1,s16,64]"
                  },
                  {
                    "key": "param_name",
                    "value": "data"
                  }
                ]
              },
              {
                "id": "1",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "val_26"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "INT64[1]"
                  },
                  {
                    "key": "param_name",
                    "value": "axes"
                  }
                ]
              }
            ],
            "style": null,
            "config": null
          },
          {
            "id": "node_Unsqueeze_541",
            "label": "Unsqueeze",
            "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.1: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.1.self_attn: transformers.models.llama.modeling_llama.LlamaAttention/unsqueeze_13: aten.unsqueeze.default",
            "subgraphIds": [],
            "attrs": [
              {
                "key": "[metadata] namespace",
                "value": ": transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.1: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.1.self_attn: transformers.models.llama.modeling_llama.LlamaAttention/unsqueeze_13: aten.unsqueeze.default"
              },
              {
                "key": "[metadata] pkg.torch.onnx.class_hierarchy",
                "value": "['transformers.models.llama.modeling_llama.LlamaForCausalLM', 'transformers.models.llama.modeling_llama.LlamaModel', 'transformers.models.llama.modeling_llama.LlamaDecoderLayer', 'transformers.models.llama.modeling_llama.LlamaAttention', 'aten.unsqueeze.default']"
              },
              {
                "key": "[metadata] pkg.torch.onnx.fx_node",
                "value": "%unsqueeze_13 : [num_users=2] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%mul_156, 1), kwargs = {})"
              },
              {
                "key": "[metadata] pkg.torch.onnx.name_scopes",
                "value": "['', 'model', 'model.layers.1', 'model.layers.1.self_attn', 'unsqueeze_13']"
              },
              {
                "key": "[metadata] pkg.torch.onnx.stack_trace",
                "value": "File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 688, in forward\n    outputs: BaseModelOutputWithPast = self.model(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 453, in forward\n    layer_outputs = decoder_layer(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 308, in forward\n    hidden_states, self_attn_weights = self.self_attn(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 247, in forward\n    query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)"
              }
            ],
            "incomingEdges": [
              {
                "sourceNodeId": "node_Sin_326",
                "sourceNodeOutputId": "0",
                "targetNodeInputId": "0"
              },
              {
                "sourceNodeId": "node_Constant_60",
                "sourceNodeOutputId": "0",
                "targetNodeInputId": "1"
              }
            ],
            "outputsMetadata": [
              {
                "id": "0",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "unsqueeze_13"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "FLOAT[1,1,s16,64]"
                  },
                  {
                    "key": "param_name",
                    "value": "expanded"
                  }
                ]
              }
            ],
            "inputsMetadata": [
              {
                "id": "0",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "sin"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "FLOAT[1,s16,64]"
                  },
                  {
                    "key": "param_name",
                    "value": "data"
                  }
                ]
              },
              {
                "id": "1",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "val_26"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "INT64[1]"
                  },
                  {
                    "key": "param_name",
                    "value": "axes"
                  }
                ]
              }
            ],
            "style": null,
            "config": null
          },
          {
            "id": "node_Mul_542",
            "label": "Mul",
            "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.1: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.1.self_attn: transformers.models.llama.modeling_llama.LlamaAttention/mul_427: aten.mul.Tensor",
            "subgraphIds": [],
            "attrs": [
              {
                "key": "[metadata] namespace",
                "value": ": transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.1: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.1.self_attn: transformers.models.llama.modeling_llama.LlamaAttention/mul_427: aten.mul.Tensor"
              },
              {
                "key": "[metadata] pkg.torch.onnx.class_hierarchy",
                "value": "['transformers.models.llama.modeling_llama.LlamaForCausalLM', 'transformers.models.llama.modeling_llama.LlamaModel', 'transformers.models.llama.modeling_llama.LlamaDecoderLayer', 'transformers.models.llama.modeling_llama.LlamaAttention', 'aten.mul.Tensor']"
              },
              {
                "key": "[metadata] pkg.torch.onnx.fx_node",
                "value": "%mul_427 : [num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%transpose_6, %unsqueeze_12), kwargs = {})"
              },
              {
                "key": "[metadata] pkg.torch.onnx.name_scopes",
                "value": "['', 'model', 'model.layers.1', 'model.layers.1.self_attn', 'mul_427']"
              },
              {
                "key": "[metadata] pkg.torch.onnx.stack_trace",
                "value": "File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 688, in forward\n    outputs: BaseModelOutputWithPast = self.model(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 453, in forward\n    layer_outputs = decoder_layer(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 308, in forward\n    hidden_states, self_attn_weights = self.self_attn(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 247, in forward\n    query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)"
              }
            ],
            "incomingEdges": [
              {
                "sourceNodeId": "node_Transpose_519",
                "sourceNodeOutputId": "0",
                "targetNodeInputId": "0"
              },
              {
                "sourceNodeId": "node_Unsqueeze_540",
                "sourceNodeOutputId": "0",
                "targetNodeInputId": "1"
              }
            ],
            "outputsMetadata": [
              {
                "id": "0",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "mul_427"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "FLOAT[s34,32,s16,64]"
                  },
                  {
                    "key": "param_name",
                    "value": "C"
                  }
                ]
              }
            ],
            "inputsMetadata": [
              {
                "id": "0",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "transpose_6"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "FLOAT[s34,32,s16,64]"
                  },
                  {
                    "key": "param_name",
                    "value": "A"
                  }
                ]
              },
              {
                "id": "1",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "unsqueeze_12"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "FLOAT[1,1,s16,64]"
                  },
                  {
                    "key": "param_name",
                    "value": "B"
                  }
                ]
              }
            ],
            "style": null,
            "config": null
          },
          {
            "id": "node_Constant_931",
            "label": "Constant",
            "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.1: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.1.self_attn: transformers.models.llama.modeling_llama.LlamaAttention/slice_38: aten.slice.Tensor",
            "subgraphIds": [],
            "attrs": [
              {
                "key": "__value",
                "value": "[0]"
              },
              {
                "key": "value",
                "value": "TensorProtoTensor<INT64,[1]>(array([0]), name='val_386')"
              }
            ],
            "incomingEdges": [],
            "outputsMetadata": [
              {
                "id": "0",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "val_386"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "INT64[1]"
                  },
                  {
                    "key": "param_name",
                    "value": "output"
                  }
                ]
              }
            ],
            "inputsMetadata": [],
            "style": null,
            "config": null
          },
          {
            "id": "node_Constant_934",
            "label": "Constant",
            "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.1: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.1.self_attn: transformers.models.llama.modeling_llama.LlamaAttention/slice_38: aten.slice.Tensor",
            "subgraphIds": [],
            "attrs": [
              {
                "key": "__value",
                "value": "[32]"
              },
              {
                "key": "value",
                "value": "TensorProtoTensor<INT64,[1]>(array([32]), name='val_389')"
              }
            ],
            "incomingEdges": [],
            "outputsMetadata": [
              {
                "id": "0",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "val_389"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "INT64[1]"
                  },
                  {
                    "key": "param_name",
                    "value": "output"
                  }
                ]
              }
            ],
            "inputsMetadata": [],
            "style": null,
            "config": null
          },
          {
            "id": "node_Constant_937",
            "label": "Constant",
            "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.1: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.1.self_attn: transformers.models.llama.modeling_llama.LlamaAttention/slice_38: aten.slice.Tensor",
            "subgraphIds": [],
            "attrs": [
              {
                "key": "__value",
                "value": "[3]"
              },
              {
                "key": "value",
                "value": "TensorProtoTensor<INT64,[1]>(array([3]), name='val_392')"
              }
            ],
            "incomingEdges": [],
            "outputsMetadata": [
              {
                "id": "0",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "val_392"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "INT64[1]"
                  },
                  {
                    "key": "param_name",
                    "value": "output"
                  }
                ]
              }
            ],
            "inputsMetadata": [],
            "style": null,
            "config": null
          },
          {
            "id": "node_Constant_552",
            "label": "Constant",
            "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.1: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.1.self_attn: transformers.models.llama.modeling_llama.LlamaAttention/slice_38: aten.slice.Tensor",
            "subgraphIds": [],
            "attrs": [
              {
                "key": "__value",
                "value": "[1]"
              },
              {
                "key": "value_ints",
                "value": "[1]"
              },
              {
                "key": "[metadata] namespace",
                "value": ": transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.1: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.1.self_attn: transformers.models.llama.modeling_llama.LlamaAttention/slice_38: aten.slice.Tensor"
              },
              {
                "key": "[metadata] pkg.torch.onnx.class_hierarchy",
                "value": "['transformers.models.llama.modeling_llama.LlamaForCausalLM', 'transformers.models.llama.modeling_llama.LlamaModel', 'transformers.models.llama.modeling_llama.LlamaDecoderLayer', 'transformers.models.llama.modeling_llama.LlamaAttention', 'aten.slice.Tensor']"
              },
              {
                "key": "[metadata] pkg.torch.onnx.fx_node",
                "value": "%slice_38 : [num_users=1] = call_function[target=torch.ops.aten.slice.Tensor](args = (%transpose_6, 3, 0, 32), kwargs = {})"
              },
              {
                "key": "[metadata] pkg.torch.onnx.name_scopes",
                "value": "['', 'model', 'model.layers.1', 'model.layers.1.self_attn', 'slice_38']"
              },
              {
                "key": "[metadata] pkg.torch.onnx.stack_trace",
                "value": "File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 688, in forward\n    outputs: BaseModelOutputWithPast = self.model(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 453, in forward\n    layer_outputs = decoder_layer(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 308, in forward\n    hidden_states, self_attn_weights = self.self_attn(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 247, in forward\n    query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)"
              }
            ],
            "incomingEdges": [],
            "outputsMetadata": [
              {
                "id": "0",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "val_393"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "INT64[1]"
                  },
                  {
                    "key": "param_name",
                    "value": "output"
                  }
                ]
              }
            ],
            "inputsMetadata": [],
            "style": null,
            "config": null
          },
          {
            "id": "node_Slice_553",
            "label": "Slice",
            "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.1: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.1.self_attn: transformers.models.llama.modeling_llama.LlamaAttention/slice_38: aten.slice.Tensor",
            "subgraphIds": [],
            "attrs": [
              {
                "key": "[metadata] namespace",
                "value": ": transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.1: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.1.self_attn: transformers.models.llama.modeling_llama.LlamaAttention/slice_38: aten.slice.Tensor"
              },
              {
                "key": "[metadata] pkg.torch.onnx.class_hierarchy",
                "value": "['transformers.models.llama.modeling_llama.LlamaForCausalLM', 'transformers.models.llama.modeling_llama.LlamaModel', 'transformers.models.llama.modeling_llama.LlamaDecoderLayer', 'transformers.models.llama.modeling_llama.LlamaAttention', 'aten.slice.Tensor']"
              },
              {
                "key": "[metadata] pkg.torch.onnx.fx_node",
                "value": "%slice_38 : [num_users=1] = call_function[target=torch.ops.aten.slice.Tensor](args = (%transpose_6, 3, 0, 32), kwargs = {})"
              },
              {
                "key": "[metadata] pkg.torch.onnx.name_scopes",
                "value": "['', 'model', 'model.layers.1', 'model.layers.1.self_attn', 'slice_38']"
              },
              {
                "key": "[metadata] pkg.torch.onnx.stack_trace",
                "value": "File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 688, in forward\n    outputs: BaseModelOutputWithPast = self.model(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 453, in forward\n    layer_outputs = decoder_layer(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 308, in forward\n    hidden_states, self_attn_weights = self.self_attn(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 247, in forward\n    query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)"
              }
            ],
            "incomingEdges": [
              {
                "sourceNodeId": "node_Transpose_519",
                "sourceNodeOutputId": "0",
                "targetNodeInputId": "0"
              },
              {
                "sourceNodeId": "node_Constant_931",
                "sourceNodeOutputId": "0",
                "targetNodeInputId": "1"
              },
              {
                "sourceNodeId": "node_Constant_934",
                "sourceNodeOutputId": "0",
                "targetNodeInputId": "2"
              },
              {
                "sourceNodeId": "node_Constant_937",
                "sourceNodeOutputId": "0",
                "targetNodeInputId": "3"
              },
              {
                "sourceNodeId": "node_Constant_552",
                "sourceNodeOutputId": "0",
                "targetNodeInputId": "4"
              }
            ],
            "outputsMetadata": [
              {
                "id": "0",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "slice_38"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "FLOAT[s34,32,s16,32]"
                  },
                  {
                    "key": "param_name",
                    "value": "output"
                  }
                ]
              }
            ],
            "inputsMetadata": [
              {
                "id": "0",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "transpose_6"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "FLOAT[s34,32,s16,64]"
                  },
                  {
                    "key": "param_name",
                    "value": "data"
                  }
                ]
              },
              {
                "id": "1",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "val_386"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "INT64[1]"
                  },
                  {
                    "key": "param_name",
                    "value": "starts"
                  }
                ]
              },
              {
                "id": "2",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "val_389"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "INT64[1]"
                  },
                  {
                    "key": "param_name",
                    "value": "ends"
                  }
                ]
              },
              {
                "id": "3",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "val_392"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "INT64[1]"
                  },
                  {
                    "key": "param_name",
                    "value": "axes"
                  }
                ]
              },
              {
                "id": "4",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "val_393"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "INT64[1]"
                  },
                  {
                    "key": "param_name",
                    "value": "steps"
                  }
                ]
              }
            ],
            "style": null,
            "config": null
          },
          {
            "id": "node_Constant_940",
            "label": "Constant",
            "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.1: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.1.self_attn: transformers.models.llama.modeling_llama.LlamaAttention/slice_39: aten.slice.Tensor",
            "subgraphIds": [],
            "attrs": [
              {
                "key": "__value",
                "value": "[32]"
              },
              {
                "key": "value",
                "value": "TensorProtoTensor<INT64,[1]>(array([32]), name='val_396')"
              }
            ],
            "incomingEdges": [],
            "outputsMetadata": [
              {
                "id": "0",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "val_396"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "INT64[1]"
                  },
                  {
                    "key": "param_name",
                    "value": "output"
                  }
                ]
              }
            ],
            "inputsMetadata": [],
            "style": null,
            "config": null
          },
          {
            "id": "node_Constant_943",
            "label": "Constant",
            "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.1: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.1.self_attn: transformers.models.llama.modeling_llama.LlamaAttention/slice_39: aten.slice.Tensor",
            "subgraphIds": [],
            "attrs": [
              {
                "key": "__value",
                "value": "[9223372036854775807]"
              },
              {
                "key": "value",
                "value": "TensorProtoTensor<INT64,[1]>(array([9223372036854775807]), name='val_399')"
              }
            ],
            "incomingEdges": [],
            "outputsMetadata": [
              {
                "id": "0",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "val_399"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "INT64[1]"
                  },
                  {
                    "key": "param_name",
                    "value": "output"
                  }
                ]
              }
            ],
            "inputsMetadata": [],
            "style": null,
            "config": null
          },
          {
            "id": "node_Constant_946",
            "label": "Constant",
            "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.1: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.1.self_attn: transformers.models.llama.modeling_llama.LlamaAttention/slice_39: aten.slice.Tensor",
            "subgraphIds": [],
            "attrs": [
              {
                "key": "__value",
                "value": "[3]"
              },
              {
                "key": "value",
                "value": "TensorProtoTensor<INT64,[1]>(array([3]), name='val_402')"
              }
            ],
            "incomingEdges": [],
            "outputsMetadata": [
              {
                "id": "0",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "val_402"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "INT64[1]"
                  },
                  {
                    "key": "param_name",
                    "value": "output"
                  }
                ]
              }
            ],
            "inputsMetadata": [],
            "style": null,
            "config": null
          },
          {
            "id": "node_Constant_563",
            "label": "Constant",
            "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.1: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.1.self_attn: transformers.models.llama.modeling_llama.LlamaAttention/slice_39: aten.slice.Tensor",
            "subgraphIds": [],
            "attrs": [
              {
                "key": "__value",
                "value": "[1]"
              },
              {
                "key": "value_ints",
                "value": "[1]"
              },
              {
                "key": "[metadata] namespace",
                "value": ": transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.1: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.1.self_attn: transformers.models.llama.modeling_llama.LlamaAttention/slice_39: aten.slice.Tensor"
              },
              {
                "key": "[metadata] pkg.torch.onnx.class_hierarchy",
                "value": "['transformers.models.llama.modeling_llama.LlamaForCausalLM', 'transformers.models.llama.modeling_llama.LlamaModel', 'transformers.models.llama.modeling_llama.LlamaDecoderLayer', 'transformers.models.llama.modeling_llama.LlamaAttention', 'aten.slice.Tensor']"
              },
              {
                "key": "[metadata] pkg.torch.onnx.fx_node",
                "value": "%slice_39 : [num_users=1] = call_function[target=torch.ops.aten.slice.Tensor](args = (%transpose_6, 3, 32, 9223372036854775807), kwargs = {})"
              },
              {
                "key": "[metadata] pkg.torch.onnx.name_scopes",
                "value": "['', 'model', 'model.layers.1', 'model.layers.1.self_attn', 'slice_39']"
              },
              {
                "key": "[metadata] pkg.torch.onnx.stack_trace",
                "value": "File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 688, in forward\n    outputs: BaseModelOutputWithPast = self.model(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 453, in forward\n    layer_outputs = decoder_layer(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 308, in forward\n    hidden_states, self_attn_weights = self.self_attn(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 247, in forward\n    query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)"
              }
            ],
            "incomingEdges": [],
            "outputsMetadata": [
              {
                "id": "0",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "val_403"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "INT64[1]"
                  },
                  {
                    "key": "param_name",
                    "value": "output"
                  }
                ]
              }
            ],
            "inputsMetadata": [],
            "style": null,
            "config": null
          },
          {
            "id": "node_Slice_564",
            "label": "Slice",
            "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.1: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.1.self_attn: transformers.models.llama.modeling_llama.LlamaAttention/slice_39: aten.slice.Tensor",
            "subgraphIds": [],
            "attrs": [
              {
                "key": "[metadata] namespace",
                "value": ": transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.1: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.1.self_attn: transformers.models.llama.modeling_llama.LlamaAttention/slice_39: aten.slice.Tensor"
              },
              {
                "key": "[metadata] pkg.torch.onnx.class_hierarchy",
                "value": "['transformers.models.llama.modeling_llama.LlamaForCausalLM', 'transformers.models.llama.modeling_llama.LlamaModel', 'transformers.models.llama.modeling_llama.LlamaDecoderLayer', 'transformers.models.llama.modeling_llama.LlamaAttention', 'aten.slice.Tensor']"
              },
              {
                "key": "[metadata] pkg.torch.onnx.fx_node",
                "value": "%slice_39 : [num_users=1] = call_function[target=torch.ops.aten.slice.Tensor](args = (%transpose_6, 3, 32, 9223372036854775807), kwargs = {})"
              },
              {
                "key": "[metadata] pkg.torch.onnx.name_scopes",
                "value": "['', 'model', 'model.layers.1', 'model.layers.1.self_attn', 'slice_39']"
              },
              {
                "key": "[metadata] pkg.torch.onnx.stack_trace",
                "value": "File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 688, in forward\n    outputs: BaseModelOutputWithPast = self.model(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 453, in forward\n    layer_outputs = decoder_layer(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 308, in forward\n    hidden_states, self_attn_weights = self.self_attn(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 247, in forward\n    query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)"
              }
            ],
            "incomingEdges": [
              {
                "sourceNodeId": "node_Transpose_519",
                "sourceNodeOutputId": "0",
                "targetNodeInputId": "0"
              },
              {
                "sourceNodeId": "node_Constant_940",
                "sourceNodeOutputId": "0",
                "targetNodeInputId": "1"
              },
              {
                "sourceNodeId": "node_Constant_943",
                "sourceNodeOutputId": "0",
                "targetNodeInputId": "2"
              },
              {
                "sourceNodeId": "node_Constant_946",
                "sourceNodeOutputId": "0",
                "targetNodeInputId": "3"
              },
              {
                "sourceNodeId": "node_Constant_563",
                "sourceNodeOutputId": "0",
                "targetNodeInputId": "4"
              }
            ],
            "outputsMetadata": [
              {
                "id": "0",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "slice_39"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "FLOAT[s34,32,s16,32]"
                  },
                  {
                    "key": "param_name",
                    "value": "output"
                  }
                ]
              }
            ],
            "inputsMetadata": [
              {
                "id": "0",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "transpose_6"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "FLOAT[s34,32,s16,64]"
                  },
                  {
                    "key": "param_name",
                    "value": "data"
                  }
                ]
              },
              {
                "id": "1",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "val_396"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "INT64[1]"
                  },
                  {
                    "key": "param_name",
                    "value": "starts"
                  }
                ]
              },
              {
                "id": "2",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "val_399"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "INT64[1]"
                  },
                  {
                    "key": "param_name",
                    "value": "ends"
                  }
                ]
              },
              {
                "id": "3",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "val_402"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "INT64[1]"
                  },
                  {
                    "key": "param_name",
                    "value": "axes"
                  }
                ]
              },
              {
                "id": "4",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "val_403"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "INT64[1]"
                  },
                  {
                    "key": "param_name",
                    "value": "steps"
                  }
                ]
              }
            ],
            "style": null,
            "config": null
          },
          {
            "id": "node_Neg_565",
            "label": "Neg",
            "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.1: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.1.self_attn: transformers.models.llama.modeling_llama.LlamaAttention/neg_2: aten.neg.default",
            "subgraphIds": [],
            "attrs": [
              {
                "key": "[metadata] namespace",
                "value": ": transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.1: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.1.self_attn: transformers.models.llama.modeling_llama.LlamaAttention/neg_2: aten.neg.default"
              },
              {
                "key": "[metadata] pkg.torch.onnx.class_hierarchy",
                "value": "['transformers.models.llama.modeling_llama.LlamaForCausalLM', 'transformers.models.llama.modeling_llama.LlamaModel', 'transformers.models.llama.modeling_llama.LlamaDecoderLayer', 'transformers.models.llama.modeling_llama.LlamaAttention', 'aten.neg.default']"
              },
              {
                "key": "[metadata] pkg.torch.onnx.fx_node",
                "value": "%neg_2 : [num_users=1] = call_function[target=torch.ops.aten.neg.default](args = (%slice_39,), kwargs = {})"
              },
              {
                "key": "[metadata] pkg.torch.onnx.name_scopes",
                "value": "['', 'model', 'model.layers.1', 'model.layers.1.self_attn', 'neg_2']"
              },
              {
                "key": "[metadata] pkg.torch.onnx.stack_trace",
                "value": "File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 688, in forward\n    outputs: BaseModelOutputWithPast = self.model(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 453, in forward\n    layer_outputs = decoder_layer(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 308, in forward\n    hidden_states, self_attn_weights = self.self_attn(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 247, in forward\n    query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)"
              }
            ],
            "incomingEdges": [
              {
                "sourceNodeId": "node_Slice_564",
                "sourceNodeOutputId": "0",
                "targetNodeInputId": "0"
              }
            ],
            "outputsMetadata": [
              {
                "id": "0",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "neg_2"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "FLOAT[s34,32,s16,32]"
                  },
                  {
                    "key": "param_name",
                    "value": "Y"
                  }
                ]
              }
            ],
            "inputsMetadata": [
              {
                "id": "0",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "slice_39"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "FLOAT[s34,32,s16,32]"
                  },
                  {
                    "key": "param_name",
                    "value": "X"
                  }
                ]
              }
            ],
            "style": null,
            "config": null
          },
          {
            "id": "node_Concat_566",
            "label": "Concat",
            "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.1: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.1.self_attn: transformers.models.llama.modeling_llama.LlamaAttention/cat_5: aten.cat.default",
            "subgraphIds": [],
            "attrs": [
              {
                "key": "axis",
                "value": "-1"
              },
              {
                "key": "[metadata] namespace",
                "value": ": transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.1: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.1.self_attn: transformers.models.llama.modeling_llama.LlamaAttention/cat_5: aten.cat.default"
              },
              {
                "key": "[metadata] pkg.torch.onnx.class_hierarchy",
                "value": "['transformers.models.llama.modeling_llama.LlamaForCausalLM', 'transformers.models.llama.modeling_llama.LlamaModel', 'transformers.models.llama.modeling_llama.LlamaDecoderLayer', 'transformers.models.llama.modeling_llama.LlamaAttention', 'aten.cat.default']"
              },
              {
                "key": "[metadata] pkg.torch.onnx.fx_node",
                "value": "%cat_5 : [num_users=1] = call_function[target=torch.ops.aten.cat.default](args = ([%neg_2, %slice_38], -1), kwargs = {})"
              },
              {
                "key": "[metadata] pkg.torch.onnx.name_scopes",
                "value": "['', 'model', 'model.layers.1', 'model.layers.1.self_attn', 'cat_5']"
              },
              {
                "key": "[metadata] pkg.torch.onnx.stack_trace",
                "value": "File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 688, in forward\n    outputs: BaseModelOutputWithPast = self.model(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 453, in forward\n    layer_outputs = decoder_layer(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 308, in forward\n    hidden_states, self_attn_weights = self.self_attn(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 247, in forward\n    query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)"
              }
            ],
            "incomingEdges": [
              {
                "sourceNodeId": "node_Neg_565",
                "sourceNodeOutputId": "0",
                "targetNodeInputId": "0"
              },
              {
                "sourceNodeId": "node_Slice_553",
                "sourceNodeOutputId": "0",
                "targetNodeInputId": "1"
              }
            ],
            "outputsMetadata": [
              {
                "id": "0",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "cat_5"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "FLOAT[s34,32,s16,64]"
                  },
                  {
                    "key": "param_name",
                    "value": "concat_result"
                  }
                ]
              }
            ],
            "inputsMetadata": [
              {
                "id": "0",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "neg_2"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "FLOAT[s34,32,s16,32]"
                  },
                  {
                    "key": "param_name",
                    "value": "inputs"
                  }
                ]
              },
              {
                "id": "1",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "slice_38"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "FLOAT[s34,32,s16,32]"
                  },
                  {
                    "key": "param_name",
                    "value": "inputs"
                  }
                ]
              }
            ],
            "style": null,
            "config": null
          },
          {
            "id": "node_Mul_567",
            "label": "Mul",
            "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.1: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.1.self_attn: transformers.models.llama.modeling_llama.LlamaAttention/mul_444: aten.mul.Tensor",
            "subgraphIds": [],
            "attrs": [
              {
                "key": "[metadata] namespace",
                "value": ": transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.1: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.1.self_attn: transformers.models.llama.modeling_llama.LlamaAttention/mul_444: aten.mul.Tensor"
              },
              {
                "key": "[metadata] pkg.torch.onnx.class_hierarchy",
                "value": "['transformers.models.llama.modeling_llama.LlamaForCausalLM', 'transformers.models.llama.modeling_llama.LlamaModel', 'transformers.models.llama.modeling_llama.LlamaDecoderLayer', 'transformers.models.llama.modeling_llama.LlamaAttention', 'aten.mul.Tensor']"
              },
              {
                "key": "[metadata] pkg.torch.onnx.fx_node",
                "value": "%mul_444 : [num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%cat_5, %unsqueeze_13), kwargs = {})"
              },
              {
                "key": "[metadata] pkg.torch.onnx.name_scopes",
                "value": "['', 'model', 'model.layers.1', 'model.layers.1.self_attn', 'mul_444']"
              },
              {
                "key": "[metadata] pkg.torch.onnx.stack_trace",
                "value": "File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 688, in forward\n    outputs: BaseModelOutputWithPast = self.model(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 453, in forward\n    layer_outputs = decoder_layer(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 308, in forward\n    hidden_states, self_attn_weights = self.self_attn(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 247, in forward\n    query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)"
              }
            ],
            "incomingEdges": [
              {
                "sourceNodeId": "node_Concat_566",
                "sourceNodeOutputId": "0",
                "targetNodeInputId": "0"
              },
              {
                "sourceNodeId": "node_Unsqueeze_541",
                "sourceNodeOutputId": "0",
                "targetNodeInputId": "1"
              }
            ],
            "outputsMetadata": [
              {
                "id": "0",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "mul_444"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "FLOAT[s34,32,s16,64]"
                  },
                  {
                    "key": "param_name",
                    "value": "C"
                  }
                ]
              }
            ],
            "inputsMetadata": [
              {
                "id": "0",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "cat_5"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "FLOAT[s34,32,s16,64]"
                  },
                  {
                    "key": "param_name",
                    "value": "A"
                  }
                ]
              },
              {
                "id": "1",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "unsqueeze_13"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "FLOAT[1,1,s16,64]"
                  },
                  {
                    "key": "param_name",
                    "value": "B"
                  }
                ]
              }
            ],
            "style": null,
            "config": null
          },
          {
            "id": "node_Add_568",
            "label": "Add",
            "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.1: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.1.self_attn: transformers.models.llama.modeling_llama.LlamaAttention/add_571: aten.add.Tensor",
            "subgraphIds": [],
            "attrs": [
              {
                "key": "[metadata] namespace",
                "value": ": transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.1: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.1.self_attn: transformers.models.llama.modeling_llama.LlamaAttention/add_571: aten.add.Tensor"
              },
              {
                "key": "[metadata] pkg.torch.onnx.class_hierarchy",
                "value": "['transformers.models.llama.modeling_llama.LlamaForCausalLM', 'transformers.models.llama.modeling_llama.LlamaModel', 'transformers.models.llama.modeling_llama.LlamaDecoderLayer', 'transformers.models.llama.modeling_llama.LlamaAttention', 'aten.add.Tensor']"
              },
              {
                "key": "[metadata] pkg.torch.onnx.fx_node",
                "value": "%add_571 : [num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%mul_427, %mul_444), kwargs = {})"
              },
              {
                "key": "[metadata] pkg.torch.onnx.name_scopes",
                "value": "['', 'model', 'model.layers.1', 'model.layers.1.self_attn', 'add_571']"
              },
              {
                "key": "[metadata] pkg.torch.onnx.stack_trace",
                "value": "File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 688, in forward\n    outputs: BaseModelOutputWithPast = self.model(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 453, in forward\n    layer_outputs = decoder_layer(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 308, in forward\n    hidden_states, self_attn_weights = self.self_attn(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 247, in forward\n    query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)"
              }
            ],
            "incomingEdges": [
              {
                "sourceNodeId": "node_Mul_542",
                "sourceNodeOutputId": "0",
                "targetNodeInputId": "0"
              },
              {
                "sourceNodeId": "node_Mul_567",
                "sourceNodeOutputId": "0",
                "targetNodeInputId": "1"
              }
            ],
            "outputsMetadata": [
              {
                "id": "0",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "add_571"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "FLOAT[s34,32,s16,64]"
                  },
                  {
                    "key": "param_name",
                    "value": "C"
                  }
                ]
              }
            ],
            "inputsMetadata": [
              {
                "id": "0",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "mul_427"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "FLOAT[s34,32,s16,64]"
                  },
                  {
                    "key": "param_name",
                    "value": "A"
                  }
                ]
              },
              {
                "id": "1",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "mul_444"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "FLOAT[s34,32,s16,64]"
                  },
                  {
                    "key": "param_name",
                    "value": "B"
                  }
                ]
              }
            ],
            "style": null,
            "config": null
          },
          {
            "id": "node_Mul_569",
            "label": "Mul",
            "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.1: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.1.self_attn: transformers.models.llama.modeling_llama.LlamaAttention/mul_452: aten.mul.Tensor",
            "subgraphIds": [],
            "attrs": [
              {
                "key": "[metadata] namespace",
                "value": ": transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.1: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.1.self_attn: transformers.models.llama.modeling_llama.LlamaAttention/mul_452: aten.mul.Tensor"
              },
              {
                "key": "[metadata] pkg.torch.onnx.class_hierarchy",
                "value": "['transformers.models.llama.modeling_llama.LlamaForCausalLM', 'transformers.models.llama.modeling_llama.LlamaModel', 'transformers.models.llama.modeling_llama.LlamaDecoderLayer', 'transformers.models.llama.modeling_llama.LlamaAttention', 'aten.mul.Tensor']"
              },
              {
                "key": "[metadata] pkg.torch.onnx.fx_node",
                "value": "%mul_452 : [num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%transpose_7, %unsqueeze_12), kwargs = {})"
              },
              {
                "key": "[metadata] pkg.torch.onnx.name_scopes",
                "value": "['', 'model', 'model.layers.1', 'model.layers.1.self_attn', 'mul_452']"
              },
              {
                "key": "[metadata] pkg.torch.onnx.stack_trace",
                "value": "File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 688, in forward\n    outputs: BaseModelOutputWithPast = self.model(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 453, in forward\n    layer_outputs = decoder_layer(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 308, in forward\n    hidden_states, self_attn_weights = self.self_attn(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 247, in forward\n    query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)"
              }
            ],
            "incomingEdges": [
              {
                "sourceNodeId": "node_Transpose_529",
                "sourceNodeOutputId": "0",
                "targetNodeInputId": "0"
              },
              {
                "sourceNodeId": "node_Unsqueeze_540",
                "sourceNodeOutputId": "0",
                "targetNodeInputId": "1"
              }
            ],
            "outputsMetadata": [
              {
                "id": "0",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "mul_452"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "FLOAT[s34,32,s16,64]"
                  },
                  {
                    "key": "param_name",
                    "value": "C"
                  }
                ]
              }
            ],
            "inputsMetadata": [
              {
                "id": "0",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "transpose_7"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "FLOAT[s34,32,s16,64]"
                  },
                  {
                    "key": "param_name",
                    "value": "A"
                  }
                ]
              },
              {
                "id": "1",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "unsqueeze_12"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "FLOAT[1,1,s16,64]"
                  },
                  {
                    "key": "param_name",
                    "value": "B"
                  }
                ]
              }
            ],
            "style": null,
            "config": null
          },
          {
            "id": "node_Constant_949",
            "label": "Constant",
            "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.1: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.1.self_attn: transformers.models.llama.modeling_llama.LlamaAttention/slice_40: aten.slice.Tensor",
            "subgraphIds": [],
            "attrs": [
              {
                "key": "__value",
                "value": "[0]"
              },
              {
                "key": "value",
                "value": "TensorProtoTensor<INT64,[1]>(array([0]), name='val_406')"
              }
            ],
            "incomingEdges": [],
            "outputsMetadata": [
              {
                "id": "0",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "val_406"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "INT64[1]"
                  },
                  {
                    "key": "param_name",
                    "value": "output"
                  }
                ]
              }
            ],
            "inputsMetadata": [],
            "style": null,
            "config": null
          },
          {
            "id": "node_Constant_952",
            "label": "Constant",
            "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.1: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.1.self_attn: transformers.models.llama.modeling_llama.LlamaAttention/slice_40: aten.slice.Tensor",
            "subgraphIds": [],
            "attrs": [
              {
                "key": "__value",
                "value": "[32]"
              },
              {
                "key": "value",
                "value": "TensorProtoTensor<INT64,[1]>(array([32]), name='val_409')"
              }
            ],
            "incomingEdges": [],
            "outputsMetadata": [
              {
                "id": "0",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "val_409"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "INT64[1]"
                  },
                  {
                    "key": "param_name",
                    "value": "output"
                  }
                ]
              }
            ],
            "inputsMetadata": [],
            "style": null,
            "config": null
          },
          {
            "id": "node_Constant_955",
            "label": "Constant",
            "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.1: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.1.self_attn: transformers.models.llama.modeling_llama.LlamaAttention/slice_40: aten.slice.Tensor",
            "subgraphIds": [],
            "attrs": [
              {
                "key": "__value",
                "value": "[3]"
              },
              {
                "key": "value",
                "value": "TensorProtoTensor<INT64,[1]>(array([3]), name='val_412')"
              }
            ],
            "incomingEdges": [],
            "outputsMetadata": [
              {
                "id": "0",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "val_412"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "INT64[1]"
                  },
                  {
                    "key": "param_name",
                    "value": "output"
                  }
                ]
              }
            ],
            "inputsMetadata": [],
            "style": null,
            "config": null
          },
          {
            "id": "node_Constant_579",
            "label": "Constant",
            "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.1: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.1.self_attn: transformers.models.llama.modeling_llama.LlamaAttention/slice_40: aten.slice.Tensor",
            "subgraphIds": [],
            "attrs": [
              {
                "key": "__value",
                "value": "[1]"
              },
              {
                "key": "value_ints",
                "value": "[1]"
              },
              {
                "key": "[metadata] namespace",
                "value": ": transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.1: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.1.self_attn: transformers.models.llama.modeling_llama.LlamaAttention/slice_40: aten.slice.Tensor"
              },
              {
                "key": "[metadata] pkg.torch.onnx.class_hierarchy",
                "value": "['transformers.models.llama.modeling_llama.LlamaForCausalLM', 'transformers.models.llama.modeling_llama.LlamaModel', 'transformers.models.llama.modeling_llama.LlamaDecoderLayer', 'transformers.models.llama.modeling_llama.LlamaAttention', 'aten.slice.Tensor']"
              },
              {
                "key": "[metadata] pkg.torch.onnx.fx_node",
                "value": "%slice_40 : [num_users=1] = call_function[target=torch.ops.aten.slice.Tensor](args = (%transpose_7, 3, 0, 32), kwargs = {})"
              },
              {
                "key": "[metadata] pkg.torch.onnx.name_scopes",
                "value": "['', 'model', 'model.layers.1', 'model.layers.1.self_attn', 'slice_40']"
              },
              {
                "key": "[metadata] pkg.torch.onnx.stack_trace",
                "value": "File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 688, in forward\n    outputs: BaseModelOutputWithPast = self.model(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 453, in forward\n    layer_outputs = decoder_layer(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 308, in forward\n    hidden_states, self_attn_weights = self.self_attn(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 247, in forward\n    query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)"
              }
            ],
            "incomingEdges": [],
            "outputsMetadata": [
              {
                "id": "0",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "val_413"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "INT64[1]"
                  },
                  {
                    "key": "param_name",
                    "value": "output"
                  }
                ]
              }
            ],
            "inputsMetadata": [],
            "style": null,
            "config": null
          },
          {
            "id": "node_Slice_580",
            "label": "Slice",
            "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.1: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.1.self_attn: transformers.models.llama.modeling_llama.LlamaAttention/slice_40: aten.slice.Tensor",
            "subgraphIds": [],
            "attrs": [
              {
                "key": "[metadata] namespace",
                "value": ": transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.1: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.1.self_attn: transformers.models.llama.modeling_llama.LlamaAttention/slice_40: aten.slice.Tensor"
              },
              {
                "key": "[metadata] pkg.torch.onnx.class_hierarchy",
                "value": "['transformers.models.llama.modeling_llama.LlamaForCausalLM', 'transformers.models.llama.modeling_llama.LlamaModel', 'transformers.models.llama.modeling_llama.LlamaDecoderLayer', 'transformers.models.llama.modeling_llama.LlamaAttention', 'aten.slice.Tensor']"
              },
              {
                "key": "[metadata] pkg.torch.onnx.fx_node",
                "value": "%slice_40 : [num_users=1] = call_function[target=torch.ops.aten.slice.Tensor](args = (%transpose_7, 3, 0, 32), kwargs = {})"
              },
              {
                "key": "[metadata] pkg.torch.onnx.name_scopes",
                "value": "['', 'model', 'model.layers.1', 'model.layers.1.self_attn', 'slice_40']"
              },
              {
                "key": "[metadata] pkg.torch.onnx.stack_trace",
                "value": "File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 688, in forward\n    outputs: BaseModelOutputWithPast = self.model(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 453, in forward\n    layer_outputs = decoder_layer(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 308, in forward\n    hidden_states, self_attn_weights = self.self_attn(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 247, in forward\n    query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)"
              }
            ],
            "incomingEdges": [
              {
                "sourceNodeId": "node_Transpose_529",
                "sourceNodeOutputId": "0",
                "targetNodeInputId": "0"
              },
              {
                "sourceNodeId": "node_Constant_949",
                "sourceNodeOutputId": "0",
                "targetNodeInputId": "1"
              },
              {
                "sourceNodeId": "node_Constant_952",
                "sourceNodeOutputId": "0",
                "targetNodeInputId": "2"
              },
              {
                "sourceNodeId": "node_Constant_955",
                "sourceNodeOutputId": "0",
                "targetNodeInputId": "3"
              },
              {
                "sourceNodeId": "node_Constant_579",
                "sourceNodeOutputId": "0",
                "targetNodeInputId": "4"
              }
            ],
            "outputsMetadata": [
              {
                "id": "0",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "slice_40"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "FLOAT[s34,32,s16,32]"
                  },
                  {
                    "key": "param_name",
                    "value": "output"
                  }
                ]
              }
            ],
            "inputsMetadata": [
              {
                "id": "0",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "transpose_7"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "FLOAT[s34,32,s16,64]"
                  },
                  {
                    "key": "param_name",
                    "value": "data"
                  }
                ]
              },
              {
                "id": "1",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "val_406"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "INT64[1]"
                  },
                  {
                    "key": "param_name",
                    "value": "starts"
                  }
                ]
              },
              {
                "id": "2",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "val_409"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "INT64[1]"
                  },
                  {
                    "key": "param_name",
                    "value": "ends"
                  }
                ]
              },
              {
                "id": "3",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "val_412"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "INT64[1]"
                  },
                  {
                    "key": "param_name",
                    "value": "axes"
                  }
                ]
              },
              {
                "id": "4",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "val_413"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "INT64[1]"
                  },
                  {
                    "key": "param_name",
                    "value": "steps"
                  }
                ]
              }
            ],
            "style": null,
            "config": null
          },
          {
            "id": "node_Constant_958",
            "label": "Constant",
            "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.1: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.1.self_attn: transformers.models.llama.modeling_llama.LlamaAttention/slice_41: aten.slice.Tensor",
            "subgraphIds": [],
            "attrs": [
              {
                "key": "__value",
                "value": "[32]"
              },
              {
                "key": "value",
                "value": "TensorProtoTensor<INT64,[1]>(array([32]), name='val_416')"
              }
            ],
            "incomingEdges": [],
            "outputsMetadata": [
              {
                "id": "0",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "val_416"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "INT64[1]"
                  },
                  {
                    "key": "param_name",
                    "value": "output"
                  }
                ]
              }
            ],
            "inputsMetadata": [],
            "style": null,
            "config": null
          },
          {
            "id": "node_Constant_961",
            "label": "Constant",
            "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.1: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.1.self_attn: transformers.models.llama.modeling_llama.LlamaAttention/slice_41: aten.slice.Tensor",
            "subgraphIds": [],
            "attrs": [
              {
                "key": "__value",
                "value": "[9223372036854775807]"
              },
              {
                "key": "value",
                "value": "TensorProtoTensor<INT64,[1]>(array([9223372036854775807]), name='val_419')"
              }
            ],
            "incomingEdges": [],
            "outputsMetadata": [
              {
                "id": "0",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "val_419"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "INT64[1]"
                  },
                  {
                    "key": "param_name",
                    "value": "output"
                  }
                ]
              }
            ],
            "inputsMetadata": [],
            "style": null,
            "config": null
          },
          {
            "id": "node_Constant_964",
            "label": "Constant",
            "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.1: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.1.self_attn: transformers.models.llama.modeling_llama.LlamaAttention/slice_41: aten.slice.Tensor",
            "subgraphIds": [],
            "attrs": [
              {
                "key": "__value",
                "value": "[3]"
              },
              {
                "key": "value",
                "value": "TensorProtoTensor<INT64,[1]>(array([3]), name='val_422')"
              }
            ],
            "incomingEdges": [],
            "outputsMetadata": [
              {
                "id": "0",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "val_422"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "INT64[1]"
                  },
                  {
                    "key": "param_name",
                    "value": "output"
                  }
                ]
              }
            ],
            "inputsMetadata": [],
            "style": null,
            "config": null
          },
          {
            "id": "node_Constant_590",
            "label": "Constant",
            "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.1: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.1.self_attn: transformers.models.llama.modeling_llama.LlamaAttention/slice_41: aten.slice.Tensor",
            "subgraphIds": [],
            "attrs": [
              {
                "key": "__value",
                "value": "[1]"
              },
              {
                "key": "value_ints",
                "value": "[1]"
              },
              {
                "key": "[metadata] namespace",
                "value": ": transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.1: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.1.self_attn: transformers.models.llama.modeling_llama.LlamaAttention/slice_41: aten.slice.Tensor"
              },
              {
                "key": "[metadata] pkg.torch.onnx.class_hierarchy",
                "value": "['transformers.models.llama.modeling_llama.LlamaForCausalLM', 'transformers.models.llama.modeling_llama.LlamaModel', 'transformers.models.llama.modeling_llama.LlamaDecoderLayer', 'transformers.models.llama.modeling_llama.LlamaAttention', 'aten.slice.Tensor']"
              },
              {
                "key": "[metadata] pkg.torch.onnx.fx_node",
                "value": "%slice_41 : [num_users=1] = call_function[target=torch.ops.aten.slice.Tensor](args = (%transpose_7, 3, 32, 9223372036854775807), kwargs = {})"
              },
              {
                "key": "[metadata] pkg.torch.onnx.name_scopes",
                "value": "['', 'model', 'model.layers.1', 'model.layers.1.self_attn', 'slice_41']"
              },
              {
                "key": "[metadata] pkg.torch.onnx.stack_trace",
                "value": "File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 688, in forward\n    outputs: BaseModelOutputWithPast = self.model(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 453, in forward\n    layer_outputs = decoder_layer(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 308, in forward\n    hidden_states, self_attn_weights = self.self_attn(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 247, in forward\n    query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)"
              }
            ],
            "incomingEdges": [],
            "outputsMetadata": [
              {
                "id": "0",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "val_423"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "INT64[1]"
                  },
                  {
                    "key": "param_name",
                    "value": "output"
                  }
                ]
              }
            ],
            "inputsMetadata": [],
            "style": null,
            "config": null
          },
          {
            "id": "node_Slice_591",
            "label": "Slice",
            "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.1: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.1.self_attn: transformers.models.llama.modeling_llama.LlamaAttention/slice_41: aten.slice.Tensor",
            "subgraphIds": [],
            "attrs": [
              {
                "key": "[metadata] namespace",
                "value": ": transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.1: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.1.self_attn: transformers.models.llama.modeling_llama.LlamaAttention/slice_41: aten.slice.Tensor"
              },
              {
                "key": "[metadata] pkg.torch.onnx.class_hierarchy",
                "value": "['transformers.models.llama.modeling_llama.LlamaForCausalLM', 'transformers.models.llama.modeling_llama.LlamaModel', 'transformers.models.llama.modeling_llama.LlamaDecoderLayer', 'transformers.models.llama.modeling_llama.LlamaAttention', 'aten.slice.Tensor']"
              },
              {
                "key": "[metadata] pkg.torch.onnx.fx_node",
                "value": "%slice_41 : [num_users=1] = call_function[target=torch.ops.aten.slice.Tensor](args = (%transpose_7, 3, 32, 9223372036854775807), kwargs = {})"
              },
              {
                "key": "[metadata] pkg.torch.onnx.name_scopes",
                "value": "['', 'model', 'model.layers.1', 'model.layers.1.self_attn', 'slice_41']"
              },
              {
                "key": "[metadata] pkg.torch.onnx.stack_trace",
                "value": "File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 688, in forward\n    outputs: BaseModelOutputWithPast = self.model(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 453, in forward\n    layer_outputs = decoder_layer(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 308, in forward\n    hidden_states, self_attn_weights = self.self_attn(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 247, in forward\n    query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)"
              }
            ],
            "incomingEdges": [
              {
                "sourceNodeId": "node_Transpose_529",
                "sourceNodeOutputId": "0",
                "targetNodeInputId": "0"
              },
              {
                "sourceNodeId": "node_Constant_958",
                "sourceNodeOutputId": "0",
                "targetNodeInputId": "1"
              },
              {
                "sourceNodeId": "node_Constant_961",
                "sourceNodeOutputId": "0",
                "targetNodeInputId": "2"
              },
              {
                "sourceNodeId": "node_Constant_964",
                "sourceNodeOutputId": "0",
                "targetNodeInputId": "3"
              },
              {
                "sourceNodeId": "node_Constant_590",
                "sourceNodeOutputId": "0",
                "targetNodeInputId": "4"
              }
            ],
            "outputsMetadata": [
              {
                "id": "0",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "slice_41"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "FLOAT[s34,32,s16,32]"
                  },
                  {
                    "key": "param_name",
                    "value": "output"
                  }
                ]
              }
            ],
            "inputsMetadata": [
              {
                "id": "0",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "transpose_7"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "FLOAT[s34,32,s16,64]"
                  },
                  {
                    "key": "param_name",
                    "value": "data"
                  }
                ]
              },
              {
                "id": "1",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "val_416"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "INT64[1]"
                  },
                  {
                    "key": "param_name",
                    "value": "starts"
                  }
                ]
              },
              {
                "id": "2",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "val_419"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "INT64[1]"
                  },
                  {
                    "key": "param_name",
                    "value": "ends"
                  }
                ]
              },
              {
                "id": "3",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "val_422"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "INT64[1]"
                  },
                  {
                    "key": "param_name",
                    "value": "axes"
                  }
                ]
              },
              {
                "id": "4",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "val_423"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "INT64[1]"
                  },
                  {
                    "key": "param_name",
                    "value": "steps"
                  }
                ]
              }
            ],
            "style": null,
            "config": null
          },
          {
            "id": "node_Neg_592",
            "label": "Neg",
            "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.1: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.1.self_attn: transformers.models.llama.modeling_llama.LlamaAttention/neg_3: aten.neg.default",
            "subgraphIds": [],
            "attrs": [
              {
                "key": "[metadata] namespace",
                "value": ": transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.1: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.1.self_attn: transformers.models.llama.modeling_llama.LlamaAttention/neg_3: aten.neg.default"
              },
              {
                "key": "[metadata] pkg.torch.onnx.class_hierarchy",
                "value": "['transformers.models.llama.modeling_llama.LlamaForCausalLM', 'transformers.models.llama.modeling_llama.LlamaModel', 'transformers.models.llama.modeling_llama.LlamaDecoderLayer', 'transformers.models.llama.modeling_llama.LlamaAttention', 'aten.neg.default']"
              },
              {
                "key": "[metadata] pkg.torch.onnx.fx_node",
                "value": "%neg_3 : [num_users=1] = call_function[target=torch.ops.aten.neg.default](args = (%slice_41,), kwargs = {})"
              },
              {
                "key": "[metadata] pkg.torch.onnx.name_scopes",
                "value": "['', 'model', 'model.layers.1', 'model.layers.1.self_attn', 'neg_3']"
              },
              {
                "key": "[metadata] pkg.torch.onnx.stack_trace",
                "value": "File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 688, in forward\n    outputs: BaseModelOutputWithPast = self.model(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 453, in forward\n    layer_outputs = decoder_layer(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 308, in forward\n    hidden_states, self_attn_weights = self.self_attn(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 247, in forward\n    query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)"
              }
            ],
            "incomingEdges": [
              {
                "sourceNodeId": "node_Slice_591",
                "sourceNodeOutputId": "0",
                "targetNodeInputId": "0"
              }
            ],
            "outputsMetadata": [
              {
                "id": "0",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "neg_3"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "FLOAT[s34,32,s16,32]"
                  },
                  {
                    "key": "param_name",
                    "value": "Y"
                  }
                ]
              }
            ],
            "inputsMetadata": [
              {
                "id": "0",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "slice_41"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "FLOAT[s34,32,s16,32]"
                  },
                  {
                    "key": "param_name",
                    "value": "X"
                  }
                ]
              }
            ],
            "style": null,
            "config": null
          },
          {
            "id": "node_Concat_593",
            "label": "Concat",
            "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.1: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.1.self_attn: transformers.models.llama.modeling_llama.LlamaAttention/cat_6: aten.cat.default",
            "subgraphIds": [],
            "attrs": [
              {
                "key": "axis",
                "value": "-1"
              },
              {
                "key": "[metadata] namespace",
                "value": ": transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.1: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.1.self_attn: transformers.models.llama.modeling_llama.LlamaAttention/cat_6: aten.cat.default"
              },
              {
                "key": "[metadata] pkg.torch.onnx.class_hierarchy",
                "value": "['transformers.models.llama.modeling_llama.LlamaForCausalLM', 'transformers.models.llama.modeling_llama.LlamaModel', 'transformers.models.llama.modeling_llama.LlamaDecoderLayer', 'transformers.models.llama.modeling_llama.LlamaAttention', 'aten.cat.default']"
              },
              {
                "key": "[metadata] pkg.torch.onnx.fx_node",
                "value": "%cat_6 : [num_users=1] = call_function[target=torch.ops.aten.cat.default](args = ([%neg_3, %slice_40], -1), kwargs = {})"
              },
              {
                "key": "[metadata] pkg.torch.onnx.name_scopes",
                "value": "['', 'model', 'model.layers.1', 'model.layers.1.self_attn', 'cat_6']"
              },
              {
                "key": "[metadata] pkg.torch.onnx.stack_trace",
                "value": "File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 688, in forward\n    outputs: BaseModelOutputWithPast = self.model(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 453, in forward\n    layer_outputs = decoder_layer(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 308, in forward\n    hidden_states, self_attn_weights = self.self_attn(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 247, in forward\n    query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)"
              }
            ],
            "incomingEdges": [
              {
                "sourceNodeId": "node_Neg_592",
                "sourceNodeOutputId": "0",
                "targetNodeInputId": "0"
              },
              {
                "sourceNodeId": "node_Slice_580",
                "sourceNodeOutputId": "0",
                "targetNodeInputId": "1"
              }
            ],
            "outputsMetadata": [
              {
                "id": "0",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "cat_6"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "FLOAT[s34,32,s16,64]"
                  },
                  {
                    "key": "param_name",
                    "value": "concat_result"
                  }
                ]
              }
            ],
            "inputsMetadata": [
              {
                "id": "0",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "neg_3"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "FLOAT[s34,32,s16,32]"
                  },
                  {
                    "key": "param_name",
                    "value": "inputs"
                  }
                ]
              },
              {
                "id": "1",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "slice_40"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "FLOAT[s34,32,s16,32]"
                  },
                  {
                    "key": "param_name",
                    "value": "inputs"
                  }
                ]
              }
            ],
            "style": null,
            "config": null
          },
          {
            "id": "node_Mul_594",
            "label": "Mul",
            "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.1: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.1.self_attn: transformers.models.llama.modeling_llama.LlamaAttention/mul_469: aten.mul.Tensor",
            "subgraphIds": [],
            "attrs": [
              {
                "key": "[metadata] namespace",
                "value": ": transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.1: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.1.self_attn: transformers.models.llama.modeling_llama.LlamaAttention/mul_469: aten.mul.Tensor"
              },
              {
                "key": "[metadata] pkg.torch.onnx.class_hierarchy",
                "value": "['transformers.models.llama.modeling_llama.LlamaForCausalLM', 'transformers.models.llama.modeling_llama.LlamaModel', 'transformers.models.llama.modeling_llama.LlamaDecoderLayer', 'transformers.models.llama.modeling_llama.LlamaAttention', 'aten.mul.Tensor']"
              },
              {
                "key": "[metadata] pkg.torch.onnx.fx_node",
                "value": "%mul_469 : [num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%cat_6, %unsqueeze_13), kwargs = {})"
              },
              {
                "key": "[metadata] pkg.torch.onnx.name_scopes",
                "value": "['', 'model', 'model.layers.1', 'model.layers.1.self_attn', 'mul_469']"
              },
              {
                "key": "[metadata] pkg.torch.onnx.stack_trace",
                "value": "File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 688, in forward\n    outputs: BaseModelOutputWithPast = self.model(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 453, in forward\n    layer_outputs = decoder_layer(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 308, in forward\n    hidden_states, self_attn_weights = self.self_attn(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 247, in forward\n    query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)"
              }
            ],
            "incomingEdges": [
              {
                "sourceNodeId": "node_Concat_593",
                "sourceNodeOutputId": "0",
                "targetNodeInputId": "0"
              },
              {
                "sourceNodeId": "node_Unsqueeze_541",
                "sourceNodeOutputId": "0",
                "targetNodeInputId": "1"
              }
            ],
            "outputsMetadata": [
              {
                "id": "0",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "mul_469"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "FLOAT[s34,32,s16,64]"
                  },
                  {
                    "key": "param_name",
                    "value": "C"
                  }
                ]
              }
            ],
            "inputsMetadata": [
              {
                "id": "0",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "cat_6"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "FLOAT[s34,32,s16,64]"
                  },
                  {
                    "key": "param_name",
                    "value": "A"
                  }
                ]
              },
              {
                "id": "1",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "unsqueeze_13"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "FLOAT[1,1,s16,64]"
                  },
                  {
                    "key": "param_name",
                    "value": "B"
                  }
                ]
              }
            ],
            "style": null,
            "config": null
          },
          {
            "id": "node_Add_595",
            "label": "Add",
            "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.1: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.1.self_attn: transformers.models.llama.modeling_llama.LlamaAttention/add_607: aten.add.Tensor",
            "subgraphIds": [],
            "attrs": [
              {
                "key": "[metadata] namespace",
                "value": ": transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.1: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.1.self_attn: transformers.models.llama.modeling_llama.LlamaAttention/add_607: aten.add.Tensor"
              },
              {
                "key": "[metadata] pkg.torch.onnx.class_hierarchy",
                "value": "['transformers.models.llama.modeling_llama.LlamaForCausalLM', 'transformers.models.llama.modeling_llama.LlamaModel', 'transformers.models.llama.modeling_llama.LlamaDecoderLayer', 'transformers.models.llama.modeling_llama.LlamaAttention', 'aten.add.Tensor']"
              },
              {
                "key": "[metadata] pkg.torch.onnx.fx_node",
                "value": "%add_607 : [num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%mul_452, %mul_469), kwargs = {})"
              },
              {
                "key": "[metadata] pkg.torch.onnx.name_scopes",
                "value": "['', 'model', 'model.layers.1', 'model.layers.1.self_attn', 'add_607']"
              },
              {
                "key": "[metadata] pkg.torch.onnx.stack_trace",
                "value": "File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 688, in forward\n    outputs: BaseModelOutputWithPast = self.model(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 453, in forward\n    layer_outputs = decoder_layer(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 308, in forward\n    hidden_states, self_attn_weights = self.self_attn(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 247, in forward\n    query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)"
              }
            ],
            "incomingEdges": [
              {
                "sourceNodeId": "node_Mul_569",
                "sourceNodeOutputId": "0",
                "targetNodeInputId": "0"
              },
              {
                "sourceNodeId": "node_Mul_594",
                "sourceNodeOutputId": "0",
                "targetNodeInputId": "1"
              }
            ],
            "outputsMetadata": [
              {
                "id": "0",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "add_607"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "FLOAT[s34,32,s16,64]"
                  },
                  {
                    "key": "param_name",
                    "value": "C"
                  }
                ]
              }
            ],
            "inputsMetadata": [
              {
                "id": "0",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "mul_452"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "FLOAT[s34,32,s16,64]"
                  },
                  {
                    "key": "param_name",
                    "value": "A"
                  }
                ]
              },
              {
                "id": "1",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "mul_469"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "FLOAT[s34,32,s16,64]"
                  },
                  {
                    "key": "param_name",
                    "value": "B"
                  }
                ]
              }
            ],
            "style": null,
            "config": null
          },
          {
            "id": "node_Concat_596",
            "label": "Concat",
            "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.1: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.1.self_attn: transformers.models.llama.modeling_llama.LlamaAttention/cat_7: aten.cat.default",
            "subgraphIds": [],
            "attrs": [
              {
                "key": "axis",
                "value": "-2"
              },
              {
                "key": "[metadata] namespace",
                "value": ": transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.1: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.1.self_attn: transformers.models.llama.modeling_llama.LlamaAttention/cat_7: aten.cat.default"
              },
              {
                "key": "[metadata] pkg.torch.onnx.class_hierarchy",
                "value": "['transformers.models.llama.modeling_llama.LlamaForCausalLM', 'transformers.models.llama.modeling_llama.LlamaModel', 'transformers.models.llama.modeling_llama.LlamaDecoderLayer', 'transformers.models.llama.modeling_llama.LlamaAttention', 'aten.cat.default']"
              },
              {
                "key": "[metadata] pkg.torch.onnx.fx_node",
                "value": "%cat_7 : [num_users=2] = call_function[target=torch.ops.aten.cat.default](args = ([%past_key_values_key_cache_1, %add_607], -2), kwargs = {})"
              },
              {
                "key": "[metadata] pkg.torch.onnx.name_scopes",
                "value": "['', 'model', 'model.layers.1', 'model.layers.1.self_attn', 'cat_7']"
              },
              {
                "key": "[metadata] pkg.torch.onnx.stack_trace",
                "value": "File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 688, in forward\n    outputs: BaseModelOutputWithPast = self.model(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 453, in forward\n    layer_outputs = decoder_layer(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 308, in forward\n    hidden_states, self_attn_weights = self.self_attn(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 252, in forward\n    key_states, value_states = past_key_value.update(key_states, value_states, self.layer_idx, cache_kwargs)"
              }
            ],
            "incomingEdges": [
              {
                "sourceNodeId": "[value] past_key_values_key_cache_1",
                "sourceNodeOutputId": "0",
                "targetNodeInputId": "0"
              },
              {
                "sourceNodeId": "node_Add_595",
                "sourceNodeOutputId": "0",
                "targetNodeInputId": "1"
              }
            ],
            "outputsMetadata": [
              {
                "id": "0",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "cat_7"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "FLOAT[s34,32,s16 + s17,64]"
                  },
                  {
                    "key": "[metadata] pkg.torch.export.graph_signature.OutputSpec.kind",
                    "value": "USER_OUTPUT"
                  },
                  {
                    "key": "param_name",
                    "value": "concat_result"
                  }
                ]
              }
            ],
            "inputsMetadata": [
              {
                "id": "0",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "past_key_values_key_cache_1"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "FLOAT[s34,32,s17,64]"
                  },
                  {
                    "key": "[metadata] pkg.torch.export.graph_signature.InputSpec.kind",
                    "value": "USER_INPUT"
                  },
                  {
                    "key": "[metadata] pkg.torch.export.graph_signature.InputSpec.persistent",
                    "value": "None"
                  },
                  {
                    "key": "param_name",
                    "value": "inputs"
                  }
                ]
              },
              {
                "id": "1",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "add_607"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "FLOAT[s34,32,s16,64]"
                  },
                  {
                    "key": "param_name",
                    "value": "inputs"
                  }
                ]
              }
            ],
            "style": null,
            "config": null
          },
          {
            "id": "node_Concat_597",
            "label": "Concat",
            "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.1: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.1.self_attn: transformers.models.llama.modeling_llama.LlamaAttention/cat_8: aten.cat.default",
            "subgraphIds": [],
            "attrs": [
              {
                "key": "axis",
                "value": "-2"
              },
              {
                "key": "[metadata] namespace",
                "value": ": transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.1: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.1.self_attn: transformers.models.llama.modeling_llama.LlamaAttention/cat_8: aten.cat.default"
              },
              {
                "key": "[metadata] pkg.torch.onnx.class_hierarchy",
                "value": "['transformers.models.llama.modeling_llama.LlamaForCausalLM', 'transformers.models.llama.modeling_llama.LlamaModel', 'transformers.models.llama.modeling_llama.LlamaDecoderLayer', 'transformers.models.llama.modeling_llama.LlamaAttention', 'aten.cat.default']"
              },
              {
                "key": "[metadata] pkg.torch.onnx.fx_node",
                "value": "%cat_8 : [num_users=2] = call_function[target=torch.ops.aten.cat.default](args = ([%past_key_values_value_cache_1, %transpose_8], -2), kwargs = {})"
              },
              {
                "key": "[metadata] pkg.torch.onnx.name_scopes",
                "value": "['', 'model', 'model.layers.1', 'model.layers.1.self_attn', 'cat_8']"
              },
              {
                "key": "[metadata] pkg.torch.onnx.stack_trace",
                "value": "File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 688, in forward\n    outputs: BaseModelOutputWithPast = self.model(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 453, in forward\n    layer_outputs = decoder_layer(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 308, in forward\n    hidden_states, self_attn_weights = self.self_attn(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 252, in forward\n    key_states, value_states = past_key_value.update(key_states, value_states, self.layer_idx, cache_kwargs)"
              }
            ],
            "incomingEdges": [
              {
                "sourceNodeId": "[value] past_key_values_value_cache_1",
                "sourceNodeOutputId": "0",
                "targetNodeInputId": "0"
              },
              {
                "sourceNodeId": "node_Transpose_539",
                "sourceNodeOutputId": "0",
                "targetNodeInputId": "1"
              }
            ],
            "outputsMetadata": [
              {
                "id": "0",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "cat_8"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "FLOAT[s34,32,s16 + s17,64]"
                  },
                  {
                    "key": "[metadata] pkg.torch.export.graph_signature.OutputSpec.kind",
                    "value": "USER_OUTPUT"
                  },
                  {
                    "key": "param_name",
                    "value": "concat_result"
                  }
                ]
              }
            ],
            "inputsMetadata": [
              {
                "id": "0",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "past_key_values_value_cache_1"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "FLOAT[s34,32,s17,64]"
                  },
                  {
                    "key": "[metadata] pkg.torch.export.graph_signature.InputSpec.kind",
                    "value": "USER_INPUT"
                  },
                  {
                    "key": "[metadata] pkg.torch.export.graph_signature.InputSpec.persistent",
                    "value": "None"
                  },
                  {
                    "key": "param_name",
                    "value": "inputs"
                  }
                ]
              },
              {
                "id": "1",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "transpose_8"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "FLOAT[s34,32,s16,64]"
                  },
                  {
                    "key": "param_name",
                    "value": "inputs"
                  }
                ]
              }
            ],
            "style": null,
            "config": null
          },
          {
            "id": "node_Transpose_598",
            "label": "Transpose",
            "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.1: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.1.self_attn: transformers.models.llama.modeling_llama.LlamaAttention/transpose_9: aten.transpose.int",
            "subgraphIds": [],
            "attrs": [
              {
                "key": "perm",
                "value": "[0, 1, 3, 2]"
              },
              {
                "key": "[metadata] namespace",
                "value": ": transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.1: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.1.self_attn: transformers.models.llama.modeling_llama.LlamaAttention/transpose_9: aten.transpose.int"
              },
              {
                "key": "[metadata] pkg.torch.onnx.class_hierarchy",
                "value": "['transformers.models.llama.modeling_llama.LlamaForCausalLM', 'transformers.models.llama.modeling_llama.LlamaModel', 'transformers.models.llama.modeling_llama.LlamaDecoderLayer', 'transformers.models.llama.modeling_llama.LlamaAttention', 'aten.transpose.int']"
              },
              {
                "key": "[metadata] pkg.torch.onnx.fx_node",
                "value": "%transpose_9 : [num_users=1] = call_function[target=torch.ops.aten.transpose.int](args = (%cat_7, 2, 3), kwargs = {})"
              },
              {
                "key": "[metadata] pkg.torch.onnx.name_scopes",
                "value": "['', 'model', 'model.layers.1', 'model.layers.1.self_attn', 'transpose_9']"
              },
              {
                "key": "[metadata] pkg.torch.onnx.stack_trace",
                "value": "File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 688, in forward\n    outputs: BaseModelOutputWithPast = self.model(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 453, in forward\n    layer_outputs = decoder_layer(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 308, in forward\n    hidden_states, self_attn_weights = self.self_attn(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 265, in forward\n    attn_output, attn_weights = attention_interface("
              }
            ],
            "incomingEdges": [
              {
                "sourceNodeId": "node_Concat_596",
                "sourceNodeOutputId": "0",
                "targetNodeInputId": "0"
              }
            ],
            "outputsMetadata": [
              {
                "id": "0",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "transpose_9"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "FLOAT[s34,32,64,s16 + s17]"
                  },
                  {
                    "key": "param_name",
                    "value": "transposed"
                  }
                ]
              }
            ],
            "inputsMetadata": [
              {
                "id": "0",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "cat_7"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "FLOAT[s34,32,s16 + s17,64]"
                  },
                  {
                    "key": "[metadata] pkg.torch.export.graph_signature.OutputSpec.kind",
                    "value": "USER_OUTPUT"
                  },
                  {
                    "key": "param_name",
                    "value": "data"
                  }
                ]
              }
            ],
            "style": null,
            "config": null
          },
          {
            "id": "node_MatMul_599",
            "label": "MatMul",
            "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.1: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.1.self_attn: transformers.models.llama.modeling_llama.LlamaAttention/matmul_3: aten.matmul.default",
            "subgraphIds": [],
            "attrs": [
              {
                "key": "[metadata] namespace",
                "value": ": transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.1: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.1.self_attn: transformers.models.llama.modeling_llama.LlamaAttention/matmul_3: aten.matmul.default"
              },
              {
                "key": "[metadata] pkg.torch.onnx.class_hierarchy",
                "value": "['transformers.models.llama.modeling_llama.LlamaForCausalLM', 'transformers.models.llama.modeling_llama.LlamaModel', 'transformers.models.llama.modeling_llama.LlamaDecoderLayer', 'transformers.models.llama.modeling_llama.LlamaAttention', 'aten.matmul.default']"
              },
              {
                "key": "[metadata] pkg.torch.onnx.fx_node",
                "value": "%matmul_3 : [num_users=1] = call_function[target=torch.ops.aten.matmul.default](args = (%add_571, %transpose_9), kwargs = {})"
              },
              {
                "key": "[metadata] pkg.torch.onnx.name_scopes",
                "value": "['', 'model', 'model.layers.1', 'model.layers.1.self_attn', 'matmul_3']"
              },
              {
                "key": "[metadata] pkg.torch.onnx.stack_trace",
                "value": "File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 688, in forward\n    outputs: BaseModelOutputWithPast = self.model(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 453, in forward\n    layer_outputs = decoder_layer(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 308, in forward\n    hidden_states, self_attn_weights = self.self_attn(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 265, in forward\n    attn_output, attn_weights = attention_interface("
              }
            ],
            "incomingEdges": [
              {
                "sourceNodeId": "node_Add_568",
                "sourceNodeOutputId": "0",
                "targetNodeInputId": "0"
              },
              {
                "sourceNodeId": "node_Transpose_598",
                "sourceNodeOutputId": "0",
                "targetNodeInputId": "1"
              }
            ],
            "outputsMetadata": [
              {
                "id": "0",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "matmul_3"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "FLOAT[s34,32,s16,s16 + s17]"
                  },
                  {
                    "key": "param_name",
                    "value": "Y"
                  }
                ]
              }
            ],
            "inputsMetadata": [
              {
                "id": "0",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "add_571"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "FLOAT[s34,32,s16,64]"
                  },
                  {
                    "key": "param_name",
                    "value": "A"
                  }
                ]
              },
              {
                "id": "1",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "transpose_9"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "FLOAT[s34,32,64,s16 + s17]"
                  },
                  {
                    "key": "param_name",
                    "value": "B"
                  }
                ]
              }
            ],
            "style": null,
            "config": null
          },
          {
            "id": "node_Mul_600",
            "label": "Mul",
            "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.1: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.1.self_attn: transformers.models.llama.modeling_llama.LlamaAttention/mul_493: aten.mul.Tensor",
            "subgraphIds": [],
            "attrs": [
              {
                "key": "[metadata] namespace",
                "value": ": transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.1: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.1.self_attn: transformers.models.llama.modeling_llama.LlamaAttention/mul_493: aten.mul.Tensor"
              },
              {
                "key": "[metadata] pkg.torch.onnx.class_hierarchy",
                "value": "['transformers.models.llama.modeling_llama.LlamaForCausalLM', 'transformers.models.llama.modeling_llama.LlamaModel', 'transformers.models.llama.modeling_llama.LlamaDecoderLayer', 'transformers.models.llama.modeling_llama.LlamaAttention', 'aten.mul.Tensor']"
              },
              {
                "key": "[metadata] pkg.torch.onnx.fx_node",
                "value": "%mul_493 : [num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%matmul_3, 0.125), kwargs = {})"
              },
              {
                "key": "[metadata] pkg.torch.onnx.name_scopes",
                "value": "['', 'model', 'model.layers.1', 'model.layers.1.self_attn', 'mul_493']"
              },
              {
                "key": "[metadata] pkg.torch.onnx.stack_trace",
                "value": "File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 688, in forward\n    outputs: BaseModelOutputWithPast = self.model(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 453, in forward\n    layer_outputs = decoder_layer(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 308, in forward\n    hidden_states, self_attn_weights = self.self_attn(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 265, in forward\n    attn_output, attn_weights = attention_interface("
              }
            ],
            "incomingEdges": [
              {
                "sourceNodeId": "node_MatMul_599",
                "sourceNodeOutputId": "0",
                "targetNodeInputId": "0"
              },
              {
                "sourceNodeId": "node_Constant_431",
                "sourceNodeOutputId": "0",
                "targetNodeInputId": "1"
              }
            ],
            "outputsMetadata": [
              {
                "id": "0",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "mul_493"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "FLOAT[s34,32,s16,s16 + s17]"
                  },
                  {
                    "key": "param_name",
                    "value": "C"
                  }
                ]
              }
            ],
            "inputsMetadata": [
              {
                "id": "0",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "matmul_3"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "FLOAT[s34,32,s16,s16 + s17]"
                  },
                  {
                    "key": "param_name",
                    "value": "A"
                  }
                ]
              },
              {
                "id": "1",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "val_316"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "FLOAT[]"
                  },
                  {
                    "key": "param_name",
                    "value": "B"
                  }
                ]
              }
            ],
            "style": null,
            "config": null
          },
          {
            "id": "node_Constant_622",
            "label": "Constant",
            "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.1: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.1.self_attn: transformers.models.llama.modeling_llama.LlamaAttention/slice_49: aten.slice.Tensor",
            "subgraphIds": [],
            "attrs": [
              {
                "key": "__value",
                "value": "[0]"
              },
              {
                "key": "value_ints",
                "value": "[0]"
              },
              {
                "key": "[metadata] namespace",
                "value": ": transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.1: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.1.self_attn: transformers.models.llama.modeling_llama.LlamaAttention/slice_49: aten.slice.Tensor"
              },
              {
                "key": "[metadata] pkg.torch.onnx.class_hierarchy",
                "value": "['transformers.models.llama.modeling_llama.LlamaForCausalLM', 'transformers.models.llama.modeling_llama.LlamaModel', 'transformers.models.llama.modeling_llama.LlamaDecoderLayer', 'transformers.models.llama.modeling_llama.LlamaAttention', 'aten.slice.Tensor']"
              },
              {
                "key": "[metadata] pkg.torch.onnx.fx_node",
                "value": "%slice_49 : [num_users=1] = call_function[target=torch.ops.aten.slice.Tensor](args = (%slice_48, 3, None, %add_4), kwargs = {})"
              },
              {
                "key": "[metadata] pkg.torch.onnx.name_scopes",
                "value": "['', 'model', 'model.layers.1', 'model.layers.1.self_attn', 'slice_49']"
              },
              {
                "key": "[metadata] pkg.torch.onnx.stack_trace",
                "value": "File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 688, in forward\n    outputs: BaseModelOutputWithPast = self.model(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 453, in forward\n    layer_outputs = decoder_layer(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 308, in forward\n    hidden_states, self_attn_weights = self.self_attn(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 265, in forward\n    attn_output, attn_weights = attention_interface("
              }
            ],
            "incomingEdges": [],
            "outputsMetadata": [
              {
                "id": "0",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "val_442"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "INT64[1]"
                  },
                  {
                    "key": "param_name",
                    "value": "output"
                  }
                ]
              }
            ],
            "inputsMetadata": [],
            "style": null,
            "config": null
          },
          {
            "id": "node_Constant_624",
            "label": "Constant",
            "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.1: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.1.self_attn: transformers.models.llama.modeling_llama.LlamaAttention/slice_49: aten.slice.Tensor",
            "subgraphIds": [],
            "attrs": [
              {
                "key": "__value",
                "value": "[-1]"
              },
              {
                "key": "value_ints",
                "value": "[-1]"
              },
              {
                "key": "[metadata] namespace",
                "value": ": transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.1: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.1.self_attn: transformers.models.llama.modeling_llama.LlamaAttention/slice_49: aten.slice.Tensor"
              },
              {
                "key": "[metadata] pkg.torch.onnx.class_hierarchy",
                "value": "['transformers.models.llama.modeling_llama.LlamaForCausalLM', 'transformers.models.llama.modeling_llama.LlamaModel', 'transformers.models.llama.modeling_llama.LlamaDecoderLayer', 'transformers.models.llama.modeling_llama.LlamaAttention', 'aten.slice.Tensor']"
              },
              {
                "key": "[metadata] pkg.torch.onnx.fx_node",
                "value": "%slice_49 : [num_users=1] = call_function[target=torch.ops.aten.slice.Tensor](args = (%slice_48, 3, None, %add_4), kwargs = {})"
              },
              {
                "key": "[metadata] pkg.torch.onnx.name_scopes",
                "value": "['', 'model', 'model.layers.1', 'model.layers.1.self_attn', 'slice_49']"
              },
              {
                "key": "[metadata] pkg.torch.onnx.stack_trace",
                "value": "File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 688, in forward\n    outputs: BaseModelOutputWithPast = self.model(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 453, in forward\n    layer_outputs = decoder_layer(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 308, in forward\n    hidden_states, self_attn_weights = self.self_attn(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 265, in forward\n    attn_output, attn_weights = attention_interface("
              }
            ],
            "incomingEdges": [],
            "outputsMetadata": [
              {
                "id": "0",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "val_444"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "INT64[1]"
                  },
                  {
                    "key": "param_name",
                    "value": "output"
                  }
                ]
              }
            ],
            "inputsMetadata": [],
            "style": null,
            "config": null
          },
          {
            "id": "node_Reshape_625",
            "label": "Reshape",
            "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.1: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.1.self_attn: transformers.models.llama.modeling_llama.LlamaAttention/slice_49: aten.slice.Tensor",
            "subgraphIds": [],
            "attrs": [
              {
                "key": "allowzero",
                "value": "0"
              },
              {
                "key": "[metadata] namespace",
                "value": ": transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.1: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.1.self_attn: transformers.models.llama.modeling_llama.LlamaAttention/slice_49: aten.slice.Tensor"
              },
              {
                "key": "[metadata] pkg.torch.onnx.class_hierarchy",
                "value": "['transformers.models.llama.modeling_llama.LlamaForCausalLM', 'transformers.models.llama.modeling_llama.LlamaModel', 'transformers.models.llama.modeling_llama.LlamaDecoderLayer', 'transformers.models.llama.modeling_llama.LlamaAttention', 'aten.slice.Tensor']"
              },
              {
                "key": "[metadata] pkg.torch.onnx.fx_node",
                "value": "%slice_49 : [num_users=1] = call_function[target=torch.ops.aten.slice.Tensor](args = (%slice_48, 3, None, %add_4), kwargs = {})"
              },
              {
                "key": "[metadata] pkg.torch.onnx.name_scopes",
                "value": "['', 'model', 'model.layers.1', 'model.layers.1.self_attn', 'slice_49']"
              },
              {
                "key": "[metadata] pkg.torch.onnx.stack_trace",
                "value": "File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 688, in forward\n    outputs: BaseModelOutputWithPast = self.model(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 453, in forward\n    layer_outputs = decoder_layer(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 308, in forward\n    hidden_states, self_attn_weights = self.self_attn(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 265, in forward\n    attn_output, attn_weights = attention_interface("
              }
            ],
            "incomingEdges": [
              {
                "sourceNodeId": "node_Add_33",
                "sourceNodeOutputId": "0",
                "targetNodeInputId": "0"
              },
              {
                "sourceNodeId": "node_Constant_624",
                "sourceNodeOutputId": "0",
                "targetNodeInputId": "1"
              }
            ],
            "outputsMetadata": [
              {
                "id": "0",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "val_445"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "INT64[1]"
                  },
                  {
                    "key": "param_name",
                    "value": "reshaped"
                  }
                ]
              }
            ],
            "inputsMetadata": [
              {
                "id": "0",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "add_4"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "INT64[]"
                  },
                  {
                    "key": "param_name",
                    "value": "data"
                  }
                ]
              },
              {
                "id": "1",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "val_444"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "INT64[1]"
                  },
                  {
                    "key": "param_name",
                    "value": "shape"
                  }
                ]
              }
            ],
            "style": null,
            "config": null
          },
          {
            "id": "node_Constant_977",
            "label": "Constant",
            "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.1: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.1.self_attn: transformers.models.llama.modeling_llama.LlamaAttention/slice_49: aten.slice.Tensor",
            "subgraphIds": [],
            "attrs": [
              {
                "key": "__value",
                "value": "[3]"
              },
              {
                "key": "value",
                "value": "TensorProtoTensor<INT64,[1]>(array([3]), name='val_448')"
              }
            ],
            "incomingEdges": [],
            "outputsMetadata": [
              {
                "id": "0",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "val_448"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "INT64[1]"
                  },
                  {
                    "key": "param_name",
                    "value": "output"
                  }
                ]
              }
            ],
            "inputsMetadata": [],
            "style": null,
            "config": null
          },
          {
            "id": "node_Constant_629",
            "label": "Constant",
            "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.1: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.1.self_attn: transformers.models.llama.modeling_llama.LlamaAttention/slice_49: aten.slice.Tensor",
            "subgraphIds": [],
            "attrs": [
              {
                "key": "__value",
                "value": "[1]"
              },
              {
                "key": "value_ints",
                "value": "[1]"
              },
              {
                "key": "[metadata] namespace",
                "value": ": transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.1: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.1.self_attn: transformers.models.llama.modeling_llama.LlamaAttention/slice_49: aten.slice.Tensor"
              },
              {
                "key": "[metadata] pkg.torch.onnx.class_hierarchy",
                "value": "['transformers.models.llama.modeling_llama.LlamaForCausalLM', 'transformers.models.llama.modeling_llama.LlamaModel', 'transformers.models.llama.modeling_llama.LlamaDecoderLayer', 'transformers.models.llama.modeling_llama.LlamaAttention', 'aten.slice.Tensor']"
              },
              {
                "key": "[metadata] pkg.torch.onnx.fx_node",
                "value": "%slice_49 : [num_users=1] = call_function[target=torch.ops.aten.slice.Tensor](args = (%slice_48, 3, None, %add_4), kwargs = {})"
              },
              {
                "key": "[metadata] pkg.torch.onnx.name_scopes",
                "value": "['', 'model', 'model.layers.1', 'model.layers.1.self_attn', 'slice_49']"
              },
              {
                "key": "[metadata] pkg.torch.onnx.stack_trace",
                "value": "File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 688, in forward\n    outputs: BaseModelOutputWithPast = self.model(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 453, in forward\n    layer_outputs = decoder_layer(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 308, in forward\n    hidden_states, self_attn_weights = self.self_attn(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 265, in forward\n    attn_output, attn_weights = attention_interface("
              }
            ],
            "incomingEdges": [],
            "outputsMetadata": [
              {
                "id": "0",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "val_449"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "INT64[1]"
                  },
                  {
                    "key": "param_name",
                    "value": "output"
                  }
                ]
              }
            ],
            "inputsMetadata": [],
            "style": null,
            "config": null
          },
          {
            "id": "node_Slice_630",
            "label": "Slice",
            "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.1: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.1.self_attn: transformers.models.llama.modeling_llama.LlamaAttention/slice_49: aten.slice.Tensor",
            "subgraphIds": [],
            "attrs": [
              {
                "key": "[metadata] namespace",
                "value": ": transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.1: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.1.self_attn: transformers.models.llama.modeling_llama.LlamaAttention/slice_49: aten.slice.Tensor"
              },
              {
                "key": "[metadata] pkg.torch.onnx.class_hierarchy",
                "value": "['transformers.models.llama.modeling_llama.LlamaForCausalLM', 'transformers.models.llama.modeling_llama.LlamaModel', 'transformers.models.llama.modeling_llama.LlamaDecoderLayer', 'transformers.models.llama.modeling_llama.LlamaAttention', 'aten.slice.Tensor']"
              },
              {
                "key": "[metadata] pkg.torch.onnx.fx_node",
                "value": "%slice_49 : [num_users=1] = call_function[target=torch.ops.aten.slice.Tensor](args = (%slice_48, 3, None, %add_4), kwargs = {})"
              },
              {
                "key": "[metadata] pkg.torch.onnx.name_scopes",
                "value": "['', 'model', 'model.layers.1', 'model.layers.1.self_attn', 'slice_49']"
              },
              {
                "key": "[metadata] pkg.torch.onnx.stack_trace",
                "value": "File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 688, in forward\n    outputs: BaseModelOutputWithPast = self.model(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 453, in forward\n    layer_outputs = decoder_layer(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 308, in forward\n    hidden_states, self_attn_weights = self.self_attn(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 265, in forward\n    attn_output, attn_weights = attention_interface("
              }
            ],
            "incomingEdges": [
              {
                "sourceNodeId": "node_ScatterND_279",
                "sourceNodeOutputId": "0",
                "targetNodeInputId": "0"
              },
              {
                "sourceNodeId": "node_Constant_622",
                "sourceNodeOutputId": "0",
                "targetNodeInputId": "1"
              },
              {
                "sourceNodeId": "node_Reshape_625",
                "sourceNodeOutputId": "0",
                "targetNodeInputId": "2"
              },
              {
                "sourceNodeId": "node_Constant_977",
                "sourceNodeOutputId": "0",
                "targetNodeInputId": "3"
              },
              {
                "sourceNodeId": "node_Constant_629",
                "sourceNodeOutputId": "0",
                "targetNodeInputId": "4"
              }
            ],
            "outputsMetadata": [
              {
                "id": "0",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "slice_49"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "FLOAT[s34,1,s16,s16 + s17]"
                  },
                  {
                    "key": "param_name",
                    "value": "output"
                  }
                ]
              }
            ],
            "inputsMetadata": [
              {
                "id": "0",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "slice_scatter_2"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "FLOAT[s34,1,s16,s16 + s17]"
                  },
                  {
                    "key": "param_name",
                    "value": "data"
                  }
                ]
              },
              {
                "id": "1",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "val_442"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "INT64[1]"
                  },
                  {
                    "key": "param_name",
                    "value": "starts"
                  }
                ]
              },
              {
                "id": "2",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "val_445"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "INT64[1]"
                  },
                  {
                    "key": "param_name",
                    "value": "ends"
                  }
                ]
              },
              {
                "id": "3",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "val_448"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "INT64[1]"
                  },
                  {
                    "key": "param_name",
                    "value": "axes"
                  }
                ]
              },
              {
                "id": "4",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "val_449"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "INT64[1]"
                  },
                  {
                    "key": "param_name",
                    "value": "steps"
                  }
                ]
              }
            ],
            "style": null,
            "config": null
          },
          {
            "id": "node_Add_631",
            "label": "Add",
            "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.1: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.1.self_attn: transformers.models.llama.modeling_llama.LlamaAttention/add_658: aten.add.Tensor",
            "subgraphIds": [],
            "attrs": [
              {
                "key": "[metadata] namespace",
                "value": ": transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.1: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.1.self_attn: transformers.models.llama.modeling_llama.LlamaAttention/add_658: aten.add.Tensor"
              },
              {
                "key": "[metadata] pkg.torch.onnx.class_hierarchy",
                "value": "['transformers.models.llama.modeling_llama.LlamaForCausalLM', 'transformers.models.llama.modeling_llama.LlamaModel', 'transformers.models.llama.modeling_llama.LlamaDecoderLayer', 'transformers.models.llama.modeling_llama.LlamaAttention', 'aten.add.Tensor']"
              },
              {
                "key": "[metadata] pkg.torch.onnx.fx_node",
                "value": "%add_658 : [num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%mul_493, %slice_49), kwargs = {})"
              },
              {
                "key": "[metadata] pkg.torch.onnx.name_scopes",
                "value": "['', 'model', 'model.layers.1', 'model.layers.1.self_attn', 'add_658']"
              },
              {
                "key": "[metadata] pkg.torch.onnx.stack_trace",
                "value": "File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 688, in forward\n    outputs: BaseModelOutputWithPast = self.model(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 453, in forward\n    layer_outputs = decoder_layer(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 308, in forward\n    hidden_states, self_attn_weights = self.self_attn(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 265, in forward\n    attn_output, attn_weights = attention_interface("
              }
            ],
            "incomingEdges": [
              {
                "sourceNodeId": "node_Mul_600",
                "sourceNodeOutputId": "0",
                "targetNodeInputId": "0"
              },
              {
                "sourceNodeId": "node_Slice_630",
                "sourceNodeOutputId": "0",
                "targetNodeInputId": "1"
              }
            ],
            "outputsMetadata": [
              {
                "id": "0",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "add_658"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "FLOAT[s34,32,s16,s16 + s17]"
                  },
                  {
                    "key": "param_name",
                    "value": "C"
                  }
                ]
              }
            ],
            "inputsMetadata": [
              {
                "id": "0",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "mul_493"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "FLOAT[s34,32,s16,s16 + s17]"
                  },
                  {
                    "key": "param_name",
                    "value": "A"
                  }
                ]
              },
              {
                "id": "1",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "slice_49"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "FLOAT[s34,1,s16,s16 + s17]"
                  },
                  {
                    "key": "param_name",
                    "value": "B"
                  }
                ]
              }
            ],
            "style": null,
            "config": null
          },
          {
            "id": "node_Softmax_632",
            "label": "Softmax",
            "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.1: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.1.self_attn: transformers.models.llama.modeling_llama.LlamaAttention/softmax_1: aten.softmax.int",
            "subgraphIds": [],
            "attrs": [
              {
                "key": "axis",
                "value": "-1"
              },
              {
                "key": "[metadata] namespace",
                "value": ": transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.1: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.1.self_attn: transformers.models.llama.modeling_llama.LlamaAttention/softmax_1: aten.softmax.int"
              },
              {
                "key": "[metadata] pkg.torch.onnx.class_hierarchy",
                "value": "['transformers.models.llama.modeling_llama.LlamaForCausalLM', 'transformers.models.llama.modeling_llama.LlamaModel', 'transformers.models.llama.modeling_llama.LlamaDecoderLayer', 'transformers.models.llama.modeling_llama.LlamaAttention', 'aten.softmax.int']"
              },
              {
                "key": "[metadata] pkg.torch.onnx.fx_node",
                "value": "%softmax_1 : [num_users=1] = call_function[target=torch.ops.aten.softmax.int](args = (%add_658, -1, torch.float32), kwargs = {})"
              },
              {
                "key": "[metadata] pkg.torch.onnx.name_scopes",
                "value": "['', 'model', 'model.layers.1', 'model.layers.1.self_attn', 'softmax_1']"
              },
              {
                "key": "[metadata] pkg.torch.onnx.stack_trace",
                "value": "File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 688, in forward\n    outputs: BaseModelOutputWithPast = self.model(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 453, in forward\n    layer_outputs = decoder_layer(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 308, in forward\n    hidden_states, self_attn_weights = self.self_attn(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 265, in forward\n    attn_output, attn_weights = attention_interface("
              }
            ],
            "incomingEdges": [
              {
                "sourceNodeId": "node_Add_631",
                "sourceNodeOutputId": "0",
                "targetNodeInputId": "0"
              }
            ],
            "outputsMetadata": [
              {
                "id": "0",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "val_450"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "FLOAT[s34,32,s16,s16 + s17]"
                  },
                  {
                    "key": "param_name",
                    "value": "output"
                  }
                ]
              }
            ],
            "inputsMetadata": [
              {
                "id": "0",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "add_658"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "FLOAT[s34,32,s16,s16 + s17]"
                  },
                  {
                    "key": "param_name",
                    "value": "input"
                  }
                ]
              }
            ],
            "style": null,
            "config": null
          },
          {
            "id": "node_MatMul_635",
            "label": "MatMul",
            "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.1: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.1.self_attn: transformers.models.llama.modeling_llama.LlamaAttention/matmul_4: aten.matmul.default",
            "subgraphIds": [],
            "attrs": [
              {
                "key": "[metadata] namespace",
                "value": ": transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.1: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.1.self_attn: transformers.models.llama.modeling_llama.LlamaAttention/matmul_4: aten.matmul.default"
              },
              {
                "key": "[metadata] pkg.torch.onnx.class_hierarchy",
                "value": "['transformers.models.llama.modeling_llama.LlamaForCausalLM', 'transformers.models.llama.modeling_llama.LlamaModel', 'transformers.models.llama.modeling_llama.LlamaDecoderLayer', 'transformers.models.llama.modeling_llama.LlamaAttention', 'aten.matmul.default']"
              },
              {
                "key": "[metadata] pkg.torch.onnx.fx_node",
                "value": "%matmul_4 : [num_users=1] = call_function[target=torch.ops.aten.matmul.default](args = (%clone_3, %cat_8), kwargs = {})"
              },
              {
                "key": "[metadata] pkg.torch.onnx.name_scopes",
                "value": "['', 'model', 'model.layers.1', 'model.layers.1.self_attn', 'matmul_4']"
              },
              {
                "key": "[metadata] pkg.torch.onnx.stack_trace",
                "value": "File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 688, in forward\n    outputs: BaseModelOutputWithPast = self.model(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 453, in forward\n    layer_outputs = decoder_layer(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 308, in forward\n    hidden_states, self_attn_weights = self.self_attn(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 265, in forward\n    attn_output, attn_weights = attention_interface("
              }
            ],
            "incomingEdges": [
              {
                "sourceNodeId": "node_Softmax_632",
                "sourceNodeOutputId": "0",
                "targetNodeInputId": "0"
              },
              {
                "sourceNodeId": "node_Concat_597",
                "sourceNodeOutputId": "0",
                "targetNodeInputId": "1"
              }
            ],
            "outputsMetadata": [
              {
                "id": "0",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "matmul_4"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "FLOAT[s34,32,s16,64]"
                  },
                  {
                    "key": "param_name",
                    "value": "Y"
                  }
                ]
              }
            ],
            "inputsMetadata": [
              {
                "id": "0",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "val_450"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "FLOAT[s34,32,s16,s16 + s17]"
                  },
                  {
                    "key": "param_name",
                    "value": "A"
                  }
                ]
              },
              {
                "id": "1",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "cat_8"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "FLOAT[s34,32,s16 + s17,64]"
                  },
                  {
                    "key": "[metadata] pkg.torch.export.graph_signature.OutputSpec.kind",
                    "value": "USER_OUTPUT"
                  },
                  {
                    "key": "param_name",
                    "value": "B"
                  }
                ]
              }
            ],
            "style": null,
            "config": null
          },
          {
            "id": "node_Transpose_636",
            "label": "Transpose",
            "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.1: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.1.self_attn: transformers.models.llama.modeling_llama.LlamaAttention/transpose_10: aten.transpose.int",
            "subgraphIds": [],
            "attrs": [
              {
                "key": "perm",
                "value": "[0, 2, 1, 3]"
              },
              {
                "key": "[metadata] namespace",
                "value": ": transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.1: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.1.self_attn: transformers.models.llama.modeling_llama.LlamaAttention/transpose_10: aten.transpose.int"
              },
              {
                "key": "[metadata] pkg.torch.onnx.class_hierarchy",
                "value": "['transformers.models.llama.modeling_llama.LlamaForCausalLM', 'transformers.models.llama.modeling_llama.LlamaModel', 'transformers.models.llama.modeling_llama.LlamaDecoderLayer', 'transformers.models.llama.modeling_llama.LlamaAttention', 'aten.transpose.int']"
              },
              {
                "key": "[metadata] pkg.torch.onnx.fx_node",
                "value": "%transpose_10 : [num_users=1] = call_function[target=torch.ops.aten.transpose.int](args = (%matmul_4, 1, 2), kwargs = {})"
              },
              {
                "key": "[metadata] pkg.torch.onnx.name_scopes",
                "value": "['', 'model', 'model.layers.1', 'model.layers.1.self_attn', 'transpose_10']"
              },
              {
                "key": "[metadata] pkg.torch.onnx.stack_trace",
                "value": "File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 688, in forward\n    outputs: BaseModelOutputWithPast = self.model(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 453, in forward\n    layer_outputs = decoder_layer(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 308, in forward\n    hidden_states, self_attn_weights = self.self_attn(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 265, in forward\n    attn_output, attn_weights = attention_interface("
              }
            ],
            "incomingEdges": [
              {
                "sourceNodeId": "node_MatMul_635",
                "sourceNodeOutputId": "0",
                "targetNodeInputId": "0"
              }
            ],
            "outputsMetadata": [
              {
                "id": "0",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "transpose_10"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "FLOAT[s34,s16,32,64]"
                  },
                  {
                    "key": "param_name",
                    "value": "transposed"
                  }
                ]
              }
            ],
            "inputsMetadata": [
              {
                "id": "0",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "matmul_4"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "FLOAT[s34,32,s16,64]"
                  },
                  {
                    "key": "param_name",
                    "value": "data"
                  }
                ]
              }
            ],
            "style": null,
            "config": null
          },
          {
            "id": "node_Concat_642",
            "label": "Concat",
            "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.1: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.1.self_attn: transformers.models.llama.modeling_llama.LlamaAttention/view_8: aten.view.default",
            "subgraphIds": [],
            "attrs": [
              {
                "key": "axis",
                "value": "0"
              },
              {
                "key": "[metadata] namespace",
                "value": ": transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.1: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.1.self_attn: transformers.models.llama.modeling_llama.LlamaAttention/view_8: aten.view.default"
              },
              {
                "key": "[metadata] pkg.torch.onnx.class_hierarchy",
                "value": "['transformers.models.llama.modeling_llama.LlamaForCausalLM', 'transformers.models.llama.modeling_llama.LlamaModel', 'transformers.models.llama.modeling_llama.LlamaDecoderLayer', 'transformers.models.llama.modeling_llama.LlamaAttention', 'aten.view.default']"
              },
              {
                "key": "[metadata] pkg.torch.onnx.fx_node",
                "value": "%view_8 : [num_users=1] = call_function[target=torch.ops.aten.view.default](args = (%clone_4, [%sym_size_int_68, %sym_size_int_61, -1]), kwargs = {})"
              },
              {
                "key": "[metadata] pkg.torch.onnx.name_scopes",
                "value": "['', 'model', 'model.layers.1', 'model.layers.1.self_attn', 'view_8']"
              },
              {
                "key": "[metadata] pkg.torch.onnx.stack_trace",
                "value": "File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 688, in forward\n    outputs: BaseModelOutputWithPast = self.model(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 453, in forward\n    layer_outputs = decoder_layer(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 308, in forward\n    hidden_states, self_attn_weights = self.self_attn(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 276, in forward\n    attn_output = attn_output.reshape(*input_shape, -1).contiguous()"
              }
            ],
            "incomingEdges": [
              {
                "sourceNodeId": "node_Shape_12",
                "sourceNodeOutputId": "0",
                "targetNodeInputId": "0"
              },
              {
                "sourceNodeId": "node_Shape_2",
                "sourceNodeOutputId": "0",
                "targetNodeInputId": "1"
              },
              {
                "sourceNodeId": "node_Constant_90",
                "sourceNodeOutputId": "0",
                "targetNodeInputId": "2"
              }
            ],
            "outputsMetadata": [
              {
                "id": "0",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "val_455"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "INT64[3]"
                  },
                  {
                    "key": "param_name",
                    "value": "concat_result"
                  }
                ]
              }
            ],
            "inputsMetadata": [
              {
                "id": "0",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "val_6"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "INT64[1]"
                  },
                  {
                    "key": "param_name",
                    "value": "inputs"
                  }
                ]
              },
              {
                "id": "1",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "val_1"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "INT64[1]"
                  },
                  {
                    "key": "param_name",
                    "value": "inputs"
                  }
                ]
              },
              {
                "id": "2",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "val_53"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "INT64[1]"
                  },
                  {
                    "key": "param_name",
                    "value": "inputs"
                  }
                ]
              }
            ],
            "style": null,
            "config": null
          },
          {
            "id": "node_Reshape_644",
            "label": "Reshape",
            "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.1: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.1.self_attn: transformers.models.llama.modeling_llama.LlamaAttention/view_8: aten.view.default",
            "subgraphIds": [],
            "attrs": [
              {
                "key": "allowzero",
                "value": "1"
              },
              {
                "key": "[metadata] namespace",
                "value": ": transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.1: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.1.self_attn: transformers.models.llama.modeling_llama.LlamaAttention/view_8: aten.view.default"
              },
              {
                "key": "[metadata] pkg.torch.onnx.class_hierarchy",
                "value": "['transformers.models.llama.modeling_llama.LlamaForCausalLM', 'transformers.models.llama.modeling_llama.LlamaModel', 'transformers.models.llama.modeling_llama.LlamaDecoderLayer', 'transformers.models.llama.modeling_llama.LlamaAttention', 'aten.view.default']"
              },
              {
                "key": "[metadata] pkg.torch.onnx.fx_node",
                "value": "%view_8 : [num_users=1] = call_function[target=torch.ops.aten.view.default](args = (%clone_4, [%sym_size_int_68, %sym_size_int_61, -1]), kwargs = {})"
              },
              {
                "key": "[metadata] pkg.torch.onnx.name_scopes",
                "value": "['', 'model', 'model.layers.1', 'model.layers.1.self_attn', 'view_8']"
              },
              {
                "key": "[metadata] pkg.torch.onnx.stack_trace",
                "value": "File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 688, in forward\n    outputs: BaseModelOutputWithPast = self.model(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 453, in forward\n    layer_outputs = decoder_layer(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 308, in forward\n    hidden_states, self_attn_weights = self.self_attn(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 276, in forward\n    attn_output = attn_output.reshape(*input_shape, -1).contiguous()"
              }
            ],
            "incomingEdges": [
              {
                "sourceNodeId": "node_Transpose_636",
                "sourceNodeOutputId": "0",
                "targetNodeInputId": "0"
              },
              {
                "sourceNodeId": "node_Concat_642",
                "sourceNodeOutputId": "0",
                "targetNodeInputId": "1"
              }
            ],
            "outputsMetadata": [
              {
                "id": "0",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "view_8"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "FLOAT[s34,s16,2048]"
                  },
                  {
                    "key": "param_name",
                    "value": "reshaped"
                  }
                ]
              }
            ],
            "inputsMetadata": [
              {
                "id": "0",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "transpose_10"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "FLOAT[s34,s16,32,64]"
                  },
                  {
                    "key": "param_name",
                    "value": "data"
                  }
                ]
              },
              {
                "id": "1",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "val_455"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "INT64[3]"
                  },
                  {
                    "key": "param_name",
                    "value": "shape"
                  }
                ]
              }
            ],
            "style": null,
            "config": null
          },
          {
            "id": "node_Transpose_645",
            "label": "Transpose",
            "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.1: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.1.self_attn: transformers.models.llama.modeling_llama.LlamaAttention/model.layers.1.self_attn.o_proj: torch.nn.modules.linear.Linear/linear_10: aten.linear.default",
            "subgraphIds": [],
            "attrs": [
              {
                "key": "perm",
                "value": "[1, 0]"
              },
              {
                "key": "[metadata] namespace",
                "value": ": transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.1: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.1.self_attn: transformers.models.llama.modeling_llama.LlamaAttention/model.layers.1.self_attn.o_proj: torch.nn.modules.linear.Linear/linear_10: aten.linear.default"
              },
              {
                "key": "[metadata] pkg.torch.onnx.class_hierarchy",
                "value": "['transformers.models.llama.modeling_llama.LlamaForCausalLM', 'transformers.models.llama.modeling_llama.LlamaModel', 'transformers.models.llama.modeling_llama.LlamaDecoderLayer', 'transformers.models.llama.modeling_llama.LlamaAttention', 'torch.nn.modules.linear.Linear', 'aten.linear.default']"
              },
              {
                "key": "[metadata] pkg.torch.onnx.fx_node",
                "value": "%linear_10 : [num_users=1] = call_function[target=torch.ops.aten.linear.default](args = (%view_8, %p_model_layers_1_self_attn_o_proj_weight), kwargs = {})"
              },
              {
                "key": "[metadata] pkg.torch.onnx.name_scopes",
                "value": "['', 'model', 'model.layers.1', 'model.layers.1.self_attn', 'model.layers.1.self_attn.o_proj', 'linear_10']"
              },
              {
                "key": "[metadata] pkg.torch.onnx.stack_trace",
                "value": "File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 688, in forward\n    outputs: BaseModelOutputWithPast = self.model(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 453, in forward\n    layer_outputs = decoder_layer(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 308, in forward\n    hidden_states, self_attn_weights = self.self_attn(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 277, in forward\n    attn_output = self.o_proj(attn_output)\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/torch/nn/modules/linear.py\", line 125, in forward\n    return F.linear(input, self.weight, self.bias)"
              }
            ],
            "incomingEdges": [
              {
                "sourceNodeId": "[value] model.layers.1.self_attn.o_proj.weight",
                "sourceNodeOutputId": "0",
                "targetNodeInputId": "0"
              }
            ],
            "outputsMetadata": [
              {
                "id": "0",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "val_457"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "FLOAT[2048,2048]"
                  },
                  {
                    "key": "param_name",
                    "value": "transposed"
                  }
                ]
              }
            ],
            "inputsMetadata": [
              {
                "id": "0",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "model.layers.1.self_attn.o_proj.weight"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "FLOAT[2048,2048]"
                  },
                  {
                    "key": "param_name",
                    "value": "data"
                  }
                ]
              }
            ],
            "style": null,
            "config": null
          },
          {
            "id": "node_MatMul_646",
            "label": "MatMul",
            "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.1: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.1.self_attn: transformers.models.llama.modeling_llama.LlamaAttention/model.layers.1.self_attn.o_proj: torch.nn.modules.linear.Linear/linear_10: aten.linear.default",
            "subgraphIds": [],
            "attrs": [
              {
                "key": "[metadata] namespace",
                "value": ": transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.1: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.1.self_attn: transformers.models.llama.modeling_llama.LlamaAttention/model.layers.1.self_attn.o_proj: torch.nn.modules.linear.Linear/linear_10: aten.linear.default"
              },
              {
                "key": "[metadata] pkg.torch.onnx.class_hierarchy",
                "value": "['transformers.models.llama.modeling_llama.LlamaForCausalLM', 'transformers.models.llama.modeling_llama.LlamaModel', 'transformers.models.llama.modeling_llama.LlamaDecoderLayer', 'transformers.models.llama.modeling_llama.LlamaAttention', 'torch.nn.modules.linear.Linear', 'aten.linear.default']"
              },
              {
                "key": "[metadata] pkg.torch.onnx.fx_node",
                "value": "%linear_10 : [num_users=1] = call_function[target=torch.ops.aten.linear.default](args = (%view_8, %p_model_layers_1_self_attn_o_proj_weight), kwargs = {})"
              },
              {
                "key": "[metadata] pkg.torch.onnx.name_scopes",
                "value": "['', 'model', 'model.layers.1', 'model.layers.1.self_attn', 'model.layers.1.self_attn.o_proj', 'linear_10']"
              },
              {
                "key": "[metadata] pkg.torch.onnx.stack_trace",
                "value": "File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 688, in forward\n    outputs: BaseModelOutputWithPast = self.model(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 453, in forward\n    layer_outputs = decoder_layer(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 308, in forward\n    hidden_states, self_attn_weights = self.self_attn(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 277, in forward\n    attn_output = self.o_proj(attn_output)\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/torch/nn/modules/linear.py\", line 125, in forward\n    return F.linear(input, self.weight, self.bias)"
              }
            ],
            "incomingEdges": [
              {
                "sourceNodeId": "node_Reshape_644",
                "sourceNodeOutputId": "0",
                "targetNodeInputId": "0"
              },
              {
                "sourceNodeId": "node_Transpose_645",
                "sourceNodeOutputId": "0",
                "targetNodeInputId": "1"
              }
            ],
            "outputsMetadata": [
              {
                "id": "0",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "linear_10"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "FLOAT[s34,s16,2048]"
                  },
                  {
                    "key": "param_name",
                    "value": "Y"
                  }
                ]
              }
            ],
            "inputsMetadata": [
              {
                "id": "0",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "view_8"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "FLOAT[s34,s16,2048]"
                  },
                  {
                    "key": "param_name",
                    "value": "A"
                  }
                ]
              },
              {
                "id": "1",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "val_457"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "FLOAT[2048,2048]"
                  },
                  {
                    "key": "param_name",
                    "value": "B"
                  }
                ]
              }
            ],
            "style": null,
            "config": null
          },
          {
            "id": "node_Add_647",
            "label": "Add",
            "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.1: transformers.models.llama.modeling_llama.LlamaDecoderLayer/add_697: aten.add.Tensor",
            "subgraphIds": [],
            "attrs": [
              {
                "key": "[metadata] namespace",
                "value": ": transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.1: transformers.models.llama.modeling_llama.LlamaDecoderLayer/add_697: aten.add.Tensor"
              },
              {
                "key": "[metadata] pkg.torch.onnx.class_hierarchy",
                "value": "['transformers.models.llama.modeling_llama.LlamaForCausalLM', 'transformers.models.llama.modeling_llama.LlamaModel', 'transformers.models.llama.modeling_llama.LlamaDecoderLayer', 'aten.add.Tensor']"
              },
              {
                "key": "[metadata] pkg.torch.onnx.fx_node",
                "value": "%add_697 : [num_users=3] = call_function[target=torch.ops.aten.add.Tensor](args = (%add_463, %linear_10), kwargs = {})"
              },
              {
                "key": "[metadata] pkg.torch.onnx.name_scopes",
                "value": "['', 'model', 'model.layers.1', 'add_697']"
              },
              {
                "key": "[metadata] pkg.torch.onnx.stack_trace",
                "value": "File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 688, in forward\n    outputs: BaseModelOutputWithPast = self.model(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 453, in forward\n    layer_outputs = decoder_layer(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 319, in forward\n    hidden_states = residual + hidden_states"
              }
            ],
            "incomingEdges": [
              {
                "sourceNodeId": "node_Add_499",
                "sourceNodeOutputId": "0",
                "targetNodeInputId": "0"
              },
              {
                "sourceNodeId": "node_MatMul_646",
                "sourceNodeOutputId": "0",
                "targetNodeInputId": "1"
              }
            ],
            "outputsMetadata": [
              {
                "id": "0",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "add_697"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "FLOAT[s34,s16,2048]"
                  },
                  {
                    "key": "param_name",
                    "value": "C"
                  }
                ]
              }
            ],
            "inputsMetadata": [
              {
                "id": "0",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "add_463"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "FLOAT[s34,s16,2048]"
                  },
                  {
                    "key": "param_name",
                    "value": "A"
                  }
                ]
              },
              {
                "id": "1",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "linear_10"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "FLOAT[s34,s16,2048]"
                  },
                  {
                    "key": "param_name",
                    "value": "B"
                  }
                ]
              }
            ],
            "style": null,
            "config": null
          },
          {
            "id": "node_Constant_648",
            "label": "Constant",
            "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.1: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.1.post_attention_layernorm: transformers.models.llama.modeling_llama.LlamaRMSNorm/pow_4: aten.pow.Tensor_Scalar",
            "subgraphIds": [],
            "attrs": [
              {
                "key": "__value",
                "value": "2.0"
              },
              {
                "key": "value",
                "value": "TensorProtoTensor<FLOAT,[]>(array(2., dtype=float32), name='')"
              },
              {
                "key": "[metadata] namespace",
                "value": ": transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.1: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.1.post_attention_layernorm: transformers.models.llama.modeling_llama.LlamaRMSNorm/pow_4: aten.pow.Tensor_Scalar"
              },
              {
                "key": "[metadata] pkg.torch.onnx.class_hierarchy",
                "value": "['transformers.models.llama.modeling_llama.LlamaForCausalLM', 'transformers.models.llama.modeling_llama.LlamaModel', 'transformers.models.llama.modeling_llama.LlamaDecoderLayer', 'transformers.models.llama.modeling_llama.LlamaRMSNorm', 'aten.pow.Tensor_Scalar']"
              },
              {
                "key": "[metadata] pkg.torch.onnx.fx_node",
                "value": "%pow_4 : [num_users=1] = call_function[target=torch.ops.aten.pow.Tensor_Scalar](args = (%add_697, 2), kwargs = {})"
              },
              {
                "key": "[metadata] pkg.torch.onnx.name_scopes",
                "value": "['', 'model', 'model.layers.1', 'model.layers.1.post_attention_layernorm', 'pow_4']"
              },
              {
                "key": "[metadata] pkg.torch.onnx.stack_trace",
                "value": "File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 688, in forward\n    outputs: BaseModelOutputWithPast = self.model(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 453, in forward\n    layer_outputs = decoder_layer(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 323, in forward\n    hidden_states = self.post_attention_layernorm(hidden_states)\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 71, in forward\n    variance = hidden_states.pow(2).mean(-1, keepdim=True)"
              }
            ],
            "incomingEdges": [],
            "outputsMetadata": [
              {
                "id": "0",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "val_458"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "FLOAT[]"
                  },
                  {
                    "key": "param_name",
                    "value": "output"
                  }
                ]
              }
            ],
            "inputsMetadata": [],
            "style": null,
            "config": null
          },
          {
            "id": "node_Pow_649",
            "label": "Pow",
            "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.1: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.1.post_attention_layernorm: transformers.models.llama.modeling_llama.LlamaRMSNorm/pow_4: aten.pow.Tensor_Scalar",
            "subgraphIds": [],
            "attrs": [
              {
                "key": "[metadata] namespace",
                "value": ": transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.1: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.1.post_attention_layernorm: transformers.models.llama.modeling_llama.LlamaRMSNorm/pow_4: aten.pow.Tensor_Scalar"
              },
              {
                "key": "[metadata] pkg.torch.onnx.class_hierarchy",
                "value": "['transformers.models.llama.modeling_llama.LlamaForCausalLM', 'transformers.models.llama.modeling_llama.LlamaModel', 'transformers.models.llama.modeling_llama.LlamaDecoderLayer', 'transformers.models.llama.modeling_llama.LlamaRMSNorm', 'aten.pow.Tensor_Scalar']"
              },
              {
                "key": "[metadata] pkg.torch.onnx.fx_node",
                "value": "%pow_4 : [num_users=1] = call_function[target=torch.ops.aten.pow.Tensor_Scalar](args = (%add_697, 2), kwargs = {})"
              },
              {
                "key": "[metadata] pkg.torch.onnx.name_scopes",
                "value": "['', 'model', 'model.layers.1', 'model.layers.1.post_attention_layernorm', 'pow_4']"
              },
              {
                "key": "[metadata] pkg.torch.onnx.stack_trace",
                "value": "File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 688, in forward\n    outputs: BaseModelOutputWithPast = self.model(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 453, in forward\n    layer_outputs = decoder_layer(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 323, in forward\n    hidden_states = self.post_attention_layernorm(hidden_states)\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 71, in forward\n    variance = hidden_states.pow(2).mean(-1, keepdim=True)"
              }
            ],
            "incomingEdges": [
              {
                "sourceNodeId": "node_Add_647",
                "sourceNodeOutputId": "0",
                "targetNodeInputId": "0"
              },
              {
                "sourceNodeId": "node_Constant_648",
                "sourceNodeOutputId": "0",
                "targetNodeInputId": "1"
              }
            ],
            "outputsMetadata": [
              {
                "id": "0",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "pow_4"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "FLOAT[s34,s16,2048]"
                  },
                  {
                    "key": "param_name",
                    "value": "Z"
                  }
                ]
              }
            ],
            "inputsMetadata": [
              {
                "id": "0",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "add_697"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "FLOAT[s34,s16,2048]"
                  },
                  {
                    "key": "param_name",
                    "value": "X"
                  }
                ]
              },
              {
                "id": "1",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "val_458"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "FLOAT[]"
                  },
                  {
                    "key": "param_name",
                    "value": "Y"
                  }
                ]
              }
            ],
            "style": null,
            "config": null
          },
          {
            "id": "node_Constant_980",
            "label": "Constant",
            "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.1: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.1.post_attention_layernorm: transformers.models.llama.modeling_llama.LlamaRMSNorm/mean_3: aten.mean.dim",
            "subgraphIds": [],
            "attrs": [
              {
                "key": "__value",
                "value": "[-1]"
              },
              {
                "key": "value",
                "value": "TensorProtoTensor<INT64,[1]>(array([-1]), name='val_460')"
              }
            ],
            "incomingEdges": [],
            "outputsMetadata": [
              {
                "id": "0",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "val_460"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "INT64[1]"
                  },
                  {
                    "key": "param_name",
                    "value": "output"
                  }
                ]
              }
            ],
            "inputsMetadata": [],
            "style": null,
            "config": null
          },
          {
            "id": "node_ReduceMean_652",
            "label": "ReduceMean",
            "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.1: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.1.post_attention_layernorm: transformers.models.llama.modeling_llama.LlamaRMSNorm/mean_3: aten.mean.dim",
            "subgraphIds": [],
            "attrs": [
              {
                "key": "noop_with_empty_axes",
                "value": "0"
              },
              {
                "key": "keepdims",
                "value": "1"
              },
              {
                "key": "[metadata] namespace",
                "value": ": transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.1: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.1.post_attention_layernorm: transformers.models.llama.modeling_llama.LlamaRMSNorm/mean_3: aten.mean.dim"
              },
              {
                "key": "[metadata] pkg.torch.onnx.class_hierarchy",
                "value": "['transformers.models.llama.modeling_llama.LlamaForCausalLM', 'transformers.models.llama.modeling_llama.LlamaModel', 'transformers.models.llama.modeling_llama.LlamaDecoderLayer', 'transformers.models.llama.modeling_llama.LlamaRMSNorm', 'aten.mean.dim']"
              },
              {
                "key": "[metadata] pkg.torch.onnx.fx_node",
                "value": "%mean_3 : [num_users=1] = call_function[target=torch.ops.aten.mean.dim](args = (%pow_4, [-1], True), kwargs = {})"
              },
              {
                "key": "[metadata] pkg.torch.onnx.name_scopes",
                "value": "['', 'model', 'model.layers.1', 'model.layers.1.post_attention_layernorm', 'mean_3']"
              },
              {
                "key": "[metadata] pkg.torch.onnx.stack_trace",
                "value": "File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 688, in forward\n    outputs: BaseModelOutputWithPast = self.model(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 453, in forward\n    layer_outputs = decoder_layer(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 323, in forward\n    hidden_states = self.post_attention_layernorm(hidden_states)\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 71, in forward\n    variance = hidden_states.pow(2).mean(-1, keepdim=True)"
              }
            ],
            "incomingEdges": [
              {
                "sourceNodeId": "node_Pow_649",
                "sourceNodeOutputId": "0",
                "targetNodeInputId": "0"
              },
              {
                "sourceNodeId": "node_Constant_980",
                "sourceNodeOutputId": "0",
                "targetNodeInputId": "1"
              }
            ],
            "outputsMetadata": [
              {
                "id": "0",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "mean_3"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "FLOAT[s34,s16,1]"
                  },
                  {
                    "key": "param_name",
                    "value": "reduced"
                  }
                ]
              }
            ],
            "inputsMetadata": [
              {
                "id": "0",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "pow_4"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "FLOAT[s34,s16,2048]"
                  },
                  {
                    "key": "param_name",
                    "value": "data"
                  }
                ]
              },
              {
                "id": "1",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "val_460"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "INT64[1]"
                  },
                  {
                    "key": "param_name",
                    "value": "axes"
                  }
                ]
              }
            ],
            "style": null,
            "config": null
          },
          {
            "id": "node_Add_653",
            "label": "Add",
            "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.1: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.1.post_attention_layernorm: transformers.models.llama.modeling_llama.LlamaRMSNorm/add_710: aten.add.Tensor",
            "subgraphIds": [],
            "attrs": [
              {
                "key": "[metadata] namespace",
                "value": ": transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.1: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.1.post_attention_layernorm: transformers.models.llama.modeling_llama.LlamaRMSNorm/add_710: aten.add.Tensor"
              },
              {
                "key": "[metadata] pkg.torch.onnx.class_hierarchy",
                "value": "['transformers.models.llama.modeling_llama.LlamaForCausalLM', 'transformers.models.llama.modeling_llama.LlamaModel', 'transformers.models.llama.modeling_llama.LlamaDecoderLayer', 'transformers.models.llama.modeling_llama.LlamaRMSNorm', 'aten.add.Tensor']"
              },
              {
                "key": "[metadata] pkg.torch.onnx.fx_node",
                "value": "%add_710 : [num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%mean_3, 1e-05), kwargs = {})"
              },
              {
                "key": "[metadata] pkg.torch.onnx.name_scopes",
                "value": "['', 'model', 'model.layers.1', 'model.layers.1.post_attention_layernorm', 'add_710']"
              },
              {
                "key": "[metadata] pkg.torch.onnx.stack_trace",
                "value": "File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 688, in forward\n    outputs: BaseModelOutputWithPast = self.model(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 453, in forward\n    layer_outputs = decoder_layer(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 323, in forward\n    hidden_states = self.post_attention_layernorm(hidden_states)\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 72, in forward\n    hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)"
              }
            ],
            "incomingEdges": [
              {
                "sourceNodeId": "node_ReduceMean_652",
                "sourceNodeOutputId": "0",
                "targetNodeInputId": "0"
              },
              {
                "sourceNodeId": "node_Constant_333",
                "sourceNodeOutputId": "0",
                "targetNodeInputId": "1"
              }
            ],
            "outputsMetadata": [
              {
                "id": "0",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "add_710"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "FLOAT[s34,s16,1]"
                  },
                  {
                    "key": "param_name",
                    "value": "C"
                  }
                ]
              }
            ],
            "inputsMetadata": [
              {
                "id": "0",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "mean_3"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "FLOAT[s34,s16,1]"
                  },
                  {
                    "key": "param_name",
                    "value": "A"
                  }
                ]
              },
              {
                "id": "1",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "val_251"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "FLOAT[]"
                  },
                  {
                    "key": "param_name",
                    "value": "B"
                  }
                ]
              }
            ],
            "style": null,
            "config": null
          },
          {
            "id": "node_Sqrt_654",
            "label": "Sqrt",
            "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.1: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.1.post_attention_layernorm: transformers.models.llama.modeling_llama.LlamaRMSNorm/rsqrt_3: aten.rsqrt.default",
            "subgraphIds": [],
            "attrs": [
              {
                "key": "[metadata] namespace",
                "value": ": transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.1: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.1.post_attention_layernorm: transformers.models.llama.modeling_llama.LlamaRMSNorm/rsqrt_3: aten.rsqrt.default"
              },
              {
                "key": "[metadata] pkg.torch.onnx.class_hierarchy",
                "value": "['transformers.models.llama.modeling_llama.LlamaForCausalLM', 'transformers.models.llama.modeling_llama.LlamaModel', 'transformers.models.llama.modeling_llama.LlamaDecoderLayer', 'transformers.models.llama.modeling_llama.LlamaRMSNorm', 'aten.rsqrt.default']"
              },
              {
                "key": "[metadata] pkg.torch.onnx.fx_node",
                "value": "%rsqrt_3 : [num_users=1] = call_function[target=torch.ops.aten.rsqrt.default](args = (%add_710,), kwargs = {})"
              },
              {
                "key": "[metadata] pkg.torch.onnx.name_scopes",
                "value": "['', 'model', 'model.layers.1', 'model.layers.1.post_attention_layernorm', 'rsqrt_3']"
              },
              {
                "key": "[metadata] pkg.torch.onnx.stack_trace",
                "value": "File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 688, in forward\n    outputs: BaseModelOutputWithPast = self.model(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 453, in forward\n    layer_outputs = decoder_layer(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 323, in forward\n    hidden_states = self.post_attention_layernorm(hidden_states)\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 72, in forward\n    hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)"
              }
            ],
            "incomingEdges": [
              {
                "sourceNodeId": "node_Add_653",
                "sourceNodeOutputId": "0",
                "targetNodeInputId": "0"
              }
            ],
            "outputsMetadata": [
              {
                "id": "0",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "val_461"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "FLOAT[s34,s16,1]"
                  },
                  {
                    "key": "param_name",
                    "value": "Y"
                  }
                ]
              }
            ],
            "inputsMetadata": [
              {
                "id": "0",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "add_710"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "FLOAT[s34,s16,1]"
                  },
                  {
                    "key": "param_name",
                    "value": "X"
                  }
                ]
              }
            ],
            "style": null,
            "config": null
          },
          {
            "id": "node_Reciprocal_655",
            "label": "Reciprocal",
            "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.1: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.1.post_attention_layernorm: transformers.models.llama.modeling_llama.LlamaRMSNorm/rsqrt_3: aten.rsqrt.default",
            "subgraphIds": [],
            "attrs": [
              {
                "key": "[metadata] namespace",
                "value": ": transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.1: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.1.post_attention_layernorm: transformers.models.llama.modeling_llama.LlamaRMSNorm/rsqrt_3: aten.rsqrt.default"
              },
              {
                "key": "[metadata] pkg.torch.onnx.class_hierarchy",
                "value": "['transformers.models.llama.modeling_llama.LlamaForCausalLM', 'transformers.models.llama.modeling_llama.LlamaModel', 'transformers.models.llama.modeling_llama.LlamaDecoderLayer', 'transformers.models.llama.modeling_llama.LlamaRMSNorm', 'aten.rsqrt.default']"
              },
              {
                "key": "[metadata] pkg.torch.onnx.fx_node",
                "value": "%rsqrt_3 : [num_users=1] = call_function[target=torch.ops.aten.rsqrt.default](args = (%add_710,), kwargs = {})"
              },
              {
                "key": "[metadata] pkg.torch.onnx.name_scopes",
                "value": "['', 'model', 'model.layers.1', 'model.layers.1.post_attention_layernorm', 'rsqrt_3']"
              },
              {
                "key": "[metadata] pkg.torch.onnx.stack_trace",
                "value": "File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 688, in forward\n    outputs: BaseModelOutputWithPast = self.model(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 453, in forward\n    layer_outputs = decoder_layer(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 323, in forward\n    hidden_states = self.post_attention_layernorm(hidden_states)\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 72, in forward\n    hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)"
              }
            ],
            "incomingEdges": [
              {
                "sourceNodeId": "node_Sqrt_654",
                "sourceNodeOutputId": "0",
                "targetNodeInputId": "0"
              }
            ],
            "outputsMetadata": [
              {
                "id": "0",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "rsqrt_3"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "FLOAT[s34,s16,1]"
                  },
                  {
                    "key": "param_name",
                    "value": "Y"
                  }
                ]
              }
            ],
            "inputsMetadata": [
              {
                "id": "0",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "val_461"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "FLOAT[s34,s16,1]"
                  },
                  {
                    "key": "param_name",
                    "value": "X"
                  }
                ]
              }
            ],
            "style": null,
            "config": null
          },
          {
            "id": "node_Mul_656",
            "label": "Mul",
            "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.1: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.1.post_attention_layernorm: transformers.models.llama.modeling_llama.LlamaRMSNorm/mul_563: aten.mul.Tensor",
            "subgraphIds": [],
            "attrs": [
              {
                "key": "[metadata] namespace",
                "value": ": transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.1: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.1.post_attention_layernorm: transformers.models.llama.modeling_llama.LlamaRMSNorm/mul_563: aten.mul.Tensor"
              },
              {
                "key": "[metadata] pkg.torch.onnx.class_hierarchy",
                "value": "['transformers.models.llama.modeling_llama.LlamaForCausalLM', 'transformers.models.llama.modeling_llama.LlamaModel', 'transformers.models.llama.modeling_llama.LlamaDecoderLayer', 'transformers.models.llama.modeling_llama.LlamaRMSNorm', 'aten.mul.Tensor']"
              },
              {
                "key": "[metadata] pkg.torch.onnx.fx_node",
                "value": "%mul_563 : [num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%add_697, %rsqrt_3), kwargs = {})"
              },
              {
                "key": "[metadata] pkg.torch.onnx.name_scopes",
                "value": "['', 'model', 'model.layers.1', 'model.layers.1.post_attention_layernorm', 'mul_563']"
              },
              {
                "key": "[metadata] pkg.torch.onnx.stack_trace",
                "value": "File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 688, in forward\n    outputs: BaseModelOutputWithPast = self.model(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 453, in forward\n    layer_outputs = decoder_layer(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 323, in forward\n    hidden_states = self.post_attention_layernorm(hidden_states)\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 72, in forward\n    hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)"
              }
            ],
            "incomingEdges": [
              {
                "sourceNodeId": "node_Add_647",
                "sourceNodeOutputId": "0",
                "targetNodeInputId": "0"
              },
              {
                "sourceNodeId": "node_Reciprocal_655",
                "sourceNodeOutputId": "0",
                "targetNodeInputId": "1"
              }
            ],
            "outputsMetadata": [
              {
                "id": "0",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "mul_563"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "FLOAT[s34,s16,2048]"
                  },
                  {
                    "key": "param_name",
                    "value": "C"
                  }
                ]
              }
            ],
            "inputsMetadata": [
              {
                "id": "0",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "add_697"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "FLOAT[s34,s16,2048]"
                  },
                  {
                    "key": "param_name",
                    "value": "A"
                  }
                ]
              },
              {
                "id": "1",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "rsqrt_3"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "FLOAT[s34,s16,1]"
                  },
                  {
                    "key": "param_name",
                    "value": "B"
                  }
                ]
              }
            ],
            "style": null,
            "config": null
          },
          {
            "id": "node_Mul_657",
            "label": "Mul",
            "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.1: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.1.post_attention_layernorm: transformers.models.llama.modeling_llama.LlamaRMSNorm/mul_567: aten.mul.Tensor",
            "subgraphIds": [],
            "attrs": [
              {
                "key": "[metadata] namespace",
                "value": ": transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.1: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.1.post_attention_layernorm: transformers.models.llama.modeling_llama.LlamaRMSNorm/mul_567: aten.mul.Tensor"
              },
              {
                "key": "[metadata] pkg.torch.onnx.class_hierarchy",
                "value": "['transformers.models.llama.modeling_llama.LlamaForCausalLM', 'transformers.models.llama.modeling_llama.LlamaModel', 'transformers.models.llama.modeling_llama.LlamaDecoderLayer', 'transformers.models.llama.modeling_llama.LlamaRMSNorm', 'aten.mul.Tensor']"
              },
              {
                "key": "[metadata] pkg.torch.onnx.fx_node",
                "value": "%mul_567 : [num_users=2] = call_function[target=torch.ops.aten.mul.Tensor](args = (%p_model_layers_1_post_attention_layernorm_weight, %mul_563), kwargs = {})"
              },
              {
                "key": "[metadata] pkg.torch.onnx.name_scopes",
                "value": "['', 'model', 'model.layers.1', 'model.layers.1.post_attention_layernorm', 'mul_567']"
              },
              {
                "key": "[metadata] pkg.torch.onnx.stack_trace",
                "value": "File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 688, in forward\n    outputs: BaseModelOutputWithPast = self.model(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 453, in forward\n    layer_outputs = decoder_layer(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 323, in forward\n    hidden_states = self.post_attention_layernorm(hidden_states)\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 73, in forward\n    return self.weight * hidden_states.to(input_dtype)"
              }
            ],
            "incomingEdges": [
              {
                "sourceNodeId": "[value] model.layers.1.post_attention_layernorm.weight",
                "sourceNodeOutputId": "0",
                "targetNodeInputId": "0"
              },
              {
                "sourceNodeId": "node_Mul_656",
                "sourceNodeOutputId": "0",
                "targetNodeInputId": "1"
              }
            ],
            "outputsMetadata": [
              {
                "id": "0",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "mul_567"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "FLOAT[s34,s16,2048]"
                  },
                  {
                    "key": "param_name",
                    "value": "C"
                  }
                ]
              }
            ],
            "inputsMetadata": [
              {
                "id": "0",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "model.layers.1.post_attention_layernorm.weight"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "FLOAT[2048]"
                  },
                  {
                    "key": "param_name",
                    "value": "A"
                  }
                ]
              },
              {
                "id": "1",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "mul_563"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "FLOAT[s34,s16,2048]"
                  },
                  {
                    "key": "param_name",
                    "value": "B"
                  }
                ]
              }
            ],
            "style": null,
            "config": null
          },
          {
            "id": "node_Transpose_658",
            "label": "Transpose",
            "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.1: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.1.mlp: transformers.models.llama.modeling_llama.LlamaMLP/model.layers.1.mlp.gate_proj: torch.nn.modules.linear.Linear/linear_11: aten.linear.default",
            "subgraphIds": [],
            "attrs": [
              {
                "key": "perm",
                "value": "[1, 0]"
              },
              {
                "key": "[metadata] namespace",
                "value": ": transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.1: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.1.mlp: transformers.models.llama.modeling_llama.LlamaMLP/model.layers.1.mlp.gate_proj: torch.nn.modules.linear.Linear/linear_11: aten.linear.default"
              },
              {
                "key": "[metadata] pkg.torch.onnx.class_hierarchy",
                "value": "['transformers.models.llama.modeling_llama.LlamaForCausalLM', 'transformers.models.llama.modeling_llama.LlamaModel', 'transformers.models.llama.modeling_llama.LlamaDecoderLayer', 'transformers.models.llama.modeling_llama.LlamaMLP', 'torch.nn.modules.linear.Linear', 'aten.linear.default']"
              },
              {
                "key": "[metadata] pkg.torch.onnx.fx_node",
                "value": "%linear_11 : [num_users=1] = call_function[target=torch.ops.aten.linear.default](args = (%mul_567, %p_model_layers_1_mlp_gate_proj_weight), kwargs = {})"
              },
              {
                "key": "[metadata] pkg.torch.onnx.name_scopes",
                "value": "['', 'model', 'model.layers.1', 'model.layers.1.mlp', 'model.layers.1.mlp.gate_proj', 'linear_11']"
              },
              {
                "key": "[metadata] pkg.torch.onnx.stack_trace",
                "value": "File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 688, in forward\n    outputs: BaseModelOutputWithPast = self.model(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 453, in forward\n    layer_outputs = decoder_layer(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 324, in forward\n    hidden_states = self.mlp(hidden_states)\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 162, in forward\n    down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/torch/nn/modules/linear.py\", line 125, in forward\n    return F.linear(input, self.weight, self.bias)"
              }
            ],
            "incomingEdges": [
              {
                "sourceNodeId": "[value] model.layers.1.mlp.gate_proj.weight",
                "sourceNodeOutputId": "0",
                "targetNodeInputId": "0"
              }
            ],
            "outputsMetadata": [
              {
                "id": "0",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "val_462"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "FLOAT[2048,8192]"
                  },
                  {
                    "key": "param_name",
                    "value": "transposed"
                  }
                ]
              }
            ],
            "inputsMetadata": [
              {
                "id": "0",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "model.layers.1.mlp.gate_proj.weight"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "FLOAT[8192,2048]"
                  },
                  {
                    "key": "param_name",
                    "value": "data"
                  }
                ]
              }
            ],
            "style": null,
            "config": null
          },
          {
            "id": "node_MatMul_659",
            "label": "MatMul",
            "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.1: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.1.mlp: transformers.models.llama.modeling_llama.LlamaMLP/model.layers.1.mlp.gate_proj: torch.nn.modules.linear.Linear/linear_11: aten.linear.default",
            "subgraphIds": [],
            "attrs": [
              {
                "key": "[metadata] namespace",
                "value": ": transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.1: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.1.mlp: transformers.models.llama.modeling_llama.LlamaMLP/model.layers.1.mlp.gate_proj: torch.nn.modules.linear.Linear/linear_11: aten.linear.default"
              },
              {
                "key": "[metadata] pkg.torch.onnx.class_hierarchy",
                "value": "['transformers.models.llama.modeling_llama.LlamaForCausalLM', 'transformers.models.llama.modeling_llama.LlamaModel', 'transformers.models.llama.modeling_llama.LlamaDecoderLayer', 'transformers.models.llama.modeling_llama.LlamaMLP', 'torch.nn.modules.linear.Linear', 'aten.linear.default']"
              },
              {
                "key": "[metadata] pkg.torch.onnx.fx_node",
                "value": "%linear_11 : [num_users=1] = call_function[target=torch.ops.aten.linear.default](args = (%mul_567, %p_model_layers_1_mlp_gate_proj_weight), kwargs = {})"
              },
              {
                "key": "[metadata] pkg.torch.onnx.name_scopes",
                "value": "['', 'model', 'model.layers.1', 'model.layers.1.mlp', 'model.layers.1.mlp.gate_proj', 'linear_11']"
              },
              {
                "key": "[metadata] pkg.torch.onnx.stack_trace",
                "value": "File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 688, in forward\n    outputs: BaseModelOutputWithPast = self.model(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 453, in forward\n    layer_outputs = decoder_layer(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 324, in forward\n    hidden_states = self.mlp(hidden_states)\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 162, in forward\n    down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/torch/nn/modules/linear.py\", line 125, in forward\n    return F.linear(input, self.weight, self.bias)"
              }
            ],
            "incomingEdges": [
              {
                "sourceNodeId": "node_Mul_657",
                "sourceNodeOutputId": "0",
                "targetNodeInputId": "0"
              },
              {
                "sourceNodeId": "node_Transpose_658",
                "sourceNodeOutputId": "0",
                "targetNodeInputId": "1"
              }
            ],
            "outputsMetadata": [
              {
                "id": "0",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "linear_11"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "FLOAT[s34,s16,8192]"
                  },
                  {
                    "key": "param_name",
                    "value": "Y"
                  }
                ]
              }
            ],
            "inputsMetadata": [
              {
                "id": "0",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "mul_567"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "FLOAT[s34,s16,2048]"
                  },
                  {
                    "key": "param_name",
                    "value": "A"
                  }
                ]
              },
              {
                "id": "1",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "val_462"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "FLOAT[2048,8192]"
                  },
                  {
                    "key": "param_name",
                    "value": "B"
                  }
                ]
              }
            ],
            "style": null,
            "config": null
          },
          {
            "id": "node_Sigmoid_660",
            "label": "Sigmoid",
            "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.1: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.1.mlp: transformers.models.llama.modeling_llama.LlamaMLP/model.layers.1.mlp.act_fn: torch.nn.modules.activation.SiLU/silu_1: aten.silu.default",
            "subgraphIds": [],
            "attrs": [
              {
                "key": "[metadata] namespace",
                "value": ": transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.1: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.1.mlp: transformers.models.llama.modeling_llama.LlamaMLP/model.layers.1.mlp.act_fn: torch.nn.modules.activation.SiLU/silu_1: aten.silu.default"
              },
              {
                "key": "[metadata] pkg.torch.onnx.class_hierarchy",
                "value": "['transformers.models.llama.modeling_llama.LlamaForCausalLM', 'transformers.models.llama.modeling_llama.LlamaModel', 'transformers.models.llama.modeling_llama.LlamaDecoderLayer', 'transformers.models.llama.modeling_llama.LlamaMLP', 'torch.nn.modules.activation.SiLU', 'aten.silu.default']"
              },
              {
                "key": "[metadata] pkg.torch.onnx.fx_node",
                "value": "%silu_1 : [num_users=1] = call_function[target=torch.ops.aten.silu.default](args = (%linear_11,), kwargs = {})"
              },
              {
                "key": "[metadata] pkg.torch.onnx.name_scopes",
                "value": "['', 'model', 'model.layers.1', 'model.layers.1.mlp', 'model.layers.1.mlp.act_fn', 'silu_1']"
              },
              {
                "key": "[metadata] pkg.torch.onnx.stack_trace",
                "value": "File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 688, in forward\n    outputs: BaseModelOutputWithPast = self.model(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 453, in forward\n    layer_outputs = decoder_layer(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 324, in forward\n    hidden_states = self.mlp(hidden_states)\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 162, in forward\n    down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/torch/nn/modules/activation.py\", line 434, in forward\n    return F.silu(input, inplace=self.inplace)"
              }
            ],
            "incomingEdges": [
              {
                "sourceNodeId": "node_MatMul_659",
                "sourceNodeOutputId": "0",
                "targetNodeInputId": "0"
              }
            ],
            "outputsMetadata": [
              {
                "id": "0",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "val_463"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "FLOAT[s34,s16,8192]"
                  },
                  {
                    "key": "param_name",
                    "value": "Y"
                  }
                ]
              }
            ],
            "inputsMetadata": [
              {
                "id": "0",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "linear_11"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "FLOAT[s34,s16,8192]"
                  },
                  {
                    "key": "param_name",
                    "value": "X"
                  }
                ]
              }
            ],
            "style": null,
            "config": null
          },
          {
            "id": "node_Mul_661",
            "label": "Mul",
            "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.1: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.1.mlp: transformers.models.llama.modeling_llama.LlamaMLP/model.layers.1.mlp.act_fn: torch.nn.modules.activation.SiLU/silu_1: aten.silu.default",
            "subgraphIds": [],
            "attrs": [
              {
                "key": "[metadata] namespace",
                "value": ": transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.1: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.1.mlp: transformers.models.llama.modeling_llama.LlamaMLP/model.layers.1.mlp.act_fn: torch.nn.modules.activation.SiLU/silu_1: aten.silu.default"
              },
              {
                "key": "[metadata] pkg.torch.onnx.class_hierarchy",
                "value": "['transformers.models.llama.modeling_llama.LlamaForCausalLM', 'transformers.models.llama.modeling_llama.LlamaModel', 'transformers.models.llama.modeling_llama.LlamaDecoderLayer', 'transformers.models.llama.modeling_llama.LlamaMLP', 'torch.nn.modules.activation.SiLU', 'aten.silu.default']"
              },
              {
                "key": "[metadata] pkg.torch.onnx.fx_node",
                "value": "%silu_1 : [num_users=1] = call_function[target=torch.ops.aten.silu.default](args = (%linear_11,), kwargs = {})"
              },
              {
                "key": "[metadata] pkg.torch.onnx.name_scopes",
                "value": "['', 'model', 'model.layers.1', 'model.layers.1.mlp', 'model.layers.1.mlp.act_fn', 'silu_1']"
              },
              {
                "key": "[metadata] pkg.torch.onnx.stack_trace",
                "value": "File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 688, in forward\n    outputs: BaseModelOutputWithPast = self.model(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 453, in forward\n    layer_outputs = decoder_layer(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 324, in forward\n    hidden_states = self.mlp(hidden_states)\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 162, in forward\n    down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/torch/nn/modules/activation.py\", line 434, in forward\n    return F.silu(input, inplace=self.inplace)"
              }
            ],
            "incomingEdges": [
              {
                "sourceNodeId": "node_MatMul_659",
                "sourceNodeOutputId": "0",
                "targetNodeInputId": "0"
              },
              {
                "sourceNodeId": "node_Sigmoid_660",
                "sourceNodeOutputId": "0",
                "targetNodeInputId": "1"
              }
            ],
            "outputsMetadata": [
              {
                "id": "0",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "silu_1"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "FLOAT[s34,s16,8192]"
                  },
                  {
                    "key": "param_name",
                    "value": "C"
                  }
                ]
              }
            ],
            "inputsMetadata": [
              {
                "id": "0",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "linear_11"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "FLOAT[s34,s16,8192]"
                  },
                  {
                    "key": "param_name",
                    "value": "A"
                  }
                ]
              },
              {
                "id": "1",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "val_463"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "FLOAT[s34,s16,8192]"
                  },
                  {
                    "key": "param_name",
                    "value": "B"
                  }
                ]
              }
            ],
            "style": null,
            "config": null
          },
          {
            "id": "node_Transpose_662",
            "label": "Transpose",
            "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.1: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.1.mlp: transformers.models.llama.modeling_llama.LlamaMLP/model.layers.1.mlp.up_proj: torch.nn.modules.linear.Linear/linear_12: aten.linear.default",
            "subgraphIds": [],
            "attrs": [
              {
                "key": "perm",
                "value": "[1, 0]"
              },
              {
                "key": "[metadata] namespace",
                "value": ": transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.1: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.1.mlp: transformers.models.llama.modeling_llama.LlamaMLP/model.layers.1.mlp.up_proj: torch.nn.modules.linear.Linear/linear_12: aten.linear.default"
              },
              {
                "key": "[metadata] pkg.torch.onnx.class_hierarchy",
                "value": "['transformers.models.llama.modeling_llama.LlamaForCausalLM', 'transformers.models.llama.modeling_llama.LlamaModel', 'transformers.models.llama.modeling_llama.LlamaDecoderLayer', 'transformers.models.llama.modeling_llama.LlamaMLP', 'torch.nn.modules.linear.Linear', 'aten.linear.default']"
              },
              {
                "key": "[metadata] pkg.torch.onnx.fx_node",
                "value": "%linear_12 : [num_users=1] = call_function[target=torch.ops.aten.linear.default](args = (%mul_567, %p_model_layers_1_mlp_up_proj_weight), kwargs = {})"
              },
              {
                "key": "[metadata] pkg.torch.onnx.name_scopes",
                "value": "['', 'model', 'model.layers.1', 'model.layers.1.mlp', 'model.layers.1.mlp.up_proj', 'linear_12']"
              },
              {
                "key": "[metadata] pkg.torch.onnx.stack_trace",
                "value": "File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 688, in forward\n    outputs: BaseModelOutputWithPast = self.model(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 453, in forward\n    layer_outputs = decoder_layer(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 324, in forward\n    hidden_states = self.mlp(hidden_states)\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 162, in forward\n    down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/torch/nn/modules/linear.py\", line 125, in forward\n    return F.linear(input, self.weight, self.bias)"
              }
            ],
            "incomingEdges": [
              {
                "sourceNodeId": "[value] model.layers.1.mlp.up_proj.weight",
                "sourceNodeOutputId": "0",
                "targetNodeInputId": "0"
              }
            ],
            "outputsMetadata": [
              {
                "id": "0",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "val_464"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "FLOAT[2048,8192]"
                  },
                  {
                    "key": "param_name",
                    "value": "transposed"
                  }
                ]
              }
            ],
            "inputsMetadata": [
              {
                "id": "0",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "model.layers.1.mlp.up_proj.weight"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "FLOAT[8192,2048]"
                  },
                  {
                    "key": "param_name",
                    "value": "data"
                  }
                ]
              }
            ],
            "style": null,
            "config": null
          },
          {
            "id": "node_MatMul_663",
            "label": "MatMul",
            "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.1: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.1.mlp: transformers.models.llama.modeling_llama.LlamaMLP/model.layers.1.mlp.up_proj: torch.nn.modules.linear.Linear/linear_12: aten.linear.default",
            "subgraphIds": [],
            "attrs": [
              {
                "key": "[metadata] namespace",
                "value": ": transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.1: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.1.mlp: transformers.models.llama.modeling_llama.LlamaMLP/model.layers.1.mlp.up_proj: torch.nn.modules.linear.Linear/linear_12: aten.linear.default"
              },
              {
                "key": "[metadata] pkg.torch.onnx.class_hierarchy",
                "value": "['transformers.models.llama.modeling_llama.LlamaForCausalLM', 'transformers.models.llama.modeling_llama.LlamaModel', 'transformers.models.llama.modeling_llama.LlamaDecoderLayer', 'transformers.models.llama.modeling_llama.LlamaMLP', 'torch.nn.modules.linear.Linear', 'aten.linear.default']"
              },
              {
                "key": "[metadata] pkg.torch.onnx.fx_node",
                "value": "%linear_12 : [num_users=1] = call_function[target=torch.ops.aten.linear.default](args = (%mul_567, %p_model_layers_1_mlp_up_proj_weight), kwargs = {})"
              },
              {
                "key": "[metadata] pkg.torch.onnx.name_scopes",
                "value": "['', 'model', 'model.layers.1', 'model.layers.1.mlp', 'model.layers.1.mlp.up_proj', 'linear_12']"
              },
              {
                "key": "[metadata] pkg.torch.onnx.stack_trace",
                "value": "File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 688, in forward\n    outputs: BaseModelOutputWithPast = self.model(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 453, in forward\n    layer_outputs = decoder_layer(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 324, in forward\n    hidden_states = self.mlp(hidden_states)\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 162, in forward\n    down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/torch/nn/modules/linear.py\", line 125, in forward\n    return F.linear(input, self.weight, self.bias)"
              }
            ],
            "incomingEdges": [
              {
                "sourceNodeId": "node_Mul_657",
                "sourceNodeOutputId": "0",
                "targetNodeInputId": "0"
              },
              {
                "sourceNodeId": "node_Transpose_662",
                "sourceNodeOutputId": "0",
                "targetNodeInputId": "1"
              }
            ],
            "outputsMetadata": [
              {
                "id": "0",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "linear_12"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "FLOAT[s34,s16,8192]"
                  },
                  {
                    "key": "param_name",
                    "value": "Y"
                  }
                ]
              }
            ],
            "inputsMetadata": [
              {
                "id": "0",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "mul_567"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "FLOAT[s34,s16,2048]"
                  },
                  {
                    "key": "param_name",
                    "value": "A"
                  }
                ]
              },
              {
                "id": "1",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "val_464"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "FLOAT[2048,8192]"
                  },
                  {
                    "key": "param_name",
                    "value": "B"
                  }
                ]
              }
            ],
            "style": null,
            "config": null
          },
          {
            "id": "node_Mul_664",
            "label": "Mul",
            "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.1: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.1.mlp: transformers.models.llama.modeling_llama.LlamaMLP/mul_580: aten.mul.Tensor",
            "subgraphIds": [],
            "attrs": [
              {
                "key": "[metadata] namespace",
                "value": ": transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.1: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.1.mlp: transformers.models.llama.modeling_llama.LlamaMLP/mul_580: aten.mul.Tensor"
              },
              {
                "key": "[metadata] pkg.torch.onnx.class_hierarchy",
                "value": "['transformers.models.llama.modeling_llama.LlamaForCausalLM', 'transformers.models.llama.modeling_llama.LlamaModel', 'transformers.models.llama.modeling_llama.LlamaDecoderLayer', 'transformers.models.llama.modeling_llama.LlamaMLP', 'aten.mul.Tensor']"
              },
              {
                "key": "[metadata] pkg.torch.onnx.fx_node",
                "value": "%mul_580 : [num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%silu_1, %linear_12), kwargs = {})"
              },
              {
                "key": "[metadata] pkg.torch.onnx.name_scopes",
                "value": "['', 'model', 'model.layers.1', 'model.layers.1.mlp', 'mul_580']"
              },
              {
                "key": "[metadata] pkg.torch.onnx.stack_trace",
                "value": "File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 688, in forward\n    outputs: BaseModelOutputWithPast = self.model(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 453, in forward\n    layer_outputs = decoder_layer(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 324, in forward\n    hidden_states = self.mlp(hidden_states)\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 162, in forward\n    down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))"
              }
            ],
            "incomingEdges": [
              {
                "sourceNodeId": "node_Mul_661",
                "sourceNodeOutputId": "0",
                "targetNodeInputId": "0"
              },
              {
                "sourceNodeId": "node_MatMul_663",
                "sourceNodeOutputId": "0",
                "targetNodeInputId": "1"
              }
            ],
            "outputsMetadata": [
              {
                "id": "0",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "mul_580"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "FLOAT[s34,s16,8192]"
                  },
                  {
                    "key": "param_name",
                    "value": "C"
                  }
                ]
              }
            ],
            "inputsMetadata": [
              {
                "id": "0",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "silu_1"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "FLOAT[s34,s16,8192]"
                  },
                  {
                    "key": "param_name",
                    "value": "A"
                  }
                ]
              },
              {
                "id": "1",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "linear_12"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "FLOAT[s34,s16,8192]"
                  },
                  {
                    "key": "param_name",
                    "value": "B"
                  }
                ]
              }
            ],
            "style": null,
            "config": null
          },
          {
            "id": "node_Transpose_665",
            "label": "Transpose",
            "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.1: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.1.mlp: transformers.models.llama.modeling_llama.LlamaMLP/model.layers.1.mlp.down_proj: torch.nn.modules.linear.Linear/linear_13: aten.linear.default",
            "subgraphIds": [],
            "attrs": [
              {
                "key": "perm",
                "value": "[1, 0]"
              },
              {
                "key": "[metadata] namespace",
                "value": ": transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.1: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.1.mlp: transformers.models.llama.modeling_llama.LlamaMLP/model.layers.1.mlp.down_proj: torch.nn.modules.linear.Linear/linear_13: aten.linear.default"
              },
              {
                "key": "[metadata] pkg.torch.onnx.class_hierarchy",
                "value": "['transformers.models.llama.modeling_llama.LlamaForCausalLM', 'transformers.models.llama.modeling_llama.LlamaModel', 'transformers.models.llama.modeling_llama.LlamaDecoderLayer', 'transformers.models.llama.modeling_llama.LlamaMLP', 'torch.nn.modules.linear.Linear', 'aten.linear.default']"
              },
              {
                "key": "[metadata] pkg.torch.onnx.fx_node",
                "value": "%linear_13 : [num_users=1] = call_function[target=torch.ops.aten.linear.default](args = (%mul_580, %p_model_layers_1_mlp_down_proj_weight), kwargs = {})"
              },
              {
                "key": "[metadata] pkg.torch.onnx.name_scopes",
                "value": "['', 'model', 'model.layers.1', 'model.layers.1.mlp', 'model.layers.1.mlp.down_proj', 'linear_13']"
              },
              {
                "key": "[metadata] pkg.torch.onnx.stack_trace",
                "value": "File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 688, in forward\n    outputs: BaseModelOutputWithPast = self.model(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 453, in forward\n    layer_outputs = decoder_layer(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 324, in forward\n    hidden_states = self.mlp(hidden_states)\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 162, in forward\n    down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/torch/nn/modules/linear.py\", line 125, in forward\n    return F.linear(input, self.weight, self.bias)"
              }
            ],
            "incomingEdges": [
              {
                "sourceNodeId": "[value] model.layers.1.mlp.down_proj.weight",
                "sourceNodeOutputId": "0",
                "targetNodeInputId": "0"
              }
            ],
            "outputsMetadata": [
              {
                "id": "0",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "val_465"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "FLOAT[8192,2048]"
                  },
                  {
                    "key": "param_name",
                    "value": "transposed"
                  }
                ]
              }
            ],
            "inputsMetadata": [
              {
                "id": "0",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "model.layers.1.mlp.down_proj.weight"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "FLOAT[2048,8192]"
                  },
                  {
                    "key": "param_name",
                    "value": "data"
                  }
                ]
              }
            ],
            "style": null,
            "config": null
          },
          {
            "id": "node_MatMul_666",
            "label": "MatMul",
            "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.1: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.1.mlp: transformers.models.llama.modeling_llama.LlamaMLP/model.layers.1.mlp.down_proj: torch.nn.modules.linear.Linear/linear_13: aten.linear.default",
            "subgraphIds": [],
            "attrs": [
              {
                "key": "[metadata] namespace",
                "value": ": transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.1: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.1.mlp: transformers.models.llama.modeling_llama.LlamaMLP/model.layers.1.mlp.down_proj: torch.nn.modules.linear.Linear/linear_13: aten.linear.default"
              },
              {
                "key": "[metadata] pkg.torch.onnx.class_hierarchy",
                "value": "['transformers.models.llama.modeling_llama.LlamaForCausalLM', 'transformers.models.llama.modeling_llama.LlamaModel', 'transformers.models.llama.modeling_llama.LlamaDecoderLayer', 'transformers.models.llama.modeling_llama.LlamaMLP', 'torch.nn.modules.linear.Linear', 'aten.linear.default']"
              },
              {
                "key": "[metadata] pkg.torch.onnx.fx_node",
                "value": "%linear_13 : [num_users=1] = call_function[target=torch.ops.aten.linear.default](args = (%mul_580, %p_model_layers_1_mlp_down_proj_weight), kwargs = {})"
              },
              {
                "key": "[metadata] pkg.torch.onnx.name_scopes",
                "value": "['', 'model', 'model.layers.1', 'model.layers.1.mlp', 'model.layers.1.mlp.down_proj', 'linear_13']"
              },
              {
                "key": "[metadata] pkg.torch.onnx.stack_trace",
                "value": "File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 688, in forward\n    outputs: BaseModelOutputWithPast = self.model(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 453, in forward\n    layer_outputs = decoder_layer(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 324, in forward\n    hidden_states = self.mlp(hidden_states)\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 162, in forward\n    down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/torch/nn/modules/linear.py\", line 125, in forward\n    return F.linear(input, self.weight, self.bias)"
              }
            ],
            "incomingEdges": [
              {
                "sourceNodeId": "node_Mul_664",
                "sourceNodeOutputId": "0",
                "targetNodeInputId": "0"
              },
              {
                "sourceNodeId": "node_Transpose_665",
                "sourceNodeOutputId": "0",
                "targetNodeInputId": "1"
              }
            ],
            "outputsMetadata": [
              {
                "id": "0",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "linear_13"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "FLOAT[s34,s16,2048]"
                  },
                  {
                    "key": "param_name",
                    "value": "Y"
                  }
                ]
              }
            ],
            "inputsMetadata": [
              {
                "id": "0",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "mul_580"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "FLOAT[s34,s16,8192]"
                  },
                  {
                    "key": "param_name",
                    "value": "A"
                  }
                ]
              },
              {
                "id": "1",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "val_465"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "FLOAT[8192,2048]"
                  },
                  {
                    "key": "param_name",
                    "value": "B"
                  }
                ]
              }
            ],
            "style": null,
            "config": null
          },
          {
            "id": "node_Add_667",
            "label": "Add",
            "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.1: transformers.models.llama.modeling_llama.LlamaDecoderLayer/add_747: aten.add.Tensor",
            "subgraphIds": [],
            "attrs": [
              {
                "key": "[metadata] namespace",
                "value": ": transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.1: transformers.models.llama.modeling_llama.LlamaDecoderLayer/add_747: aten.add.Tensor"
              },
              {
                "key": "[metadata] pkg.torch.onnx.class_hierarchy",
                "value": "['transformers.models.llama.modeling_llama.LlamaForCausalLM', 'transformers.models.llama.modeling_llama.LlamaModel', 'transformers.models.llama.modeling_llama.LlamaDecoderLayer', 'aten.add.Tensor']"
              },
              {
                "key": "[metadata] pkg.torch.onnx.fx_node",
                "value": "%add_747 : [num_users=2] = call_function[target=torch.ops.aten.add.Tensor](args = (%add_697, %linear_13), kwargs = {})"
              },
              {
                "key": "[metadata] pkg.torch.onnx.name_scopes",
                "value": "['', 'model', 'model.layers.1', 'add_747']"
              },
              {
                "key": "[metadata] pkg.torch.onnx.stack_trace",
                "value": "File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 688, in forward\n    outputs: BaseModelOutputWithPast = self.model(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 453, in forward\n    layer_outputs = decoder_layer(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 325, in forward\n    hidden_states = residual + hidden_states"
              }
            ],
            "incomingEdges": [
              {
                "sourceNodeId": "node_Add_647",
                "sourceNodeOutputId": "0",
                "targetNodeInputId": "0"
              },
              {
                "sourceNodeId": "node_MatMul_666",
                "sourceNodeOutputId": "0",
                "targetNodeInputId": "1"
              }
            ],
            "outputsMetadata": [
              {
                "id": "0",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "add_747"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "FLOAT[s34,s16,2048]"
                  },
                  {
                    "key": "param_name",
                    "value": "C"
                  }
                ]
              }
            ],
            "inputsMetadata": [
              {
                "id": "0",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "add_697"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "FLOAT[s34,s16,2048]"
                  },
                  {
                    "key": "param_name",
                    "value": "A"
                  }
                ]
              },
              {
                "id": "1",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "linear_13"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "FLOAT[s34,s16,2048]"
                  },
                  {
                    "key": "param_name",
                    "value": "B"
                  }
                ]
              }
            ],
            "style": null,
            "config": null
          },
          {
            "id": "node_Constant_668",
            "label": "Constant",
            "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.norm: transformers.models.llama.modeling_llama.LlamaRMSNorm/pow_5: aten.pow.Tensor_Scalar",
            "subgraphIds": [],
            "attrs": [
              {
                "key": "__value",
                "value": "2.0"
              },
              {
                "key": "value",
                "value": "TensorProtoTensor<FLOAT,[]>(array(2., dtype=float32), name='')"
              },
              {
                "key": "[metadata] namespace",
                "value": ": transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.norm: transformers.models.llama.modeling_llama.LlamaRMSNorm/pow_5: aten.pow.Tensor_Scalar"
              },
              {
                "key": "[metadata] pkg.torch.onnx.class_hierarchy",
                "value": "['transformers.models.llama.modeling_llama.LlamaForCausalLM', 'transformers.models.llama.modeling_llama.LlamaModel', 'transformers.models.llama.modeling_llama.LlamaRMSNorm', 'aten.pow.Tensor_Scalar']"
              },
              {
                "key": "[metadata] pkg.torch.onnx.fx_node",
                "value": "%pow_5 : [num_users=1] = call_function[target=torch.ops.aten.pow.Tensor_Scalar](args = (%add_747, 2), kwargs = {})"
              },
              {
                "key": "[metadata] pkg.torch.onnx.name_scopes",
                "value": "['', 'model', 'model.norm', 'pow_5']"
              },
              {
                "key": "[metadata] pkg.torch.onnx.stack_trace",
                "value": "File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 688, in forward\n    outputs: BaseModelOutputWithPast = self.model(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 470, in forward\n    hidden_states = self.norm(hidden_states)\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 71, in forward\n    variance = hidden_states.pow(2).mean(-1, keepdim=True)"
              }
            ],
            "incomingEdges": [],
            "outputsMetadata": [
              {
                "id": "0",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "val_466"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "FLOAT[]"
                  },
                  {
                    "key": "param_name",
                    "value": "output"
                  }
                ]
              }
            ],
            "inputsMetadata": [],
            "style": null,
            "config": null
          },
          {
            "id": "node_Pow_669",
            "label": "Pow",
            "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.norm: transformers.models.llama.modeling_llama.LlamaRMSNorm/pow_5: aten.pow.Tensor_Scalar",
            "subgraphIds": [],
            "attrs": [
              {
                "key": "[metadata] namespace",
                "value": ": transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.norm: transformers.models.llama.modeling_llama.LlamaRMSNorm/pow_5: aten.pow.Tensor_Scalar"
              },
              {
                "key": "[metadata] pkg.torch.onnx.class_hierarchy",
                "value": "['transformers.models.llama.modeling_llama.LlamaForCausalLM', 'transformers.models.llama.modeling_llama.LlamaModel', 'transformers.models.llama.modeling_llama.LlamaRMSNorm', 'aten.pow.Tensor_Scalar']"
              },
              {
                "key": "[metadata] pkg.torch.onnx.fx_node",
                "value": "%pow_5 : [num_users=1] = call_function[target=torch.ops.aten.pow.Tensor_Scalar](args = (%add_747, 2), kwargs = {})"
              },
              {
                "key": "[metadata] pkg.torch.onnx.name_scopes",
                "value": "['', 'model', 'model.norm', 'pow_5']"
              },
              {
                "key": "[metadata] pkg.torch.onnx.stack_trace",
                "value": "File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 688, in forward\n    outputs: BaseModelOutputWithPast = self.model(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 470, in forward\n    hidden_states = self.norm(hidden_states)\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 71, in forward\n    variance = hidden_states.pow(2).mean(-1, keepdim=True)"
              }
            ],
            "incomingEdges": [
              {
                "sourceNodeId": "node_Add_667",
                "sourceNodeOutputId": "0",
                "targetNodeInputId": "0"
              },
              {
                "sourceNodeId": "node_Constant_668",
                "sourceNodeOutputId": "0",
                "targetNodeInputId": "1"
              }
            ],
            "outputsMetadata": [
              {
                "id": "0",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "pow_5"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "FLOAT[s34,s16,2048]"
                  },
                  {
                    "key": "param_name",
                    "value": "Z"
                  }
                ]
              }
            ],
            "inputsMetadata": [
              {
                "id": "0",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "add_747"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "FLOAT[s34,s16,2048]"
                  },
                  {
                    "key": "param_name",
                    "value": "X"
                  }
                ]
              },
              {
                "id": "1",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "val_466"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "FLOAT[]"
                  },
                  {
                    "key": "param_name",
                    "value": "Y"
                  }
                ]
              }
            ],
            "style": null,
            "config": null
          },
          {
            "id": "node_Constant_981",
            "label": "Constant",
            "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.norm: transformers.models.llama.modeling_llama.LlamaRMSNorm/mean_4: aten.mean.dim",
            "subgraphIds": [],
            "attrs": [
              {
                "key": "__value",
                "value": "[-1]"
              },
              {
                "key": "value",
                "value": "TensorProtoTensor<INT64,[1]>(array([-1]), name='val_468')"
              }
            ],
            "incomingEdges": [],
            "outputsMetadata": [
              {
                "id": "0",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "val_468"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "INT64[1]"
                  },
                  {
                    "key": "param_name",
                    "value": "output"
                  }
                ]
              }
            ],
            "inputsMetadata": [],
            "style": null,
            "config": null
          },
          {
            "id": "node_ReduceMean_672",
            "label": "ReduceMean",
            "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.norm: transformers.models.llama.modeling_llama.LlamaRMSNorm/mean_4: aten.mean.dim",
            "subgraphIds": [],
            "attrs": [
              {
                "key": "noop_with_empty_axes",
                "value": "0"
              },
              {
                "key": "keepdims",
                "value": "1"
              },
              {
                "key": "[metadata] namespace",
                "value": ": transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.norm: transformers.models.llama.modeling_llama.LlamaRMSNorm/mean_4: aten.mean.dim"
              },
              {
                "key": "[metadata] pkg.torch.onnx.class_hierarchy",
                "value": "['transformers.models.llama.modeling_llama.LlamaForCausalLM', 'transformers.models.llama.modeling_llama.LlamaModel', 'transformers.models.llama.modeling_llama.LlamaRMSNorm', 'aten.mean.dim']"
              },
              {
                "key": "[metadata] pkg.torch.onnx.fx_node",
                "value": "%mean_4 : [num_users=1] = call_function[target=torch.ops.aten.mean.dim](args = (%pow_5, [-1], True), kwargs = {})"
              },
              {
                "key": "[metadata] pkg.torch.onnx.name_scopes",
                "value": "['', 'model', 'model.norm', 'mean_4']"
              },
              {
                "key": "[metadata] pkg.torch.onnx.stack_trace",
                "value": "File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 688, in forward\n    outputs: BaseModelOutputWithPast = self.model(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 470, in forward\n    hidden_states = self.norm(hidden_states)\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 71, in forward\n    variance = hidden_states.pow(2).mean(-1, keepdim=True)"
              }
            ],
            "incomingEdges": [
              {
                "sourceNodeId": "node_Pow_669",
                "sourceNodeOutputId": "0",
                "targetNodeInputId": "0"
              },
              {
                "sourceNodeId": "node_Constant_981",
                "sourceNodeOutputId": "0",
                "targetNodeInputId": "1"
              }
            ],
            "outputsMetadata": [
              {
                "id": "0",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "mean_4"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "FLOAT[s34,s16,1]"
                  },
                  {
                    "key": "param_name",
                    "value": "reduced"
                  }
                ]
              }
            ],
            "inputsMetadata": [
              {
                "id": "0",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "pow_5"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "FLOAT[s34,s16,2048]"
                  },
                  {
                    "key": "param_name",
                    "value": "data"
                  }
                ]
              },
              {
                "id": "1",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "val_468"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "INT64[1]"
                  },
                  {
                    "key": "param_name",
                    "value": "axes"
                  }
                ]
              }
            ],
            "style": null,
            "config": null
          },
          {
            "id": "node_Add_673",
            "label": "Add",
            "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.norm: transformers.models.llama.modeling_llama.LlamaRMSNorm/add_760: aten.add.Tensor",
            "subgraphIds": [],
            "attrs": [
              {
                "key": "[metadata] namespace",
                "value": ": transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.norm: transformers.models.llama.modeling_llama.LlamaRMSNorm/add_760: aten.add.Tensor"
              },
              {
                "key": "[metadata] pkg.torch.onnx.class_hierarchy",
                "value": "['transformers.models.llama.modeling_llama.LlamaForCausalLM', 'transformers.models.llama.modeling_llama.LlamaModel', 'transformers.models.llama.modeling_llama.LlamaRMSNorm', 'aten.add.Tensor']"
              },
              {
                "key": "[metadata] pkg.torch.onnx.fx_node",
                "value": "%add_760 : [num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%mean_4, 1e-05), kwargs = {})"
              },
              {
                "key": "[metadata] pkg.torch.onnx.name_scopes",
                "value": "['', 'model', 'model.norm', 'add_760']"
              },
              {
                "key": "[metadata] pkg.torch.onnx.stack_trace",
                "value": "File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 688, in forward\n    outputs: BaseModelOutputWithPast = self.model(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 470, in forward\n    hidden_states = self.norm(hidden_states)\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 72, in forward\n    hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)"
              }
            ],
            "incomingEdges": [
              {
                "sourceNodeId": "node_ReduceMean_672",
                "sourceNodeOutputId": "0",
                "targetNodeInputId": "0"
              },
              {
                "sourceNodeId": "node_Constant_333",
                "sourceNodeOutputId": "0",
                "targetNodeInputId": "1"
              }
            ],
            "outputsMetadata": [
              {
                "id": "0",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "add_760"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "FLOAT[s34,s16,1]"
                  },
                  {
                    "key": "param_name",
                    "value": "C"
                  }
                ]
              }
            ],
            "inputsMetadata": [
              {
                "id": "0",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "mean_4"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "FLOAT[s34,s16,1]"
                  },
                  {
                    "key": "param_name",
                    "value": "A"
                  }
                ]
              },
              {
                "id": "1",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "val_251"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "FLOAT[]"
                  },
                  {
                    "key": "param_name",
                    "value": "B"
                  }
                ]
              }
            ],
            "style": null,
            "config": null
          },
          {
            "id": "node_Sqrt_674",
            "label": "Sqrt",
            "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.norm: transformers.models.llama.modeling_llama.LlamaRMSNorm/rsqrt_4: aten.rsqrt.default",
            "subgraphIds": [],
            "attrs": [
              {
                "key": "[metadata] namespace",
                "value": ": transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.norm: transformers.models.llama.modeling_llama.LlamaRMSNorm/rsqrt_4: aten.rsqrt.default"
              },
              {
                "key": "[metadata] pkg.torch.onnx.class_hierarchy",
                "value": "['transformers.models.llama.modeling_llama.LlamaForCausalLM', 'transformers.models.llama.modeling_llama.LlamaModel', 'transformers.models.llama.modeling_llama.LlamaRMSNorm', 'aten.rsqrt.default']"
              },
              {
                "key": "[metadata] pkg.torch.onnx.fx_node",
                "value": "%rsqrt_4 : [num_users=1] = call_function[target=torch.ops.aten.rsqrt.default](args = (%add_760,), kwargs = {})"
              },
              {
                "key": "[metadata] pkg.torch.onnx.name_scopes",
                "value": "['', 'model', 'model.norm', 'rsqrt_4']"
              },
              {
                "key": "[metadata] pkg.torch.onnx.stack_trace",
                "value": "File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 688, in forward\n    outputs: BaseModelOutputWithPast = self.model(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 470, in forward\n    hidden_states = self.norm(hidden_states)\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 72, in forward\n    hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)"
              }
            ],
            "incomingEdges": [
              {
                "sourceNodeId": "node_Add_673",
                "sourceNodeOutputId": "0",
                "targetNodeInputId": "0"
              }
            ],
            "outputsMetadata": [
              {
                "id": "0",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "val_469"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "FLOAT[s34,s16,1]"
                  },
                  {
                    "key": "param_name",
                    "value": "Y"
                  }
                ]
              }
            ],
            "inputsMetadata": [
              {
                "id": "0",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "add_760"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "FLOAT[s34,s16,1]"
                  },
                  {
                    "key": "param_name",
                    "value": "X"
                  }
                ]
              }
            ],
            "style": null,
            "config": null
          },
          {
            "id": "node_Reciprocal_675",
            "label": "Reciprocal",
            "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.norm: transformers.models.llama.modeling_llama.LlamaRMSNorm/rsqrt_4: aten.rsqrt.default",
            "subgraphIds": [],
            "attrs": [
              {
                "key": "[metadata] namespace",
                "value": ": transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.norm: transformers.models.llama.modeling_llama.LlamaRMSNorm/rsqrt_4: aten.rsqrt.default"
              },
              {
                "key": "[metadata] pkg.torch.onnx.class_hierarchy",
                "value": "['transformers.models.llama.modeling_llama.LlamaForCausalLM', 'transformers.models.llama.modeling_llama.LlamaModel', 'transformers.models.llama.modeling_llama.LlamaRMSNorm', 'aten.rsqrt.default']"
              },
              {
                "key": "[metadata] pkg.torch.onnx.fx_node",
                "value": "%rsqrt_4 : [num_users=1] = call_function[target=torch.ops.aten.rsqrt.default](args = (%add_760,), kwargs = {})"
              },
              {
                "key": "[metadata] pkg.torch.onnx.name_scopes",
                "value": "['', 'model', 'model.norm', 'rsqrt_4']"
              },
              {
                "key": "[metadata] pkg.torch.onnx.stack_trace",
                "value": "File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 688, in forward\n    outputs: BaseModelOutputWithPast = self.model(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 470, in forward\n    hidden_states = self.norm(hidden_states)\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 72, in forward\n    hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)"
              }
            ],
            "incomingEdges": [
              {
                "sourceNodeId": "node_Sqrt_674",
                "sourceNodeOutputId": "0",
                "targetNodeInputId": "0"
              }
            ],
            "outputsMetadata": [
              {
                "id": "0",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "rsqrt_4"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "FLOAT[s34,s16,1]"
                  },
                  {
                    "key": "param_name",
                    "value": "Y"
                  }
                ]
              }
            ],
            "inputsMetadata": [
              {
                "id": "0",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "val_469"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "FLOAT[s34,s16,1]"
                  },
                  {
                    "key": "param_name",
                    "value": "X"
                  }
                ]
              }
            ],
            "style": null,
            "config": null
          },
          {
            "id": "node_Mul_676",
            "label": "Mul",
            "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.norm: transformers.models.llama.modeling_llama.LlamaRMSNorm/mul_599: aten.mul.Tensor",
            "subgraphIds": [],
            "attrs": [
              {
                "key": "[metadata] namespace",
                "value": ": transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.norm: transformers.models.llama.modeling_llama.LlamaRMSNorm/mul_599: aten.mul.Tensor"
              },
              {
                "key": "[metadata] pkg.torch.onnx.class_hierarchy",
                "value": "['transformers.models.llama.modeling_llama.LlamaForCausalLM', 'transformers.models.llama.modeling_llama.LlamaModel', 'transformers.models.llama.modeling_llama.LlamaRMSNorm', 'aten.mul.Tensor']"
              },
              {
                "key": "[metadata] pkg.torch.onnx.fx_node",
                "value": "%mul_599 : [num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%add_747, %rsqrt_4), kwargs = {})"
              },
              {
                "key": "[metadata] pkg.torch.onnx.name_scopes",
                "value": "['', 'model', 'model.norm', 'mul_599']"
              },
              {
                "key": "[metadata] pkg.torch.onnx.stack_trace",
                "value": "File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 688, in forward\n    outputs: BaseModelOutputWithPast = self.model(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 470, in forward\n    hidden_states = self.norm(hidden_states)\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 72, in forward\n    hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)"
              }
            ],
            "incomingEdges": [
              {
                "sourceNodeId": "node_Add_667",
                "sourceNodeOutputId": "0",
                "targetNodeInputId": "0"
              },
              {
                "sourceNodeId": "node_Reciprocal_675",
                "sourceNodeOutputId": "0",
                "targetNodeInputId": "1"
              }
            ],
            "outputsMetadata": [
              {
                "id": "0",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "mul_599"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "FLOAT[s34,s16,2048]"
                  },
                  {
                    "key": "param_name",
                    "value": "C"
                  }
                ]
              }
            ],
            "inputsMetadata": [
              {
                "id": "0",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "add_747"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "FLOAT[s34,s16,2048]"
                  },
                  {
                    "key": "param_name",
                    "value": "A"
                  }
                ]
              },
              {
                "id": "1",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "rsqrt_4"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "FLOAT[s34,s16,1]"
                  },
                  {
                    "key": "param_name",
                    "value": "B"
                  }
                ]
              }
            ],
            "style": null,
            "config": null
          },
          {
            "id": "node_Mul_677",
            "label": "Mul",
            "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.norm: transformers.models.llama.modeling_llama.LlamaRMSNorm/mul_603: aten.mul.Tensor",
            "subgraphIds": [],
            "attrs": [
              {
                "key": "[metadata] namespace",
                "value": ": transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.norm: transformers.models.llama.modeling_llama.LlamaRMSNorm/mul_603: aten.mul.Tensor"
              },
              {
                "key": "[metadata] pkg.torch.onnx.class_hierarchy",
                "value": "['transformers.models.llama.modeling_llama.LlamaForCausalLM', 'transformers.models.llama.modeling_llama.LlamaModel', 'transformers.models.llama.modeling_llama.LlamaRMSNorm', 'aten.mul.Tensor']"
              },
              {
                "key": "[metadata] pkg.torch.onnx.fx_node",
                "value": "%mul_603 : [num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%p_model_norm_weight, %mul_599), kwargs = {})"
              },
              {
                "key": "[metadata] pkg.torch.onnx.name_scopes",
                "value": "['', 'model', 'model.norm', 'mul_603']"
              },
              {
                "key": "[metadata] pkg.torch.onnx.stack_trace",
                "value": "File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 688, in forward\n    outputs: BaseModelOutputWithPast = self.model(\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 470, in forward\n    hidden_states = self.norm(hidden_states)\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 73, in forward\n    return self.weight * hidden_states.to(input_dtype)"
              }
            ],
            "incomingEdges": [
              {
                "sourceNodeId": "[value] model.norm.weight",
                "sourceNodeOutputId": "0",
                "targetNodeInputId": "0"
              },
              {
                "sourceNodeId": "node_Mul_676",
                "sourceNodeOutputId": "0",
                "targetNodeInputId": "1"
              }
            ],
            "outputsMetadata": [
              {
                "id": "0",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "mul_603"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "FLOAT[s34,s16,2048]"
                  },
                  {
                    "key": "param_name",
                    "value": "C"
                  }
                ]
              }
            ],
            "inputsMetadata": [
              {
                "id": "0",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "model.norm.weight"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "FLOAT[2048]"
                  },
                  {
                    "key": "param_name",
                    "value": "A"
                  }
                ]
              },
              {
                "id": "1",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "mul_599"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "FLOAT[s34,s16,2048]"
                  },
                  {
                    "key": "param_name",
                    "value": "B"
                  }
                ]
              }
            ],
            "style": null,
            "config": null
          },
          {
            "id": "node_Transpose_701",
            "label": "Transpose",
            "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/lm_head: torch.nn.modules.linear.Linear/linear_14: aten.linear.default",
            "subgraphIds": [],
            "attrs": [
              {
                "key": "perm",
                "value": "[1, 0]"
              },
              {
                "key": "[metadata] namespace",
                "value": ": transformers.models.llama.modeling_llama.LlamaForCausalLM/lm_head: torch.nn.modules.linear.Linear/linear_14: aten.linear.default"
              },
              {
                "key": "[metadata] pkg.torch.onnx.class_hierarchy",
                "value": "['transformers.models.llama.modeling_llama.LlamaForCausalLM', 'torch.nn.modules.linear.Linear', 'aten.linear.default']"
              },
              {
                "key": "[metadata] pkg.torch.onnx.fx_node",
                "value": "%linear_14 : [num_users=1] = call_function[target=torch.ops.aten.linear.default](args = (%slice_52, %p_lm_head_weight), kwargs = {})"
              },
              {
                "key": "[metadata] pkg.torch.onnx.name_scopes",
                "value": "['', 'lm_head', 'linear_14']"
              },
              {
                "key": "[metadata] pkg.torch.onnx.stack_trace",
                "value": "File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 704, in forward\n    logits = self.lm_head(hidden_states[:, slice_indices, :])\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/torch/nn/modules/linear.py\", line 125, in forward\n    return F.linear(input, self.weight, self.bias)"
              }
            ],
            "incomingEdges": [
              {
                "sourceNodeId": "[value] lm_head.weight",
                "sourceNodeOutputId": "0",
                "targetNodeInputId": "0"
              }
            ],
            "outputsMetadata": [
              {
                "id": "0",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "val_490"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "FLOAT[2048,49152]"
                  },
                  {
                    "key": "param_name",
                    "value": "transposed"
                  }
                ]
              }
            ],
            "inputsMetadata": [
              {
                "id": "0",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "lm_head.weight"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "FLOAT[49152,2048]"
                  },
                  {
                    "key": "param_name",
                    "value": "data"
                  }
                ]
              }
            ],
            "style": null,
            "config": null
          },
          {
            "id": "node_MatMul_702",
            "label": "MatMul",
            "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/lm_head: torch.nn.modules.linear.Linear/linear_14: aten.linear.default",
            "subgraphIds": [],
            "attrs": [
              {
                "key": "[metadata] namespace",
                "value": ": transformers.models.llama.modeling_llama.LlamaForCausalLM/lm_head: torch.nn.modules.linear.Linear/linear_14: aten.linear.default"
              },
              {
                "key": "[metadata] pkg.torch.onnx.class_hierarchy",
                "value": "['transformers.models.llama.modeling_llama.LlamaForCausalLM', 'torch.nn.modules.linear.Linear', 'aten.linear.default']"
              },
              {
                "key": "[metadata] pkg.torch.onnx.fx_node",
                "value": "%linear_14 : [num_users=1] = call_function[target=torch.ops.aten.linear.default](args = (%slice_52, %p_lm_head_weight), kwargs = {})"
              },
              {
                "key": "[metadata] pkg.torch.onnx.name_scopes",
                "value": "['', 'lm_head', 'linear_14']"
              },
              {
                "key": "[metadata] pkg.torch.onnx.stack_trace",
                "value": "File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 704, in forward\n    logits = self.lm_head(hidden_states[:, slice_indices, :])\n  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/torch/nn/modules/linear.py\", line 125, in forward\n    return F.linear(input, self.weight, self.bias)"
              }
            ],
            "incomingEdges": [
              {
                "sourceNodeId": "node_Mul_677",
                "sourceNodeOutputId": "0",
                "targetNodeInputId": "0"
              },
              {
                "sourceNodeId": "node_Transpose_701",
                "sourceNodeOutputId": "0",
                "targetNodeInputId": "1"
              }
            ],
            "outputsMetadata": [
              {
                "id": "0",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "linear_14"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "FLOAT[s34,s16,49152]"
                  },
                  {
                    "key": "[metadata] pkg.torch.export.graph_signature.OutputSpec.kind",
                    "value": "USER_OUTPUT"
                  },
                  {
                    "key": "param_name",
                    "value": "Y"
                  }
                ]
              }
            ],
            "inputsMetadata": [
              {
                "id": "0",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "mul_603"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "FLOAT[s34,s16,2048]"
                  },
                  {
                    "key": "param_name",
                    "value": "A"
                  }
                ]
              },
              {
                "id": "1",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "val_490"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "FLOAT[2048,49152]"
                  },
                  {
                    "key": "param_name",
                    "value": "B"
                  }
                ]
              }
            ],
            "style": null,
            "config": null
          },
          {
            "id": "[value] model.layers.0.self_attn.q_proj.weight",
            "label": "Initializer",
            "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.0: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.0.self_attn: transformers.models.llama.modeling_llama.LlamaAttention/model.layers.0.self_attn.q_proj: torch.nn.modules.linear.Linear/linear: aten.linear.default",
            "subgraphIds": [],
            "attrs": [],
            "incomingEdges": [],
            "outputsMetadata": [
              {
                "id": "0",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "model.layers.0.self_attn.q_proj.weight"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "FLOAT[2048,2048]"
                  },
                  {
                    "key": "value",
                    "value": "ExternalTensor<FLOAT,[2048,2048]>(location='model_SmolLM17b_2Layer-onnx_dynamo-ir-d1rt1.onnx.data', name='model.layers.0.self_attn.q_proj.weight', offset=65536, length=16777216, base_dir='')"
                  }
                ]
              }
            ],
            "inputsMetadata": [],
            "style": null,
            "config": null
          },
          {
            "id": "[value] model.layers.0.self_attn.k_proj.weight",
            "label": "Initializer",
            "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.0: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.0.self_attn: transformers.models.llama.modeling_llama.LlamaAttention/model.layers.0.self_attn.k_proj: torch.nn.modules.linear.Linear/linear_1: aten.linear.default",
            "subgraphIds": [],
            "attrs": [],
            "incomingEdges": [],
            "outputsMetadata": [
              {
                "id": "0",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "model.layers.0.self_attn.k_proj.weight"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "FLOAT[2048,2048]"
                  },
                  {
                    "key": "value",
                    "value": "ExternalTensor<FLOAT,[2048,2048]>(location='model_SmolLM17b_2Layer-onnx_dynamo-ir-d1rt1.onnx.data', name='model.layers.0.self_attn.k_proj.weight', offset=16842752, length=16777216, base_dir='')"
                  }
                ]
              }
            ],
            "inputsMetadata": [],
            "style": null,
            "config": null
          },
          {
            "id": "[value] model.layers.0.self_attn.v_proj.weight",
            "label": "Initializer",
            "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.0: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.0.self_attn: transformers.models.llama.modeling_llama.LlamaAttention/model.layers.0.self_attn.v_proj: torch.nn.modules.linear.Linear/linear_2: aten.linear.default",
            "subgraphIds": [],
            "attrs": [],
            "incomingEdges": [],
            "outputsMetadata": [
              {
                "id": "0",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "model.layers.0.self_attn.v_proj.weight"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "FLOAT[2048,2048]"
                  },
                  {
                    "key": "value",
                    "value": "ExternalTensor<FLOAT,[2048,2048]>(location='model_SmolLM17b_2Layer-onnx_dynamo-ir-d1rt1.onnx.data', name='model.layers.0.self_attn.v_proj.weight', offset=33619968, length=16777216, base_dir='')"
                  }
                ]
              }
            ],
            "inputsMetadata": [],
            "style": null,
            "config": null
          },
          {
            "id": "[value] model.layers.0.self_attn.o_proj.weight",
            "label": "Initializer",
            "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.0: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.0.self_attn: transformers.models.llama.modeling_llama.LlamaAttention/model.layers.0.self_attn.o_proj: torch.nn.modules.linear.Linear/linear_3: aten.linear.default",
            "subgraphIds": [],
            "attrs": [],
            "incomingEdges": [],
            "outputsMetadata": [
              {
                "id": "0",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "model.layers.0.self_attn.o_proj.weight"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "FLOAT[2048,2048]"
                  },
                  {
                    "key": "value",
                    "value": "ExternalTensor<FLOAT,[2048,2048]>(location='model_SmolLM17b_2Layer-onnx_dynamo-ir-d1rt1.onnx.data', name='model.layers.0.self_attn.o_proj.weight', offset=50397184, length=16777216, base_dir='')"
                  }
                ]
              }
            ],
            "inputsMetadata": [],
            "style": null,
            "config": null
          },
          {
            "id": "[value] model.layers.0.mlp.gate_proj.weight",
            "label": "Initializer",
            "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.0: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.0.mlp: transformers.models.llama.modeling_llama.LlamaMLP/model.layers.0.mlp.gate_proj: torch.nn.modules.linear.Linear/linear_4: aten.linear.default",
            "subgraphIds": [],
            "attrs": [],
            "incomingEdges": [],
            "outputsMetadata": [
              {
                "id": "0",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "model.layers.0.mlp.gate_proj.weight"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "FLOAT[8192,2048]"
                  },
                  {
                    "key": "value",
                    "value": "ExternalTensor<FLOAT,[8192,2048]>(location='model_SmolLM17b_2Layer-onnx_dynamo-ir-d1rt1.onnx.data', name='model.layers.0.mlp.gate_proj.weight', offset=134283264, length=67108864, base_dir='')"
                  }
                ]
              }
            ],
            "inputsMetadata": [],
            "style": null,
            "config": null
          },
          {
            "id": "[value] model.layers.0.mlp.up_proj.weight",
            "label": "Initializer",
            "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.0: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.0.mlp: transformers.models.llama.modeling_llama.LlamaMLP/model.layers.0.mlp.up_proj: torch.nn.modules.linear.Linear/linear_5: aten.linear.default",
            "subgraphIds": [],
            "attrs": [],
            "incomingEdges": [],
            "outputsMetadata": [
              {
                "id": "0",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "model.layers.0.mlp.up_proj.weight"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "FLOAT[8192,2048]"
                  },
                  {
                    "key": "value",
                    "value": "ExternalTensor<FLOAT,[8192,2048]>(location='model_SmolLM17b_2Layer-onnx_dynamo-ir-d1rt1.onnx.data', name='model.layers.0.mlp.up_proj.weight', offset=201392128, length=67108864, base_dir='')"
                  }
                ]
              }
            ],
            "inputsMetadata": [],
            "style": null,
            "config": null
          },
          {
            "id": "[value] model.layers.0.mlp.down_proj.weight",
            "label": "Initializer",
            "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.0: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.0.mlp: transformers.models.llama.modeling_llama.LlamaMLP/model.layers.0.mlp.down_proj: torch.nn.modules.linear.Linear/linear_6: aten.linear.default",
            "subgraphIds": [],
            "attrs": [],
            "incomingEdges": [],
            "outputsMetadata": [
              {
                "id": "0",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "model.layers.0.mlp.down_proj.weight"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "FLOAT[2048,8192]"
                  },
                  {
                    "key": "value",
                    "value": "ExternalTensor<FLOAT,[2048,8192]>(location='model_SmolLM17b_2Layer-onnx_dynamo-ir-d1rt1.onnx.data', name='model.layers.0.mlp.down_proj.weight', offset=268500992, length=67108864, base_dir='')"
                  }
                ]
              }
            ],
            "inputsMetadata": [],
            "style": null,
            "config": null
          },
          {
            "id": "[value] model.layers.0.input_layernorm.weight",
            "label": "Initializer",
            "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.0: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.0.input_layernorm: transformers.models.llama.modeling_llama.LlamaRMSNorm/mul_173: aten.mul.Tensor",
            "subgraphIds": [],
            "attrs": [],
            "incomingEdges": [],
            "outputsMetadata": [
              {
                "id": "0",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "model.layers.0.input_layernorm.weight"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "FLOAT[2048]"
                  },
                  {
                    "key": "value",
                    "value": "ExternalTensor<FLOAT,[2048]>(location='model_SmolLM17b_2Layer-onnx_dynamo-ir-d1rt1.onnx.data', name='model.layers.0.input_layernorm.weight', offset=0, length=8192, base_dir='')"
                  }
                ]
              }
            ],
            "inputsMetadata": [],
            "style": null,
            "config": null
          },
          {
            "id": "[value] model.layers.0.post_attention_layernorm.weight",
            "label": "Initializer",
            "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.0: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.0.post_attention_layernorm: transformers.models.llama.modeling_llama.LlamaRMSNorm/mul_352: aten.mul.Tensor",
            "subgraphIds": [],
            "attrs": [],
            "incomingEdges": [],
            "outputsMetadata": [
              {
                "id": "0",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "model.layers.0.post_attention_layernorm.weight"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "FLOAT[2048]"
                  },
                  {
                    "key": "value",
                    "value": "ExternalTensor<FLOAT,[2048]>(location='model_SmolLM17b_2Layer-onnx_dynamo-ir-d1rt1.onnx.data', name='model.layers.0.post_attention_layernorm.weight', offset=8192, length=8192, base_dir='')"
                  }
                ]
              }
            ],
            "inputsMetadata": [],
            "style": null,
            "config": null
          },
          {
            "id": "[value] model.layers.1.self_attn.q_proj.weight",
            "label": "Initializer",
            "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.1: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.1.self_attn: transformers.models.llama.modeling_llama.LlamaAttention/model.layers.1.self_attn.q_proj: torch.nn.modules.linear.Linear/linear_7: aten.linear.default",
            "subgraphIds": [],
            "attrs": [],
            "incomingEdges": [],
            "outputsMetadata": [
              {
                "id": "0",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "model.layers.1.self_attn.q_proj.weight"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "FLOAT[2048,2048]"
                  },
                  {
                    "key": "value",
                    "value": "ExternalTensor<FLOAT,[2048,2048]>(location='model_SmolLM17b_2Layer-onnx_dynamo-ir-d1rt1.onnx.data', name='model.layers.1.self_attn.q_proj.weight', offset=67174400, length=16777216, base_dir='')"
                  }
                ]
              }
            ],
            "inputsMetadata": [],
            "style": null,
            "config": null
          },
          {
            "id": "[value] model.layers.1.self_attn.k_proj.weight",
            "label": "Initializer",
            "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.1: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.1.self_attn: transformers.models.llama.modeling_llama.LlamaAttention/model.layers.1.self_attn.k_proj: torch.nn.modules.linear.Linear/linear_8: aten.linear.default",
            "subgraphIds": [],
            "attrs": [],
            "incomingEdges": [],
            "outputsMetadata": [
              {
                "id": "0",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "model.layers.1.self_attn.k_proj.weight"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "FLOAT[2048,2048]"
                  },
                  {
                    "key": "value",
                    "value": "ExternalTensor<FLOAT,[2048,2048]>(location='model_SmolLM17b_2Layer-onnx_dynamo-ir-d1rt1.onnx.data', name='model.layers.1.self_attn.k_proj.weight', offset=83951616, length=16777216, base_dir='')"
                  }
                ]
              }
            ],
            "inputsMetadata": [],
            "style": null,
            "config": null
          },
          {
            "id": "[value] model.layers.1.self_attn.v_proj.weight",
            "label": "Initializer",
            "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.1: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.1.self_attn: transformers.models.llama.modeling_llama.LlamaAttention/model.layers.1.self_attn.v_proj: torch.nn.modules.linear.Linear/linear_9: aten.linear.default",
            "subgraphIds": [],
            "attrs": [],
            "incomingEdges": [],
            "outputsMetadata": [
              {
                "id": "0",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "model.layers.1.self_attn.v_proj.weight"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "FLOAT[2048,2048]"
                  },
                  {
                    "key": "value",
                    "value": "ExternalTensor<FLOAT,[2048,2048]>(location='model_SmolLM17b_2Layer-onnx_dynamo-ir-d1rt1.onnx.data', name='model.layers.1.self_attn.v_proj.weight', offset=100728832, length=16777216, base_dir='')"
                  }
                ]
              }
            ],
            "inputsMetadata": [],
            "style": null,
            "config": null
          },
          {
            "id": "[value] model.layers.1.self_attn.o_proj.weight",
            "label": "Initializer",
            "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.1: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.1.self_attn: transformers.models.llama.modeling_llama.LlamaAttention/model.layers.1.self_attn.o_proj: torch.nn.modules.linear.Linear/linear_10: aten.linear.default",
            "subgraphIds": [],
            "attrs": [],
            "incomingEdges": [],
            "outputsMetadata": [
              {
                "id": "0",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "model.layers.1.self_attn.o_proj.weight"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "FLOAT[2048,2048]"
                  },
                  {
                    "key": "value",
                    "value": "ExternalTensor<FLOAT,[2048,2048]>(location='model_SmolLM17b_2Layer-onnx_dynamo-ir-d1rt1.onnx.data', name='model.layers.1.self_attn.o_proj.weight', offset=117506048, length=16777216, base_dir='')"
                  }
                ]
              }
            ],
            "inputsMetadata": [],
            "style": null,
            "config": null
          },
          {
            "id": "[value] model.layers.1.mlp.gate_proj.weight",
            "label": "Initializer",
            "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.1: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.1.mlp: transformers.models.llama.modeling_llama.LlamaMLP/model.layers.1.mlp.gate_proj: torch.nn.modules.linear.Linear/linear_11: aten.linear.default",
            "subgraphIds": [],
            "attrs": [],
            "incomingEdges": [],
            "outputsMetadata": [
              {
                "id": "0",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "model.layers.1.mlp.gate_proj.weight"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "FLOAT[8192,2048]"
                  },
                  {
                    "key": "value",
                    "value": "ExternalTensor<FLOAT,[8192,2048]>(location='model_SmolLM17b_2Layer-onnx_dynamo-ir-d1rt1.onnx.data', name='model.layers.1.mlp.gate_proj.weight', offset=335609856, length=67108864, base_dir='')"
                  }
                ]
              }
            ],
            "inputsMetadata": [],
            "style": null,
            "config": null
          },
          {
            "id": "[value] model.layers.1.mlp.up_proj.weight",
            "label": "Initializer",
            "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.1: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.1.mlp: transformers.models.llama.modeling_llama.LlamaMLP/model.layers.1.mlp.up_proj: torch.nn.modules.linear.Linear/linear_12: aten.linear.default",
            "subgraphIds": [],
            "attrs": [],
            "incomingEdges": [],
            "outputsMetadata": [
              {
                "id": "0",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "model.layers.1.mlp.up_proj.weight"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "FLOAT[8192,2048]"
                  },
                  {
                    "key": "value",
                    "value": "ExternalTensor<FLOAT,[8192,2048]>(location='model_SmolLM17b_2Layer-onnx_dynamo-ir-d1rt1.onnx.data', name='model.layers.1.mlp.up_proj.weight', offset=402718720, length=67108864, base_dir='')"
                  }
                ]
              }
            ],
            "inputsMetadata": [],
            "style": null,
            "config": null
          },
          {
            "id": "[value] model.layers.1.mlp.down_proj.weight",
            "label": "Initializer",
            "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.1: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.1.mlp: transformers.models.llama.modeling_llama.LlamaMLP/model.layers.1.mlp.down_proj: torch.nn.modules.linear.Linear/linear_13: aten.linear.default",
            "subgraphIds": [],
            "attrs": [],
            "incomingEdges": [],
            "outputsMetadata": [
              {
                "id": "0",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "model.layers.1.mlp.down_proj.weight"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "FLOAT[2048,8192]"
                  },
                  {
                    "key": "value",
                    "value": "ExternalTensor<FLOAT,[2048,8192]>(location='model_SmolLM17b_2Layer-onnx_dynamo-ir-d1rt1.onnx.data', name='model.layers.1.mlp.down_proj.weight', offset=469827584, length=67108864, base_dir='')"
                  }
                ]
              }
            ],
            "inputsMetadata": [],
            "style": null,
            "config": null
          },
          {
            "id": "[value] model.layers.1.input_layernorm.weight",
            "label": "Initializer",
            "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.1: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.1.input_layernorm: transformers.models.llama.modeling_llama.LlamaRMSNorm/mul_388: aten.mul.Tensor",
            "subgraphIds": [],
            "attrs": [],
            "incomingEdges": [],
            "outputsMetadata": [
              {
                "id": "0",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "model.layers.1.input_layernorm.weight"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "FLOAT[2048]"
                  },
                  {
                    "key": "value",
                    "value": "ExternalTensor<FLOAT,[2048]>(location='model_SmolLM17b_2Layer-onnx_dynamo-ir-d1rt1.onnx.data', name='model.layers.1.input_layernorm.weight', offset=16384, length=8192, base_dir='')"
                  }
                ]
              }
            ],
            "inputsMetadata": [],
            "style": null,
            "config": null
          },
          {
            "id": "[value] model.layers.1.post_attention_layernorm.weight",
            "label": "Initializer",
            "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.layers.1: transformers.models.llama.modeling_llama.LlamaDecoderLayer/model.layers.1.post_attention_layernorm: transformers.models.llama.modeling_llama.LlamaRMSNorm/mul_567: aten.mul.Tensor",
            "subgraphIds": [],
            "attrs": [],
            "incomingEdges": [],
            "outputsMetadata": [
              {
                "id": "0",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "model.layers.1.post_attention_layernorm.weight"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "FLOAT[2048]"
                  },
                  {
                    "key": "value",
                    "value": "ExternalTensor<FLOAT,[2048]>(location='model_SmolLM17b_2Layer-onnx_dynamo-ir-d1rt1.onnx.data', name='model.layers.1.post_attention_layernorm.weight', offset=24576, length=8192, base_dir='')"
                  }
                ]
              }
            ],
            "inputsMetadata": [],
            "style": null,
            "config": null
          },
          {
            "id": "[value] model.norm.weight",
            "label": "Initializer",
            "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.norm: transformers.models.llama.modeling_llama.LlamaRMSNorm/mul_603: aten.mul.Tensor",
            "subgraphIds": [],
            "attrs": [],
            "incomingEdges": [],
            "outputsMetadata": [
              {
                "id": "0",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "model.norm.weight"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "FLOAT[2048]"
                  },
                  {
                    "key": "value",
                    "value": "ExternalTensor<FLOAT,[2048]>(location='model_SmolLM17b_2Layer-onnx_dynamo-ir-d1rt1.onnx.data', name='model.norm.weight', offset=32768, length=8192, base_dir='')"
                  }
                ]
              }
            ],
            "inputsMetadata": [],
            "style": null,
            "config": null
          },
          {
            "id": "[value] lm_head.weight",
            "label": "Initializer",
            "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM",
            "subgraphIds": [],
            "attrs": [],
            "incomingEdges": [],
            "outputsMetadata": [
              {
                "id": "0",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "lm_head.weight"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "FLOAT[49152,2048]"
                  },
                  {
                    "key": "value",
                    "value": "ExternalTensor<FLOAT,[49152,2048]>(location='model_SmolLM17b_2Layer-onnx_dynamo-ir-d1rt1.onnx.data', name='lm_head.weight', offset=536936448, length=402653184, base_dir='')"
                  }
                ]
              }
            ],
            "inputsMetadata": [],
            "style": null,
            "config": null
          },
          {
            "id": "[value] expand_2",
            "label": "Initializer",
            "namespace": "main_graph/: transformers.models.llama.modeling_llama.LlamaForCausalLM/model: transformers.models.llama.modeling_llama.LlamaModel/model.rotary_emb: transformers.models.llama.modeling_llama.LlamaRotaryEmbedding/matmul: aten.matmul.default",
            "subgraphIds": [],
            "attrs": [
              {
                "key": "__value",
                "value": "[1.0,0.7498942017555237,0.5623413324356079,0.4216965138912201,0.3162277638912201,0.23713736236095428,0.17782793939113617,0.1333521455526352,0.10000000149011612,0.07498941570520401,0.05623412877321243,0.04216964915394783,0.03162277862429619,0.0237137358635664,0.017782794311642647,0.01333521492779255]"
              }
            ],
            "incomingEdges": [],
            "outputsMetadata": [
              {
                "id": "0",
                "attrs": [
                  {
                    "key": "__tensor_tag",
                    "value": "expand_2"
                  },
                  {
                    "key": "tensor_shape",
                    "value": "FLOAT[1,32,1]"
                  },
                  {
                    "key": "value",
                    "value": "TensorProtoTensor<FLOAT,[1,32,1]>(name='expand_2')"
                  }
                ]
              }
            ],
            "inputsMetadata": [],
            "style": null,
            "config": null
          },
          {
            "id": "[value] linear_14",
            "label": "Output",
            "namespace": "",
            "subgraphIds": [],
            "attrs": [
              {
                "key": "name",
                "value": "linear_14"
              },
              {
                "key": "index",
                "value": "0"
              }
            ],
            "incomingEdges": [
              {
                "sourceNodeId": "node_MatMul_702",
                "sourceNodeOutputId": "0",
                "targetNodeInputId": "0"
              }
            ],
            "outputsMetadata": [],
            "inputsMetadata": [],
            "style": null,
            "config": null
          },
          {
            "id": "[value] cat_3",
            "label": "Output",
            "namespace": "",
            "subgraphIds": [],
            "attrs": [
              {
                "key": "name",
                "value": "cat_3"
              },
              {
                "key": "index",
                "value": "1"
              }
            ],
            "incomingEdges": [
              {
                "sourceNodeId": "node_Concat_427",
                "sourceNodeOutputId": "0",
                "targetNodeInputId": "0"
              }
            ],
            "outputsMetadata": [],
            "inputsMetadata": [],
            "style": null,
            "config": null
          },
          {
            "id": "[value] cat_7",
            "label": "Output",
            "namespace": "",
            "subgraphIds": [],
            "attrs": [
              {
                "key": "name",
                "value": "cat_7"
              },
              {
                "key": "index",
                "value": "2"
              }
            ],
            "incomingEdges": [
              {
                "sourceNodeId": "node_Concat_596",
                "sourceNodeOutputId": "0",
                "targetNodeInputId": "0"
              }
            ],
            "outputsMetadata": [],
            "inputsMetadata": [],
            "style": null,
            "config": null
          },
          {
            "id": "[value] cat_4",
            "label": "Output",
            "namespace": "",
            "subgraphIds": [],
            "attrs": [
              {
                "key": "name",
                "value": "cat_4"
              },
              {
                "key": "index",
                "value": "3"
              }
            ],
            "incomingEdges": [
              {
                "sourceNodeId": "node_Concat_428",
                "sourceNodeOutputId": "0",
                "targetNodeInputId": "0"
              }
            ],
            "outputsMetadata": [],
            "inputsMetadata": [],
            "style": null,
            "config": null
          },
          {
            "id": "[value] cat_8",
            "label": "Output",
            "namespace": "",
            "subgraphIds": [],
            "attrs": [
              {
                "key": "name",
                "value": "cat_8"
              },
              {
                "key": "index",
                "value": "4"
              }
            ],
            "incomingEdges": [
              {
                "sourceNodeId": "node_Concat_597",
                "sourceNodeOutputId": "0",
                "targetNodeInputId": "0"
              }
            ],
            "outputsMetadata": [],
            "inputsMetadata": [],
            "style": null,
            "config": null
          }
        ],
        "groupNodeAttributes": {
          "": {
            "opset_imports": "{'': 18}",
            "producer_name": "pytorch",
            "producer_version": "2.8.0.dev20250530+cu126",
            "domain": "None",
            "model_version": "None",
            "doc_string": "None"
          },
          "main_graph": {
            "[metadata] pkg.torch.export.ExportedProgram.graph_signature": "\n# inputs\np_model_embed_tokens_weight: PARAMETER target='model.embed_tokens.weight'\np_model_layers_0_self_attn_q_proj_weight: PARAMETER target='model.layers.0.self_attn.q_proj.weight'\np_model_layers_0_self_attn_k_proj_weight: PARAMETER target='model.layers.0.self_attn.k_proj.weight'\np_model_layers_0_self_attn_v_proj_weight: PARAMETER target='model.layers.0.self_attn.v_proj.weight'\np_model_layers_0_self_attn_o_proj_weight: PARAMETER target='model.layers.0.self_attn.o_proj.weight'\np_model_layers_0_mlp_gate_proj_weight: PARAMETER target='model.layers.0.mlp.gate_proj.weight'\np_model_layers_0_mlp_up_proj_weight: PARAMETER target='model.layers.0.mlp.up_proj.weight'\np_model_layers_0_mlp_down_proj_weight: PARAMETER target='model.layers.0.mlp.down_proj.weight'\np_model_layers_0_input_layernorm_weight: PARAMETER target='model.layers.0.input_layernorm.weight'\np_model_layers_0_post_attention_layernorm_weight: PARAMETER target='model.layers.0.post_attention_layernorm.weight'\np_model_layers_1_self_attn_q_proj_weight: PARAMETER target='model.layers.1.self_attn.q_proj.weight'\np_model_layers_1_self_attn_k_proj_weight: PARAMETER target='model.layers.1.self_attn.k_proj.weight'\np_model_layers_1_self_attn_v_proj_weight: PARAMETER target='model.layers.1.self_attn.v_proj.weight'\np_model_layers_1_self_attn_o_proj_weight: PARAMETER target='model.layers.1.self_attn.o_proj.weight'\np_model_layers_1_mlp_gate_proj_weight: PARAMETER target='model.layers.1.mlp.gate_proj.weight'\np_model_layers_1_mlp_up_proj_weight: PARAMETER target='model.layers.1.mlp.up_proj.weight'\np_model_layers_1_mlp_down_proj_weight: PARAMETER target='model.layers.1.mlp.down_proj.weight'\np_model_layers_1_input_layernorm_weight: PARAMETER target='model.layers.1.input_layernorm.weight'\np_model_layers_1_post_attention_layernorm_weight: PARAMETER target='model.layers.1.post_attention_layernorm.weight'\np_model_norm_weight: PARAMETER target='model.norm.weight'\np_lm_head_weight: PARAMETER target='lm_head.weight'\nb_model_rotary_emb_inv_freq: BUFFER target='model.rotary_emb.inv_freq' persistent=False\ninput_ids: USER_INPUT\nattention_mask: USER_INPUT\nposition_ids: USER_INPUT\npast_key_values_key_cache_0: USER_INPUT\npast_key_values_key_cache_1: USER_INPUT\npast_key_values_value_cache_0: USER_INPUT\npast_key_values_value_cache_1: USER_INPUT\ninputs_embeds: USER_INPUT\nlabels: USER_INPUT\nuse_cache: USER_INPUT\noutput_attentions: USER_INPUT\noutput_hidden_states: USER_INPUT\ncache_position: USER_INPUT\nlogits_to_keep: USER_INPUT\n\n# outputs\nlinear_14: USER_OUTPUT\ncat_3: USER_OUTPUT\ncat_7: USER_OUTPUT\ncat_4: USER_OUTPUT\ncat_8: USER_OUTPUT\n",
            "[metadata] pkg.torch.export.ExportedProgram.range_constraints": "{s34: VR[1, 1024], s16: VR[8, 32768], s16 + s17: VR[10, 36864], s17: VR[1, 4096], s61: VR[2, int_oo], s75: VR[2, int_oo], s54: VR[2, int_oo]}"
          }
        },
        "collectionLabel": "model_SmolLM17b_2Layer-onnx_dynamo-ir-d1rt1.onnx"
      },
      "level": 0
    }
  ]
}