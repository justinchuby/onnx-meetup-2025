{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3f786336",
   "metadata": {},
   "source": [
    "# Demos for Talk Working with Large Models in ONNX IR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "1985be7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare environment\n",
    "\n",
    "# %pip install --upgrade onnxscript onnx-ir onnx-safetensors model-explorer-onnx onnxruntime"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bedf1490",
   "metadata": {},
   "source": [
    "## Demo 1: Safetensors in ONNX\n",
    "\n",
    "**Q1: Is there a way to use the safetensors format as an external data format for ONNX?**\n",
    "\n",
    "**A1:** Yes. The data is contiguous, row-major, and little-endian (same as ONNX). Data offset can be found by parsing the json header.\n",
    "\n",
    "<img src=\"resources/safetensors-format.svg\" width=\"500\"/>\n",
    "\n",
    "Image source: https://huggingface.co/docs/safetensors/en/index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9337aa3c",
   "metadata": {},
   "source": [
    "**Q2: How do we do it efficiently?**\n",
    "\n",
    "**A2:** Use onnx_ir to replace the tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "a54e0bdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<\n",
      "    ir_version=10,\n",
      "    opset_imports={'': 21},\n",
      "    producer_name='onnx-safetensors-example',\n",
      "    producer_version=None,\n",
      "    domain=None,\n",
      "    model_version=None,\n",
      ">\n",
      "graph(\n",
      "    name=SimpleGraph,\n",
      "    inputs=(\n",
      "        %\"input\"<FLOAT,[1,3]>\n",
      "    ),\n",
      "    outputs=(\n",
      "        %\"output\"<FLOAT,[1,3]>\n",
      "    ),\n",
      "    initializers=(\n",
      "        %\"weights\"<FLOAT,[3]>{TensorProtoTensor<FLOAT,[3]>(array([1., 2., 3.], dtype=float32), name='weights')}\n",
      "    ),\n",
      ") {\n",
      "    0 |  # :anonymous_node:130470374283680\n",
      "         %\"output\"<FLOAT,[1,3]> ⬅️ ::Add(%\"input\", %\"weights\"{[1.0, 2.0, 3.0]})\n",
      "    return %\"output\"<FLOAT,[1,3]>\n",
      "}\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import onnx_ir as ir\n",
    "\n",
    "model = ir.load(\"resources/model.textproto\")\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04626ef6",
   "metadata": {},
   "source": [
    "### Loading tensors from a safetensors file into an ONNX model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cecd56d8",
   "metadata": {},
   "source": [
    "Use `load_file_as_external_data` to load safetensors as external data and replace weights in the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "72d642a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<\n",
      "   ir_version: 10,\n",
      "   opset_import: [\"\" : 21],\n",
      "   producer_name: \"onnx-safetensors-example\"\n",
      ">\n",
      "SimpleGraph (float[1,3] input) => (float[1,3] output) \n",
      "   <float[3] weights = [\"location\": \"resources/weights.safetensors\", \"offset\": \"72\", \"length\": \"12\"], float[3] weights>\n",
      "{\n",
      "   output = Add (input, weights)\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "model_with_external_data = onnx_safetensors.load_file_as_external_data(\n",
    "    model,\n",
    "    \"resources/weights.safetensors\",  # weights containing [4, 5, 6]\n",
    ")\n",
    "\n",
    "print(onnx.printer.to_text(ir.to_proto(model_with_external_data)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7ff107f",
   "metadata": {},
   "source": [
    "### Using safetensors as external data for ONNX\n",
    "\n",
    "We can similarly save external data file from an ONNX model to safetensors. By storing the tensor dtype in ONNX file, we can even use types safetensors doesn't yet support, like INT4.\n",
    "\n",
    "You can read more at https://github.com/justinchuby/onnx-safetensors/blob/main/examples/tutorial.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eb20a2e",
   "metadata": {},
   "source": [
    "### Inference with ONNX Runtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "39c668a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.0, 2.0, 3.0]] + [4, 5, 6] = [array([[5., 7., 9.]], dtype=float32)]\n"
     ]
    }
   ],
   "source": [
    "import onnxruntime as ort\n",
    "\n",
    "ir.save(model_with_external_data, \"model_with_external_data.onnx\")\n",
    "session = ort.InferenceSession(\"model_with_external_data.onnx\")\n",
    "output = session.run(None, {\"input\": np.array([[1.0, 2.0, 3.0]], dtype=np.float32)})\n",
    "print(\"[[1.0, 2.0, 3.0]] + [4, 5, 6] =\", output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c03361ce",
   "metadata": {},
   "source": [
    "## Demo 2: Fusion in onnxscript for ONNX Runtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "160e2559",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b358607f",
   "metadata": {},
   "source": [
    "## Demo 3: Putting it together\n",
    "\n",
    "1. Build a model with the tape module.\n",
    "2. Replace some initializers\n",
    "3. Build a pass to modify the model (merge QKV weights)\n",
    "4. Use rewriter to do the same thing\n",
    "5. Show it on model explorer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07724b5a",
   "metadata": {},
   "source": [
    "### 1. Build a model with the tape module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "48ee05ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def build_model() -> ir.Model:\n",
    "    R\"\"\"\n",
    "      Input Embeddings  [batch_size, seq_len, hidden_size]\n",
    "          |\n",
    "          V\n",
    "      [Linear Layer: W_QKV]  <-- Single merged weight matrix  [hidden_size, 3 * hidden_size]\n",
    "          |\n",
    "          V\n",
    "      [QKV Tensor]  [batch_size, seq_len, 3 * hidden_size]\n",
    "       /    |    \\\n",
    "      /     |     \\\n",
    "     V      V      V\n",
    "    Query  Key   Value    <-- Q, K, V are now derived by splitting the QKV Tensor\n",
    "    # Each of these has shape [batch_size, seq_len, hidden_size]\n",
    "        \\   |   /\n",
    "         \\  |  /\n",
    "     [Scaled Dot-Product Attention]\n",
    "                |\n",
    "                V\n",
    "        [Attention Output]  [batch_size, seq_len, hidden_size]\n",
    "    \"\"\"\n",
    "    batch_size = 2\n",
    "    seq_len = 3\n",
    "    hidden_size = 16\n",
    "\n",
    "    # Initializer the Tape. It is simply a recorder of operations and initializers.\n",
    "    tape = ir.tape.Tape()\n",
    "\n",
    "    # Create initializers\n",
    "    q_weight = tape.initializer(\n",
    "        ir.tensor(np.random.rand(hidden_size, hidden_size).astype(np.float32)),\n",
    "        name=\"q_weight\",\n",
    "    )\n",
    "    k_weight = tape.initializer(\n",
    "        ir.tensor(np.random.rand(hidden_size, hidden_size).astype(np.float32)),\n",
    "        name=\"k_weight\",\n",
    "    )\n",
    "    v_weight = tape.initializer(\n",
    "        ir.tensor(np.random.rand(hidden_size, hidden_size).astype(np.float32)),\n",
    "        name=\"v_weight\",\n",
    "    )\n",
    "\n",
    "    # Create graph inputs\n",
    "    input = ir.Value(\n",
    "        name=\"input\",\n",
    "        type=ir.TensorType(ir.DataType.FLOAT),\n",
    "        shape=ir.Shape([batch_size, seq_len, hidden_size]),\n",
    "    )\n",
    "\n",
    "    query = tape.op(\"MatMul\", inputs=[input, q_weight])\n",
    "    key = tape.op(\"MatMul\", inputs=[input, k_weight])\n",
    "    value = tape.op(\"MatMul\", inputs=[input, v_weight])\n",
    "    attention_output = tape.op(\"Attention\", inputs=[query, key, value], attributes={\"q_num_heads\": 1, \"kv_num_heads\": 1})\n",
    "    attention_output.shape = ir.Shape([batch_size, seq_len, hidden_size])\n",
    "    attention_output.type = ir.TensorType(ir.DataType.FLOAT)\n",
    "\n",
    "    model = ir.Model(\n",
    "        graph=ir.Graph(\n",
    "            inputs=[input],\n",
    "            outputs=[attention_output],\n",
    "            nodes=tape.nodes,\n",
    "            initializers=tape.initializers,\n",
    "            opset_imports={\"\": 23},\n",
    "            name=\"main_graph\",\n",
    "        ),\n",
    "        ir_version=10,\n",
    "    )\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9da7a8dd",
   "metadata": {},
   "source": [
    "### 2. Replace some initializers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "053b163c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor<FLOAT,[16,16]>(array([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]], dtype=float32), name='q_weight')\n",
      "\n",
      "Min: 0.0, Max: 0.0, NaN count: 0, Inf count: 0\n",
      "Sparsity (abs<1e-06): 1.00\n",
      "Histogram:\n",
      "     256 ┼                                       ╭╮\n",
      "     224 ┤                                       ││\n",
      "     192 ┤                                       ││\n",
      "     160 ┤                                       ││\n",
      "     128 ┤                                       ││\n",
      "      96 ┤                                       ││\n",
      "      64 ┤                                       ││\n",
      "      32 ┤                                       ││\n",
      "       0 ┼───────────────────────────────────────╯╰──────────────────────────────────────\n",
      "    -0.5000  -0.4000  -0.3000  -0.1875  -0.0875  0.0250  0.1125  0.2125  0.3000  0.4000  0.5000\n"
     ]
    }
   ],
   "source": [
    "model = build_model()\n",
    "\n",
    "model.graph.initializers[\"q_weight\"].const_value = (\n",
    "    ir.tensor(np.zeros((hidden_size, hidden_size)).astype(np.float32), name=\"q_weight\")\n",
    ")\n",
    "model.graph.initializers[\"q_weight\"].const_value.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98851a49",
   "metadata": {},
   "source": [
    "### 3. Build a pass to modify the model (merge QKV weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "36f09674",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q weight value:\n",
      "Tensor<FLOAT,[16,16]>(array([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]], dtype=float32), name='q_weight')\n",
      "\n",
      "Min: 0.0, Max: 0.0, NaN count: 0, Inf count: 0\n",
      "Sparsity (abs<1e-06): 1.00\n",
      "Histogram:\n",
      "     256 ┼                                       ╭╮\n",
      "     224 ┤                                       ││\n",
      "     192 ┤                                       ││\n",
      "     160 ┤                                       ││\n",
      "     128 ┤                                       ││\n",
      "      96 ┤                                       ││\n",
      "      64 ┤                                       ││\n",
      "      32 ┤                                       ││\n",
      "       0 ┼───────────────────────────────────────╯╰──────────────────────────────────────\n",
      "    -0.5000  -0.4000  -0.3000  -0.1875  -0.0875  0.0250  0.1125  0.2125  0.3000  0.4000  0.5000\n"
     ]
    }
   ],
   "source": [
    "# Now we want to combine the weights into a single QKV weight matrix\n",
    "\n",
    "for node in model.graph:\n",
    "    if node.op_type != \"Attention\":\n",
    "        continue\n",
    "\n",
    "    # Find the weights for Q, K, V\n",
    "    input_val = node.inputs[0].producer().inputs[0]\n",
    "    q_weight_val = node.inputs[0].producer().inputs[1]\n",
    "    k_weight_val = node.inputs[1].producer().inputs[1]\n",
    "    v_weight_val = node.inputs[2].producer().inputs[1]\n",
    "    assert q_weight_val.const_value is not None\n",
    "    assert k_weight_val.const_value is not None\n",
    "    assert v_weight_val.const_value is not None\n",
    "    # Show the values of the weights\n",
    "    print(\"Q weight value:\")\n",
    "    q_weight_val.const_value.display()\n",
    "\n",
    "    qkv_weight = ir.Value(\n",
    "        name=\"qkv_weight\",\n",
    "        type=ir.TensorType(ir.DataType.FLOAT),\n",
    "        shape=ir.Shape([hidden_size, 3 * hidden_size]),\n",
    "        const_value=ir.tensor(\n",
    "            np.concatenate(\n",
    "                [\n",
    "                    q_weight_val.const_value,\n",
    "                    k_weight_val.const_value,\n",
    "                    v_weight_val.const_value,\n",
    "                ],\n",
    "                axis=1,\n",
    "            )\n",
    "        ),\n",
    "    )\n",
    "    # Create a new MatMul node uses the combined Q, K, V weights\n",
    "    combined_matmul_node = ir.node(\"MatMul\", inputs=[input_val, qkv_weight])\n",
    "    new_qkv = ir.node(\n",
    "        \"Split\",\n",
    "        inputs=combined_matmul_node.outputs,\n",
    "        attributes={\n",
    "            \"axis\": 2,\n",
    "            \"num_outputs\": 3,\n",
    "        },\n",
    "        num_outputs=3,\n",
    "    )\n",
    "    # Add the new node to the graph and register the new initializer\n",
    "    node.prepend((combined_matmul_node, new_qkv))\n",
    "    # Reconnect the Attention node to use the new QKV outputs\n",
    "    ir.convenience.replace_all_uses_with(\n",
    "        node.inputs,\n",
    "        new_qkv.outputs,\n",
    "    )\n",
    "    # Add the new initializer to the graph\n",
    "    model.graph.register_initializer(qkv_weight)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "62c92959",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined QKV model:\n",
      "<\n",
      "    ir_version=10,\n",
      "    opset_imports={'': 23},\n",
      "    producer_name=None,\n",
      "    producer_version=None,\n",
      "    domain=None,\n",
      "    model_version=None,\n",
      ">\n",
      "graph(\n",
      "    name=main_graph,\n",
      "    inputs=(\n",
      "        %\"input\"<FLOAT,[2,3,16]>\n",
      "    ),\n",
      "    outputs=(\n",
      "        %\"val_3\"<FLOAT,[2,3,16]>\n",
      "    ),\n",
      "    initializers=(\n",
      "        %\"q_weight\"<FLOAT,[16,16]>{Tensor(...)},\n",
      "        %\"k_weight\"<FLOAT,[16,16]>{Tensor(...)},\n",
      "        %\"v_weight\"<FLOAT,[16,16]>{Tensor(...)},\n",
      "        %\"qkv_weight\"<FLOAT,[16,48]>{Tensor(...)}\n",
      "    ),\n",
      ") {\n",
      "    0 |  # node_MatMul_0\n",
      "         %\"val_0\"<?,?> ⬅️ ::MatMul(%\"input\", %\"q_weight\"{...})\n",
      "    1 |  # node_MatMul_1\n",
      "         %\"val_1\"<?,?> ⬅️ ::MatMul(%\"input\", %\"k_weight\"{...})\n",
      "    2 |  # node_MatMul_2\n",
      "         %\"val_2\"<?,?> ⬅️ ::MatMul(%\"input\", %\"v_weight\"{...})\n",
      "    3 |  # node_MatMul_4\n",
      "         %\"val_4\"<?,?> ⬅️ ::MatMul(%\"input\", %\"qkv_weight\"{...})\n",
      "    4 |  # node_Split_5\n",
      "         %\"val_5\"<?,?>, %\"val_6\"<?,?>, %\"val_7\"<?,?> ⬅️ ::Split(%\"val_4\") {axis=2, num_outputs=3}\n",
      "    5 |  # node_Attention_3\n",
      "         %\"val_3\"<FLOAT,[2,3,16]> ⬅️ ::Attention(%\"val_5\", %\"val_6\", %\"val_7\") {q_num_heads=1, kv_num_heads=1}\n",
      "    return %\"val_3\"<FLOAT,[2,3,16]>\n",
      "}\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Combined QKV model:\")\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbe6ebc8",
   "metadata": {},
   "source": [
    "### 3.5. Run passes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "69416d61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model after removing unused nodes:\n",
      "<\n",
      "    ir_version=10,\n",
      "    opset_imports={'': 23},\n",
      "    producer_name=None,\n",
      "    producer_version=None,\n",
      "    domain=None,\n",
      "    model_version=None,\n",
      ">\n",
      "graph(\n",
      "    name=main_graph,\n",
      "    inputs=(\n",
      "        %\"input\"<FLOAT,[2,3,16]>\n",
      "    ),\n",
      "    outputs=(\n",
      "        %\"val_3\"<FLOAT,[2,3,16]>\n",
      "    ),\n",
      "    initializers=(\n",
      "        %\"qkv_weight\"<FLOAT,[16,48]>{Tensor(...)}\n",
      "    ),\n",
      ") {\n",
      "    0 |  # node_MatMul_4\n",
      "         %\"val_4\"<?,?> ⬅️ ::MatMul(%\"input\", %\"qkv_weight\"{...})\n",
      "    1 |  # node_Split_5\n",
      "         %\"val_5\"<?,?>, %\"val_6\"<?,?>, %\"val_7\"<?,?> ⬅️ ::Split(%\"val_4\") {axis=2, num_outputs=3}\n",
      "    2 |  # node_Attention_3\n",
      "         %\"val_3\"<FLOAT,[2,3,16]> ⬅️ ::Attention(%\"val_5\", %\"val_6\", %\"val_7\") {q_num_heads=1, kv_num_heads=1}\n",
      "    return %\"val_3\"<FLOAT,[2,3,16]>\n",
      "}\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import onnx_ir.passes.common as common_passes\n",
    "\n",
    "common_passes.RemoveUnusedNodesPass()(model)\n",
    "\n",
    "print(\"Model after removing unused nodes:\")\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "0e295db9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model after shape inference:\n",
      "<\n",
      "    ir_version=10,\n",
      "    opset_imports={'': 23},\n",
      "    producer_name=None,\n",
      "    producer_version=None,\n",
      "    domain=None,\n",
      "    model_version=None,\n",
      ">\n",
      "graph(\n",
      "    name=main_graph,\n",
      "    inputs=(\n",
      "        %\"input\"<FLOAT,[2,3,16]>\n",
      "    ),\n",
      "    outputs=(\n",
      "        %\"val_3\"<FLOAT,[2,3,16]>\n",
      "    ),\n",
      "    initializers=(\n",
      "        %\"qkv_weight\"<FLOAT,[16,48]>{Tensor(...)}\n",
      "    ),\n",
      ") {\n",
      "    0 |  # node_MatMul_4\n",
      "         %\"val_4\"<FLOAT,[2,3,48]> ⬅️ ::MatMul(%\"input\", %\"qkv_weight\"{...})\n",
      "    1 |  # node_Split_5\n",
      "         %\"val_5\"<FLOAT,[2,3,16]>, %\"val_6\"<FLOAT,[2,3,16]>, %\"val_7\"<FLOAT,[2,3,16]> ⬅️ ::Split(%\"val_4\") {axis=2, num_outputs=3}\n",
      "    2 |  # node_Attention_3\n",
      "         %\"val_3\"<FLOAT,[2,3,16]> ⬅️ ::Attention(%\"val_5\", %\"val_6\", %\"val_7\") {q_num_heads=1, kv_num_heads=1}\n",
      "    return %\"val_3\"<FLOAT,[2,3,16]>\n",
      "}\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "common_passes.ShapeInferencePass()(model)\n",
    "\n",
    "print(\"Model after shape inference:\")\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1ffbe3c",
   "metadata": {},
   "source": [
    "### 4. Use rewriter to do the same thing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "a2200331",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<\n",
      "    ir_version=10,\n",
      "    opset_imports={'': 23},\n",
      "    producer_name=None,\n",
      "    producer_version=None,\n",
      "    domain=None,\n",
      "    model_version=None,\n",
      ">\n",
      "graph(\n",
      "    name=main_graph,\n",
      "    inputs=(\n",
      "        %\"input\"<FLOAT,[2,3,16]>\n",
      "    ),\n",
      "    outputs=(\n",
      "        %\"val_3\"<FLOAT,[2,3,16]>\n",
      "    ),\n",
      "    initializers=(\n",
      "        %\"qkv_weight\"<FLOAT,[16,48]>{Tensor(...)}\n",
      "    ),\n",
      ") {\n",
      "    0 |  # node_MatMul_4\n",
      "         %\"val_4\"<FLOAT,[2,3,48]> ⬅️ ::MatMul(%\"input\", %\"qkv_weight\"{...})\n",
      "    1 |  # node_Split_5\n",
      "         %\"val_5\"<FLOAT,[2,3,16]>, %\"val_6\"<FLOAT,[2,3,16]>, %\"val_7\"<FLOAT,[2,3,16]> ⬅️ ::Split(%\"val_4\") {axis=2, num_outputs=3}\n",
      "    2 |  # node_Attention_6\n",
      "         %\"val_3\"<FLOAT,[2,3,16]> ⬅️ ::Attention(%\"val_5\", %\"val_6\", %\"val_7\") {q_num_heads=1, kv_num_heads=1}\n",
      "    return %\"val_3\"<FLOAT,[2,3,16]>\n",
      "}\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Now use the rewriter\n",
    "\n",
    "import onnxscript.rewriter as rewriter\n",
    "\n",
    "class CombineQKVWeights(rewriter.pattern.RewriteRuleClassBase):\n",
    "    def pattern(cls, op, input, q_weight, k_weight, v_weight):\n",
    "        q = op.MatMul(input, q_weight)\n",
    "        k = op.MatMul(input, k_weight)\n",
    "        v = op.MatMul(input, v_weight)\n",
    "        return op.Attention(q, k, v, q_num_heads=1, kv_num_heads=1)\n",
    "\n",
    "    def rewrite(cls, op, input, q_weight, k_weight, v_weight):\n",
    "        qkv_weight = op.initializer(\n",
    "            ir.tensor(\n",
    "                np.concatenate(\n",
    "                    [\n",
    "                        q_weight.const_value.numpy(),\n",
    "                        k_weight.const_value.numpy(),\n",
    "                        v_weight.const_value.numpy(),\n",
    "                    ],\n",
    "                    axis=1,\n",
    "                )\n",
    "            ),\n",
    "            name=\"qkv_weight\",\n",
    "        )\n",
    "        combined_matmul = op.MatMul(input, qkv_weight)\n",
    "        new_q, new_k, new_v = op.Split(combined_matmul, axis=2, num_outputs=3, _outputs=3)\n",
    "        return op.Attention(new_q, new_k, new_v, q_num_heads=1, kv_num_heads=1)\n",
    "\n",
    "model = build_model()\n",
    "# Create the rewrite rule\n",
    "rule = CombineQKVWeights.rule()\n",
    "# Apply the rewrite rule to the model\n",
    "rule.apply_to_model(model)\n",
    "# Clean up and run shape inference. Note that you can use the Sequential pass to chain multiple passes together.\n",
    "ir.passes.Sequential(\n",
    "    common_passes.RemoveUnusedNodesPass(),\n",
    "    common_passes.ShapeInferencePass(),\n",
    ")(model)\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "533cab58",
   "metadata": {},
   "source": [
    "### 5. Show it on model explorer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "d9521cc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "ir.save(model, \"merged_qkv.onnx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "8fca9b7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading extensions...\n",
      "2025-06-07 18:56:49.125906: I external/org_tensorflow/tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "Loaded 9 extensions:\n",
      " - TFLite adapter (Flatbuffer)\n",
      " - TFLite adapter (MLIR)\n",
      " - TF adapter (MLIR)\n",
      " - TF adapter (direct)\n",
      " - GraphDef adapter\n",
      " - Pytorch adapter (exported program)\n",
      " - MLIR adapter\n",
      " - ONNX adapter\n",
      " - JSON adapter\n",
      "\n",
      "Starting Model Explorer server at:\n",
      "http://localhost:8083/?data=%7B%22models%22%3A%20%5B%7B%22url%22%3A%20%22/home/justinchu/dev/onnx-meetup-2025/merged_qkv.onnx%22%7D%5D%7D\n",
      "\n",
      "Press Ctrl+C to stop.\n",
      "gio: http://localhost:8083/?data=%7B%22models%22%3A%20%5B%7B%22url%22%3A%20%22/home/justinchu/dev/onnx-meetup-2025/merged_qkv.onnx%22%7D%5D%7D: Operation not supported\n",
      "Stopping server...\n",
      "^C\n"
     ]
    }
   ],
   "source": [
    "!onnxvis merged_qkv.onnx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7b27674",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
